[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#main-formatting",
    "href": "index.html#main-formatting",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Main Formatting",
    "text": "Main Formatting\n\nHtml Comments",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#syntax-for-tasks",
    "href": "index.html#syntax-for-tasks",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Syntax for Tasks",
    "text": "Syntax for Tasks\n\nTasks with ToDo Tree\n\nSimple “One-line tasks”\nUse Code ticks and html comment and task format for tasks distinctly visible across all formats including the ToDo-Tree overview:\n&lt;!-- [ ] ToDos for things to do / tasks / reminders (allows \"jump to with Taks Tree extension\") --&gt;\nUse html comment and task format for open or uncertain tasks, visible in the .qmd file:\n\n\n\nMore Complex Tasks with Notes\n&lt;!-- [ ] Task Title: short description--&gt;\n\n  More Information about task\n\n  Relevant notes\n\n  Step-by-step implementation Plan\n\n  Etc.\n\n\n\nCompleted Tasks\nRetain completed tasks in ToDo-Tree by adding an x in the brackets: [x] &lt;!-- [x] Tasks which have been finished but should remain for later verification --&gt;\n\nMark and remove completed tasks from ToDo-Tree by adding a minus in the brackets: [-]\n&lt;!-- [-] Tasks which have been finished but should remain visible for later verification --&gt;\n\n\n\nMissing Citations\n&lt;!-- [ ] FIND: @CITATION_KEY_PURPOSE: \"Description of the appropriate/idea source, including ideas /suggestions / search terms etc.\" --&gt;\n\n\nSuggested Citation\n&lt;!-- [ ] VERIFY: @CITATION_KEY_SUGGESTED: \"Description of the appropriate paper, book, source\" [Include BibTex if known] --&gt;\n\n\nMissing Graphic\n&lt;!-- [ ] FIND: {#fig-GRAPHIC_IDEA}]: \"Description of the appropriate/idea source, including ideas /suggestions / search terms etc.\" --&gt;\n\n\nSuggested Graphic\n&lt;!-- [ ] VERIFY: {#fig-GRAPHIC_IDEA}: \"Description of the appropriate paper, book, source\" [Include figure syntax if known] --&gt;\nMissing and/or suggested tables, concepts, explanations as well as other elements should be suggested similarily.\n\n\n\nTask Syntax Examples\n&lt;!-- [ ] (Example short: open and visible in text)   Find and list the names of the MTAIR team-members responsible for the Analytica Implementation --&gt;\n&lt;!-- [ ] (Example longer: open and visible in text)    Review/Plan/Discuss integrating Live Prediction Markets --&gt;\n\n  Live prediction market integration requires:\n    (1) API connections to platforms (Metaculus, Manifold),\n    (2) Question-to-variable mapping algorithms,\n    (3) Probability update mechanisms, \n    (4) Handling of market dynamics (thin markets, manipulation).\n    Current mentions may overstate readiness or underestimate complexity.\n    Need realistic assessment of what's achievable.\n\n  Implementation Steps:\n      0. List/mention all relevant platforms with a brief description each\n      1. Review all existing prediction market mentions for accuracy\n      2. Assess actual API availability and limitations\n      3. Describe/explain/discuss how to implement basic proof-of-concept with single platform\n      4. Document challenges: question mapping, market interpretation\n      5. Create realistic timeline for full implementation\n      6. Revise thesis claims to match reality\n      7. Add \"Future Work\" and/or extension section on complete integration\n      8. Include descriptions of mockups/designs even if not fully built \n      9. Highlight/discuss the advantages of such integrations\n      10. Quickly brainstorm for downsides worth mentioning\n\n\n\n\nVerbatim Code Formatting\nverbatim code formatting for notes and ideas to be included (here)\n\n\nCode Block formatting\nAlso code blocks for more extensive notes and ideas to be included and checklists\n- test 1. \n- test 2. \n- test 3.\n2. second\n3. third\ncode\nAdd a language to syntax highlight code blocks:\n1 + 1\n\n\nBlockquote Formatting\n\nBlockquote formatting for “Suggested Citations (e.g. carlsmith 2024 on …)” and/or claims which require a citation (e.g. claim x should be backed-up by a ciation from the literature)\n\n\n\nTables\n\n\n\nTable 1.1: Demonstration of pipe table syntax\n\n\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\nTable 1.2: My Caption 1\n\n\n\n\n\nCol1\nCol2\nCol3\n\n\n\n\nA\nB\nC\n\n\nE\nF\nG\n\n\nA\nG\nG\n\n\n\n\n\n\nReferencing tables with @tbl-KEY: See Table 1.2.\n\n\n\nTable 1.3: Main Caption\n\n\n\n\n\n\n\n(a) First Table\n\n\n\n\n\nCol1\nCol2\nCol3\n\n\n\n\nA\nB\nC\n\n\nE\nF\nG\n\n\nA\nG\nG\n\n\n\n\n\n\n\n\n\n\n\n(b) Second Table\n\n\n\n\n\nCol1\nCol2\nCol3\n\n\n\n\nA\nB\nC\n\n\nE\nF\nG\n\n\nA\nG\nG\n\n\n\n\n\n\n\n\n\n\n\nSee Table 1.3 for details, especially Table 1.3 (b).\npython\n#| label: tbl-planets\n#| tbl-cap: Astronomical object\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",\"696,000\",1.989e30],\n         [\"Earth\",\"6,371\",5.972e24],\n         [\"Moon\",\"1,737\",7.34e22],\n         [\"Mars\",\"3,390\",6.39e23]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Astronomical object\",\"R (km)\", \"mass (kg)\"]\n))\n\nSample grid table.\n\n\n\n\n\n\n\nFruit\nPrice\nAdvantages\n\n\n\n\nBananas\n$1.34\n\nbuilt-in wrapper\nbright color\n\n\n\nOranges\n$2.10\n\ncures scurvy\ntasty\n\n\n\n\nContent with HTML tables you don’t want processed.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-heading",
    "href": "index.html#sec-heading",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Headings & Potential Headings in Standard Markdown formatting (‘##’)",
    "text": "Headings & Potential Headings in Standard Markdown formatting (‘##’)\n\nHeading 3\n\nHeading 4",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#text-formatting-options",
    "href": "index.html#text-formatting-options",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Text Formatting Options",
    "text": "Text Formatting Options\nitalics, bold, bold italics\nsuperscript2 and subscript2\nstrikethrough\nThis text is highlighted\nThis text is underlined\nThis text is smallcaps",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#lists",
    "href": "index.html#lists",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Lists",
    "text": "Lists\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1\n\n\nitem 2\nContinued (indent 4 spaces)\n\n\nordered list\nitem 2\n\nsub-item 1\n\nsub-sub-item 1",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#math",
    "href": "index.html#math",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Math",
    "text": "Math\ninline math: \\(E = mc^{2}\\)\ndisplay math:\n\\[E = mc^{2}\\]\nIf you want to define custom TeX macros, include them within $$ delimiters enclosed in a .hidden block. For example:\n\n\\[\n\\def\\RR{{\\bf R}}\n\\def\\bold#1{{\\bf #1}}\n\\]\n\nFor HTML math processed using MathJax (the default) you can use the \\def, \\newcommand, \\renewcommand, \\newenvironment, \\renewenvironment, and \\let commands to create your own macros and environments.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "Inlines notes are easier to write, since you don’t have to pick an identifier and move down to type the note.↩︎\nHere is the footnote.↩︎\nHere’s one with multiple blocks.\nSubsequent paragraphs are indented to show that they belong to the previous footnote.\n{ some.code }\nThe whole paragraph can be indented, or just the first line. In this way, multi-paragraph footnotes work like multi-paragraph list items.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-callouts",
    "href": "index.html#sec-callouts",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Callouts",
    "text": "Callouts\nQuarto’s native callouts work without additional packages:\n\nThis is written in a ‘note’ environment – but it does not seem to produce any special rendering.\n\n\n\n\n\n\n\nOptional Title\n\n\n\nContent here\n\n\n\n\n\n\n\n\nImportant Note2\n\n\n\nThis renders perfectly in both HTML and PDF.\n\n\nAlso for markdown:\n::: {.render_as_markdown_example}\n## Markdown Heading\nThis renders perfectly in both HTML and PDF but as markdown \"plain text\"\n:::",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Links",
    "text": "Links\n&lt;https://quarto.org/docs/authoring/markdown-basics.html&gt; produces: https://quarto.org/docs/authoring/markdown-basics.html\n[Quarto Book Cross-References](https://quarto.org/docs/books/book-crossrefs.html) produces: Quarto Book Cross-References",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-figures1",
    "href": "index.html#sec-figures1",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Images & Figures",
    "text": "Images & Figures\n[![AMTAIR Automation Pipeline from @bucknall2022](/images/pipeline.png){\n  #fig-automation_pipeline\n  fig-scap=\"Five-step AMTAIR automation pipeline from PDFs to Bayesian networks\" \n  fig-alt=\"FLOWCHART: Five-step automation pipeline workflow for AMTAIR project.\n          DATA: The pipeline transforms PDFs through ArgDown, BayesDown, CSV, and HTML into Bayesian network visualizations.\n          PURPOSE: Illustrates the core technical process that enables automated extraction of probabilistic models from AI safety literature.\n          DETAILS: Five numbered green steps show: (1) LLM-based extraction from PDFs to ArgDown, (2) ArgDown to BayesDown completion with probabilities, (3) Extracting world-models as CSV data, (4) Software tools for data inference, and (5) Visualization of the resulting Bayesian network.\n          Each step includes example outputs, with the final visualization showing a Rain-Sprinkler-Grass Wet Bayesian network with probability tables.\n          SOURCE: Created by the author to explain the AMTAIR methodology\n          \"\n  fig-align=\"center\" \n  width=\"100%\"\n  }](https://github.com/VJMeyer/submission)\n\n\nTesting crossreferencing grapics @fig-automation_pipeline.\n\n![Caption/Title 2](/images/cover.png){#fig-testgraphic2 fig-scap=\"Short 2 caption\" fig-alt=\"2nd Alt Text / Description.\" fig-align=\"left\" width=\"30%\"}\n\nTesting crossreferencing grapics @fig-testgraphic2.\n\n\n\n\n\n\nFigure 1.1: AMTAIR Automation Pipeline from\n\n\n\nTesting crossreferencing grapics Figure 1.1. Note that the indentations of graphic inclusions get messed up by viewing them in “view mode” in VS code.\n\n\n\n\n\n\nFigure 1.2: Caption/Title 2\n\n\n\nTesting crossreferencing grapics Figure 1.2.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#page-breaks",
    "href": "index.html#page-breaks",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Page Breaks",
    "text": "Page Breaks\npage 1\n\n\n\npage 2\npage 1\n\npage 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-code",
    "href": "index.html#sec-code",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Including Code",
    "text": "Including Code\n\nCode\nimport pandas as pd\nprint(\"AMTAIR is working!\")\n\n\n\n\n\nAMTAIR is working!\n\n\n\nFigure 1.3\n\n\n\n\nIn-Line LaTeX\n\n\n\nIn-Line HTML\nHere’s some raw inline HTML: html",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#reference-or-embed-code-from-.ipynb-files",
    "href": "index.html#reference-or-embed-code-from-.ipynb-files",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Reference or Embed Code from .ipynb files",
    "text": "Reference or Embed Code from .ipynb files\n\nCode chunks from .ipynb notebooks can be embedded in the .qmd text with:\n{{&lt; embed /AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb#my_code_cell_test &gt;}}\n\n\nwhich produces the output of executing the code cell:\n\n\n\nConnecting to repository: https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/\nAttempting to load: https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/ArgDown.md\n✅ Successfully connected to repository and loaded test files.\n[Existential_Catastrophe]: The destruction of humanity's long-term potential due to AI systems we've lost control over. {\"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"]}\n- [Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"]}\n    - [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {\"instantiations\": [\"scale_of_power_seeking_TRUE\", \"scale_of_power_seeking_FALSE\"]}\n        - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n            - [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {\"instantiations\": [\"aps_systems_TRUE\", \"aps_systems_FALSE\"]}\n                - [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {\"instantiations\": [\"advanced_ai_capability_TRUE\", \"advanced_ai_capability_FALSE\"]}\n                - [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {\"instantiations\": [\"agentic_planning_TRUE\", \"agentic_planning_FALSE\"]}\n                - [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {\"instantiations\": [\"strategic_awareness_TRUE\", \"strategic_awareness_FALSE\"]}\n            - [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {\"instantiations\": [\"difficulty_of_alignment_TRUE\", \"difficulty_of_alignment_FALSE\"]}\n                - [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {\"instantiations\": [\"instrumental_convergence_TRUE\", \"instrumental_convergence_FALSE\"]}\n                - [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {\"instantiations\": [\"problems_with_proxies_TRUE\", \"problems_with_proxies_FALSE\"]}\n                - [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {\"instantiations\": [\"problems_with_search_TRUE\", \"problems_with_search_FALSE\"]}\n            - [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"]}\n                - [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {\"instantiations\": [\"incentives_to_build_aps_STRONG\", \"incentives_to_build_aps_WEAK\"]}\n                    - [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {\"instantiations\": [\"usefulness_of_aps_HIGH\", \"usefulness_of_aps_LOW\"]}\n                    - [Competitive_Dynamics]: Competitive pressures between AI developers. {\"instantiations\": [\"competitive_dynamics_STRONG\", \"competitive_dynamics_WEAK\"]}\n                - [Deception_By_AI]: AI systems deceiving humans about their true objectives. {\"instantiations\": [\"deception_by_ai_TRUE\", \"deception_by_ai_FALSE\"]}\n        - [Corrective_Feedback]: Human society implementing corrections after observing problems. {\"instantiations\": [\"corrective_feedback_EFFECTIVE\", \"corrective_feedback_INEFFECTIVE\"]}\n            - [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {\"instantiations\": [\"warning_shots_OBSERVED\", \"warning_shots_UNOBSERVED\"]}\n            - [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {\"instantiations\": [\"rapid_capability_escalation_TRUE\", \"rapid_capability_escalation_FALSE\"]}\n[Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {\"instantiations\": [\"barriers_to_understanding_HIGH\", \"barriers_to_understanding_LOW\"]}\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n[Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {\"instantiations\": [\"adversarial_dynamics_TRUE\", \"adversarial_dynamics_FALSE\"]}\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n[Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {\"instantiations\": [\"stakes_of_error_HIGH\", \"stakes_of_error_LOW\"]}\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n\n\n\n\n\n\nincluding ‘echo=true’ renders the code of the cell:\n{{&lt; embed /AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb#my_code_cell_test echo=true &gt;}}\n\n\n\nCode\n# @title 0.2 --- Connect to GitHub Repository --- Load Files\n\n\"\"\"\nBLOCK PURPOSE: Establishes connection to the AMTAIR GitHub repository and provides\nfunctions to load example data files for processing.\n\nThis block creates a reusable function for accessing files from the project's\nGitHub repository, enabling access to example files like the rain-sprinkler-lawn\nBayesian network that serves as our canonical test case.\n\nDEPENDENCIES: requests library, io library\nOUTPUTS: load_file_from_repo function and test file loads\n\"\"\"\n\nfrom requests.exceptions import HTTPError\n\n# Specify the base repository URL for the AMTAIR project\nrepo_url = \"https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/\"\nprint(f\"Connecting to repository: {repo_url}\")\n\ndef load_file_from_repo(relative_path):\n    \"\"\"\n    Loads a file from the specified GitHub repository using a relative path.\n\n    Args:\n        relative_path (str): Path to the file relative to the repo_url\n\n    Returns:\n        For CSV/JSON: pandas DataFrame\n        For MD: string containing file contents\n\n    Raises:\n        HTTPError: If file not found or other HTTP error occurs\n        ValueError: If unsupported file type is requested\n    \"\"\"\n    file_url = repo_url + relative_path\n    print(f\"Attempting to load: {file_url}\")\n\n    # Fetch the file content from GitHub\n    response = requests.get(file_url)\n\n    # Check for bad status codes with enhanced error messages\n    if response.status_code == 404:\n        raise HTTPError(f\"File not found at URL: {file_url}. Check the file path/name and ensure the file is publicly accessible.\", response=response)\n    else:\n        response.raise_for_status()  # Raise for other error codes\n\n    # Convert response to file-like object\n    file_object = io.StringIO(response.text)\n\n    # Process different file types appropriately\n    if relative_path.endswith(\".csv\"):\n        return pd.read_csv(file_object)  # Return DataFrame for CSV\n    elif relative_path.endswith(\".json\"):\n        return pd.read_json(file_object)  # Return DataFrame for JSON\n    elif relative_path.endswith(\".md\"):\n        return file_object.read()  # Return raw content for MD files\n    else:\n        raise ValueError(f\"Unsupported file type: {relative_path.split('.')[-1]}. Add support in the GitHub Connection section of this notebook.\")\n\n# Load example files to test connection\ntry:\n    # Load the extracted data CSV file\n#    df = load_file_from_repo(\"extracted_data.csv\")\n\n    # Load the ArgDown test text\n    md_content = load_file_from_repo(\"ArgDown.md\")\n\n    print(\"✅ Successfully connected to repository and loaded test files.\")\nexcept Exception as e:\n    print(f\"❌ Error loading files: {str(e)}\")\n    print(\"Please check your internet connection and the repository URL.\")\n\n# Display preview of loaded content (commented out to avoid cluttering output)\nprint(md_content)\n\n\nConnecting to repository: https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/\nAttempting to load: https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/ArgDown.md\n✅ Successfully connected to repository and loaded test files.\n[Existential_Catastrophe]: The destruction of humanity's long-term potential due to AI systems we've lost control over. {\"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"]}\n- [Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"]}\n    - [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {\"instantiations\": [\"scale_of_power_seeking_TRUE\", \"scale_of_power_seeking_FALSE\"]}\n        - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n            - [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {\"instantiations\": [\"aps_systems_TRUE\", \"aps_systems_FALSE\"]}\n                - [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {\"instantiations\": [\"advanced_ai_capability_TRUE\", \"advanced_ai_capability_FALSE\"]}\n                - [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {\"instantiations\": [\"agentic_planning_TRUE\", \"agentic_planning_FALSE\"]}\n                - [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {\"instantiations\": [\"strategic_awareness_TRUE\", \"strategic_awareness_FALSE\"]}\n            - [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {\"instantiations\": [\"difficulty_of_alignment_TRUE\", \"difficulty_of_alignment_FALSE\"]}\n                - [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {\"instantiations\": [\"instrumental_convergence_TRUE\", \"instrumental_convergence_FALSE\"]}\n                - [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {\"instantiations\": [\"problems_with_proxies_TRUE\", \"problems_with_proxies_FALSE\"]}\n                - [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {\"instantiations\": [\"problems_with_search_TRUE\", \"problems_with_search_FALSE\"]}\n            - [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"]}\n                - [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {\"instantiations\": [\"incentives_to_build_aps_STRONG\", \"incentives_to_build_aps_WEAK\"]}\n                    - [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {\"instantiations\": [\"usefulness_of_aps_HIGH\", \"usefulness_of_aps_LOW\"]}\n                    - [Competitive_Dynamics]: Competitive pressures between AI developers. {\"instantiations\": [\"competitive_dynamics_STRONG\", \"competitive_dynamics_WEAK\"]}\n                - [Deception_By_AI]: AI systems deceiving humans about their true objectives. {\"instantiations\": [\"deception_by_ai_TRUE\", \"deception_by_ai_FALSE\"]}\n        - [Corrective_Feedback]: Human society implementing corrections after observing problems. {\"instantiations\": [\"corrective_feedback_EFFECTIVE\", \"corrective_feedback_INEFFECTIVE\"]}\n            - [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {\"instantiations\": [\"warning_shots_OBSERVED\", \"warning_shots_UNOBSERVED\"]}\n            - [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {\"instantiations\": [\"rapid_capability_escalation_TRUE\", \"rapid_capability_escalation_FALSE\"]}\n[Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {\"instantiations\": [\"barriers_to_understanding_HIGH\", \"barriers_to_understanding_LOW\"]}\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n[Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {\"instantiations\": [\"adversarial_dynamics_TRUE\", \"adversarial_dynamics_FALSE\"]}\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n[Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {\"instantiations\": [\"stakes_of_error_HIGH\", \"stakes_of_error_LOW\"]}\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n\n\n\n\nLink:\nFull Notebooks are embedded in the Appendix through the _quarto.yml file with:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#diagrams",
    "href": "index.html#diagrams",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Diagrams",
    "text": "Diagrams\nQuarto has native support for embedding Mermaid and Graphviz diagrams. This enables you to create flowcharts, sequence diagrams, state diagrams, Gantt charts, and more using a plain text syntax inspired by markdown.\nFor example, here we embed a flowchart created using Mermaid:\n\n\nCode\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-citations",
    "href": "index.html#sec-citations",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Citations",
    "text": "Citations\nSoares and Fallenstein (2014) \n(Soares and Fallenstein 2014) and (Knuth 1984)\nBlah Blah (see Knuth 1984, 33–35; also Growiec 2024, chap. 1)\nBlah Blah (Knuth 1984, 33–35, 38–39 and passim)\nBlah Blah (Growiec 2024; Knuth 1984).\nGrowiec says blah (2024)\n\nNarrative citations (author as subject)\nSoares and Fallenstein (2014) argues that AI alignment requires…\n\n\nParenthetical citations (supporting reference)\nRecent work supports this view (Soares and Fallenstein 2014; Knuth 1984).\n\n\nAuthor-only citation (when discussing the person)\nAs (2014) demonstrates in their analysis…\n\n\nYear-only citation (when author already mentioned)\nSoares (2014) later revised this position.\n\n\nPage-specific references\nThe key insight appears in (Soares and Fallenstein 2014, 45–67).\n\n\nMultiple works, different pages\nThis view is supported (Soares and Fallenstein 2014, 23; Knuth 1984, 156–59).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-crossref",
    "href": "index.html#sec-crossref",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Section Cross-References",
    "text": "Section Cross-References\nRefer to sections like: ?sec-adaptive-governance and Section Cross-References \nCaveat: refering to sections with @sec-HEADINGS works only for sections with:\n## Heading {#sec-HEADINGS}\nIt does not work for sections with \".unnumbered and/or .unlisted\":\n## Heading {#sec-HEADINGS .unnumbered .unlisted}\nFurthermore the .qmd and/or .md yml settings (~ numbering have to be just right)\n\nSection Numbers\nBy default, all headings in your document create a numbered section. You customize numbering depth using the number-depth option. For example, to only number sections immediately below the chapter level, use this:\nnumber-depth: 2\nNote that toc-depth is independent of number-depth (i.e. you can have unnumbered entries in the TOC if they are masked out from numbering by number-depth).\nTesting crossreferencing grapics Figure 1.1. See Chapter Quarto Syntax for more details on visualizing model diagnostics.\nTesting crossreferencing headings ?sec-carlsmith-model\nTesting crossreferencing headings @sec-rain-sprinkler-grass which does not work yet. \nChapter Cross-Reference Section Cross-References",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#pages-in-landscape",
    "href": "index.html#pages-in-landscape",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Pages in Landscape",
    "text": "Pages in Landscape\n\nThis will appear in landscape but only in PDF format. Testing crossreferencing headings ?sec-carlsmith-model",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#illustrations-and-terminology-quick-references",
    "href": "index.html#illustrations-and-terminology-quick-references",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Illustrations and Terminology — Quick References",
    "text": "Illustrations and Terminology — Quick References\n\nAcknowledgments\n\nAcademic supervisor (Prof. Timo Speith) and institution (University of Bayreuth)\n\nResearch collaborators, especially those connected to the original MTAIR project\n\nTechnical advisors who provided feedback on implementation aspects\n\nPersonal supporters who enabled the research through encouragement and feedback",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#list-of-graphics-figures",
    "href": "index.html#list-of-graphics-figures",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "List of Graphics & Figures",
    "text": "List of Graphics & Figures\n\n\n\nFigure 1.1: The coordination crisis in AI governance - visualization of fragmentation\n\nFigure 2.1: The Carlsmith model - DAG representation\n\nFigure 3.1: Research design overview - workflow diagram\n\nFigure 3.2: From natural language to BayesDown - transformation process\n\nFigure 4.1: ARPA system architecture - component diagram\n\nFigure 4.2: Visualization of Rain-Sprinkler-Grass_Wet Bayesian network - screenshot\n\nFigure 5.1: Extraction quality metrics - comparative chart\n\nFigure 5.2: Comparative analysis of AI governance worldviews - network visualization",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#list-of-abbreviations",
    "href": "index.html#list-of-abbreviations",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "List of Abbreviations",
    "text": "List of Abbreviations\n\n\nesp. especially\nf., ff. following\nincl. including\np., pp. page(s)\nMAD Mutually Assured Destruction\n\nAI - Artificial Intelligence\n\nAGI - Artificial General Intelligence\n\nARPA - AI Risk Pathway Analyzer\n\nDAG - Directed Acyclic Graph\n\nLLM - Large Language Model\n\nMTAIR - Modeling Transformative AI Risks\n\nP(Doom) - Probability of existential catastrophe from misaligned AI\n\nCPT - Conditional Probability Table",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#glossary",
    "href": "index.html#glossary",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Glossary",
    "text": "Glossary\n\n\n\nArgument mapping: A method for visually representing the structure of arguments\n\nBayesDown: An extension of ArgDown that incorporates probabilistic information\n\nBayesian network: A probabilistic graphical model representing variables and their dependencies\n\nConditional probability: The probability of an event given that another event has occurred\n\nDirected Acyclic Graph (DAG): A graph with directed edges and no cycles\n\nExistential risk: Risk of permanent curtailment of humanity’s potential\n\nPower-seeking AI: AI systems with instrumental incentives to acquire resources and power\n\nPrediction market: A market where participants trade contracts that resolve based on future events\n\nd-separation: A criterion for identifying conditional independence relationships in Bayesian networks\n\nMonte Carlo sampling: A computational technique using random sampling to obtain numerical results\n\n\n\n\nQuarto Features Previously Incompatible with LaTeX (Below)\n\n\n\n\n\n\n\n\n\nGrowiec, Jakub. 2024. “Existential Risk from Transformative AI: An Economic Perspective.” Technological and Economic Development of Economy, 1–27.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nSoares, Nate, and Benja Fallenstein. 2014. “Aligning Superintelligence with Human Interests: A Technical Research Agenda.”",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/code_improve/Code-Improvement-Plan_12.2.html",
    "href": "chapters/code_improve/Code-Improvement-Plan_12.2.html",
    "title": "1  Comprehensive Jupyter Notebook Enhancement Plan 12.2",
    "section": "",
    "text": "1.0.1 Executive Improvements\n\n1.0.1.1 1. Enhanced Executive Summary\n# Add comprehensive overview cell\n\"\"\"\nAMTAIR Prototype: Production-Ready Demonstration\n===============================================\n\nThis notebook demonstrates the complete AMTAIR pipeline with:\n- Validated extraction accuracy: 85%+ structure, 73%+ probabilities\n- Real-world application to Carlsmith's AI risk model\n- Interactive visualizations for policy evaluation\n- Performance benchmarks and scaling analysis\n\nQuick Start:\n1. Run all cells in Section 0 for setup\n2. Skip to Section 4 for visualizations\n3. See Section 3.3 for technical metrics\n\"\"\"\n\n\n1.0.1.2 2. Add Navigation Cell\n# Create clickable table of contents\nfrom IPython.display import Markdown\ntoc = \"\"\"\n## 📍 Quick Navigation\n\n- [🚀 Setup & Installation](#setup) \n- [📄 Document Processing](#processing)\n- [🔍 Extraction Pipeline](#extraction)\n- [📊 Visualization](#visualization)\n- [💾 Export Results](#export)\n- [📈 Performance Metrics](#metrics)\n- [🔬 Validation Results](#validation)\n\"\"\"\ndisplay(Markdown(toc))\n\n\n\n1.0.2 Technical Enhancements\n\n1.0.2.1 3. Performance Monitoring\n#| label: performance-monitor\nimport time\nimport psutil\nimport pandas as pd\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.metrics = []\n    \n    def checkpoint(self, stage_name):\n        self.metrics.append({\n            'stage': stage_name,\n            'time': time.time(),\n            'memory': psutil.Process().memory_info().rss / 1024 / 1024,\n            'cpu': psutil.cpu_percent()\n        })\n    \n    def report(self):\n        df = pd.DataFrame(self.metrics)\n        df['duration'] = df['time'].diff()\n        return df[['stage', 'duration', 'memory', 'cpu']]\n\nmonitor = PerformanceMonitor()\n\n\n1.0.2.2 4. Validation Framework\n#| label: validation-framework\nclass ExtractionValidator:\n    \"\"\"Comprehensive validation of extraction results\"\"\"\n    \n    def __init__(self, ground_truth_path):\n        self.ground_truth = self.load_ground_truth(ground_truth_path)\n        self.results = {}\n    \n    def validate_structure(self, extracted, ground_truth):\n        \"\"\"Calculate precision, recall, F1 for structure\"\"\"\n        # Node identification metrics\n        # Edge extraction metrics\n        # Return comprehensive metrics dict\n        \n    def validate_probabilities(self, extracted, ground_truth):\n        \"\"\"Calculate MAE, KL divergence for probabilities\"\"\"\n        # Probability accuracy metrics\n        # Distribution comparison\n        # Return metrics dict\n        \n    def generate_report(self):\n        \"\"\"Create formatted validation report with confidence intervals\"\"\"\n        # Statistical analysis\n        # Confidence bounds\n        # Visualizations\n\n\n1.0.2.3 5. Error Analysis Dashboard\n#| label: error-analysis\ndef create_error_analysis_dashboard(validation_results):\n    \"\"\"Interactive dashboard for error pattern analysis\"\"\"\n    \n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=['Error Types', 'Extraction Confidence',\n                       'Node Complexity vs Accuracy', 'Improvement Over Time']\n    )\n    \n    # Error categorization pie chart\n    # Confidence distribution histogram  \n    # Complexity correlation scatter\n    # Learning curve over iterations\n    \n    return fig.show()\n\n\n\n1.0.3 Visualization Upgrades\n\n1.0.3.1 6. Enhanced Network Visualization\n#| label: enhanced-viz\ndef create_advanced_visualization(network, policy_interventions=None):\n    \"\"\"Production-ready visualization with policy overlay\"\"\"\n    \n    # Base network with advanced layout algorithms\n    net = Network(height=\"800px\", width=\"100%\", \n                  bgcolor=\"#ffffff\", font_color=\"#000000\")\n    \n    # Add policy intervention highlights\n    if policy_interventions:\n        for intervention in policy_interventions:\n            # Highlight affected paths\n            # Show probability changes\n            # Add intervention annotations\n    \n    # Advanced interaction features\n    net.add_node_menu()  # Right-click context menu\n    net.add_search_bar()  # Node search functionality\n    net.add_minimap()     # Navigation minimap\n    \n    return net\n\n\n1.0.3.2 7. Comparative Analysis Tools\n#| label: comparison-tools\nclass ModelComparator:\n    \"\"\"Compare multiple extracted models\"\"\"\n    \n    def structural_similarity(self, model1, model2):\n        \"\"\"Graph edit distance and alignment visualization\"\"\"\n        \n    def probability_divergence(self, model1, model2):\n        \"\"\"KL divergence heatmap between models\"\"\"\n        \n    def intervention_robustness(self, models, intervention):\n        \"\"\"Test intervention across multiple worldviews\"\"\"\n        \n    def generate_comparison_report(self):\n        \"\"\"Comprehensive comparison with visualizations\"\"\"\n\n\n\n1.0.4 Case Study Enhancements\n\n1.0.4.1 8. Multiple Model Demonstrations\n#| label: multi-model-demo\n# Add extraction examples beyond Carlsmith\nmodels = {\n    'carlsmith': load_model('carlsmith_2022.md'),\n    'christiano': load_model('christiano_failure.md'),\n    'critch': load_model('critch_arches.md')\n}\n\n# Comparative extraction accuracy\nresults = {}\nfor name, model in models.items():\n    results[name] = extract_and_validate(model)\n    \n# Convergence analysis across models\nconvergence_matrix = analyze_convergence(results)\nvisualize_convergence_patterns(convergence_matrix)\n\n\n1.0.4.2 9. Policy Evaluation Suite\n#| label: policy-evaluation\ndef evaluate_policy_suite():\n    \"\"\"Evaluate multiple real policies\"\"\"\n    \n    policies = {\n        'sb_1047': {\n            'compute_threshold': 10^26,\n            'safety_testing': 'required',\n            'kill_switch': 'mandatory'\n        },\n        'narrow_path': {\n            'capability_monitoring': 'continuous',\n            'international_coordination': 'high',\n            'research_priorities': 'safety_first'\n        }\n    }\n    \n    for policy_name, parameters in policies.items():\n        # Map to model variables\n        # Calculate intervention effects\n        # Generate policy dashboard\n        # Export policy brief\n\n\n\n1.0.5 Data Integration\n\n1.0.5.1 10. Live Data Connectors\n#| label: data-connectors\nclass PredictionMarketConnector:\n    \"\"\"Connect to live prediction markets (demonstration)\"\"\"\n    \n    def __init__(self, mock_mode=True):\n        self.mock_mode = mock_mode\n        self.markets = {\n            'metaculus': MetaculusAPI() if not mock_mode else MockAPI(),\n            'manifold': ManifoldAPI() if not mock_mode else MockAPI()\n        }\n    \n    def find_relevant_questions(self, model_variables):\n        \"\"\"Semantic matching to market questions\"\"\"\n        \n    def update_probabilities(self, model, market_data):\n        \"\"\"Integrate market probabilities with confidence weighting\"\"\"\n\n\n1.0.5.2 11. Export Enhancements\n#| label: enhanced-export\nclass ComprehensiveExporter:\n    \"\"\"Export results in multiple formats with metadata\"\"\"\n    \n    def export_for_researchers(self, results):\n        \"\"\"Technical details, full data, replication package\"\"\"\n        \n    def export_for_policymakers(self, results):\n        \"\"\"Executive summary, key insights, recommendations\"\"\"\n        \n    def export_for_public(self, results):\n        \"\"\"Accessible visualizations, plain language, FAQs\"\"\"\n        \n    def create_interactive_report(self, results):\n        \"\"\"Standalone HTML with all visualizations\"\"\"\n\n\n\n1.0.6 Documentation and Usability\n\n1.0.6.1 12. Inline Documentation\n# Add docstring examples for every major function\ndef extract_bayesdown(text: str, model: str = 'gpt-4') -&gt; BayesDownResult:\n    \"\"\"\n    Extract BayesDown representation from natural language text.\n    \n    Parameters\n    ----------\n    text : str\n        The source text containing argument structure\n    model : str, default='gpt-4'\n        LLM model to use for extraction\n        \n    Returns\n    -------\n    BayesDownResult\n        Structured result with nodes, edges, and probabilities\n        \n    Examples\n    --------\n    &gt;&gt;&gt; text = \"AI systems with advanced capabilities likely pose risks...\"\n    &gt;&gt;&gt; result = extract_bayesdown(text)\n    &gt;&gt;&gt; print(f\"Extracted {len(result.nodes)} nodes with {result.accuracy:.1%} confidence\")\n    \n    Notes\n    -----\n    Extraction accuracy depends on text structure and clarity.\n    For best results, use texts with explicit causal claims.\n    \"\"\"\n\n\n1.0.6.2 13. Interactive Tutorials\n#| label: tutorial-system\ndef create_interactive_tutorial():\n    \"\"\"Step-by-step guided tutorial with exercises\"\"\"\n    \n    tutorial_steps = [\n        \"Understanding ArgDown syntax\",\n        \"Creating your first extraction\",\n        \"Adding probability information\",\n        \"Visualizing the network\",\n        \"Evaluating interventions\"\n    ]\n    \n    for step in tutorial_steps:\n        display_tutorial_section(step)\n        if not check_exercise_completion(step):\n            provide_hints()\n\n\n\n1.0.7 Testing and Quality Assurance\n\n1.0.7.1 14. Comprehensive Test Suite\n#| label: test-suite\nimport pytest\n\nclass TestAMTAIRPipeline:\n    def test_extraction_accuracy(self):\n        \"\"\"Verify extraction meets claimed accuracy\"\"\"\n        \n    def test_probability_coherence(self):\n        \"\"\"Ensure probabilities sum to 1.0\"\"\"\n        \n    def test_visualization_rendering(self):\n        \"\"\"Check all visual elements render correctly\"\"\"\n        \n    def test_policy_evaluation(self):\n        \"\"\"Verify intervention calculations\"\"\"\n        \n    def test_performance_benchmarks(self):\n        \"\"\"Ensure processing times meet targets\"\"\"\n\n\n1.0.7.2 15. Continuous Improvement Tracking\n#| label: improvement-tracking\nclass ImprovementTracker:\n    \"\"\"Track extraction quality over time\"\"\"\n    \n    def log_extraction(self, text, result, ground_truth=None):\n        \"\"\"Log each extraction for analysis\"\"\"\n        \n    def analyze_trends(self):\n        \"\"\"Identify improving/degrading performance areas\"\"\"\n        \n    def suggest_prompt_improvements(self):\n        \"\"\"Data-driven prompt engineering suggestions\"\"\"\n\n\n\n1.0.8 Implementation Priority\n\nImmediate (for thesis submission):\n\nItems 1, 2, 4, 6, 12 (core functionality and documentation)\n\nHigh Priority (for defense):\n\nItems 3, 5, 8, 9 (validation and multiple examples)\n\nFuture Development:\n\nItems 7, 10, 11, 13-15 (advanced features)\n\n\n\n\n1.0.9 Success Metrics\n\nNotebook runs end-to-end without errors\nAll cells have descriptive labels and documentation\n\nPerformance metrics are automatically tracked\nValidation results are clearly presented\nMultiple case studies demonstrate versatility\nExport functions produce publication-ready outputs",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Comprehensive Jupyter Notebook Enhancement Plan 12.2</span>"
    ]
  },
  {
    "objectID": "chapters/code_improve/Code-Improvement-Plan_11.7.html",
    "href": "chapters/code_improve/Code-Improvement-Plan_11.7.html",
    "title": "2  Comprehensive Jupyter Notebook Enhancement Plan 11.7",
    "section": "",
    "text": "2.0.1 1. Structural Alignment with Thesis\n\n2.0.1.1 1.1 Executive Summary Enhancement\n\nCurrent: Brief overview\nImprove:\n\nAdd explicit thesis connection for each section\nInclude visual pipeline diagram at start\nAdd “How to Read This Notebook” guide for different audiences\nCross-reference specific thesis chapters\n\n\n\n\n2.0.1.2 1.2 Section Mapping\n# Add at beginning of each section:\n\"\"\"\nTHESIS CONNECTION: This section implements the concepts from Chapter 3.1 \n(ArgDown Extraction) of the thesis. It demonstrates the automated extraction \npipeline that transforms unstructured text into formal argument representations.\n\nKEY CONCEPTS DEMONSTRATED:\n- Two-stage extraction architecture\n- LLM prompt engineering for argument identification  \n- Structural validation of extracted arguments\n\"\"\"\n\n\n\n2.0.2 2. Code Quality and Documentation\n\n2.0.2.1 2.1 Enhanced Function Documentation\ndef parse_markdown_hierarchy_fixed(markdown_text, ArgDown=False):\n    \"\"\"\n    Parse ArgDown or BayesDown format into structured DataFrame.\n    \n    This function implements the core extraction algorithm described in \n    Section 3.2 of the thesis. It demonstrates how hierarchical argument \n    structures are transformed into relational data suitable for network analysis.\n    \n    Algorithm Overview:\n    1. Clean text and remove comments\n    2. Extract node information with indentation levels\n    3. Establish parent-child relationships using BayesDown semantics\n    4. Convert to DataFrame with network properties\n    \n    Args:\n        markdown_text (str): Text in ArgDown/BayesDown format\n        ArgDown (bool): If True, extract structure only (no probabilities)\n        \n    Returns:\n        pd.DataFrame: Structured representation with columns:\n            - Title: Node identifier\n            - Description: Natural language description\n            - Parents/Children: Network relationships\n            - instantiations: Possible states\n            - priors/posteriors: Probability information (if BayesDown)\n            \n    Example:\n        &gt;&gt;&gt; argdown_text = \"[Claim]: Description. {\\\"instantiations\\\": [\\\"TRUE\\\", \\\"FALSE\\\"]}\"\n        &gt;&gt;&gt; df = parse_markdown_hierarchy_fixed(argdown_text, ArgDown=True)\n        \n    See Also:\n        - Thesis Section 3.2: Extraction Algorithm\n        - BayesDownSyntax.md: Format specification\n    \"\"\"\n\n\n2.0.2.2 2.2 Algorithm Visualization\nAdd visual representations of key algorithms:\n\n\n\n\n2.0.3 3. Enhanced Demonstrations\n\n2.0.3.1 3.1 Progressive Complexity Examples\n\nToy Example: Single claim with one premise\nRain-Sprinkler: Canonical 3-node network\nMini-Carlsmith: 5-node subset for clarity\nFull Carlsmith: Complete 23-node implementation\n\n\n\n2.0.3.2 3.2 Extraction Quality Metrics\ndef evaluate_extraction_quality(manual_extraction, automated_extraction):\n    \"\"\"\n    Compare automated extraction against manual ground truth.\n    Implements validation methodology from Thesis Section 4.1.\n    \"\"\"\n    metrics = {\n        'node_precision': calculate_node_precision(),\n        'edge_recall': calculate_edge_recall(),\n        'probability_mae': calculate_probability_mae()\n    }\n    \n    # Visualize results\n    create_extraction_quality_dashboard(metrics)\n    return metrics\n\n\n\n2.0.4 4. Interactive Enhancements\n\n2.0.4.1 4.1 Parameter Exploration Widgets\nimport ipywidgets as widgets\n\ndef create_extraction_interface():\n    \"\"\"Interactive interface for testing extraction parameters\"\"\"\n    \n    temperature = widgets.FloatSlider(\n        value=0.3, min=0.1, max=1.0, step=0.1,\n        description='LLM Temperature:'\n    )\n    \n    model = widgets.Dropdown(\n        options=['gpt-4-turbo', 'claude-3-opus'],\n        description='Model:'\n    )\n    \n    def run_extraction(temp, model_name):\n        results = extract_argdown_from_text(\n            sample_text, \n            temperature=temp,\n            model=model_name\n        )\n        display_extraction_results(results)\n    \n    widgets.interact(run_extraction, temp=temperature, model_name=model)\n\n\n2.0.4.2 4.2 Visualization Customization\ndef create_enhanced_visualization(df, style_options):\n    \"\"\"\n    Enhanced network visualization with thesis-specific features:\n    - Probability encoding (green-red gradient)\n    - Node type classification (border colors)\n    - Interactive probability tables\n    - Policy intervention overlays\n    \"\"\"\n    # Add intervention visualization\n    if style_options.show_interventions:\n        add_intervention_effects(network, intervention_data)\n\n\n\n2.0.5 5. Policy Analysis Integration\n\n2.0.5.1 5.1 Policy Evaluation Demonstration\nclass PolicyEvaluator:\n    \"\"\"\n    Implements policy evaluation framework from Thesis Chapter 4.\n    \"\"\"\n    \n    def evaluate_narrow_path(self, network):\n        \"\"\"Evaluate 'A Narrow Path' interventions\"\"\"\n        interventions = {\n            'compute_governance': {'node': 'APS_Systems', 'value': 0.3},\n            'international_coordination': {'node': 'Deployment_Decisions', 'value': 'WITHHOLD'}\n        }\n        \n        baseline = self.calculate_baseline_risk(network)\n        results = {}\n        \n        for name, intervention in interventions.items():\n            modified_risk = self.apply_intervention(network, intervention)\n            results[name] = {\n                'baseline_risk': baseline,\n                'modified_risk': modified_risk,\n                'reduction': (baseline - modified_risk) / baseline\n            }\n            \n        self.visualize_policy_impacts(results)\n        return results\n\n\n\n2.0.6 6. Validation and Testing\n\n2.0.6.1 6.1 Comprehensive Test Suite\nclass TestAMTAIRPipeline:\n    \"\"\"Test suite validating thesis claims\"\"\"\n    \n    def test_extraction_accuracy(self):\n        \"\"\"Verify 85% structural extraction accuracy claim\"\"\"\n        \n    def test_probability_extraction(self):\n        \"\"\"Verify 73% probability extraction accuracy claim\"\"\"\n        \n    def test_scaling_performance(self):\n        \"\"\"Verify performance with networks up to 50 nodes\"\"\"\n\n\n2.0.6.2 6.2 Error Analysis\ndef analyze_extraction_errors(manual, automated):\n    \"\"\"\n    Categorize and visualize extraction errors.\n    Implements error taxonomy from Thesis Section 4.2.\n    \"\"\"\n    error_categories = {\n        'missed_nodes': [],\n        'incorrect_edges': [],\n        'probability_errors': []\n    }\n    \n    # Detailed error analysis with examples\n    create_error_analysis_report(error_categories)\n\n\n\n2.0.7 7. Export and Documentation\n\n2.0.7.1 7.1 Multiple Output Formats\ndef export_analysis_package(analysis_results):\n    \"\"\"\n    Export complete analysis package for thesis appendix:\n    - Jupyter notebook (with outputs)\n    - PDF report (formal documentation)\n    - Interactive HTML (for presentations)\n    - Raw data files (CSV, JSON)\n    - Standalone Python package\n    \"\"\"\n\n\n2.0.7.2 7.2 Reproducibility Package\ndef create_reproducibility_package():\n    \"\"\"\n    Generate complete package for reproducing results:\n    - Environment specification (requirements.txt)\n    - Data files with checksums\n    - Random seeds for all stochastic processes\n    - Step-by-step reproduction guide\n    \"\"\"\n\n\n\n2.0.8 8. Performance and Optimization\n\n2.0.8.1 8.1 Computational Benchmarks\ndef benchmark_pipeline_performance():\n    \"\"\"\n    Comprehensive performance testing matching thesis claims:\n    - Small networks (&lt;10 nodes): &lt;1 second\n    - Medium networks (10-30 nodes): 2-8 seconds  \n    - Large networks (30-50 nodes): 15-45 seconds\n    \"\"\"\n\n\n2.0.8.2 8.2 Memory Profiling\ndef profile_memory_usage():\n    \"\"\"Track memory usage throughout pipeline stages\"\"\"\n\n\n\n2.0.9 9. User Experience Enhancements\n\n2.0.9.1 9.1 Progress Indicators\nfrom tqdm.notebook import tqdm\n\ndef extract_with_progress(documents):\n    \"\"\"Show clear progress for long-running extractions\"\"\"\n    results = []\n    for doc in tqdm(documents, desc=\"Extracting arguments\"):\n        result = extract_argdown(doc)\n        results.append(result)\n    return results\n\n\n2.0.9.2 9.2 Error Handling and Recovery\ndef robust_extraction(text, max_retries=3):\n    \"\"\"\n    Robust extraction with automatic retry and error recovery.\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            return extract_argdown_from_text(text)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                return handle_extraction_failure(text, e)\n            time.sleep(2 ** attempt)  # Exponential backoff\n\n\n\n2.0.10 10. Integration with Thesis Claims\n\n2.0.10.1 10.1 Claim Validation Cells\nMark specific cells that validate thesis claims:\n#| label: validate-extraction-accuracy\n#| fig-cap: \"Validation of 85% extraction accuracy claim from Section 4.1\"\n\n# This cell specifically validates the claim made in thesis section 4.1\n# that structural extraction achieves 85% accuracy\n\n\n2.0.10.2 10.2 Cross-Reference Generation\ndef generate_thesis_crossref_table():\n    \"\"\"\n    Generate table mapping notebook sections to thesis chapters:\n    \n    | Notebook Section | Thesis Chapter | Key Claims Demonstrated |\n    |-----------------|----------------|------------------------|\n    | 1.0 ArgDown     | 3.1 Methods    | Two-stage extraction   |\n    | 4.0 Visualization| 4.3 Results   | Interactive networks   |\n    \"\"\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Comprehensive Jupyter Notebook Enhancement Plan 11.7</span>"
    ]
  }
]