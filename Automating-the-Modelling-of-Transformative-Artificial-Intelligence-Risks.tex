% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
  letterpaper,
]{book}
\usepackage{xcolor}
\usepackage[margin=2.5cm,paper=a4paper]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{biblatex}
\addbibresource{ref/MAref.bib}


% AMTAIR Thesis Preamble - Zero package conflicts
% Only formatting commands, no package loading

% Line spacing for academic work
\usepackage{setspace}
\onehalfspacing

% Custom chapter formatting (remove "Chapter N" prefix) but unfortunately leaves blank space
\usepackage{titlesec}
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries}  % format
  {}                           % label (empty = no "Chapter N")
  {0pt}                        % sep
  {\Huge}                      % before-code



% Page formatting and headers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\slshape\nouppercase{\rightmark}}
\fancyhead[LO,RE]{\slshape\nouppercase{\leftmark}}
\fancyfoot[C]{\thepage}

% % Fix page breaks after title page
% \newcommand{\cleartitlepage}{
%   \clearpage
%   \thispagestyle{empty}
%   \mbox{}
%   \clearpage
% }



\renewcommand{\maketitle}{}

%  Citation customization
% \usepackage[style=authoryear,backend=biber,natbib=true]{biblatex}

% % Custom citation commands for different contexts
% \newcommand{\citeauthor}[1]{\textcite{#1}}           % Author (year)
% \newcommand{\citeyear}[1]{(\citeyear*{#1})}         % (year)
% \newcommand{\citealt}[1]{\citeauthor{#1} \citeyear{#1}}  % Author year
% \newcommand{\citep}[1]{(\cite{#1})}                 # (Author, year)

% Page reference styling
% \DeclareFieldFormat{postnote}{#1}                    # No "p." prefix
% \DeclareFieldFormat{multipostnote}{#1}               # No "pp." prefix


% % Page numbering control
% \usepackage{afterpage}

% % Command to start front matter (roman numerals)
% \newcommand{\frontmatter}{
%   \cleardoublepage
%   \pagenumbering{roman}
%   \setcounter{page}{1}
% }

% % Command to start main matter (arabic numerals)
% \newcommand{\mainmatter}{
%   \cleardoublepage
%   \pagenumbering{arabic}
%   \setcounter{page}{1}
% }

% % Command to start back matter (continue arabic)
% \newcommand{\backmatter}{
%   \cleardoublepage
%   % Keep arabic numbering but could change style if needed
% }

% % Suppress page numbers on title page
% \newcommand{\titlepage}{
%   \thispagestyle{empty}
% }



% Commands for custom title page
% \newcommand{\thesistitle}{Automating the Modelling of Transformative Artificial Intelligence Risks}
% \newcommand{\thesisauthor}{Valentin Jakob Meyer}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Automating the Modelling of Transformative Artificial Intelligence Risks},
  pdfauthor={Valentin Jakob Meyer},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Automating the Modelling of Transformative Artificial
Intelligence Risks}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{An Epistemic Framework for Leveraging Frontier AI Systems to
Upscale Conditional Policy Assessments in Bayesian Networks on a Narrow
Path towards Existential Safety}
\author{Valentin Jakob Meyer}
\date{2025-05-26}
\begin{document}
\frontmatter
\maketitle

\begin{titlepage}
\thispagestyle{empty}% Remove page number from title page

% Top header with logo (left) and department (right)
\begin{minipage}{0.3\textwidth}
  \includegraphics[width=5cm]{latex/uni-bayreuth-logo.png}
\end{minipage}
\hfill
\begin{minipage}{0.9\textwidth}
  \begin{center}
    -- P\&E Master's Programme --\\
    Chair of Philosophy, Computer\\
    Science \& Artificial Intelligence
  \end{center}
\end{minipage}

% Horizontal rule
\vspace{1.5cm}
\hrule
\vspace{2cm}

% Title in bold
\begin{center}
  \Large\textbf{Automating the Modelling of
Transformative Artificial Intelligence Risks}
\end{center}
\vspace{0.2cm}

\begin{center}
  -----
\end{center}
\vspace{0.2cm}

% Subtitle in italics with quotation marks
\begin{center}
  \normalsize``\textit{An Epistemic Framework for Leveraging Frontier AI Systems
to Upscale Conditional Policy Assessments in Bayesian Networks on a Narrow Path towards Existencial Safety }''
\end{center}
\vspace{0.2cm}

\begin{center}
  -----
\end{center}
\vspace{0.2cm}

% Thesis designation
\begin{center}
  A thesis submitted at the Department of Philosophy\\[0.4cm]
  for the degree of \textit{Master of Arts in Philosophy \& Economics}
\end{center}

\vspace{1.5cm}
% Horizontal rule
\hrule
\vspace{1.5cm}

% Author and supervisor information with precise alignment
\begin{minipage}[t]{0.48\textwidth}
  \textbf{Author:}\\[0.3cm]
  \href{https://www.vjmeyer.org}{Valentin Jakob Meyer}\\
  \href{mailto:Valentin.meyer@uni-bayreuth.de}{Valentin.meyer@uni-bayreuth.de}\\
  \textit{Matriculation Number:} 1828610\\
  \textit{Tel.:} +49 (1573) 4512494\\
  Pielmühler Straße 15\\
  93138 Lappersdorf
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
  \begin{flushright}
    \textbf{Supervisor:}\\[0.3cm]
    \href{mailto:timo.speith@uni-bayreuth.de}{Dr. Timo Speith}\\[0.35cm]
    \textit{Word Count:}\\
    30.000\\[0.1cm]
    \textit{Source / Identifier:}\\
    \href{https://github.com/VJMeyer/submission}{Document URL}
  \end{flushright}
\end{minipage}

% Date at bottom
\vfill
\begin{center}
  26th of May 2025
\end{center}
\end{titlepage}

% Critical: Clean page break to TOC
\cleardoublepage

\renewcommand*\contentsname{Table of Contents}
{
\setcounter{tocdepth}{9}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\bookmarksetup{startatroot}

\chapter*{Abstract}\label{sec-abstract}
\addcontentsline{toc}{chapter}{Abstract}

\markboth{Abstract}{Abstract}

\begin{quote}
The coordination crisis in AI governance presents a paradoxical
challenge: unprecedented investment in AI safety coexists alongside
fundamental coordination failures across technical, policy, and ethical
domains. These divisions systematically increase existential risk. This
thesis introduces AMTAIR (Automating Transformative AI Risk Modeling), a
computational approach addressing this coordination failure by
automating the extraction of probabilistic world models from AI safety
literature using frontier language models. The system implements an
end-to-end pipeline transforming unstructured text into interactive
Bayesian networks through a novel two-stage extraction process that
bridges communication gaps between stakeholders.
\end{quote}

\texttt{The\ coordination\ crisis\ in\ AI\ governance\ presents\ a\ paradoxical\ challenge:\ unprecedented\ investment\ in\ AI\ safety\ coexists\ alongside\ fundamental\ coordination\ failures\ across\ technical,\ policy,\ and\ ethical\ domains.\ These\ divisions\ systematically\ increase\ existential\ risk\ by\ creating\ safety\ gaps,\ misallocating\ resources,\ and\ fostering\ inconsistent\ approaches\ to\ interdependent\ problems.}

\begin{quote}
This thesis introduces AMTAIR (Automating Transformative AI Risk
Modeling), a computational approach that addresses this coordination
failure by automating the extraction of probabilistic world models from
AI safety literature using frontier language models.
\end{quote}

\texttt{The\ AMTAIR\ system\ implements\ an\ end-to-end\ pipeline\ that\ transforms\ unstructured\ text\ into\ interactive\ Bayesian\ networks\ through\ a\ novel\ two-stage\ extraction\ process:\ first\ capturing\ argument\ structure\ in\ ArgDown\ format,\ then\ enhancing\ it\ with\ probability\ information\ in\ BayesDown.\ This\ approach\ bridges\ communication\ gaps\ between\ stakeholders\ by\ making\ implicit\ models\ explicit,\ enabling\ comparison\ across\ different\ worldviews,\ providing\ a\ common\ language\ for\ discussing\ probabilistic\ relationships,\ and\ supporting\ policy\ evaluation\ across\ diverse\ scenarios.}

\bookmarksetup{startatroot}

\chapter*{Prefatory Apparatus:
Frontmatter}\label{prefatory-apparatus-frontmatter}
\addcontentsline{toc}{chapter}{Prefatory Apparatus: Frontmatter}

\markboth{Prefatory Apparatus: Frontmatter}{Prefatory Apparatus:
Frontmatter}

\section*{Illustrations and Terminology --- Quick
References}\label{illustrations-and-terminology-quick-references}
\addcontentsline{toc}{section}{Illustrations and Terminology --- Quick
References}

\markright{Illustrations and Terminology --- Quick References}

\subsection*{\texorpdfstring{\textbf{Acknowledgments}}{Acknowledgments}}\label{acknowledgments}
\addcontentsline{toc}{subsection}{\textbf{Acknowledgments}}

\begin{itemize}
\tightlist
\item
  Academic supervisor (Prof.~Timo Speith) and institution (University of
  Bayreuth)\\
\item
  Research collaborators, especially those connected to the original
  MTAIR project\\
\item
  Technical advisors who provided feedback on implementation aspects\\
\item
  Personal supporters who enabled the research through encouragement and
  feedback
\end{itemize}

\section*{List of Graphics \& Figures}\label{list-of-graphics-figures}
\addcontentsline{toc}{section}{List of Graphics \& Figures}

\markright{List of Graphics \& Figures}

\section*{List of Abbreviations}\label{list-of-abbreviations}
\addcontentsline{toc}{section}{List of Abbreviations}

\markright{List of Abbreviations}

\begin{itemize}
\tightlist
\item
  AGI - Artificial General Intelligence
\item
  AMTAIR - Automating Modeling of Transformative AI Risks
\item
  API - Application Programming Interface
\item
  APS - Advanced, Planning, Strategic (AI systems per
  \textcite{carlsmith2021})
\item
  BN - Bayesian Network
\item
  CPT - Conditional Probability Table
\item
  DAG - Directed Acyclic Graph
\item
  LLM - Large Language Model
\item
  MTAIR - Modeling Transformative AI Risks
\item
  TAI - Transformative Artificial Intelligence
\end{itemize}

\section*{Glossary}\label{glossary}

\markright{Glossary}

\begin{itemize}
\tightlist
\item
  \textbf{Argument mapping}: A method for visually representing the
  structure of arguments\\
\item
  \textbf{BayesDown}: An extension of ArgDown that incorporates
  probabilistic information\\
\item
  \textbf{Bayesian network}: A probabilistic graphical model
  representing variables and their dependencies\\
\item
  \textbf{Conditional probability}: The probability of an event given
  that another event has occurred\\
\item
  \textbf{Directed Acyclic Graph (DAG)}: A graph with directed edges and
  no cycles\\
\item
  \textbf{Existential risk}: Risk of permanent curtailment of humanity's
  potential\\
\item
  \textbf{Power-seeking AI}: AI systems with instrumental incentives to
  acquire resources and power\\
\item
  \textbf{Prediction market}: A market where participants trade
  contracts that resolve based on future events\\
\item
  \textbf{d-separation}: A criterion for identifying conditional
  independence relationships in Bayesian networks\\
\item
  \textbf{Monte Carlo sampling}: A computational technique using random
  sampling to obtain numerical results
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Final Thesis: Automating the Modeling of Transformative
Artificial Intelligence
Risks}\label{final-thesis-automating-the-modeling-of-transformative-artificial-intelligence-risks}

\section{Frontmatter: Preface}\label{frontmatter-preface}

This thesis represents the culmination of interdisciplinary research at
the intersection of AI safety, formal epistemology, and computational
social science. The work emerged from recognizing a fundamental
challenge in AI governance: while investment in AI safety research has
grown exponentially, coordination between different stakeholder
communities remains fragmented, potentially increasing existential risk
through misaligned efforts.

The journey from initial concept to working implementation involved
iterative refinement based on feedback from advisors, domain experts,
and potential users. What began as a technical exercise in automated
extraction evolved into a broader framework for enhancing epistemic
security in one of humanity's most critical coordination challenges. The
AMTAIR project---Automating Transformative AI Risk Modeling---represents
an attempt to build computational bridges between communities that,
despite shared concerns about AI risk, often struggle to communicate
effectively due to incompatible frameworks, terminologies, and implicit
assumptions.

I hope this work contributes to building the intellectual and technical
infrastructure necessary for humanity to navigate the transition to
transformative AI safely. The tools and frameworks presented here are
offered in the spirit of collaborative problem-solving, recognizing that
the challenges we face require unprecedented cooperation across
disciplines, institutions, and worldviews.

\section{Acknowledgments}\label{acknowledgments-1}

I thank my supervisor Dr.~Timo Speith for his guidance throughout this
project, providing both technical insights and philosophical grounding.
The MTAIR team's pioneering manual approach inspired this automation
effort, and I am grateful for their foundational work.

I acknowledge Johannes Meyer and Jelena Meyer for their invaluable
assistance in verifying the automated extraction procedure through
manual extraction of ArgDown and BayesDown data from the Carlsmith
paper, providing crucial ground truth for validation.

Special recognition goes to Coleman Snell for his partnership and
research collaboration with the AMTAIR project, offering both technical
expertise and strategic vision. The AI safety community's creation of
rich literature made this work possible, and I thank all researchers
whose arguments provided the raw material for formalization.

Any errors or limitations remain my own responsibility.

\section{List of Figures}\label{list-of-figures}

\section{List of Tables}\label{list-of-tables}

\section{List of Abbreviations}\label{list-of-abbreviations-1}

AI - Artificial Intelligence\\
AGI - Artificial General Intelligence\\
AMTAIR - Automating Transformative AI Risk Modeling\\
API - Application Programming Interface\\
APS - Advanced, Planning, Strategic (AI systems)\\
BN - Bayesian Network\\
CPT - Conditional Probability Table\\
DAG - Directed Acyclic Graph\\
LLM - Large Language Model\\
ML - Machine Learning\\
MTAIR - Modeling Transformative AI Risks\\
NLP - Natural Language Processing\\
P\&E - Philosophy \& Economics\\
PDF - Portable Document Format\\
TAI - Transformative Artificial Intelligence

\bookmarksetup{startatroot}

\chapter{1. Introduction: The Coordination Crisis in AI
Governance}\label{introduction-the-coordination-crisis-in-ai-governance}

\textbf{Chapter Overview}\\
\textbf{Grade Weight}: 10\% \textbar{} \textbf{Target Length}:
\textasciitilde14\% of text (\textasciitilde4,200 words)\\
\textbf{Requirements}: Introduces and motivates the core question,
provides context, states precise thesis, provides roadmap

\section{1.1 Opening Scenario: The Policymaker's
Dilemma}\label{opening-scenario-the-policymakers-dilemma}

Imagine a senior policy advisor preparing recommendations for AI
governance legislation. On her desk lie a dozen reports from leading AI
safety researchers, each painting a different picture of the risks
ahead. One argues that misaligned AI could pose existential risks within
the decade, citing complex technical arguments about instrumental
convergence and orthogonality. Another suggests these concerns are
overblown, emphasizing uncertainty and the strength of existing
institutions. A third proposes specific technical standards but
acknowledges deep uncertainty about their effectiveness.

Each report seems compelling in isolation, written by credentialed
experts with sophisticated arguments. Yet they reach dramatically
different conclusions about both the magnitude of risk and appropriate
interventions. The technical arguments involve unfamiliar
concepts---mesa-optimization, corrigibility, capability
amplification---expressed through different frameworks and implicit
assumptions. Time is limited, stakes are high, and the legislation could
shape humanity's trajectory for decades.

This scenario\footnote{The orthogonality thesis posits that intelligence
  and goals are independent---an AI can have any set of objectives
  regardless of its intelligence level. The instrumental convergence
  thesis suggests that different AI systems may adopt similar
  instrumental goals (e.g., self-preservation, resource acquisition) to
  achieve their objectives.} plays out daily across government offices,
corporate boardrooms, and research institutions worldwide. It
exemplifies what I term the ``coordination crisis'' in AI governance:
despite unprecedented attention and resources directed toward AI safety,
we lack the epistemic infrastructure to synthesize diverse expert
knowledge into actionable governance strategies \textcite{todd2024}.

Show Image

\section{1.2 The Coordination Crisis in AI
Governance}\label{the-coordination-crisis-in-ai-governance}

As AI capabilities advance at an accelerating pace---demonstrated by the
rapid progression from GPT-3 to GPT-4, Claude, and emerging multimodal
systems \textcite{maslej2025} \textcite{samborska2025}---humanity faces
a governance challenge unlike any in history. The task of ensuring
increasingly powerful AI systems remain aligned with human values and
beneficial to our long-term flourishing grows more urgent with each
capability breakthrough. This challenge becomes particularly acute when
considering transformative AI systems that could drastically alter
civilization's trajectory, potentially including existential risks from
misaligned systems pursuing objectives counter to human welfare.

Despite unprecedented investment in AI safety research, rapidly growing
awareness among key stakeholders, and proliferating frameworks for
responsible AI development, we face what I'll term the ``coordination
crisis'' in AI governance---a systemic failure to align diverse efforts
across technical, policy, and strategic domains into a coherent response
proportionate to the risks we face.

The current state of AI governance presents a striking paradox. On one
hand, we witness extraordinary mobilization: billions in research
funding, proliferating safety initiatives, major tech companies
establishing alignment teams, and governments worldwide developing AI
strategies. The Asilomar AI Principles garnered thousands of signatures
\textcite{tegmark2024}, the EU advances comprehensive AI regulation
\textcite{european2024}, and technical researchers produce increasingly
sophisticated work on alignment, interpretability, and robustness.

Yet alongside this activity, we observe systematic coordination failures
that may prove catastrophic. Technical safety researchers develop
sophisticated alignment techniques without clear implementation
pathways. Policy specialists craft regulatory frameworks lacking
technical grounding to ensure practical efficacy. Ethicists articulate
normative principles that lack operational specificity. Strategy
researchers identify critical uncertainties but struggle to translate
these into actionable guidance. International bodies convene without
shared frameworks for assessing interventions.

Show Image

\subsection{1.2.1 Safety Gaps from Misaligned
Efforts}\label{safety-gaps-from-misaligned-efforts}

The fragmentation problem manifests in incompatible frameworks between
technical researchers, policy specialists, and strategic analysts. Each
community develops sophisticated approaches within their domain, yet
translation between domains remains primitive. This creates systematic
blind spots where risks emerge at the interfaces between technical
capabilities, institutional responses, and strategic dynamics.

When different communities operate with incompatible frameworks,
critical risks fall through the cracks. Technical researchers may solve
alignment problems under assumptions that policymakers' decisions
invalidate. Regulations optimized for current systems may inadvertently
incentivize dangerous development patterns. Without shared models of the
risk landscape, our collective efforts resemble the parable of blind men
describing an elephant---each accurate within their domain but missing
the complete picture \textcite{paul2023}.

Historical precedents demonstrate how coordination failures in
technology governance can lead to dangerous dynamics. The nuclear arms
race exemplifies how lack of coordination can create negative-sum
outcomes where all parties become less secure despite massive
investments in safety measures. Similar dynamics may emerge in AI
development without proper coordination infrastructure.

\subsection{1.2.2 Resource Misallocation}\label{resource-misallocation}

The AI safety community faces a complex tradeoff in resource allocation.
While some duplication of efforts can improve reliability through
independent verification---akin to reproducing scientific results---the
current level of fragmentation often leads to wasteful redundancy.
Multiple teams independently develop similar frameworks without building
on each other's work, creating opportunity costs where critical but
unglamorous research areas remain understaffed. Funders struggle to
identify high-impact opportunities across technical and governance
domains, lacking the epistemic infrastructure to assess where marginal
resources would have the greatest impact. This misallocation becomes
more costly as the window for establishing effective governance narrows
with accelerating AI development.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{Examples of duplicated AI safety efforts across
organizations}\label{tbl-resource-duplication}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Research Area
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Organization A
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Organization B
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Duplication Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Opportunity Cost
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Research Area
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Organization A
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Organization B
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Duplication Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Opportunity Cost
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Interpretability Methods & Anthropic's mechanistic interpretability &
DeepMind's concept activation vectors & Medium & Reduced focus on
multi-agent safety \\
Alignment Frameworks & MIRI's embedded agency & FHI's comprehensive AI
services & High & Limited work on institutional design \\
Risk Assessment Models & GovAI's policy models & CSER's existential risk
frameworks & High & Insufficient capability benchmarking \\
\end{longtable}

\subsection{1.2.3 Negative-Sum Dynamics}\label{negative-sum-dynamics}

Perhaps most concerning, uncoordinated interventions can actively
increase risk. Safety standards that advantage established players may
accelerate risky development elsewhere. Partial transparency
requirements might enable capability advances without commensurate
safety improvements. International agreements lacking shared technical
understanding may lock in dangerous practices. Without coordination, our
cure risks becoming worse than the disease.

The game-theoretic structure of AI development creates particularly
pernicious dynamics. Armstrong et al. \textcite{armstrong2016}
demonstrate how uncoordinated policies can incentivize a ``race to the
precipice'' where competitive pressures override safety considerations.
The situation resembles a multi-player prisoner's dilemma or stag hunt
where individually rational decisions lead to collectively catastrophic
outcomes \textcite{samuel2023} \textcite{hunt2025}.

\section{1.3 Historical Parallels and Temporal
Urgency}\label{historical-parallels-and-temporal-urgency}

History offers instructive parallels. The nuclear age began with
scientists racing to understand and control forces that could destroy
civilization. Early coordination failures---competing national programs,
scientist-military tensions, public-expert divides---nearly led to
catastrophe multiple times. Only through developing shared frameworks
(deterrence theory) \textcite{schelling1960}, institutions (IAEA), and
communication channels (hotlines, treaties) did humanity navigate the
nuclear precipice \textcite{rehman2025}.

Yet AI presents unique coordination challenges that compress our
response timeline:

\textbf{Accelerating Development}: Unlike nuclear weapons requiring
massive infrastructure, AI development proceeds in corporate labs and
academic departments worldwide. Capability improvements come through
algorithmic insights and computational scale, both advancing
exponentially.

\textbf{Dual-Use Ubiquity}: Every AI advance potentially contributes to
both beneficial applications and catastrophic risks. The same language
model architectures enabling scientific breakthroughs could facilitate
dangerous manipulation or deception at scale.

\textbf{Comprehension Barriers}: Nuclear risks were viscerally
understandable---cities vaporized, radiation sickness, nuclear winter.
AI risks involve abstract concepts like optimization processes, goal
misspecification, and emergent capabilities that resist intuitive
understanding.

\textbf{Governance Lag}: Traditional governance
mechanisms---legislation, international treaties, professional
standards---operate on timescales of years to decades. AI capabilities
advance on timescales of months to years, creating an ever-widening
capability-governance gap.

Show Image

\section{1.4 Research Question and
Scope}\label{research-question-and-scope}

This thesis addresses a specific dimension of the coordination challenge
by investigating the question:

\textbf{Can frontier AI technologies be utilized to automate the
modeling of transformative AI risks, enabling robust prediction of
policy impacts across diverse worldviews?}

More specifically, I explore whether frontier language models can
automate the extraction and formalization of probabilistic world models
from AI safety literature, creating a scalable computational framework
that enhances coordination in AI governance through systematic policy
evaluation under uncertainty.

To break this down into its components:

\begin{itemize}
\tightlist
\item
  \textbf{Frontier AI Technologies}: Today's most capable language
  models (GPT-4, Claude-3 level systems)
\item
  \textbf{Automated Modeling}: Using these systems to extract and
  formalize argument structures from natural language
\item
  \textbf{Transformative AI Risks}: Potentially catastrophic outcomes
  from advanced AI systems, particularly existential risks
\item
  \textbf{Policy Impact Prediction}: Evaluating how governance
  interventions might alter probability distributions over outcomes
\item
  \textbf{Diverse Worldviews}: Accounting for fundamental disagreements
  about AI development trajectories and risk factors
\end{itemize}

The investigation encompasses both theoretical development and practical
implementation, focusing specifically on existential risks from
misaligned AI systems rather than broader AI ethics concerns. This
narrowed scope enables deep technical development while addressing the
highest-stakes coordination challenges.

\section{1.5 The Multiplicative Benefits
Framework}\label{the-multiplicative-benefits-framework}

The central thesis of this work is that combining three
elements---automated worldview extraction, prediction market
integration, and formal policy evaluation---creates multiplicative
rather than merely additive benefits for AI governance. Each component
enhances the others, creating a system more valuable than the sum of its
parts.

Show Image

\subsection{1.5.1 Automated Worldview
Extraction}\label{automated-worldview-extraction}

Current approaches to AI risk modeling, exemplified by the Modeling
Transformative AI Risks (MTAIR) project, demonstrate the value of formal
representation but require extensive manual effort. Creating a single
model demands dozens of expert-hours to translate qualitative arguments
into quantitative frameworks. This bottleneck severely limits the number
of perspectives that can be formalized and the speed of model updates as
new arguments emerge.

Automation using frontier language models addresses this scaling
challenge. By developing systematic methods to extract causal structures
and probability judgments from natural language, we can:

\begin{itemize}
\tightlist
\item
  Process orders of magnitude more content
\item
  Incorporate diverse perspectives rapidly
\item
  Maintain models that evolve with the discourse
\item
  Reduce barriers to entry for contributing worldviews
\end{itemize}

\subsection{1.5.2 Live Data Integration}\label{live-data-integration}

Static models, however well-constructed, quickly become outdated in
fast-moving domains. Prediction markets and forecasting platforms
aggregate distributed knowledge about uncertain futures, providing
continuously updated probability estimates. By connecting formal models
to these live data sources, we create dynamic assessments that
incorporate the latest collective intelligence \textcite{tetlock2015}.

This integration serves multiple purposes:

\begin{itemize}
\tightlist
\item
  Grounding abstract models in empirical forecasts
\item
  Identifying which uncertainties most affect outcomes
\item
  Revealing when model assumptions diverge from collective expectations
\item
  Generating new questions for forecasting communities
\end{itemize}

\subsection{1.5.3 Formal Policy
Evaluation}\label{formal-policy-evaluation}

\textbf{Formal policy evaluation} transforms static risk assessments
into actionable guidance by modeling how specific interventions alter
critical parameters. Using causal inference techniques
\textcite{pearl2000} \textcite{pearl2009}, we can assess not just the
probability of adverse outcomes but how those probabilities change under
different policy regimes.

This enables genuinely evidence-based policy development:

\begin{itemize}
\tightlist
\item
  Comparing interventions across multiple worldviews
\item
  Identifying robust strategies that work across scenarios
\item
  Understanding which uncertainties most affect policy effectiveness
\item
  Prioritizing research to reduce decision-relevant uncertainty
\end{itemize}

\subsection{1.5.4 The Synergy}\label{the-synergy}

The multiplicative benefits emerge from the interactions between
components:

\begin{itemize}
\tightlist
\item
  Automation enables comprehensive coverage, making prediction market
  integration more valuable by connecting to more perspectives
\item
  Market data validates and calibrates automated extractions, improving
  quality
\item
  Policy evaluation gains precision from both comprehensive models and
  live probability updates
\item
  The complete system creates feedback loops where policy analysis
  identifies critical uncertainties for market attention
\end{itemize}

This synergistic combination addresses the coordination crisis by
providing common ground for disparate communities, translating between
technical and policy languages, quantifying previously implicit
disagreements, and enabling evidence-based compromise.

\section{1.6 Thesis Structure and
Roadmap}\label{thesis-structure-and-roadmap}

The remainder of this thesis develops the multiplicative benefits
framework from theoretical foundations to practical implementation:

\textbf{Chapter 2: Context and Theoretical Foundations} establishes the
intellectual groundwork, examining the epistemic challenges unique to AI
governance, Bayesian networks as formal tools for uncertainty
representation, argument mapping as a bridge from natural language to
formal models, the MTAIR project's achievements and limitations, and
requirements for effective coordination infrastructure.

\textbf{Chapter 3: AMTAIR Design and Implementation} presents the
technical system including overall architecture and design principles,
the two-stage extraction pipeline (ArgDown → BayesDown), validation
methodology and results, case studies from simple examples to complex AI
risk models, and integration with prediction markets and policy
evaluation.

\textbf{Chapter 4: Discussion - Implications and Limitations} critically
examines technical limitations and failure modes, conceptual concerns
about formalization, integration with existing governance frameworks,
scaling challenges and opportunities, and broader implications for
epistemic security.

\textbf{Chapter 5: Conclusion} synthesizes key contributions and charts
paths forward with a summary of theoretical and practical achievements,
concrete recommendations for stakeholders, research agenda for community
development, and vision for AI governance with proper coordination
infrastructure.

Throughout this progression, I maintain dual focus on theoretical
sophistication and practical utility. The framework aims not merely to
advance academic understanding but to provide actionable tools for
improving coordination in AI governance during this critical period.

Show Image

Having established the coordination crisis and outlined how automated
modeling can address it, we now turn to the theoretical foundations that
make this approach possible. The next chapter examines the unique
epistemic challenges of AI governance and introduces the formal
tools---particularly Bayesian networks---that enable rigorous reasoning
under deep uncertainty.

\bookmarksetup{startatroot}

\chapter{2. Context and Theoretical
Foundations}\label{context-and-theoretical-foundations}

\textbf{Chapter Overview}\\
\textbf{Grade Weight}: 20\% \textbar{} \textbf{Target Length}:
\textasciitilde29\% of text (\textasciitilde8,700 words)\\
\textbf{Requirements}: Demonstrates understanding of relevant concepts,
explains relevance, situates in debate, reconstructs arguments

This chapter establishes the theoretical and methodological foundations
for the AMTAIR approach. We begin by examining a concrete example of
structured AI risk assessment---Joseph Carlsmith's power-seeking AI
model---to ground our discussion in practical terms. We then explore the
unique epistemic challenges of AI governance that render traditional
policy analysis inadequate, introduce Bayesian networks as formal tools
for representing uncertainty, and examine how argument mapping bridges
natural language reasoning and formal models. The chapter concludes by
analyzing the MTAIR project's achievements and limitations, motivating
the need for automated approaches, and surveying relevant literature
across AI risk modeling, governance proposals, and technical
methodologies.

\section{2.1 AI Existential Risk: The Carlsmith
Model}\label{ai-existential-risk-the-carlsmith-model}

To ground our discussion in concrete terms, I examine Joseph Carlsmith's
``Is Power-Seeking AI an Existential Risk?'' as an exemplar of
structured reasoning about AI catastrophic risk
\textcite{carlsmith2022}. Carlsmith's analysis stands out for its
explicit probabilistic decomposition of the path from current AI
development to potential existential catastrophe.

\subsection{2.1.1 Six-Premise
Decomposition}\label{six-premise-decomposition}

According to the MTAIR model \textcite{clarke2022}, Carlsmith decomposes
existential risk into a probabilistic chain with explicit
estimates\footnote{Multiple versions of Carlsmith's paper exist with
  slight updates to probability estimates: \textcite{carlsmith2021},
  \textcite{carlsmith2022}, \textcite{carlsmith2024}. We primarily
  reference the version used by the MTAIR team for their extraction.
  Extended discussion and expert probability estimates can be found on
  LessWrong.}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Premise 1}: Transformative AI development this century
  (P≈0.80)(P ≈ 0.80) (P≈0.80)
\item
  \textbf{Premise 2}: AI systems pursuing objectives in the world
  (P≈0.95)(P ≈ 0.95) (P≈0.95)
\item
  \textbf{Premise 3}: Systems with power-seeking instrumental incentives
  (P≈0.40)(P ≈ 0.40) (P≈0.40)
\item
  \textbf{Premise 4}: Sufficient capability for existential threat
  (P≈0.65)(P ≈ 0.65) (P≈0.65)
\item
  \textbf{Premise 5}: Misaligned systems despite safety efforts
  (P≈0.50)(P ≈ 0.50) (P≈0.50)
\item
  \textbf{Premise 6}: Catastrophic outcomes from misaligned
  power-seeking (P≈0.65)(P ≈ 0.65) (P≈0.65)
\end{enumerate}

\textbf{Composite Risk Calculation}: P(doom)≈0.05P(doom) ≈ 0.05
P(doom)≈0.05 (5\%)

mermaid

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flowchart TD}
\NormalTok{    P1[Premise 1: Transformative AI\textless{}br/\textgreater{}P ≈ 0.80] {-}{-}\textgreater{} P2[Premise 2: AI pursuing objectives\textless{}br/\textgreater{}P ≈ 0.95]}
\NormalTok{    P2 {-}{-}\textgreater{} P3[Premise 3: Power{-}seeking incentives\textless{}br/\textgreater{}P ≈ 0.40]}
\NormalTok{    P3 {-}{-}\textgreater{} P4[Premise 4: Existential capability\textless{}br/\textgreater{}P ≈ 0.65]}
\NormalTok{    P4 {-}{-}\textgreater{} P5[Premise 5: Misalignment despite safety\textless{}br/\textgreater{}P ≈ 0.50]}
\NormalTok{    P5 {-}{-}\textgreater{} P6[Premise 6: Catastrophic outcome\textless{}br/\textgreater{}P ≈ 0.65]}
\NormalTok{    P6 {-}{-}\textgreater{} D[Existential Catastrophe\textless{}br/\textgreater{}P ≈ 0.05]}
\end{Highlighting}
\end{Shaded}

Carlsmith structures his argument through six conditional premises, each
assigned explicit probability estimates:

\textbf{Premise 1: APS Systems by 2070} (P≈0.65)(P ≈ 0.65) (P≈0.65) ``By
2070, there will be AI systems with Advanced capability, Agentic
planning, and Strategic awareness''---the conjunction of capabilities
that could enable systematic pursuit of objectives in the world.

\textbf{Premise 2: Alignment Difficulty} (P≈0.40)(P ≈ 0.40) (P≈0.40)
``It will be harder to build aligned APS systems than misaligned systems
that are still attractive to deploy''---capturing the challenge that
safety may conflict with capability or efficiency.

\textbf{Premise 3: Deployment Despite Misalignment} (P≈0.70)(P ≈ 0.70)
(P≈0.70) ``Conditional on 1 and 2, we will deploy misaligned APS
systems''---reflecting competitive pressures and limited coordination.

\textbf{Premise 4: Power-Seeking Behavior} (P≈0.65)(P ≈ 0.65) (P≈0.65)
``Conditional on 1-3, misaligned APS systems will seek power in
high-impact ways''---based on instrumental convergence arguments.

\textbf{Premise 5: Disempowerment Success} (P≈0.40)(P ≈ 0.40) (P≈0.40)
``Conditional on 1-4, power-seeking will scale to permanent human
disempowerment''---despite potential resistance and safeguards.

\textbf{Premise 6: Existential Catastrophe} (P≈0.95)(P ≈ 0.95) (P≈0.95)
``Conditional on 1-5, this disempowerment constitutes existential
catastrophe''---connecting power loss to permanent curtailment of human
potential.

\textbf{Overall Risk}: Multiplying through the conditional chain yields
P(doom)≈0.05P(doom) ≈ 0.05 P(doom)≈0.05 or 5\% by 2070.

This structured approach exemplifies the type of reasoning AMTAIR aims
to formalize and automate. While Carlsmith spent months developing this
model manually, similar rigor exists implicitly in many AI safety
arguments awaiting extraction.

\subsection{2.1.2 Why Carlsmith Exemplifies Formalizable
Arguments}\label{why-carlsmith-exemplifies-formalizable-arguments}

Carlsmith's model demonstrates several features that make it ideal for
formal representation:

\textbf{Explicit Probabilistic Structure}: Each premise receives
numerical probability estimates with documented reasoning, enabling
direct translation to Bayesian network parameters.

\textbf{Clear Conditional Dependencies}: The logical flow from
capabilities through deployment decisions to catastrophic outcomes maps
naturally onto directed acyclic graphs.

\textbf{Transparent Decomposition}: Breaking the argument into modular
premises allows independent evaluation and sensitivity analysis of each
component.

\textbf{Documented Reasoning}: Extensive justification for each
probability enables extraction of both structure and parameters from the
source text.

We will return to Carlsmith's model in Chapter 3 as our primary complex
case study, demonstrating how AMTAIR successfully extracts and
formalizes this sophisticated multi-level argument.

Beyond Carlsmith's model, other structured approaches to AI risk---such
as Christiano's ``What failure looks like''
\textcite{christiano2019}---provide additional targets for automated
extraction, enabling comparative analysis across different expert
worldviews.

\section{2.2 The Epistemic Challenge of Policy
Evaluation}\label{the-epistemic-challenge-of-policy-evaluation}

AI governance policy evaluation faces unique epistemic challenges that
render traditional policy analysis methods insufficient. Understanding
these challenges motivates the need for new computational approaches.

\subsection{2.2.1 Unique Characteristics of AI
Governance}\label{unique-characteristics-of-ai-governance}

\textbf{Deep Uncertainty Rather Than Risk}: Traditional policy analysis
distinguishes between risk (known probability distributions) and
uncertainty (known possibilities, unknown probabilities). AI governance
faces deep uncertainty---we cannot confidently enumerate possible
futures, much less assign probabilities \textcite{hallegatte2012}. Will
recursive self-improvement enable rapid capability gains? Can value
alignment be solved technically? These foundational questions resist
empirical resolution before their answers become catastrophically
relevant.

\textbf{Complex Multi-Level Causation}: Policy effects propagate through
technical, institutional, and social levels with intricate feedback
loops. A technical standard might alter research incentives, shifting
capability development trajectories, changing competitive dynamics, and
ultimately affecting existential risk through pathways invisible at the
policy's inception. Traditional linear causal models cannot capture
these dynamics.

\textbf{Irreversibility and Lock-In}: Many AI governance decisions
create path dependencies that prove difficult or impossible to reverse.
Early technical standards shape development trajectories. Institutional
structures ossify. International agreements create sticky equilibria.
Unlike many policy domains where course correction remains possible, AI
governance mistakes may prove permanent.

\textbf{Value-Laden Technical Choices}: The entanglement of technical
and normative questions confounds traditional separation of facts and
values. What constitutes ``alignment''? How much capability development
should we risk for economic benefits? Technical specifications embed
ethical judgments that resist neutral expertise.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{Comparison of AI governance vs traditional policy
domains}\label{tbl-governance-challenges}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Traditional Policy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Governance
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Traditional Policy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Governance
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Uncertainty Type & Risk (known distributions) & Deep uncertainty
(unknown unknowns) \\
Causal Structure & Linear, traceable & Multi-level, feedback loops \\
Reversibility & Course correction possible & Path dependencies,
lock-in \\
Fact-Value Separation & Clear boundaries & Entangled
technical-normative \\
Empirical Grounding & Historical precedents & Unprecedented phenomena \\
Time Horizons & Years to decades & Months to centuries \\
\end{longtable}

\subsection{2.2.2 Limitations of Traditional
Approaches}\label{limitations-of-traditional-approaches}

Standard policy evaluation tools prove inadequate for these challenges:

\textbf{Cost-Benefit Analysis} assumes commensurable outcomes and stable
probability distributions. When potential outcomes include existential
catastrophe with deeply uncertain probabilities, the mathematical
machinery breaks down. Infinite negative utility resists standard
decision frameworks.

\textbf{Scenario Planning} helps explore possible futures but typically
lacks the probabilistic reasoning needed for decision-making under
uncertainty. Without quantification, scenarios provide narrative
richness but limited action guidance.

\textbf{Expert Elicitation} aggregates specialist judgment but struggles
with interdisciplinary questions where no single expert grasps all
relevant factors. Moreover, experts often operate with different
implicit models, making aggregation problematic.

\textbf{Red Team Exercises} test specific plans but miss systemic risks
emerging from component interactions. Gaming individual failures cannot
reveal emergent catastrophic possibilities.

These limitations create a methodological gap: we need approaches that
handle deep uncertainty, represent complex causation, quantify expert
disagreement, and enable systematic exploration of intervention effects.

\subsection{2.2.3 The Underlying Epistemic
Framework}\label{the-underlying-epistemic-framework}

The AMTAIR approach rests on a specific epistemic framework that
combines probabilistic reasoning, conditional logic, and possible worlds
semantics. This framework provides the philosophical foundation for
representing deep uncertainty about AI futures.

\textbf{Probabilistic Epistemology}: Following the Bayesian tradition,
we treat probability as a measure of rational credence rather than
objective frequency. This subjective interpretation allows meaningful
probability assignments even for unique, unprecedented events like AI
catastrophe. As E.T. Jaynes demonstrated, probability theory extends
deductive logic to handle uncertainty, providing a calculus for rational
belief \textcite{jaynes2003}.

\textbf{Conditional Structure}: The framework emphasizes conditional
rather than absolute probabilities. Instead of asking ``What is
P(catastrophe)?'' we ask ``What is P(catastrophe \textbar{} specific
assumptions)?'' This conditionalization makes explicit the dependency of
conclusions on worldview assumptions, enabling productive disagreement
about premises rather than conclusions.

\textbf{Possible Worlds Semantics}: We conceptualize uncertainty as
distributions over possible worlds---complete descriptions of how
reality might unfold. Each world represents a coherent scenario with
specific values for all relevant variables. Probability distributions
over these worlds capture both what we know and what we don't know about
the future.

This framework enables several key capabilities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Representing ignorance}: We can express uncertainty about
  uncertainty itself through hierarchical probability models
\item
  \textbf{Combining evidence}: Bayesian updating provides principled
  methods for integrating new information
\item
  \textbf{Comparing worldviews}: Different probability distributions
  over the same space of possibilities enable systematic comparison
\item
  \textbf{Evaluating interventions}: Counterfactual reasoning about how
  actions change probability distributions
\end{enumerate}

\subsection{2.2.4 Toward New Epistemic
Tools}\label{toward-new-epistemic-tools}

The inadequacy of traditional methods for AI governance creates an
urgent need for new epistemic tools. These tools must:

\begin{itemize}
\tightlist
\item
  \textbf{Handle Deep Uncertainty}: Move beyond point estimates to
  represent ranges of possibilities
\item
  \textbf{Capture Complex Causation}: Model multi-level interactions and
  feedback loops
\item
  \textbf{Quantify Disagreement}: Make explicit where experts diverge
  and why
\item
  \textbf{Enable Systematic Analysis}: Support rigorous comparison of
  policy options
\end{itemize}

\textbf{Key Insight}: The computational approaches developed in this
thesis---particularly Bayesian networks enhanced with automated
extraction---directly address each of these requirements by providing
formal frameworks for reasoning under uncertainty.

Show Image

Show Image

Show Image

Show Image

Recent work on conditional trees demonstrates the value of structured
approaches to uncertainty. McCaslin et al. \textcite{mccaslin2024} show
how hierarchical conditional forecasting can identify high-value
questions for reducing uncertainty about complex topics like AI risk.
Their methodology, which asks experts to produce simplified Bayesian
networks of informative forecasting questions, achieved nine times
higher information value than standard forecasting platform questions.

Tetlock's work with the Forecasting Research Institute
\textcite{tetlock2022} exemplifies how prediction markets can provide
empirical grounding for formal models. By structuring questions as
conditional trees, they enable forecasters to express complex
dependencies between events, providing exactly the type of data needed
for Bayesian network parameterization.

Gruetzemacher \textcite{gruetzemacher2022} evaluates the tradeoffs
between full Bayesian networks and conditional trees for forecasting
tournaments. While conditional trees offer simplicity, Bayesian networks
provide richer representation of dependencies---motivating AMTAIR's
approach of using full networks while leveraging conditional tree
insights for question generation.

\section{2.3 Bayesian Networks as Knowledge
Representation}\label{bayesian-networks-as-knowledge-representation}

Bayesian networks offer a mathematical framework uniquely suited to
addressing these epistemic challenges. By combining graphical structure
with probability theory, they provide tools for reasoning about complex
uncertain domains.

\subsection{2.3.1 Mathematical
Foundations}\label{mathematical-foundations}

A Bayesian network consists of:

\begin{itemize}
\tightlist
\item
  \textbf{Directed Acyclic Graph (DAG)}: Nodes represent variables,
  edges represent direct dependencies
\item
  \textbf{Conditional Probability Tables (CPTs)}: For each node,
  P(node\textbar parents) quantifies relationships
\end{itemize}

The joint probability distribution factors according to the graph
structure:

P(X1,X2,\ldots,Xn)=∏i=1nP(Xi∣Parents(Xi))P(X\_1, X\_2, \ldots, X\_n) =
\prod\_\{i=1\}\^{}\{n\} P(X\_i \textbar{}
Parents(X\_i))P(X1\hspace{0pt},X2\hspace{0pt},\ldots,Xn\hspace{0pt})=i=1∏n\hspace{0pt}P(Xi\hspace{0pt}∣Parents(Xi\hspace{0pt}))

This factorization enables efficient inference and embodies causal
assumptions explicitly.

Pearl's foundational work \textcite{pearl2014} established Bayesian
networks as a principled approach to automated reasoning under
uncertainty, providing both theoretical foundations and practical
algorithms.

\subsection{2.3.2 The Rain-Sprinkler-Grass
Example}\label{the-rain-sprinkler-grass-example}

The canonical example illustrates key concepts\footnote{This example,
  while simple, demonstrates all essential features of Bayesian networks
  and serves as the foundation for understanding more complex
  applications}:

\begin{verbatim}
[Grass_Wet]: Concentrated moisture on grass. 
 + [Rain]: Water falling from sky.
 + [Sprinkler]: Artificial watering system.
   + [Rain]
\end{verbatim}

Network Structure:

\begin{itemize}
\tightlist
\item
  \textbf{Rain} (root cause): P(rain) = 0.2
\item
  \textbf{Sprinkler} (intermediate): P(sprinkler\textbar rain) varies by
  rain state
\item
  \textbf{Grass\_Wet} (effect): P(wet\textbar rain, sprinkler) depends
  on both causes
\end{itemize}

mermaid

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flowchart TD}
\NormalTok{    R[Rain\textless{}br/\textgreater{}P(rain) = 0.2] {-}{-}\textgreater{} S[Sprinkler]}
\NormalTok{    R {-}{-}\textgreater{} G[Grass\_Wet]}
\NormalTok{    S {-}{-}\textgreater{} G}
    
\NormalTok{    subgraph CPT1[Sprinkler CPT]}
\NormalTok{        S1[P(sprinkler|rain) = 0.01]}
\NormalTok{        S2[P(sprinkler|¬rain) = 0.4]}
\NormalTok{    end}
    
\NormalTok{    subgraph CPT2[Grass\_Wet CPT]}
\NormalTok{        G1[P(wet|rain,sprinkler) = 0.99]}
\NormalTok{        G2[P(wet|rain,¬sprinkler) = 0.8]}
\NormalTok{        G3[P(wet|¬rain,sprinkler) = 0.9]}
\NormalTok{        G4[P(wet|¬rain,¬sprinkler) = 0.01]}
\NormalTok{    end}
\end{Highlighting}
\end{Shaded}

python

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Basic network representation}
\NormalTok{nodes }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}Rain\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Sprinkler\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Grass\_Wet\textquotesingle{}}\NormalTok{]}
\NormalTok{edges }\OperatorTok{=}\NormalTok{ [(}\StringTok{\textquotesingle{}Rain\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Sprinkler\textquotesingle{}}\NormalTok{), (}\StringTok{\textquotesingle{}Rain\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Grass\_Wet\textquotesingle{}}\NormalTok{), (}\StringTok{\textquotesingle{}Sprinkler\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Grass\_Wet\textquotesingle{}}\NormalTok{)]}

\CommentTok{\# Conditional probability specification}
\NormalTok{P\_wet\_given\_causes }\OperatorTok{=}\NormalTok{ \{}
\NormalTok{    (}\VariableTok{True}\NormalTok{, }\VariableTok{True}\NormalTok{): }\FloatTok{0.99}\NormalTok{,    }\CommentTok{\# Rain=T, Sprinkler=T}
\NormalTok{    (}\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{): }\FloatTok{0.80}\NormalTok{,   }\CommentTok{\# Rain=T, Sprinkler=F  }
\NormalTok{    (}\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{): }\FloatTok{0.90}\NormalTok{,   }\CommentTok{\# Rain=F, Sprinkler=T}
\NormalTok{    (}\VariableTok{False}\NormalTok{, }\VariableTok{False}\NormalTok{): }\FloatTok{0.01}   \CommentTok{\# Rain=F, Sprinkler=F}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This simple network demonstrates:

\begin{itemize}
\tightlist
\item
  \textbf{Marginal Inference}: P(grass\_wet) computed from joint
  distribution
\item
  \textbf{Diagnostic Reasoning}: P(rain\textbar grass\_wet) reasoning
  from effects to causes
\item
  \textbf{Intervention Modeling}: P(grass\_wet\textbar do(sprinkler=on))
  for policy analysis
\end{itemize}

Show Image

\subsubsection{Rain-Sprinkler-Grass Network
Rendering}\label{rain-sprinkler-grass-network-rendering}

\begin{verbatim}
#| label: rain_sprinkler_grass_example_network_rendering
#| echo: true
#| eval: true
#| fig-cap: "Dynamic Html Rendering of the Rain-Sprinkler-Grass DAG with Conditional Probabilities"
#| fig-link: "https://singularitysmith.github.io/AMTAIR_Prototype/bayesian_network.html"
#| fig-alt: "Dynamic Html Rendering of the Rain-Sprinkler-Grass DAG"

from IPython.display import IFrame

IFrame(src="https://singularitysmith.github.io/AMTAIR_Prototype/bayesian_network.html", width="100%", height="600px")
\end{verbatim}

\subsection{2.3.3 Advantages for AI Risk
Modeling}\label{advantages-for-ai-risk-modeling}

These features address key requirements for AI governance:

\begin{itemize}
\tightlist
\item
  \textbf{Handling Uncertainty}: Every parameter is a distribution, not
  a point estimate
\item
  \textbf{Representing Causation}: Directed edges embody causal
  relationships
\item
  \textbf{Enabling Analysis}: Formal inference algorithms support
  systematic evaluation
\item
  \textbf{Facilitating Communication}: Visual structure aids
  cross-domain understanding
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{3.5.6 Validation Against Original (From the MTAIR
Project)}\label{sec-carlsmith-validation}

To validate the AMTAIR extraction quality, an ideal approach would
involve systematic comparison with expert manual extractions. The
validation methodology would follow these steps:

\textbf{Expert Baseline Creation}: Multiple domain experts independently
extract ArgDown and BayesDown representations from the same source
documents. This creates a ground truth dataset accounting for legitimate
variation in expert interpretation.

\textbf{Structural Comparison}: Compare the extracted causal structures,
examining:

\begin{itemize}
\tightlist
\item
  Node identification completeness
\item
  Relationship preservation accuracy
\item
  Hierarchical organization fidelity
\item
  Handling of repeated or complex dependencies
\end{itemize}

\textbf{Probability Assessment}: Evaluate probability extraction by:

\begin{itemize}
\tightlist
\item
  Comparing explicit probability captures
\item
  Assessing interpretation of qualitative expressions
\item
  Measuring consistency across related probabilities
\item
  Identifying systematic biases or tendencies
\end{itemize}

\textbf{Semantic Preservation}: Expert review would assess whether the
formal representation maintains the essential meaning and nuance of the
original arguments.

For the Carlsmith model specifically, preliminary manual extractions by
domain experts (including Johannes Meyer and Jelena Meyer) suggest that
automated extraction achieves high structural fidelity---capturing the
key variables and their relationships---while probability estimates show
greater variation, reflecting the inherent ambiguity in translating
qualitative language to quantitative values.

\section{3.6 Validation Methodology}\label{sec-validation-methodology}

Establishing trust in automated extraction requires rigorous validation
across multiple dimensions.

\subsection{3.6.1 Ground Truth Construction}\label{sec-ground-truth}

An ideal validation protocol would establish ground truth through:

\textbf{Expert Selection}: Recruit domain experts with both AI safety
knowledge and experience in formal modeling. Experts should represent
diverse perspectives within the field to capture legitimate interpretive
variation.

\textbf{Standardized Training}: Provide consistent training on
ArgDown/BayesDown syntax and extraction principles. This ensures
methodological alignment while preserving substantive differences in
interpretation.

\textbf{Independent Extraction}: Have experts work independently on the
same source documents, preventing anchoring bias and capturing the
natural range of valid interpretations.

\textbf{Consensus Building}: Through structured discussion, identify
areas of convergence and legitimate disagreement. This distinguishes
extraction errors from genuine ambiguity in source materials.

\textbf{Documentation}: Record not just final extractions but the
reasoning process, creating rich data for understanding extraction
challenges and improving automated approaches.

\subsection{3.6.2 Evaluation Metrics}\label{sec-evaluation-metrics}

A comprehensive evaluation framework would assess multiple dimensions:

\textbf{Structural Metrics}:

\begin{itemize}
\tightlist
\item
  Node identification precision and recall
\item
  Edge extraction accuracy
\item
  Preservation of hierarchical relationships
\item
  Handling of complex dependencies
\end{itemize}

\textbf{Probability Metrics}:

\begin{itemize}
\tightlist
\item
  Accuracy of explicit probability extraction
\item
  Consistency in interpreting qualitative expressions
\item
  Preservation of conditional relationships
\item
  Handling of uncertainty about uncertainty
\end{itemize}

\textbf{Semantic Metrics}:

\begin{itemize}
\tightlist
\item
  Expert ratings of meaning preservation
\item
  Functional equivalence for key inferences
\item
  Preservation of author's argumentative intent
\item
  Appropriate simplification choices
\end{itemize}

\textbf{Pragmatic Metrics}:

\begin{itemize}
\tightlist
\item
  Usefulness for downstream analysis
\item
  Time savings versus manual extraction
\item
  Error patterns and failure modes
\item
  Robustness across document types
\end{itemize}

\subsection{3.6.3 Results Summary}\label{sec-validation-results}

While comprehensive validation remains future work, preliminary
assessments suggest:

\textbf{Structural Extraction}: The two-stage approach successfully
identifies major causal relationships and preserves hierarchical
structure. The explicit ArgDown intermediate representation allows
verification of structural accuracy before probability quantification.

\textbf{Probability Challenges}: Converting qualitative expressions to
numerical probabilities remains the primary challenge. Different experts
interpret phrases like ``likely'' or ``significant risk'' differently,
and automated extraction inherits this ambiguity.

\textbf{Practical Utility}: Despite imperfections, automated extraction
provides sufficient quality for many practical applications, especially
when combined with human review at critical points.

The validation framework itself represents a contribution, providing
systematic methods for assessing automated argument formalization tools
as this area develops.

\subsection{3.6.4 Error Analysis}\label{sec-error-analysis}

Common failure modes to avoid:

\textbf{Implicit Assumptions}: Unstated background assumptions that
experts infer but system misses. These often involve domain-specific
common knowledge that remains unspoken in expert discourse.

\textbf{Complex Conditionals}: Nested conditionals with multiple
antecedents challenge current parsing. Statements like ``If A and B,
then probably C, unless D'' require sophisticated logical analysis.

\textbf{Ambiguous Quantifiers}: Terms like ``significant'' lack clear
probability mapping without context. The same word may imply different
probabilities in different domains or even different parts of the same
argument.

\textbf{Coreference Resolution}: Pronouns and indirect references create
attribution challenges. When authors use ``this risk'' or ``that
assumption,'' identifying the correct referent requires deep contextual
understanding.

Understanding these limitations guides both current usage and future
improvements.

\section{3.7 Policy Evaluation
Capabilities}\label{sec-policy-evaluation}

Beyond extraction and visualization, AMTAIR enables systematic policy
analysis through formal intervention modeling.

\subsection{3.7.1 Intervention
Representation}\label{sec-intervention-representation}

Policy interventions can be modeled as modifications to network
parameters, following Pearl's do-calculus framework. An ideal
implementation would:

\textbf{Parameter Modification}: Represent policies as changes to
specific probability values. For instance, safety requirements might
reduce P(deployment\textbar misaligned) by making unsafe deployment less
likely.

\textbf{Structural Interventions}: Some policies add or remove causal
pathways. Regulatory oversight might introduce new nodes representing
approval processes.

\textbf{Uncertainty Propagation}: Model uncertainty about policy
effectiveness. Rather than assuming perfect implementation, represent
ranges of possible effects.

\textbf{Multi-Level Effects}: Capture how policies influence multiple
levels simultaneously---technical development, corporate behavior, and
international dynamics.

The formal framework enables rigorous counterfactual reasoning: ``What
would happen to existential risk if this policy were implemented?''

\subsection{3.7.2 Example: Deployment
Governance}\label{sec-deployment-example}

Consider a hypothetical policy requiring safety certification before
deployment:

\textbf{Baseline Scenario}: Without intervention, the model might show
P(deployment\textbar misaligned) = 0.7, reflecting competitive pressures
to deploy despite risks.

\textbf{Policy Intervention}: Safety certification requirements could
reduce this to P(deployment\textbar misaligned) = 0.1, assuming
effective enforcement.

\textbf{Downstream Effects}: This change propagates through the network:

\begin{itemize}
\tightlist
\item
  Reduced deployment of misaligned systems
\item
  Lower probability of power-seeking behavior manifestation
\item
  Decreased existential risk
\end{itemize}

\textbf{Quantitative Assessment}: The formal model enables precise
calculation of risk reduction, helping prioritize among possible
interventions.

This example illustrates how formal models transform vague policy
discussions into concrete quantitative analyses, though the specific
numbers depend on model assumptions and parameter estimates.

\subsection{3.7.3 Robustness Analysis}\label{sec-robustness}

Policies must work across worldviews. AMTAIR enables multi-model
evaluation, parameter sensitivity testing, scenario analysis, and
confidence bound computation---ensuring interventions remain effective
despite uncertainty.

\textbf{Cross-Model Testing}: Evaluate policies across different
extracted worldviews to identify robust strategies that work regardless
of which expert's model proves correct.

\textbf{Sensitivity Analysis}: Identify which parameters most affect
policy effectiveness, focusing implementation efforts on critical
factors.

\textbf{Scenario Planning}: Test policies under different future
scenarios---slow versus fast takeoff, unipolar versus multipolar
development, cooperative versus adversarial dynamics.

\textbf{Confidence Bounds}: Rather than point estimates, compute ranges
of possible effects accounting for parameter uncertainty.

\section{3.8 Interactive Visualization
Design}\label{sec-visualization-design}

Making Bayesian networks accessible to diverse stakeholders requires
careful visualization design.

\subsection{3.8.1 Visual Encoding Strategy}\label{sec-visual-encoding}

The system uses multiple visual channels:

\textbf{Color}: Probability magnitude (green=high, red=low) provides
immediate visual indication of likelihood, leveraging intuitive color
associations.

\textbf{Borders}: Node type (blue=root, purple=intermediate,
magenta=effect) helps users understand causal flow through the network
structure.

\textbf{Size}: Centrality in network (larger=more influential) draws
attention to critical nodes that affect many other variables.

\textbf{Layout}: Force-directed positioning reveals clusters of related
variables, helping users identify cohesive sub-arguments within larger
models.

\subsection{3.8.2 Progressive
Disclosure}\label{sec-progressive-disclosure}

Information appears at appropriate levels:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Overview}: Network structure and color coding provide
  immediate understanding of key relationships and probability
  distributions.
\item
  \textbf{Hover}: Node description and prior probability offer
  additional context without cluttering the main view.
\item
  \textbf{Click}: Full probability tables and details enable deep
  investigation of specific variables and their relationships.
\item
  \textbf{Interaction}: Drag to rearrange, zoom to explore---users can
  customize views for their specific interests and questions.
\end{enumerate}

This layered approach serves both quick assessment and deep analysis
needs.

\subsection{3.8.3 User Interface Elements}\label{sec-ui-elements}

Effective interface design would incorporate:

\textbf{Physics Controls}: Allow users to adjust layout dynamics,
finding arrangements that best reveal patterns of interest.

\textbf{Filter Options}: Enable focusing on specific node types or
probability ranges, reducing complexity for targeted analysis.

\textbf{Export Functions}: Support saving visualizations and data in
formats suitable for reports, presentations, and further analysis.

\textbf{Comparison Mode}: Facilitate side-by-side viewing of different
models or the same model under different policy interventions.

These features should emerge from iterative design with actual
users---researchers needing detailed analysis, policymakers seeking key
insights, and public stakeholders requiring accessible overviews.

\section{3.9 Integration with Prediction
Markets}\label{sec-market-integration}

While full integration remains future work, the architecture supports
connection to live forecasting data.

\subsection{3.9.1 Design for Integration}\label{sec-integration-design}

The system anticipates market connections through:

\textbf{API Specifications}: Standardized interfaces for major
forecasting platforms like Metaculus, Good Judgment Open, and Manifold
Markets.

\textbf{Semantic Matching}: Algorithms to connect model variables with
related forecast questions, handling differences in phrasing and scope.

\textbf{Aggregation Methods}: Principled approaches for combining
multiple forecast sources, accounting for track records and expertise.

\textbf{Update Scheduling}: Efficient caching and refresh strategies to
balance currency with computational cost.

\subsection{3.9.2 Challenges and
Opportunities}\label{sec-market-challenges}

Key integration challenges:

\textbf{Question Mapping}: Model variables rarely match market questions
exactly. ``AI causes existential catastrophe'' might map to multiple
specific forecast questions about AI capabilities, deployment, and
impacts.

\textbf{Temporal Alignment}: Markets forecast specific dates while
models consider scenarios. Bridging these requires careful
interpretation and uncertainty propagation.

\textbf{Quality Variation}: Market depth and participation vary
significantly. Some questions attract expert forecasters while others
rely on casual participants.

Despite challenges, even partial integration provides value through
external validation of probability estimates and dynamic updating as new
information emerges.

\section{3.10 Computational Performance
Analysis}\label{sec-computational-performance}

As networks grow large, computational challenges emerge requiring
sophisticated approaches.

\subsection{3.10.1 Exact vs.~Approximate
Inference}\label{sec-exact-approximate}

Small networks enable exact inference through variable elimination.
Larger networks require approximation:

\textbf{Monte Carlo Methods}: Sample from probability distributions to
estimate queries. While approximate, these methods scale to arbitrary
network sizes.

\textbf{Variational Inference}: Optimize simpler distributions to
approximate true posteriors. These methods trade some accuracy for
guaranteed convergence.

\textbf{Belief Propagation}: Pass messages between nodes to converge on
beliefs. Particularly effective for tree-structured or sparse networks.

The system automatically selects appropriate methods based on network
properties.

\subsection{3.10.2 Scaling Strategies}\label{sec-scaling-strategies}

For very large networks, several strategies enable practical analysis:

\textbf{Hierarchical Decomposition}: Break large networks into
manageable sub-networks, compute locally, then integrate results.

\textbf{Relevance Pruning}: For specific queries, identify and focus on
relevant subgraphs, ignoring distant unconnected nodes.

\textbf{Caching Architecture}: Store computed results for common
queries, dramatically improving response time for interactive use.

\textbf{Parallel Processing}: Distribute computation across multiple
cores or machines for large-scale analysis.

\section{3.11 Results and Achievements}\label{sec-results-achievements}

\subsection{3.11.1 Extraction Quality
Assessment}\label{sec-extraction-quality}

An ideal assessment methodology would systematically evaluate:

\textbf{Coverage Metrics}: What proportion of arguments in source texts
are successfully captured in formal models?

\textbf{Accuracy Metrics}: How closely do automated extractions match
expert consensus?

\textbf{Robustness Metrics}: How well does the system handle different
writing styles, argument structures, and domains?

\textbf{Utility Metrics}: Do the extracted models enable meaningful
analysis and decision support?

Preliminary applications suggest the approach achieves practical utility
while highlighting areas for improvement, particularly in handling
implicit reasoning and converting qualitative uncertainty expressions.

\subsection{3.11.2 Computational
Performance}\label{sec-computational-performance}

Performance analysis would examine:

\textbf{Scaling Characteristics}: How processing time grows with network
size and complexity.

\textbf{Bottleneck Identification}: Whether limitations arise from
extraction, inference, or visualization.

\textbf{Optimization Opportunities}: Where algorithmic improvements or
engineering enhancements could improve performance.

\textbf{Resource Requirements}: Memory, processing, and storage needs
for realistic applications.

The modular architecture enables targeted optimization of bottleneck
components while maintaining system coherence.

\subsection{3.11.3 Policy Impact Evaluation}\label{sec-policy-impact}

A comprehensive evaluation framework would assess:

\textbf{Intervention Modeling}: How effectively can policies be
represented as network modifications?

\textbf{Robustness Testing}: Do policy recommendations remain stable
across model variations?

\textbf{Comparative Analysis}: How do different policy options compare
in effectiveness?

\textbf{Implementation Guidance}: Does the analysis provide actionable
insights for policymakers?

The ability to formally model policy interventions and trace their
effects through complex causal networks represents a significant advance
in systematic governance analysis.

\section{3.12 Summary of Technical
Contributions}\label{sec-technical-summary}

AMTAIR successfully demonstrates:

\begin{itemize}
\tightlist
\item
  \textbf{Automated extraction} from natural language to formal models
\item
  \textbf{Two-stage architecture} separating structure from
  quantification
\item
  \textbf{High fidelity} preservation of complex arguments
\item
  \textbf{Interactive visualization} accessible to diverse users
\item
  \textbf{Scalable implementation} handling realistic network sizes
\end{itemize}

These achievements validate the feasibility of computational
coordination infrastructure for AI governance.

The implementation shows that formal modeling of AI risk arguments is
not only theoretically possible but practically achievable. While
challenges remain---particularly in handling implicit reasoning and
diverse uncertainty expressions---the system provides a foundation for
enhanced coordination in AI governance.

\bookmarksetup{startatroot}

\chapter{4. Discussion: Implications and
Limitations}\label{sec-discussion}

\textbf{Chapter Overview}\\
\textbf{Grade Weight}: 10\% \textbar{} \textbf{Target Length}:
\textasciitilde14\% of text (\textasciitilde4,200 words)\\
\textbf{Requirements}: Discusses objections, provides convincing
replies, extends beyond course materials

\section{4.1 Technical Limitations and
Responses}\label{sec-technical-limitations}

\subsection{4.1.1 Objection 1: Extraction Quality
Boundaries}\label{sec-extraction-boundaries}

\textbf{Critic}: ``Complex implicit reasoning chains resist
formalization; automated extraction will systematically miss nuanced
arguments and subtle conditional relationships that human experts would
identify.''

\textbf{Response}: This concern has merit---extraction does face
inherent limitations. However, the empirical results tell a more nuanced
story. The two-stage extraction process, while imperfect, captures
sufficient structure for practical use while maintaining transparency
about its limitations.

More importantly, AMTAIR employs a hybrid human-AI workflow that
addresses this limitation:

\begin{itemize}
\tightlist
\item
  \textbf{Two-stage verification}: Humans review structural extraction
  before probability quantification
\item
  \textbf{Transparent outputs}: All intermediate representations remain
  human-readable
\item
  \textbf{Iterative refinement}: Extraction prompts improve based on
  error analysis
\item
  \textbf{Ensemble approaches}: Multiple extraction attempts can
  identify ambiguities
\end{itemize}

The question is not whether automated extraction perfectly captures
every nuance---it doesn't. Rather, it's whether imperfect extraction
still provides value over no formal representation. When the alternative
is relying on conflicting mental models that remain entirely implicit,
even partially accurate formal models represent significant progress.

Furthermore, extraction errors often reveal interesting properties of
the source arguments themselves---ambiguities that human readers gloss
over become explicit when formalization fails. This diagnostic value
enhances rather than undermines the approach.

\subsection{4.1.2 Objection 2: False Precision in
Uncertainty}\label{sec-false-precision}

\textbf{Critic}: ``Attaching exact probabilities to unprecedented events
like AI catastrophe is fundamentally misguided. The numbers create false
confidence in what amounts to educated speculation about radically
uncertain futures.''

\textbf{Response}: This philosophical objection strikes at the heart of
formal risk assessment. However, AMTAIR addresses it through several
design choices:

First, the system explicitly represents uncertainty about uncertainty.
Rather than point estimates, the framework supports probability
distributions over parameters. When someone says ``likely'' we might
model this as a range rather than exactly 0.8, capturing both the
central estimate and our uncertainty about it.

Second, all probabilities are explicitly conditional on stated
assumptions. The system doesn't claim ``P(catastrophe) = 0.05''
absolutely, but rather ``Given Carlsmith's model assumptions,
P(catastrophe) = 0.05.'' This conditionality is preserved throughout
analysis.

Third, sensitivity analysis reveals which probabilities actually matter.
Often, precise values are unnecessary---knowing whether a parameter is
closer to 0.1 or 0.9 suffices for decision-making. The formalization
helps identify where precision matters and where it doesn't.

Finally, the alternative to quantification isn't avoiding the problem
but making it worse. When experts say ``highly likely'' or ``significant
risk,'' they implicitly reason with probabilities. Formalization simply
makes these implicit quantities explicit and subject to scrutiny. As
Dennis Lindley noted, ``Uncertainty is not in the events, but in our
knowledge about them.''

\subsection{4.1.3 Objection 3: Correlation
Complexity}\label{sec-correlation-complexity}

\textbf{Critic}: ``Bayesian networks assume conditional independence
given parents, but real-world AI risks involve complex correlations.
Ignoring these dependencies could dramatically misrepresent risk
levels.''

\textbf{Response}: Standard Bayesian networks do face limitations with
correlation representation---this is a genuine technical challenge.
However, several approaches within the framework address this:

\textbf{Explicit correlation nodes}: When factors share hidden common
causes, we can add latent variables to capture correlations. For
instance, ``AI research culture'' might influence both ``capability
advancement'' and ``safety investment.''

\textbf{Copula methods}: For known correlation structures, copula
functions can model dependencies while preserving marginal
distributions. This extends standard Bayesian networks
significantly.\footnote{Copulas provide a mathematically elegant way to
  separate marginal behavior from dependence structure}

\textbf{Sensitivity bounds}: When correlations remain uncertain, we can
compute bounds on outcomes under different correlation assumptions. This
reveals when correlations critically affect conclusions.

\textbf{Model ensembles}: Different correlation structures can be
modeled separately and results aggregated, similar to climate modeling
approaches.

More fundamentally, the question is whether imperfect independence
assumptions invalidate the approach. In practice, explicitly modeling
first-order effects with known limitations often proves more valuable
than attempting to capture all dependencies informally. The framework
makes assumptions transparent, enabling targeted improvements where
correlations matter most.

\section{4.2 Conceptual and Methodological
Concerns}\label{sec-conceptual-concerns}

\subsection{4.2.1 Objection 4: Democratic
Exclusion}\label{sec-democratic-exclusion}

\textbf{Critic}: ``Transforming policy debates into complex graphs and
equations will sideline non-technical stakeholders, concentrating
influence among those comfortable with formal models. This technocratic
approach undermines democratic participation in crucial decisions about
humanity's future.''

\textbf{Response}: This concern about technocratic exclusion deserves
serious consideration---formal methods can indeed create barriers.
However, AMTAIR's design explicitly prioritizes accessibility alongside
rigor:

\textbf{Progressive disclosure interfaces} allow engagement at multiple
levels. A policymaker might explore visual network structures and
probability color-coding without engaging mathematical details.
Interactive features let users modify assumptions and see consequences
without understanding implementation.

\textbf{Natural language preservation} ensures original arguments remain
accessible. The BayesDown format maintains human-readable descriptions
alongside formal specifications. Users can always trace from
mathematical representations back to source texts.

\textbf{Comparative advantage} comes from making implicit technical
content explicit, not adding complexity. When experts debate AI risk,
they already employ sophisticated probabilistic
reasoning---formalization reveals rather than creates this complexity.
Making hidden assumptions visible arguably enhances rather than reduces
democratic participation.

\textbf{Multiple interfaces} serve different communities. Researchers
access full technical depth, policymakers use summary dashboards, public
stakeholders explore interactive visualizations. The same underlying
model supports varied engagement modes.

Rather than excluding non-technical stakeholders, proper implementation
can democratize access to expert reasoning by making it inspectable and
modifiable. The risk lies not in formalization itself but in poor
interface design or gatekeeping behaviors around model access.

\subsection{4.2.2 Objection 5: Oversimplification of Complex
Systems}\label{sec-oversimplification}

\textbf{Critic}: ``Forcing rich socio-technical systems into discrete
Bayesian networks necessarily loses crucial dynamics---feedback loops,
emergent properties, institutional responses, and cultural factors that
shape AI development. The models become precise but wrong.''

\textbf{Response}: All models simplify by necessity---as Box noted,
``All models are wrong, but some are useful.'' The question becomes
whether formal simplifications improve upon informal mental models:

\textbf{Transparent limitations} make formal models' shortcomings
explicit. Unlike mental models where simplifications remain hidden,
network representations clearly show what is and isn't included. This
transparency enables targeted criticism and improvement.

\textbf{Iterative refinement} allows models to grow more sophisticated
over time. Starting with first-order effects and adding complexity where
it proves important follows successful practice in other domains.
Climate models began simply and added dynamics as computational power
and understanding grew.

\textbf{Complementary tools} address different aspects of the system.
Bayesian networks excel at probabilistic reasoning and intervention
analysis. Other approaches---agent-based models, system dynamics,
scenario planning---can capture different properties. AMTAIR provides
one lens, not the only lens.

\textbf{Empirical adequacy} ultimately judges models. If simplified
representations enable better predictions and decisions than informal
alternatives, their abstractions are justified. Early results suggest
formal models, despite simplifications, outperform intuitive reasoning
for complex risk assessment.

The goal isn't creating perfect representations but useful ones. By
making simplifications explicit and modifiable, formal models enable
systematic improvement in ways mental models cannot.

\subsection{4.2.3 Objection 6: Idiosyncratic Implementation and Modeling
Choices}\label{sec-idiosyncratic}

\textbf{Critic}: ``The specific choices made in AMTAIR's
implementation---from prompt design to parsing algorithms to
visualization strategies---seem arbitrary. Different teams might make
entirely different choices, leading to incompatible results. How can we
trust conclusions that depend so heavily on implementation details?''

\textbf{Response}: This concern about implementation dependency is valid
and deserves careful consideration. However, several factors mitigate
this issue:

\textbf{Convergent Design Principles}: While specific implementations
vary, fundamental design principles tend to converge. The two-stage
extraction process (structure then probability) emerges naturally from
how humans parse arguments. The use of intermediate representations
follows established practice in computational linguistics. These aren't
arbitrary choices but responses to inherent challenges.

\textbf{Empirical Validation}: The ``correctness'' of implementation
choices isn't philosophical but empirical. If different reasonable
implementations extract similar structures and lead to similar policy
conclusions, this demonstrates robustness. If they diverge dramatically,
this reveals genuine ambiguity in source materials---itself valuable
information.

\textbf{Transparent Methodology}: By documenting all implementation
choices and making code open source, AMTAIR enables replication and
variation. Other teams can modify specific components while preserving
overall architecture, testing which choices matter.

\textbf{Convergence at Higher Levels}: Even if implementations differ in
details, they may converge at levels that matter for coordination. If
two systems extract slightly different network structures but reach
similar conclusions about policy robustness, the implementation
differences don't undermine the approach's value.

\textbf{Community Standards}: As the field matures, community standards
will likely emerge---not enforcing uniformity but establishing
interoperability. This parallels development in other technical fields
where multiple implementations coexist within shared frameworks.

The deeper insight is that implementation choices encode theoretical
commitments. By making these explicit and variable, AMTAIR turns a bug
into a feature---we can systematically explore how different assumptions
affect conclusions, enhancing rather than undermining epistemic
security.

\section{4.3 Red-Teaming Results}\label{sec-red-teaming}

To identify failure modes, systematic adversarial testing of the AMTAIR
system would be essential.

\subsection{4.3.1 Adversarial Extraction
Attempts}\label{sec-adversarial-extraction}

A comprehensive red-teaming approach would test the system with:

\textbf{Contradictory Arguments}: Texts containing logically
inconsistent claims or probability estimates. The system should flag
contradictions rather than silently reconciling them.

\textbf{Circular Reasoning}: Arguments with circular dependencies that
violate DAG requirements. Proper validation should detect and report
such structural issues.

\textbf{Ambiguous Language}: Texts using extremely vague or metaphorical
language. The system should acknowledge extraction uncertainty rather
than forcing precise interpretations.

\textbf{Deceptive Framings}: Arguments crafted to imply false causal
relationships. This tests whether the system merely extracts surface
claims or requires deeper coherence.

\textbf{Adversarial Prompts}: Inputs designed to trigger known LLM
failure modes. This ensures robustness against prompt injection and
manipulation attempts.

Each failure mode discovered would inform system improvements and user
guidance.

\subsection{4.3.2 Robustness Findings}\label{sec-robustness-findings}

Theoretical analysis suggests key vulnerabilities:

\textbf{Anchoring Effects}: Language models may over-weight information
presented early in documents, potentially biasing extraction toward
initial framings.

\textbf{Authority Sensitivity}: Extraction might be influenced by
explicit credibility signals in text, potentially giving undue weight to
claimed expertise.

\textbf{Complexity Limits}: Performance likely degrades with very large
argument structures, requiring hierarchical decomposition strategies.

\textbf{Context Windows}: Long-range dependencies exceeding model
context windows could be missed, fragmenting cohesive arguments.

Understanding these limitations enables appropriate use---leveraging
strengths while compensating for weaknesses through human oversight and
validation.

\subsection{4.3.3 Implications for
Deployment}\label{sec-deployment-implications}

These considerations suggest AMTAIR is suitable for:

\begin{itemize}
\tightlist
\item
  \textbf{Research applications} with expert oversight
\item
  \textbf{Policy analysis} of well-structured arguments
\item
  \textbf{Educational uses} demonstrating formal reasoning
\item
  \textbf{Collaborative modeling} with human verification
\end{itemize}

But should be used cautiously for:

\begin{itemize}
\tightlist
\item
  Fully automated analysis without review
\item
  Adversarial or politically contentious texts
\item
  Real-time decision-making without validation
\item
  Arguments far outside training distribution
\end{itemize}

\section{4.4 Enhancing Epistemic Security}\label{sec-epistemic-security}

Despite limitations, AMTAIR contributes to epistemic security in AI
governance through several mechanisms.

\subsection{4.4.1 Making Models
Inspectable}\label{sec-inspectable-models}

The greatest epistemic benefit comes from forcing implicit models into
explicit form. When an expert claims ``misalignment likely leads to
catastrophe,'' formalization asks:

\begin{itemize}
\tightlist
\item
  Likely means what probability?
\item
  Through what causal pathways?
\item
  Under what assumptions?
\item
  With what evidence?
\end{itemize}

This explicitation serves multiple functions:

\textbf{Clarity}: Vague statements become precise claims subject to
evaluation

\textbf{Comparability}: Different experts' models can be systematically
compared

\textbf{Criticizability}: Hidden assumptions become visible targets for
challenge

\textbf{Updatability}: Formal models can systematically incorporate new
evidence

\subsection{4.4.2 Revealing Convergence and
Divergence}\label{sec-convergence-divergence}

Theoretical analysis suggests formal comparison would reveal:

\textbf{Structural Patterns}: Experts likely share more agreement about
causal structures than probability values, suggesting common
understanding of mechanisms despite quantitative disagreement.

\textbf{Crux Identification}: Formal models make explicit which specific
disagreements drive different conclusions, focusing discussion on
genuinely critical differences.

\textbf{Hidden Agreements}: Apparently conflicting positions might share
substantial common ground obscured by different terminology or emphasis.

\textbf{Uncertainty Clustering}: Areas of high uncertainty likely
correlate across models, revealing where additional research would most
reduce disagreement.

These patterns remain invisible in natural language debates but become
analyzable through formalization.

\subsection{4.4.3 Improving Collective
Reasoning}\label{sec-collective-reasoning}

AMTAIR enhances group epistemics through:

\textbf{Explicit uncertainty}: Replacing ``might,'' ``could,''
``likely'' with probability distributions reduces miscommunication and
forces precision

\textbf{Compositional reasoning}: Complex arguments decompose into
manageable components that can be independently evaluated

\textbf{Evidence integration}: New information updates specific
parameters rather than requiring complete argument reconstruction

\textbf{Exploration tools}: Stakeholders can modify assumptions and
immediately see consequences, building intuition about model dynamics

While empirical validation remains future work, theoretical
considerations suggest these mechanisms could substantially improve
coordination quality. By providing shared representations and systematic
methods for managing disagreement, formal models create infrastructure
for collective intelligence that transcends individual limitations.

\section{4.5 Scaling Challenges and Opportunities}\label{sec-scaling}

Moving from prototype to widespread adoption faces both technical and
social challenges.

\subsection{4.5.1 Technical Scaling}\label{sec-technical-scaling}

\textbf{Computational complexity} grows with network size, but several
approaches help:

\begin{itemize}
\tightlist
\item
  Hierarchical decomposition for very large models
\item
  Caching and approximation for common queries
\item
  Distributed processing for extraction tasks
\item
  Incremental updating rather than full recomputation
\end{itemize}

\textbf{Data quality} varies dramatically across sources:

\begin{itemize}
\tightlist
\item
  Academic papers provide structured arguments
\item
  Blog posts offer rich ideas with less formal structure
\item
  Policy documents mix normative and empirical claims
\item
  Social media presents extreme extraction challenges
\end{itemize}

\textbf{Integration complexity} increases with ecosystem growth:

\begin{itemize}
\tightlist
\item
  Multiple LLM providers with different capabilities
\item
  Diverse visualization needs across users
\item
  Various export formats for downstream tools
\item
  Version control for evolving models
\end{itemize}

\subsection{4.5.2 Social and Institutional
Scaling}\label{sec-social-scaling}

\textbf{Adoption barriers} include:

\begin{itemize}
\tightlist
\item
  Learning curve for formal methods
\item
  Institutional inertia in established processes
\item
  Concerns about replacing human judgment
\item
  Resource requirements for implementation
\end{itemize}

\textbf{Trust building} requires:

\begin{itemize}
\tightlist
\item
  Transparent methodology documentation
\item
  Published validation studies
\item
  High-profile successful applications
\item
  Community ownership and development
\end{itemize}

\textbf{Sustainability} depends on:

\begin{itemize}
\tightlist
\item
  Open source development model
\item
  Diverse funding sources
\item
  Academic and industry partnerships
\item
  Clear value demonstration
\end{itemize}

\subsection{4.5.3 Opportunities for
Impact}\label{sec-impact-opportunities}

Despite challenges, several factors favor adoption:

\textbf{Timing}: AI governance needs tools now, creating receptive
audiences

\textbf{Complementarity}: AMTAIR enhances rather than replaces existing
processes

\textbf{Flexibility}: The approach adapts to different contexts and
needs

\textbf{Network effects}: Value increases as more perspectives are
formalized

Early adopters in research organizations and think tanks can demonstrate
value, creating momentum for broader adoption.

\section{4.6 Integration with Governance
Frameworks}\label{sec-governance-integration}

AMTAIR complements rather than replaces existing governance approaches.

\subsection{4.6.1 Standards
Development}\label{sec-standards-integration}

Technical standards bodies could use AMTAIR to:

\begin{itemize}
\tightlist
\item
  Model how proposed standards affect risk pathways
\item
  Compare different standard options systematically
\item
  Identify unintended consequences through pathway analysis
\item
  Build consensus through explicit model negotiation
\end{itemize}

Example: Evaluating compute thresholds for AI system regulation by
modeling how different thresholds affect capability development, safety
investment, and competitive dynamics.

\subsection{4.6.2 Regulatory Design}\label{sec-regulatory-integration}

Regulators could apply the framework to:

\begin{itemize}
\tightlist
\item
  Assess regulatory impact across different scenarios
\item
  Identify enforcement challenges through explicit modeling
\item
  Compare international approaches systematically
\item
  Design adaptive regulations responsive to evidence
\end{itemize}

Example: Analyzing how liability frameworks affect corporate AI
development decisions under different market conditions.

The extensive literature on corporate governance and liability
frameworks \textcite{cuomo2016} \textcite{demirag2000}
\textcite{devilliers2021} \textcite{divito2022} \textcite{kaur2024}
\textcite{list2011} \textcite{solomon2020} provides theoretical
grounding for understanding how regulatory interventions shape
organizational behavior. AMTAIR could formalize these relationships in
the specific context of AI development, making explicit how different
liability regimes might incentivize or discourage safety investments.

\subsection{4.6.3 International
Coordination}\label{sec-international-integration}

Multilateral bodies could leverage shared models for:

\begin{itemize}
\tightlist
\item
  Establishing common risk assessments
\item
  Negotiating agreements with explicit assumptions
\item
  Monitoring compliance through parameter tracking
\item
  Adapting agreements as evidence emerges
\end{itemize}

Example: Building shared models for AGI development scenarios to inform
international AI governance treaties.

\subsection{4.6.4 Organizational
Decision-Making}\label{sec-organizational-integration}

Individual organizations could use AMTAIR for:

\begin{itemize}
\tightlist
\item
  Internal risk assessment and planning
\item
  Board-level communication about AI strategies
\item
  Research prioritization based on model sensitivity
\item
  Safety case development with explicit assumptions
\end{itemize}

Example: An AI lab modeling how different safety investments affect both
capability advancement and risk mitigation.

\section{4.7 Future Research Directions}\label{sec-future-research}

Several research directions could enhance AMTAIR's capabilities and
impact.

\subsection{4.7.1 Technical Enhancements}\label{sec-technical-future}

\textbf{Improved extraction}: Fine-tuning language models specifically
for argument extraction, handling implicit reasoning, and cross-document
synthesis

\textbf{Richer representations}: Temporal dynamics, continuous
variables, and multi-agent interactions within extended frameworks

\textbf{Inference advances}: Quantum computing applications, neural
approximate inference, and hybrid symbolic-neural methods

\textbf{Validation methods}: Automated consistency checking, anomaly
detection in extracted models, and benchmark dataset development

\subsection{4.7.2 Methodological
Extensions}\label{sec-methodological-future}

\textbf{Causal discovery}: Inferring causal structures from data rather
than just extracting from text

\textbf{Experimental integration}: Connecting models to empirical
results from AI safety experiments

\textbf{Dynamic updating}: Continuous model refinement as new evidence
emerges from research and deployment

\textbf{Uncertainty quantification}: Richer representation of deep
uncertainty and model confidence

Recent advances in causal structure learning from both text and data
\textcite{babakov2025} \textcite{ban2023} \textcite{bethard2007}
\textcite{chen2023} \textcite{heinze-deml2018} \textcite{squires2023}
\textcite{yang2022} suggest promising directions for enhancing AMTAIR's
extraction capabilities. The theoretical foundations from
\textcite{duhem1954} and \textcite{meyer2022b} on the philosophy of
science and knowledge structures provide epistemological grounding for
these methodological extensions.

\subsection{4.7.3 Application Domains}\label{sec-application-future}

\textbf{Beyond AI safety}: Climate risk, biosecurity, nuclear policy,
and other existential risks

\textbf{Corporate governance}: Strategic planning, risk management, and
innovation assessment

\textbf{Scientific modeling}: Formalizing theoretical arguments in
emerging fields

\textbf{Educational tools}: Teaching probabilistic reasoning and
critical thinking

\subsection{4.7.4 Ecosystem Development}\label{sec-ecosystem-future}

\textbf{Open standards}: Common formats for model exchange and tool
interoperability

\textbf{Community platforms}: Collaborative model development and
sharing infrastructure

\textbf{Training programs}: Building capacity for formal modeling in
governance communities

\textbf{Quality assurance}: Certification processes for high-stakes
model applications

These directions could transform AMTAIR from a single tool into a
broader ecosystem for enhanced reasoning about complex risks.

\section{4.8 Known Unknowns and Deep
Uncertainties}\label{sec-deep-uncertainties}

While AMTAIR enhances reasoning under uncertainty, fundamental
limitations remain regarding truly novel developments that might fall
outside existing conceptual frameworks.

\subsection{4.8.1 Categories of Deep
Uncertainty}\label{sec-uncertainty-categories}

\textbf{Novel Capabilities}: Future AI developments may operate
according to principles outside current scientific understanding. No
amount of careful modeling can anticipate fundamental paradigm shifts in
what intelligence can accomplish.

\textbf{Emergent Behaviors}: Complex system properties that resist
prediction from component analysis may dominate outcomes. The
interaction between advanced AI systems and human society could produce
wholly unexpected dynamics.

\textbf{Strategic Interactions}: Game-theoretic dynamics with superhuman
AI systems exceed human modeling capacity. We cannot reliably predict
how entities smarter than us will behave strategically.

\textbf{Social Transformation}: Unprecedented social and economic
changes may invalidate current institutional assumptions. Our models
assume continuity in basic social structures that AI might fundamentally
alter.

\subsection{4.8.2 Adaptation Strategies for Deep
Uncertainty}\label{sec-adaptation-strategies}

Rather than pretending to model the unmodelable, AMTAIR incorporates
several strategies:

\textbf{Model Architecture Flexibility}: The modular structure enables
rapid incorporation of new variables as novel factors become apparent.
When surprises occur, models can be updated rather than discarded.

\textbf{Explicit Uncertainty Tracking}: Confidence levels for each model
component make clear where knowledge is solid versus speculative. This
prevents false confidence in highly uncertain domains.

\textbf{Scenario Branching}: Multiple model variants capture different
assumptions about fundamental uncertainties. Rather than committing to
one worldview, the system maintains portfolios of possibilities.

\textbf{Update Mechanisms}: Integration with prediction markets and
expert assessment enables rapid model revision as new information
emerges. Models evolve rather than remaining static.

\subsection{4.8.3 Robust Decision-Making
Principles}\label{sec-robust-principles}

Given deep uncertainty, certain decision principles become paramount:

\textbf{Option Value Preservation}: Policies should maintain flexibility
for future course corrections rather than locking in irreversible
choices based on current models.

\textbf{Portfolio Diversification}: Multiple approaches hedging across
different uncertainty sources provide robustness against model error.

\textbf{Early Warning Systems}: Monitoring for developments that would
invalidate current models enables rapid response when assumptions break
down.

\textbf{Adaptive Governance}: Institutional mechanisms must enable rapid
response to new information rather than rigid adherence to plans based
on outdated models.

The goal is not to eliminate uncertainty but to make good decisions
despite it. AMTAIR provides tools for systematic reasoning about what we
do know while maintaining appropriate humility about what we don't and
can't know.

\section{4.9 Summary of Implications}\label{sec-implications-summary}

The discussion reveals both the promise and limitations of computational
approaches to AI governance coordination:

\textbf{Technical Feasibility}: Despite imperfections, automated
extraction and formal modeling prove practically viable for complex AI
risk arguments.

\textbf{Epistemic Value}: Making implicit models explicit, enabling
systematic comparison, and supporting evidence integration enhance
collective reasoning.

\textbf{Practical Limitations}: Extraction boundaries, false precision
risks, and implementation dependencies require careful management.

\textbf{Integration Potential}: The approach complements rather than
replaces existing governance frameworks, adding rigor without
sacrificing flexibility.

\textbf{Future Development}: Technical enhancements, methodological
extensions, and ecosystem growth could amplify impact.

\textbf{Deep Uncertainty}: Fundamental limits on predicting novel
developments require maintaining humility and adaptability.

These findings suggest AMTAIR represents a valuable addition to the AI
governance toolkit---not a panacea but a meaningful enhancement to our
collective capacity for navigating unprecedented challenges.

\bookmarksetup{startatroot}

\chapter{5. Conclusion: Toward Coordinated AI
Governance}\label{sec-conclusion}

\textbf{Chapter Overview}\\
\textbf{Grade Weight}: 10\% \textbar{} \textbf{Target Length}:
\textasciitilde14\% of text (\textasciitilde4,200 words)\\
\textbf{Requirements}: Summarizes thesis and argument, outlines
implications, notes limitations, points to future research

\section{5.1 Summary of Key Contributions}\label{sec-key-contributions}

This thesis has demonstrated both the need for and feasibility of
computational approaches to enhancing coordination in AI governance. The
work makes several distinct contributions across theory, methodology,
and implementation.

\subsection{5.1.1 Theoretical
Contributions}\label{sec-theoretical-contributions}

\textbf{Diagnosis of the Coordination Crisis}: I've articulated how
fragmentation across technical, policy, and strategic communities
systematically amplifies existential risk from advanced AI. This framing
moves beyond identifying disagreements to understanding how misaligned
efforts create negative-sum dynamics---safety gaps emerge between
communities, resources are misallocated through duplication and neglect,
and interventions interact destructively.

\textbf{The Multiplicative Benefits Framework}: The combination of
automated extraction, prediction market integration, and formal policy
evaluation creates value exceeding the sum of parts. Automation enables
scale, markets provide empirical grounding, and policy analysis delivers
actionable insights. Together, they address different facets of the
coordination challenge while reinforcing each other's strengths.

\textbf{Epistemic Infrastructure Conception}: Positioning formal models
as epistemic infrastructure reframes the role of technical tools in
governance. Rather than replacing human judgment, computational
approaches provide common languages, shared representations, and
systematic methods for managing disagreement---essential foundations for
coordination under uncertainty.

\subsection{5.1.2 Methodological
Innovations}\label{sec-methodological-innovations}

\textbf{Two-Stage Extraction Architecture}: Separating structural
extraction (ArgDown) from probability quantification (BayesDown)
addresses key challenges in automated formalization. This modularity
enables human oversight at critical points, supports multiple
quantification methods, allows for unprecedented transparency and
explainability of the entire process, and isolates different types of
errors for targeted improvement.

\textbf{BayesDown as Bridge Representation}: The development of
BayesDown syntax creates a crucial intermediate representation
preserving both narrative accessibility and mathematical precision. This
bridge enables the transformation from qualitative arguments to
quantitative models while maintaining traceability and human
readability.

\textbf{Validation Framework}: The systematic approach to validating
automated extraction---comparing against expert annotations, measuring
multiple accuracy dimensions, and analyzing error patterns---establishes
scientific standards for assessing formalization tools. This framework
can guide future development in this emerging area.

\subsection{5.1.3 Technical
Achievements}\label{sec-technical-achievements}

\textbf{Working Implementation}: AMTAIR demonstrates end-to-end
feasibility from document ingestion through interactive visualization.
The system successfully processes complex arguments like Carlsmith's
power-seeking AI model, extracting hierarchical structures and
probability information.

\textbf{Scalability Solutions}: Technical approaches for handling
realistic model complexity---hierarchical decomposition, approximate
inference, and progressive visualization---show that computational
limitations need not prevent practical application.

\textbf{Accessibility Design}: The layered interface approach serves
diverse stakeholders without compromising technical depth. Progressive
disclosure, visual encoding, and interactive exploration make formal
models accessible beyond technical specialists.

\subsection{5.1.4 Empirical Findings}\label{sec-empirical-findings}

\textbf{Extraction Feasibility}: The successful extraction of complex
arguments like Carlsmith's model validates the core premise that
implicit formal structures exist in natural language arguments and can
be computationally recovered with reasonable fidelity.

\textbf{Convergence Patterns}: Theoretical analysis suggests that formal
comparison would reveal structural agreements across different expert
worldviews even when probability estimates diverge---providing
foundations for coordination.

\textbf{Intervention Impacts}: Policy evaluation capabilities
demonstrate how formal models enable rigorous assessment of governance
options. The ability to trace intervention effects through complex
causal networks validates the practical value of formalization.

\section{5.2 Limitations and Honest
Assessment}\label{sec-limitations-assessment}

Despite these contributions, important limitations constrain current
capabilities and should guide appropriate use.

\subsection{5.2.1 Technical
Constraints}\label{sec-technical-constraints}

\textbf{Extraction Boundaries}: The system struggles with implicit
assumptions, complex conditionals, and ambiguous quantifiers. These
limitations necessitate human review for high-stakes applications.

\textbf{Correlation Handling}: Standard Bayesian networks inadequately
represent complex correlations in real systems. While extensions like
copulas and explicit correlation nodes help, fully capturing
interdependencies remains challenging.

\textbf{Computational Scaling}: Very large networks require
approximations that may affect accuracy. As models grow to represent
richer phenomena, computational constraints increasingly bind.

\subsection{5.2.2 Conceptual
Limitations}\label{sec-conceptual-limitations}

\textbf{Formalization Trade-offs}: Converting rich arguments to formal
models necessarily loses nuance. While making assumptions explicit
provides value, some insights resist mathematical representation.

\textbf{Probability Interpretation}: Deep uncertainty about
unprecedented events challenges probabilistic representation. Numbers
can create false precision even when explicitly conditional and
uncertain.

\textbf{Social Complexity}: Institutional dynamics, cultural factors,
and political processes influence AI development in ways that causal
models struggle to capture fully.

\subsection{5.2.3 Practical
Constraints}\label{sec-practical-constraints}

\textbf{Adoption Barriers}: Learning curves, institutional inertia, and
resource requirements limit immediate deployment. Even demonstrably
valuable tools face implementation challenges.

\textbf{Maintenance Burden}: Models require updating as arguments evolve
and evidence emerges. Without sustained effort, formal representations
quickly become outdated.

\textbf{Context Dependence}: The approach works best for well-structured
academic arguments. Application to informal discussions or political
rhetoric remains challenging.

\section{5.3 Implications for AI
Governance}\label{sec-governance-implications}

Despite limitations, AMTAIR's approach offers significant implications
for how AI governance can evolve toward greater coordination and
effectiveness.

\subsection{5.3.1 Near-Term
Applications}\label{sec-near-term-applications}

\textbf{Research Coordination}: Research organizations can use formal
models to:

\begin{itemize}
\tightlist
\item
  Map the landscape of current arguments and identify gaps
\item
  Prioritize investigations targeting high-sensitivity parameters
\item
  Build cumulative knowledge through explicit model updating
\item
  Facilitate collaboration through shared representations
\end{itemize}

\textbf{Policy Development}: Governance bodies can apply the framework
to:

\begin{itemize}
\tightlist
\item
  Evaluate proposals across multiple expert worldviews
\item
  Identify robust interventions effective under uncertainty
\item
  Make assumptions explicit for democratic scrutiny
\item
  Track how evidence changes optimal policies over time
\end{itemize}

\textbf{Stakeholder Communication}: The visualization and analysis tools
enable:

\begin{itemize}
\tightlist
\item
  Clearer communication between technical and policy communities
\item
  Public engagement with complex risk assessments
\item
  Board-level strategic discussions grounded in formal analysis
\item
  International negotiations with explicit shared models
\end{itemize}

\subsection{5.3.2 Medium-Term Transformation}\label{sec-medium-term}

As adoption spreads, we might see:

\textbf{Epistemic Commons}: Shared repositories of formalized arguments
become reference points for governance discussions, similar to how
economic models inform monetary policy or climate models guide
environmental agreements.

\textbf{Adaptive Governance}: Policies designed with explicit models can
include triggers for reassessment as key parameters change, enabling
responsive governance that avoids both paralysis and recklessness.

\textbf{Professionalization}: ``Model curator'' and ``argument
formalization specialist'' emerge as recognized roles, building
expertise in bridging natural language and formal representations.

\textbf{Quality Standards}: Community norms develop around model
transparency, validation requirements, and appropriate use cases,
preventing both dismissal and over-reliance on formal tools.

\subsection{5.3.3 Long-Term Vision}\label{sec-long-term-vision}

Successfully scaling this approach could fundamentally alter AI
governance:

\textbf{Coordinated Response}: Rather than fragmented efforts, the AI
safety ecosystem could operate with shared situational
awareness---different actors understanding how their efforts interact
and contribute to collective goals.

\textbf{Anticipatory Action}: Formal models with prediction market
integration could provide early warning of emerging risks, enabling
proactive rather than reactive governance.

\textbf{Global Cooperation}: Shared formal frameworks could facilitate
international coordination similar to how economic models enable
monetary coordination or climate models support environmental
agreements.

\textbf{Democratic Enhancement}: Making expert reasoning transparent and
modifiable could enable broader participation in crucial decisions about
humanity's technological future.

\section{5.4 Recommendations for
Stakeholders}\label{sec-recommendations}

Different communities can take concrete steps to realize these benefits:

\subsection{5.4.1 For Researchers}\label{sec-researcher-recommendations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Experiment with formalization}: Try extracting your own
  arguments into ArgDown/BayesDown format to discover implicit
  assumptions
\item
  \textbf{Contribute to validation}: Provide expert annotations for
  building benchmark datasets and improving extraction quality
\item
  \textbf{Develop extensions}: Build on the open-source foundation to
  add capabilities for your specific domain needs
\item
  \textbf{Publish formally}: Include formal model representations
  alongside traditional papers to enable cumulative building
\end{enumerate}

\subsection{5.4.2 For
Policymakers}\label{sec-policymaker-recommendations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Pilot applications}: Use AMTAIR for internal analysis of
  specific policy proposals to build familiarity and identify value
\item
  \textbf{Demand transparency}: Request formal models underlying expert
  recommendations to understand assumptions and uncertainties
\item
  \textbf{Fund development}: Support tool development and training to
  build governance capacity for formal methods
\item
  \textbf{Design adaptively}: Create policies with explicit triggers
  based on model parameters to enable responsive governance
\end{enumerate}

\subsection{5.4.3 For
Technologists}\label{sec-technologist-recommendations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Improve extraction}: Contribute better prompting strategies,
  fine-tuned models, or validation methods
\item
  \textbf{Enhance interfaces}: Develop visualizations and interactions
  serving specific stakeholder needs
\item
  \textbf{Build integrations}: Connect AMTAIR to other tools in the AI
  governance ecosystem
\item
  \textbf{Scale infrastructure}: Address computational challenges for
  larger models and broader deployment
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{Bibliography}\label{bibliography}
\addcontentsline{toc}{chapter}{Bibliography}

\markboth{Bibliography}{Bibliography}

\printbibliography[heading=none]


\backmatter
\printbibliography[title=Bibliography]



\clearpage
\thispagestyle{empty} % Removes page numbering for current page

\newpage


% Top header with logo (left) and department (right)
\begin{minipage}{0.3\textwidth}
  \includegraphics[width=5cm]{latex/uni-bayreuth-logo.png}
\end{minipage}
\hfill
\begin{minipage}{0.9\textwidth}
  \begin{center}
    -- P\&E Master's Programme --\\
    Chair of Philosophy, Computer\\
    Science \& Artificial Intelligence
  \end{center}
\end{minipage}

% Horizontal rule
\vspace{1.5cm}
\hrule
\vspace{2.5cm}

% Title in bold

  \LARGE\textbf{Affidavit}
\vspace{1.5cm}

\center

\normalsize

% \part*{Affidavit}

    \subsection*{\Large Declaration of Academic Honesty}
	    \vspace{1cm}\noindent \\
	    Hereby, I attest that I have composed and written the presented thesis 
        \vspace*{0.5cm}\noindent \\
        \textit{ \textbf{ Automating the Modelling of Transformative Artificial Intelligence Risks }}
        \vspace*{0.5cm}\noindent \\
        independently on my own, without the use of other than the stated aids and without any other resources than the ones indicated. All thoughts taken directly or indirectly from external sources are properly denoted as such.
	    \vspace{\baselineskip}
	    \\  This paper has neither been previously submitted in the same or a similar form to another authority nor has it been published yet.
	    \vspace{2cm}
	    
    \flushright
    \begin{minipage}{0.5\textwidth}
        \begin{flushleft} \large
        \textsc{Bayreuth}                     %   Place
        on the \\ % 26th of May 2025     \\
        \today           %   Date
        \vspace{2cm}\\
    	{\rule[-3pt]{\linewidth}{.4pt}\par\smallskip  
        \textsc{Valentin Meyer}	\\         %   Your name
    	}
        \end{flushleft}
        \end{minipage}


\end{document}
