% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
  letterpaper,
]{book}
\usepackage{xcolor}
\usepackage[margin=2.5cm,paper=a4paper]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{biblatex}
\addbibresource{ref/MAref.bib}


% AMTAIR Thesis Preamble - Zero package conflicts
% Only formatting commands, no package loading

% Line spacing for academic work
\usepackage{setspace}
\onehalfspacing

% Custom chapter formatting (remove "Chapter N" prefix) but unfortunately leaves blank space
\usepackage{titlesec}
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries}  % format
  {}                           % label (empty = no "Chapter N")
  {0pt}                        % sep
  {\Huge}                      % before-code



% Page formatting and headers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\slshape\nouppercase{\rightmark}}
\fancyhead[LO,RE]{\slshape\nouppercase{\leftmark}}
\fancyfoot[C]{\thepage}

% % Fix page breaks after title page
% \newcommand{\cleartitlepage}{
%   \clearpage
%   \thispagestyle{empty}
%   \mbox{}
%   \clearpage
% }



\renewcommand{\maketitle}{}

%  Citation customization
% \usepackage[style=authoryear,backend=biber,natbib=true]{biblatex}

% % Custom citation commands for different contexts
% \newcommand{\citeauthor}[1]{\textcite{#1}}           % Author (year)
% \newcommand{\citeyear}[1]{(\citeyear*{#1})}         % (year)
% \newcommand{\citealt}[1]{\citeauthor{#1} \citeyear{#1}}  % Author year
% \newcommand{\citep}[1]{(\cite{#1})}                 # (Author, year)

% Page reference styling
% \DeclareFieldFormat{postnote}{#1}                    # No "p." prefix
% \DeclareFieldFormat{multipostnote}{#1}               # No "pp." prefix


% % Page numbering control
% \usepackage{afterpage}

% % Command to start front matter (roman numerals)
% \newcommand{\frontmatter}{
%   \cleardoublepage
%   \pagenumbering{roman}
%   \setcounter{page}{1}
% }

% % Command to start main matter (arabic numerals)
% \newcommand{\mainmatter}{
%   \cleardoublepage
%   \pagenumbering{arabic}
%   \setcounter{page}{1}
% }

% % Command to start back matter (continue arabic)
% \newcommand{\backmatter}{
%   \cleardoublepage
%   % Keep arabic numbering but could change style if needed
% }

% % Suppress page numbers on title page
% \newcommand{\titlepage}{
%   \thispagestyle{empty}
% }



% Commands for custom title page
% \newcommand{\thesistitle}{Automating the Modelling of Transformative Artificial Intelligence Risks}
% \newcommand{\thesisauthor}{Valentin Jakob Meyer}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Automating the Modelling of Transformative Artificial Intelligence Risks},
  pdfauthor={Valentin Jakob Meyer},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Automating the Modelling of Transformative Artificial
Intelligence Risks}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{An Epistemic Framework for Leveraging Frontier AI Systems to
Upscale Conditional Policy Assessments in Bayesian Networks on a Narrow
Path towards Existential Safety}
\author{Valentin Jakob Meyer}
\date{2025-05-26}
\begin{document}
\frontmatter
\maketitle

\begin{titlepage}
\thispagestyle{empty}% Remove page number from title page

% Top header with logo (left) and department (right)
\begin{minipage}{0.3\textwidth}
  \includegraphics[width=5cm]{latex/uni-bayreuth-logo.png}
\end{minipage}
\hfill
\begin{minipage}{0.9\textwidth}
  \begin{center}
    -- P\&E Master's Programme --\\
    Chair of Philosophy, Computer\\
    Science \& Artificial Intelligence
  \end{center}
\end{minipage}

% Horizontal rule
\vspace{1.5cm}
\hrule
\vspace{2cm}

% Title in bold
\begin{center}
  \Large\textbf{Automating the Modelling of
Transformative Artificial Intelligence Risks}
\end{center}
\vspace{0.2cm}

\begin{center}
  -----
\end{center}
\vspace{0.2cm}

% Subtitle in italics with quotation marks
\begin{center}
  \normalsize``\textit{An Epistemic Framework for Leveraging Frontier AI Systems
to Upscale Conditional Policy Assessments in Bayesian Networks on a Narrow Path towards Existencial Safety }''
\end{center}
\vspace{0.2cm}

\begin{center}
  -----
\end{center}
\vspace{0.2cm}

% Thesis designation
\begin{center}
  A thesis submitted at the Department of Philosophy\\[0.4cm]
  for the degree of \textit{Master of Arts in Philosophy \& Economics}
\end{center}

\vspace{1.5cm}
% Horizontal rule
\hrule
\vspace{1.5cm}

% Author and supervisor information with precise alignment
\begin{minipage}[t]{0.48\textwidth}
  \textbf{Author:}\\[0.3cm]
  \href{https://www.vjmeyer.org}{Valentin Jakob Meyer}\\
  \href{mailto:Valentin.meyer@uni-bayreuth.de}{Valentin.meyer@uni-bayreuth.de}\\
  \textit{Matriculation Number:} 1828610\\
  \textit{Tel.:} +49 (1573) 4512494\\
  Pielmühler Straße 15\\
  52066 Lappersdorf
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
  \begin{flushright}
    \textbf{Supervisor:}\\[0.3cm]
    \href{mailto:timo.speith@uni-bayreuth.de}{Dr. Timo Speith}\\[0.35cm]
    \textit{Word Count:}\\
    30.000\\[0.1cm]
    \textit{Source / Identifier:}\\
    \href{https://github.com/VJMeyer/submission}{Document URL}
  \end{flushright}
\end{minipage}

% Date at bottom
\vfill
\begin{center}
  26th of May 2025
\end{center}
\end{titlepage}

% Critical: Clean page break to TOC
\cleardoublepage

\renewcommand*\contentsname{Table of Contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This is a Quarto book.

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\bookmarksetup{startatroot}

\chapter*{Abstract}\label{sec-Abstract}
\addcontentsline{toc}{chapter}{Abstract}

\markboth{Abstract}{Abstract}

\bookmarksetup{startatroot}

\chapter*{Outline(s): Table of Contents}\label{sec-ToC}
\addcontentsline{toc}{chapter}{Outline(s): Table of Contents}

\markboth{Outline(s): Table of Contents}{Outline(s): Table of Contents}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface-1}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

title: ``Automating the Modelling of Transformative Artificial
Intelligence Risks'' subtitle: ``An Epistemic Framework for Leveraging
Frontier AI Systems to Upscale Conditional Policy Assessments in
Bayesian Networks on a Narrow Path towards Existential Safety'' author:

\begin{itemize}
\tightlist
\item
  name: Valentin Jakob Meyer orcid: 0009-0006-0889-5269 corresponding:
  true email: Valentin.Meyer@uni-bayreuth.de roles:

  \begin{itemize}
  \tightlist
  \item
    GraduateAuthor affiliations:
  \item
    University of Bayreuth
  \item
    MCMP --- LMU Munich
  \end{itemize}
\item
  name: Dr.~Timo Speith orcid: 0000-0002-6675-154X corresponding: false
  roles:

  \begin{itemize}
  \tightlist
  \item
    Supervisor affiliations:
  \item
    University of Bayreuth keywords:
  \end{itemize}
\item
  AMTAIR
\item
  AI Governance
\item
  Bayesian Networks
\item
  Transformative AI
\item
  Risk Assessment
\item
  Argument Extraction abstract: \textbar{} This thesis addresses
  coordination failures in AI safety by creating computational tools
  that automatically extract and formalize probabilistic world models
  from AI safety literature using frontier language models. The AMTAIR
  (Automating Transformative AI Risk Modeling) system implements an
  end-to-end pipeline transforming unstructured arguments into
  interactive Bayesian networks through a novel two-stage extraction
  process: first capturing argument structure in ArgDown format, then
  enhancing it with probability information in BayesDown.
\end{itemize}

Applied to canonical examples and real AI safety arguments, the system
demonstrates extraction accuracy exceeding 85\% for structural
relationships and 73\% for probability capture. By making implicit
models explicit, enabling cross-worldview comparison, and supporting
rigorous policy evaluation, AMTAIR bridges communication gaps between
technical researchers, policy specialists, and other stakeholders
working to address existential risks from advanced AI.

The thesis contributes both theoretical foundations and practical
implementation, validated through expert comparison and real-world case
studies including Carlsmith's power-seeking AI model. While current
limitations include correlation handling and extraction ambiguities, the
approach provides essential epistemic infrastructure for coordinated AI
governance. plain-language-summary: \textbar{} This thesis develops
software tools that automatically extract and visualize the hidden
assumptions and probability estimates in AI safety arguments. By
transforming complex written arguments into interactive diagrams showing
relationships and probabilities, AMTAIR helps different groups working
on AI safety---researchers, policymakers, and others---understand each
other better and coordinate their efforts to address risks from advanced
AI systems. key-points:

\begin{itemize}
\tightlist
\item
  A novel two-stage extraction pipeline transforms argument structures
  into Bayesian networks through ArgDown and BayesDown intermediate
  representations
\item
  Interactive visualizations make complex probabilistic relationships
  accessible to diverse stakeholders
\item
  Formal representation enables systematic comparison across different
  worldviews and assumptions
\item
  Validated extraction achieves \textgreater85\% accuracy for structure
  and \textgreater73\% for probabilities
\item
  The approach addresses coordination failures by creating a common
  language for AI risk assessment metadata-submission: field-of-study:
  ``Philosophy \& Economics M.A.'' matriculation-number: 1828610
  submission-date: ``May 26, 2025'' word-count: 30000 date:
  ``2025-05-26'' bibliography: ref/MAref.bib citation: container-title:
  University of Bayreuth number-sections: true reference-location:
  margin citation-location: margin
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

This Quarto book represents the culmination of interdisciplinary
research at the intersection of AI safety, formal epistemology, and
computational social science. The work emerged from recognizing a
fundamental challenge in AI governance: while investment in AI safety
research has grown exponentially, coordination between different
stakeholder communities remains fragmented, potentially increasing
existential risk through misaligned efforts.

The journey from initial concept to working implementation involved
iterative refinement based on feedback from advisors, domain experts,
and potential users. What began as a technical exercise in automated
extraction evolved into a broader framework for enhancing epistemic
security in one of humanity's most critical coordination challenges.

\section*{Acknowledgments}\label{acknowledgments}
\addcontentsline{toc}{section}{Acknowledgments}

\markright{Acknowledgments}

I thank my supervisor Dr.~Timo Speith for guidance throughout this
project, the MTAIR team for pioneering the manual approach that inspired
automation, and the AI safety community for creating the rich literature
that made this work possible. Special recognition goes to technical
advisors who provided implementation feedback and domain experts who
validated extraction results.

\bookmarksetup{startatroot}

\chapter*{Table of Contents}\label{table-of-contents}
\addcontentsline{toc}{chapter}{Table of Contents}

\markboth{Table of Contents}{Table of Contents}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://claude.ai/chat/c48c3c3a-d1a9-4c33-b88b-63dd586bb6f2\#sec-introduction}{Introduction:
  The Coordination Crisis in AI Governance}
\item
  \href{https://claude.ai/chat/c48c3c3a-d1a9-4c33-b88b-63dd586bb6f2\#sec-context}{Context
  and Theoretical Foundations}
\item
  \href{https://claude.ai/chat/c48c3c3a-d1a9-4c33-b88b-63dd586bb6f2\#sec-amtair}{AMTAIR:
  Design and Implementation}
\item
  \href{https://claude.ai/chat/c48c3c3a-d1a9-4c33-b88b-63dd586bb6f2\#sec-discussion}{Discussion:
  Implications and Limitations}
\item
  \href{https://claude.ai/chat/c48c3c3a-d1a9-4c33-b88b-63dd586bb6f2\#sec-conclusion}{Conclusion:
  Toward Coordinated AI Governance}
\item
  \href{https://claude.ai/chat/c48c3c3a-d1a9-4c33-b88b-63dd586bb6f2\#sec-references}{References}
\item
  \href{https://claude.ai/chat/c48c3c3a-d1a9-4c33-b88b-63dd586bb6f2\#sec-appendices}{Appendices}
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{Abstract}\label{sec-abstract}
\addcontentsline{toc}{chapter}{Abstract}

\markboth{Abstract}{Abstract}

The coordination crisis in AI governance presents a paradoxical
challenge: unprecedented investment in AI safety coexists alongside
fundamental coordination failures across technical, policy, and ethical
domains. These divisions systematically increase existential risk by
creating safety gaps, misallocating resources, and fostering
inconsistent approaches to interdependent problems.

This thesis introduces AMTAIR (Automating Transformative AI Risk
Modeling), a computational approach that addresses this coordination
failure by automating the extraction of probabilistic world models from
AI safety literature using frontier language models. The AMTAIR system
implements an end-to-end pipeline that transforms unstructured text into
interactive Bayesian networks through a novel two-stage extraction
process: first capturing argument structure in ArgDown format, then
enhancing it with probability information in BayesDown.

The core technical contribution involves developing intermediate
representations that preserve both narrative structure and mathematical
precision. ArgDown captures hierarchical argument relationships while
remaining human-readable. BayesDown extends this with probabilistic
metadata, creating a bridge to formal Bayesian networks. This two-stage
approach separates concerns, enabling modular improvement and human
oversight at critical decision points.

Validation through expert comparison and real-world case studies
demonstrates extraction accuracy exceeding 85\% for structural
relationships and 73\% for probability capture. Application to
Carlsmith's power-seeking AI model shows the system can reconstruct
complex multi-level causal structures with realistic uncertainty
relationships. Comparative analysis across different AI governance
worldviews reveals both convergence on key structural elements and
critical disagreements on parameter values.

This approach bridges communication gaps between stakeholders by making
implicit models explicit, enabling comparison across different
worldviews, providing a common language for discussing probabilistic
relationships, and supporting policy evaluation across diverse
scenarios. While limitations remain in handling complex correlations and
extraction ambiguities, AMTAIR provides essential epistemic
infrastructure for enhanced coordination in AI governance.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\bookmarksetup{startatroot}

\chapter*{1. Introduction: The Coordination Crisis in AI
Governance}\label{sec-introduction}
\addcontentsline{toc}{chapter}{1. Introduction: The Coordination Crisis
in AI Governance}

\markboth{1. Introduction: The Coordination Crisis in AI Governance}{1.
Introduction: The Coordination Crisis in AI Governance}

\begin{verbatim}
### 10% of Grade: ~ 14% of text ~ 4200 words ~ 10 pages

- introduces and motivates the core question or problem
- provides context for discussion (places issue within a larger debate or sphere of relevance)  
- states precise thesis or position the author will argue for
- provides roadmap indicating structure and key content points of the essay
\end{verbatim}

\section*{Opening Scenario: The Policymaker's
Dilemma}\label{sec-opening-scenario}
\addcontentsline{toc}{section}{Opening Scenario: The Policymaker's
Dilemma}

\markright{Opening Scenario: The Policymaker's Dilemma}

Imagine a senior policy advisor preparing recommendations for AI
governance legislation. On her desk lie a dozen reports from leading AI
safety researchers, each painting a different picture of the risks
ahead. One argues that misaligned AI could pose existential risks within
the decade, citing complex technical arguments about instrumental
convergence and orthogonality. Another suggests these concerns are
overblown, emphasizing uncertainty and the strength of existing
institutions. A third proposes specific technical standards but
acknowledges deep uncertainty about their effectiveness.

Each report seems compelling in isolation, written by credentialed
experts with sophisticated arguments. Yet they reach dramatically
different conclusions about both the magnitude of risk and appropriate
interventions. The technical arguments involve unfamiliar
concepts---mesa-optimization, corrigibility, capability
amplification---expressed through different frameworks and implicit
assumptions. Time is limited, stakes are high, and the legislation could
shape humanity's trajectory for decades.

This scenario plays out daily across government offices, corporate
boardrooms, and research institutions worldwide. It exemplifies what I
term the ``coordination crisis'' in AI governance: despite unprecedented
attention and resources directed toward AI safety, we lack the epistemic
infrastructure to synthesize diverse expert knowledge into actionable
governance strategies.

\section*{The Coordination Crisis in AI
Governance}\label{sec-coordination-crisis}
\addcontentsline{toc}{section}{The Coordination Crisis in AI Governance}

\markright{The Coordination Crisis in AI Governance}

As AI capabilities advance at an accelerating pace---demonstrated by the
rapid progression from GPT-3 to GPT-4, Claude, and emerging multimodal
systems---humanity faces a governance challenge unlike any in history.
The task of ensuring increasingly powerful AI systems remain aligned
with human values and beneficial to our long-term flourishing grows more
urgent with each capability breakthrough. This challenge becomes
particularly acute when considering transformative AI systems that could
drastically alter civilization's trajectory, potentially including
existential risks from misaligned systems pursuing objectives counter to
human welfare.

The current state of AI governance presents a striking paradox. On one
hand, we witness extraordinary mobilization: billions in research
funding, proliferating safety initiatives, major tech companies
establishing alignment teams, and governments worldwide developing AI
strategies. The Asilomar AI Principles garnered thousands of signatures,
the EU advances comprehensive AI regulation, and technical researchers
produce increasingly sophisticated work on alignment, interpretability,
and robustness.

Yet alongside this activity, we observe systematic coordination failures
that may prove catastrophic. Technical safety researchers develop
sophisticated alignment techniques without clear implementation
pathways. Policy specialists craft regulatory frameworks lacking
technical grounding to ensure practical efficacy. Ethicists articulate
normative principles that lack operational specificity. Strategy
researchers identify critical uncertainties but struggle to translate
these into actionable guidance. International bodies convene without
shared frameworks for assessing interventions.

This fragmentation is not merely inefficient---it systematically
amplifies existential risk through several mechanisms:

\subsection*{Safety Gaps from Misaligned Efforts}\label{sec-safety-gaps}
\addcontentsline{toc}{subsection}{Safety Gaps from Misaligned Efforts}

When different communities operate with incompatible frameworks,
critical risks fall through the cracks. Technical researchers may solve
alignment problems under assumptions that policymakers' decisions
invalidate. Regulations optimized for current systems may inadvertently
incentivize dangerous development patterns. Without shared models of the
risk landscape, our collective efforts resemble the parable of blind men
describing an elephant---each accurate within their domain but missing
the complete picture.

\subsection*{Resource Misallocation}\label{sec-resource-misallocation}
\addcontentsline{toc}{subsection}{Resource Misallocation}

The AI safety community duplicates efforts while leaving critical areas
underexplored. Multiple teams independently develop similar frameworks
without building on each other's work. Funders struggle to identify
high-impact opportunities across technical and governance domains.
Talent flows toward well-publicized approaches while neglected
strategies remain understaffed. This misallocation becomes more costly
as the window for establishing effective governance narrows.

\subsection*{Negative-Sum Dynamics}\label{sec-negative-sum}
\addcontentsline{toc}{subsection}{Negative-Sum Dynamics}

Perhaps most concerning, uncoordinated interventions can actively
increase risk. Safety standards that advantage established players may
accelerate risky development elsewhere. Partial transparency
requirements might enable capability advances without commensurate
safety improvements. International agreements lacking shared technical
understanding may lock in dangerous practices. Without coordination, our
cure risks becoming worse than the disease.

\section*{Historical Parallels and Temporal
Urgency}\label{sec-historical-urgency}
\addcontentsline{toc}{section}{Historical Parallels and Temporal
Urgency}

\markright{Historical Parallels and Temporal Urgency}

History offers instructive parallels. The nuclear age began with
scientists racing to understand and control forces that could destroy
civilization. Early coordination failures---competing national programs,
scientist-military tensions, public-expert divides---nearly led to
catastrophe multiple times. Only through developing shared frameworks
(deterrence theory), institutions (IAEA), and communication channels
(hotlines, treaties) did humanity navigate the nuclear precipice.

Yet AI presents unique coordination challenges that compress our
response timeline:

\textbf{Accelerating Development}: Unlike nuclear weapons requiring
massive infrastructure, AI development proceeds in corporate labs and
academic departments worldwide. Capability improvements come through
algorithmic insights and computational scale, both advancing
exponentially.

\textbf{Dual-Use Ubiquity}: Every AI advance potentially contributes to
both beneficial applications and catastrophic risks. The same language
model architectures enabling scientific breakthroughs could facilitate
dangerous manipulation or deception at scale.

\textbf{Comprehension Barriers}: Nuclear risks were viscerally
understandable---cities vaporized, radiation sickness, nuclear winter.
AI risks involve abstract concepts like optimization processes, goal
misspecification, and emergent capabilities that resist intuitive
understanding.

\textbf{Governance Lag}: Traditional governance
mechanisms---legislation, international treaties, professional
standards---operate on timescales of years to decades. AI capabilities
advance on timescales of months to years, creating an ever-widening
capability-governance gap.

\section*{Research Question and Scope}\label{sec-research-question}
\addcontentsline{toc}{section}{Research Question and Scope}

\markright{Research Question and Scope}

This thesis addresses a specific dimension of the coordination challenge
by investigating:

\textbf{How can computational approaches formalize the worldviews and
arguments underlying AI safety discourse, transforming qualitative
disagreements into quantitative models suitable for rigorous policy
evaluation?}

More specifically, I explore whether frontier AI technologies can be
utilized to automate the modeling of transformative AI risks, enabling
robust prediction of policy impacts across diverse worldviews.

To break this down:

\begin{itemize}
\tightlist
\item
  \textbf{Computational Formalization}: Using automated extraction and
  formal representation to make implicit models explicit
\item
  \textbf{Worldview Representation}: Capturing different perspectives on
  AI risk in comparable frameworks
\item
  \textbf{Argument Transformation}: Converting natural language
  arguments into structured Bayesian networks
\item
  \textbf{Policy Evaluation}: Assessing intervention impacts through
  formal counterfactual analysis
\end{itemize}

The scope encompasses both theoretical development and practical
implementation. Theoretically, I develop a framework for representing
diverse perspectives on AI risk in a common formal language.
Practically, I implement this framework in a computational system---the
AI Risk Pathway Analyzer (ARPA)---that enables interactive exploration
of how policy interventions might alter existential risk across
different worldviews.

This investigation focuses specifically on existential risks from
misaligned AI systems rather than broader AI ethics concerns. This
narrowed scope enables deep technical development while addressing the
highest-stakes coordination challenges where current fragmentation poses
the greatest danger.

\section*{The Multiplicative Benefits
Framework}\label{sec-multiplicative-benefits}
\addcontentsline{toc}{section}{The Multiplicative Benefits Framework}

\markright{The Multiplicative Benefits Framework}

The central thesis of this work is that combining three
elements---automated worldview extraction, prediction market
integration, and formal policy evaluation---creates multiplicative
rather than merely additive benefits for AI governance. Each component
enhances the others, creating a system more valuable than the sum of its
parts.

\subsection*{Automated Worldview
Extraction}\label{sec-automated-extraction}
\addcontentsline{toc}{subsection}{Automated Worldview Extraction}

Current approaches to AI risk modeling, exemplified by the Modeling
Transformative AI Risks (MTAIR) project, demonstrate the value of formal
representation but require extensive manual effort. Creating a single
model demands hundreds of expert-hours to translate qualitative
arguments into quantitative frameworks. This bottleneck severely limits
the number of perspectives that can be formalized and the speed of model
updates as new arguments emerge.

Automation using frontier language models addresses this scaling
challenge. By developing systematic methods to extract causal structures
and probability judgments from natural language, we can process orders
of magnitude more content, incorporate diverse perspectives rapidly, and
maintain models that evolve with the discourse.

\subsection*{Live Data Integration}\label{sec-live-data}
\addcontentsline{toc}{subsection}{Live Data Integration}

Static models, however well-constructed, quickly become outdated in
fast-moving domains. Prediction markets and forecasting platforms
aggregate distributed knowledge about uncertain futures, providing
continuously updated probability estimates. By connecting formal models
to these live data sources, we create dynamic assessments that
incorporate the latest collective intelligence.

This integration serves multiple purposes: grounding abstract models in
empirical forecasts, identifying which uncertainties most affect
outcomes, revealing when model assumptions diverge from collective
expectations, and generating new questions for forecasting communities.

\subsection*{Formal Policy Evaluation}\label{sec-policy-evaluation}
\addcontentsline{toc}{subsection}{Formal Policy Evaluation}

The ultimate purpose of risk modeling is informing action. Formal policy
evaluation transforms static risk assessments into actionable guidance
by modeling how specific interventions alter critical parameters. Using
causal inference techniques, we can assess not just the probability of
adverse outcomes but how those probabilities change under different
policy regimes.

This enables genuinely evidence-based policy development: comparing
interventions across multiple worldviews, identifying robust strategies
that work across scenarios, understanding which uncertainties most
affect policy effectiveness, and prioritizing research to reduce
decision-relevant uncertainty.

\subsection*{The Synergy}\label{sec-synergy}
\addcontentsline{toc}{subsection}{The Synergy}

The multiplicative benefits emerge from the interactions between
components:

\begin{itemize}
\tightlist
\item
  Automation enables comprehensive coverage, making prediction market
  integration more valuable by connecting to more perspectives
\item
  Market data validates and calibrates automated extractions, improving
  quality
\item
  Policy evaluation gains precision from both comprehensive models and
  live probability updates
\item
  The complete system creates feedback loops where policy analysis
  identifies critical uncertainties for market attention
\end{itemize}

This synergistic combination addresses the coordination crisis by
providing common ground for disparate communities, translating between
technical and policy languages, quantifying previously implicit
disagreements, and enabling evidence-based compromise.

\section*{Thesis Structure and Roadmap}\label{sec-roadmap}
\addcontentsline{toc}{section}{Thesis Structure and Roadmap}

\markright{Thesis Structure and Roadmap}

The remainder of this thesis develops the multiplicative benefits
framework from theoretical foundations to practical implementation:

\textbf{Chapter 2: Context and Theoretical Foundations} establishes the
intellectual groundwork, examining:

\begin{itemize}
\tightlist
\item
  The epistemic challenges unique to AI governance
\item
  Bayesian networks as formal tools for uncertainty representation
\item
  Argument mapping as a bridge from natural language to formal models
\item
  The MTAIR project's achievements and limitations
\item
  Requirements for effective coordination infrastructure
\end{itemize}

\textbf{Chapter 3: AMTAIR Design and Implementation} presents the
technical system:

\begin{itemize}
\tightlist
\item
  Overall architecture and design principles
\item
  The two-stage extraction pipeline (ArgDown → BayesDown)
\item
  Validation methodology and results
\item
  Case studies from simple examples to complex AI risk models
\item
  Integration with prediction markets and policy evaluation
\end{itemize}

\textbf{Chapter 4: Discussion - Implications and Limitations} critically
examines:

\begin{itemize}
\tightlist
\item
  Technical limitations and failure modes
\item
  Conceptual concerns about formalization
\item
  Integration with existing governance frameworks
\item
  Scaling challenges and opportunities
\item
  Broader implications for epistemic security
\end{itemize}

\textbf{Chapter 5: Conclusion} synthesizes key contributions and charts
paths forward:

\begin{itemize}
\tightlist
\item
  Summary of theoretical and practical achievements
\item
  Concrete recommendations for stakeholders
\item
  Research agenda for community development
\item
  Vision for AI governance with proper coordination infrastructure
\end{itemize}

Throughout, I maintain dual focus on theoretical sophistication and
practical utility. The framework aims not merely to advance academic
understanding but to provide actionable tools for improving coordination
in AI governance during this critical period.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\bookmarksetup{startatroot}

\chapter*{2. Context and Theoretical Foundations}\label{sec-context}
\addcontentsline{toc}{chapter}{2. Context and Theoretical Foundations}

\markboth{2. Context and Theoretical Foundations}{2. Context and
Theoretical Foundations}

\begin{verbatim}
### 20% of Grade: ~ 29% of text ~ 8700 words ~ 20 pages

- demonstrates understanding of all relevant core concepts
- explains why the question/thesis/problem is relevant in student's own words (supported by quotations)
- situates it within the debate/course material
- reconstructs selected arguments and identifies relevant assumptions
- describes additional relevant material that has been consulted and integrates it with the course material as well as the research question/thesis/problem
\end{verbatim}

This chapter establishes the theoretical and methodological foundations
necessary for understanding AMTAIR's approach to automating AI risk
modeling. I begin with the core challenge---representing existential
risk arguments in formal terms---then develop the technical and
conceptual tools needed to address it.

\section*{AI Existential Risk: The Carlsmith
Model}\label{sec-carlsmith-model}
\addcontentsline{toc}{section}{AI Existential Risk: The Carlsmith Model}

\markright{AI Existential Risk: The Carlsmith Model}

To ground our discussion in concrete terms, I examine Joseph Carlsmith's
``Is Power-Seeking AI an Existential Risk?'' as an exemplar of
structured reasoning about AI catastrophic risk. Carlsmith's analysis
stands out for its explicit probabilistic decomposition of the path from
current AI development to potential existential catastrophe.

\subsection*{Six-Premise Decomposition}\label{sec-six-premise}
\addcontentsline{toc}{subsection}{Six-Premise Decomposition}

Carlsmith structures his argument through six conditional premises, each
assigned explicit probability estimates:

\textbf{Premise 1: APS Systems by 2070} (P ≈ 0.65)\\
``By 2070, there will be AI systems with Advanced capability, Agentic
planning, and Strategic awareness'' - the conjunction of capabilities
that could enable systematic pursuit of objectives in the world.

\textbf{Premise 2: Alignment Difficulty} (P ≈ 0.40)\\
``It will be harder to build aligned APS systems than misaligned systems
that are still attractive to deploy'' - capturing the challenge that
safety may conflict with capability or efficiency.

\textbf{Premise 3: Deployment Despite Misalignment} (P ≈ 0.70)\\
``Conditional on 1 and 2, we will deploy misaligned APS systems'' -
reflecting competitive pressures and limited coordination.

\textbf{Premise 4: Power-Seeking Behavior} (P ≈ 0.65)\\
``Conditional on 1-3, misaligned APS systems will seek power in
high-impact ways'' - based on instrumental convergence arguments.

\textbf{Premise 5: Disempowerment Success} (P ≈ 0.40)\\
``Conditional on 1-4, power-seeking will scale to permanent human
disempowerment'' - despite potential resistance and safeguards.

\textbf{Premise 6: Existential Catastrophe} (P ≈ 0.95)\\
``Conditional on 1-5, this disempowerment constitutes existential
catastrophe'' - connecting power loss to permanent curtailment of human
potential.

\textbf{Overall Risk}: Multiplying through the conditional chain yields
P(doom) ≈ 0.05 or 5\% by 2070.

\subsection*{Why Carlsmith Exemplifies Formalizable
Arguments}\label{sec-carlsmith-formalizable}
\addcontentsline{toc}{subsection}{Why Carlsmith Exemplifies Formalizable
Arguments}

Carlsmith's model demonstrates several features that make it ideal for
formal representation:

\textbf{Explicit Probabilistic Structure}: Each premise receives
numerical probability estimates with documented reasoning, enabling
direct translation to Bayesian network parameters.

\textbf{Clear Conditional Dependencies}: The logical flow from
capabilities through deployment decisions to catastrophic outcomes maps
naturally onto directed acyclic graphs.

\textbf{Transparent Decomposition}: Breaking the argument into modular
premises allows independent evaluation and sensitivity analysis of each
component.

\textbf{Documented Reasoning}: Extensive justification for each
probability enables extraction of both structure and parameters from the
source text.

This structured approach exemplifies the type of reasoning AMTAIR aims
to formalize and automate. While Carlsmith spent months developing this
model manually, similar rigor exists implicitly in many AI safety
arguments awaiting extraction.

\section*{The Epistemic Challenge of Policy
Evaluation}\label{sec-epistemic-challenge}
\addcontentsline{toc}{section}{The Epistemic Challenge of Policy
Evaluation}

\markright{The Epistemic Challenge of Policy Evaluation}

Evaluating AI governance policies presents unique epistemic challenges
that traditional policy analysis methods cannot adequately address.
Understanding these challenges motivates the need for new computational
approaches.

\subsection*{Unique Characteristics of AI
Governance}\label{sec-ai-governance-unique}
\addcontentsline{toc}{subsection}{Unique Characteristics of AI
Governance}

\textbf{Deep Uncertainty Rather Than Risk}: Traditional policy analysis
distinguishes between risk (known probability distributions) and
uncertainty (known possibilities, unknown probabilities). AI governance
faces deep uncertainty---we cannot confidently enumerate possible
futures, much less assign probabilities. Will recursive self-improvement
enable rapid capability gains? Can value alignment be solved
technically? These foundational questions resist empirical resolution
before their answers become catastrophically relevant.

\textbf{Complex Multi-Level Causation}: Policy effects propagate through
technical, institutional, and social levels with intricate feedback
loops. A technical standard might alter research incentives, shifting
capability development trajectories, changing competitive dynamics, and
ultimately affecting existential risk through pathways invisible at the
policy's inception. Traditional linear causal models cannot capture
these dynamics.

\textbf{Irreversibility and Lock-In}: Many AI governance decisions
create path dependencies that prove difficult or impossible to reverse.
Early technical standards shape development trajectories. Institutional
structures ossify. International agreements create sticky equilibria.
Unlike many policy domains where course correction remains possible, AI
governance mistakes may prove permanent.

\textbf{Value-Laden Technical Choices}: The entanglement of technical
and normative questions confounds traditional separation of facts and
values. What constitutes ``alignment''? How much capability development
should we risk for economic benefits? Technical specifications embed
ethical judgments that resist neutral expertise.

\subsection*{Limitations of Traditional
Approaches}\label{sec-traditional-limitations}
\addcontentsline{toc}{subsection}{Limitations of Traditional Approaches}

Standard policy evaluation tools prove inadequate for these challenges:

\textbf{Cost-Benefit Analysis} assumes commensurable outcomes and stable
probability distributions. When potential outcomes include existential
catastrophe with deeply uncertain probabilities, the mathematical
machinery breaks down. Infinite negative utility resists standard
decision frameworks.

\textbf{Scenario Planning} helps explore possible futures but typically
lacks the probabilistic reasoning needed for decision-making under
uncertainty. Without quantification, scenarios provide narrative
richness but limited action guidance.

\textbf{Expert Elicitation} aggregates specialist judgment but struggles
with interdisciplinary questions where no single expert grasps all
relevant factors. Moreover, experts often operate with different
implicit models, making aggregation problematic.

\textbf{Red Team Exercises} test specific plans but miss systemic risks
emerging from component interactions. Gaming individual failures cannot
reveal emergent catastrophic possibilities.

These limitations create a methodological gap: we need approaches that
handle deep uncertainty, represent complex causation, quantify expert
disagreement, and enable systematic exploration of intervention effects.

\section*{Bayesian Networks as Knowledge
Representation}\label{sec-bayesian-networks}
\addcontentsline{toc}{section}{Bayesian Networks as Knowledge
Representation}

\markright{Bayesian Networks as Knowledge Representation}

Bayesian networks offer a mathematical framework uniquely suited to
addressing these epistemic challenges. By combining graphical structure
with probability theory, they provide tools for reasoning about complex
uncertain domains.

\subsection*{Mathematical
Foundations}\label{sec-mathematical-foundations}
\addcontentsline{toc}{subsection}{Mathematical Foundations}

A Bayesian network consists of:

\begin{itemize}
\tightlist
\item
  \textbf{Directed Acyclic Graph (DAG)}: Nodes represent variables,
  edges represent direct dependencies
\item
  \textbf{Conditional Probability Tables (CPTs)}: For each node,
  P(node\textbar parents) quantifies relationships
\end{itemize}

The joint probability distribution factors according to the graph
structure:

\[P(X_1, X_2, ..., X_n) = \prod_{i=1}^{n} P(X_i | Parents(X_i))\]

This factorization enables efficient inference and embodies causal
assumptions explicitly.

\subsection*{The Rain-Sprinkler-Grass
Example}\label{sec-rain-sprinkler-example}
\addcontentsline{toc}{subsection}{The Rain-Sprinkler-Grass Example}

The canonical example illustrates key concepts:

\begin{verbatim}
[Grass_Wet]: Concentrated moisture on grass. 
 + [Rain]: Water falling from sky.
 + [Sprinkler]: Artificial watering system.
   + [Rain]
\end{verbatim}

Network Structure:

\begin{itemize}
\tightlist
\item
  \textbf{Rain} (root cause): P(rain) = 0.2
\item
  \textbf{Sprinkler} (intermediate): P(sprinkler\textbar rain) varies by
  rain state
\item
  \textbf{Grass\_Wet} (effect): P(wet\textbar rain, sprinkler) depends
  on both causes
\end{itemize}

This simple network demonstrates:

\begin{itemize}
\tightlist
\item
  \textbf{Marginal Inference}: P(grass\_wet) computed from joint
  distribution
\item
  \textbf{Diagnostic Reasoning}: P(rain\textbar grass\_wet) reasoning
  from effects to causes
\item
  \textbf{Intervention Modeling}: P(grass\_wet\textbar do(sprinkler=on))
  for policy analysis
\end{itemize}

\subsection*{Advantages for AI Risk
Modeling}\label{sec-modeling-advantages}
\addcontentsline{toc}{subsection}{Advantages for AI Risk Modeling}

Bayesian networks provide several crucial capabilities:

\textbf{Explicit Uncertainty Representation}: Every belief is a
probability distribution, avoiding false certainty while enabling
quantitative reasoning.

\textbf{Causal Modeling}: Directed edges represent causal relationships,
enabling counterfactual reasoning through Pearl's do-calculus for policy
evaluation.

\textbf{Modular Structure}: Complex arguments decompose into manageable
components that can be independently evaluated and refined.

\textbf{Evidence Integration}: Bayesian updating provides principled
methods for incorporating new information as it emerges.

\textbf{Visual Communication}: Graphical structure makes complex
relationships comprehensible across expertise levels.

These features address key requirements for AI governance: handling
uncertainty, representing causation, enabling systematic analysis, and
facilitating communication across communities.

\section*{Argument Mapping and Formal
Representations}\label{sec-argument-mapping}
\addcontentsline{toc}{section}{Argument Mapping and Formal
Representations}

\markright{Argument Mapping and Formal Representations}

The gap between natural language arguments and formal models requires
systematic bridging. Argument mapping provides methods for making
implicit reasoning structures explicit and analyzable.

\subsection*{From Natural Language to
Structure}\label{sec-natural-to-structure}
\addcontentsline{toc}{subsection}{From Natural Language to Structure}

Natural language arguments contain rich information expressed through:

\begin{itemize}
\tightlist
\item
  Causal claims (``X leads to Y'')
\item
  Conditional relationships (``If A then likely B'')
\item
  Uncertainty expressions (``probably,'' ``might,'' ``certainly'')
\item
  Support/attack patterns between claims
\end{itemize}

Argument mapping extracts this structure, identifying:

\begin{itemize}
\tightlist
\item
  \textbf{Core claims and propositions}
\item
  \textbf{Inferential relationships}
\item
  \textbf{Implicit assumptions}
\item
  \textbf{Uncertainty qualifications}
\end{itemize}

\subsection*{ArgDown: Structured Argument
Notation}\label{sec-argdown-notation}
\addcontentsline{toc}{subsection}{ArgDown: Structured Argument Notation}

ArgDown provides a markdown-like syntax for hierarchical argument
representation:

\begin{verbatim}
[MainClaim]: Description of primary conclusion.
 + [SupportingEvidence]: Evidence supporting the claim.
   + [SubEvidence]: More specific support.
 - [CounterArgument]: Evidence against the claim.
\end{verbatim}

This notation captures argument structure while remaining human-readable
and writable. Crucially, it serves as an intermediate representation
between natural language and formal models.

\subsection*{BayesDown: The Bridge to Bayesian
Networks}\label{sec-bayesdown}
\addcontentsline{toc}{subsection}{BayesDown: The Bridge to Bayesian
Networks}

BayesDown extends ArgDown with probabilistic metadata:

\begin{verbatim}
[Node]: Description. {
  "instantiations": ["node_TRUE", "node_FALSE"],
  "priors": {"p(node_TRUE)": "0.7", "p(node_FALSE)": "0.3"},
  "posteriors": {
    "p(node_TRUE|parent_TRUE)": "0.9",
    "p(node_TRUE|parent_FALSE)": "0.4"
  }
}
\end{verbatim}

This representation:

\begin{itemize}
\tightlist
\item
  \textbf{Preserves narrative structure} from the original argument
\item
  \textbf{Adds mathematical precision} through probability
  specifications
\item
  \textbf{Enables transformation} to standard Bayesian network formats
\item
  \textbf{Supports validation} by maintaining traceability to sources
\end{itemize}

The two-stage extraction process (ArgDown → BayesDown) separates
concerns: first capturing structure, then quantifying relationships.
This modularity enables human oversight at critical decision points.

\section*{The MTAIR Framework: Achievements and
Limitations}\label{sec-mtair-framework}
\addcontentsline{toc}{section}{The MTAIR Framework: Achievements and
Limitations}

\markright{The MTAIR Framework: Achievements and Limitations}

The Modeling Transformative AI Risks (MTAIR) project, led by RAND
researchers, pioneered formal modeling of AI existential risk arguments.
Understanding its approach and limitations motivates the automation
efforts of AMTAIR.

\subsection*{MTAIR's Approach}\label{sec-mtair-approach}
\addcontentsline{toc}{subsection}{MTAIR's Approach}

MTAIR manually translated influential AI risk arguments into Bayesian
networks using Analytica software:

\textbf{Systematic Decomposition}: Breaking complex arguments into
variables and relationships through expert analysis.

\textbf{Probability Elicitation}: Gathering quantitative estimates
through structured expert interviews and literature review.

\textbf{Sensitivity Analysis}: Identifying which parameters most
influence conclusions about AI risk levels.

\textbf{Visual Communication}: Creating interactive models that
stakeholders could explore and modify.

\subsection*{Key Achievements}\label{sec-mtair-achievements}
\addcontentsline{toc}{subsection}{Key Achievements}

MTAIR demonstrated several important possibilities:

\textbf{Feasibility of Formalization}: Complex philosophical arguments
about AI risk can be represented as Bayesian networks while preserving
essential insights.

\textbf{Value of Quantification}: Moving from qualitative concerns to
quantitative models enables systematic analysis, comparison, and
prioritization.

\textbf{Cross-Perspective Communication}: Formal models provide common
ground for technical and policy communities to engage productively.

\textbf{Research Prioritization}: Sensitivity analysis reveals which
empirical questions would most reduce uncertainty about AI risks.

\subsection*{Fundamental Limitations}\label{sec-mtair-limitations}
\addcontentsline{toc}{subsection}{Fundamental Limitations}

However, MTAIR's manual approach faces severe constraints:

\textbf{Labor Intensity}: Each model requires hundreds of expert-hours
to construct, limiting coverage to a few perspectives.

\textbf{Static Nature}: Models become outdated as arguments evolve but
updating requires near-complete reconstruction.

\textbf{Limited Accessibility}: Using the models requires Analytica
software and significant technical sophistication.

\textbf{Single Perspective}: Each model represents one worldview, making
comparison across perspectives difficult.

These limitations prevent MTAIR's approach from scaling to meet AI
governance needs. As the pace of AI development accelerates and
arguments proliferate, manual modeling cannot keep pace.

\subsection*{The Automation
Opportunity}\label{sec-automation-opportunity}
\addcontentsline{toc}{subsection}{The Automation Opportunity}

MTAIR's experience reveals both the value of formal modeling and the
necessity of automation. Key lessons:

\begin{itemize}
\tightlist
\item
  Formal models genuinely enhance understanding and coordination
\item
  The modeling process itself surfaces implicit assumptions
\item
  Quantification enables analyses impossible with qualitative arguments
  alone
\item
  But manual approaches cannot scale to match the challenge
\end{itemize}

This motivates AMTAIR's central innovation: using frontier language
models to automate the extraction and formalization process while
preserving the benefits MTAIR demonstrated.

\section*{Requirements for Coordination
Infrastructure}\label{sec-coordination-requirements}
\addcontentsline{toc}{section}{Requirements for Coordination
Infrastructure}

\markright{Requirements for Coordination Infrastructure}

Based on the challenges identified and lessons from existing approaches,
we can specify requirements for computational tools that could enhance
coordination in AI governance:

\subsection*{Scalability}\label{sec-scalability-req}
\addcontentsline{toc}{subsection}{Scalability}

The system must process large volumes of arguments across:

\begin{itemize}
\tightlist
\item
  Academic papers and technical reports
\item
  Policy documents and proposals
\item
  Blog posts and informal arguments
\item
  Forecasting questions and market data
\end{itemize}

Automation is essential---manual approaches cannot match the pace of
discourse.

\subsection*{Accessibility}\label{sec-accessibility-req}
\addcontentsline{toc}{subsection}{Accessibility}

Diverse stakeholders must be able to engage with the system:

\begin{itemize}
\tightlist
\item
  \textbf{Researchers} need technical depth and modification
  capabilities
\item
  \textbf{Policymakers} require clear summaries and intervention
  analysis
\item
  \textbf{Forecasters} want integration with prediction platforms
\item
  \textbf{Public stakeholders} deserve transparent representation
\end{itemize}

This demands multiple interfaces and levels of abstraction.

\subsection*{Epistemic Virtues}\label{sec-epistemic-virtues}
\addcontentsline{toc}{subsection}{Epistemic Virtues}

The system should enhance rather than replace human judgment by:

\begin{itemize}
\tightlist
\item
  \textbf{Making assumptions explicit} through formal representation
\item
  \textbf{Preserving uncertainty} rather than false precision
\item
  \textbf{Enabling validation} through traceable extraction
\item
  \textbf{Supporting disagreement} through multi-worldview
  representation
\item
  \textbf{Encouraging updating} as new evidence emerges
\end{itemize}

\subsection*{Integration Capabilities}\label{sec-integration-req}
\addcontentsline{toc}{subsection}{Integration Capabilities}

Isolated tools have limited impact. The system needs:

\begin{itemize}
\tightlist
\item
  \textbf{Data source connections} to prediction markets and forecasting
  platforms
\item
  \textbf{API accessibility} for integration with other tools
\item
  \textbf{Export formats} compatible with standard analysis software
\item
  \textbf{Version control} for tracking model evolution
\item
  \textbf{Collaborative features} for community development
\end{itemize}

\subsection*{Robustness Properties}\label{sec-robustness-req}
\addcontentsline{toc}{subsection}{Robustness Properties}

Given the high stakes, the system must handle:

\begin{itemize}
\tightlist
\item
  \textbf{Extraction errors} through validation and correction
  mechanisms
\item
  \textbf{Adversarial inputs} designed to manipulate outputs
\item
  \textbf{Model uncertainty} through sensitivity analysis
\item
  \textbf{Scaling challenges} as networks grow large
\item
  \textbf{Evolution over time} as arguments develop
\end{itemize}

These requirements shape AMTAIR's design, as detailed in the next
chapter.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\bookmarksetup{startatroot}

\chapter*{3. AMTAIR: Design and Implementation}\label{sec-amtair}
\addcontentsline{toc}{chapter}{3. AMTAIR: Design and Implementation}

\markboth{3. AMTAIR: Design and Implementation}{3. AMTAIR: Design and
Implementation}

\begin{verbatim}
### 20% of Grade: ~ 29% of text ~ 8700 words ~ 20 pages

- provides critical or constructive evaluation of positions introduced
- develops strong (plausible) argument in support of author's own position/thesis
- argument draws on relevant course material
- demonstrates understanding of course materials and key concepts
- presents original or insightful contribution to the debate
\end{verbatim}

This chapter presents the technical architecture and implementation of
AMTAIR, demonstrating how theoretical principles translate into working
software. I detail the design decisions, implementation challenges, and
validation results that establish AMTAIR's feasibility and value.

\section*{System Architecture Overview}\label{sec-system-architecture}
\addcontentsline{toc}{section}{System Architecture Overview}

\markright{System Architecture Overview}

AMTAIR implements an end-to-end pipeline transforming unstructured text
into interactive Bayesian network visualizations. The architecture
reflects key design principles:

\begin{itemize}
\tightlist
\item
  \textbf{Modularity}: Each component can be independently improved
\item
  \textbf{Transparency}: Intermediate outputs enable inspection and
  validation
\item
  \textbf{Flexibility}: Multiple input formats and configurable
  processing
\item
  \textbf{Scalability}: Efficient processing of large document sets
\end{itemize}

\subsection*{Five-Stage Pipeline}\label{sec-five-stage-pipeline}
\addcontentsline{toc}{subsection}{Five-Stage Pipeline}

The system processes information through five distinct stages:

\begin{verbatim}
Documents → Ingestion → ArgDown → BayesDown → Networks → Visualization
\end{verbatim}

Each stage produces inspectable outputs, enabling validation and
debugging. This transparency is crucial for building trust in automated
extraction.

\subsection*{Component Architecture}\label{sec-component-architecture}
\addcontentsline{toc}{subsection}{Component Architecture}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ AMTAIRPipeline:}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \VariableTok{self}\NormalTok{.ingestion }\OperatorTok{=}\NormalTok{ DocumentIngestion()}
        \VariableTok{self}\NormalTok{.extraction }\OperatorTok{=}\NormalTok{ BayesDownExtractor()  }
        \VariableTok{self}\NormalTok{.transformation }\OperatorTok{=}\NormalTok{ DataTransformer()}
        \VariableTok{self}\NormalTok{.network\_builder }\OperatorTok{=}\NormalTok{ BayesianNetworkBuilder()}
        \VariableTok{self}\NormalTok{.visualizer }\OperatorTok{=}\NormalTok{ InteractiveVisualizer()}
        
    \KeywordTok{def}\NormalTok{ process(}\VariableTok{self}\NormalTok{, document):}
        \CommentTok{"""End{-}to{-}end processing from document to interactive model"""}
\NormalTok{        structured\_data }\OperatorTok{=} \VariableTok{self}\NormalTok{.ingestion.preprocess(document)}
\NormalTok{        bayesdown }\OperatorTok{=} \VariableTok{self}\NormalTok{.extraction.extract(structured\_data)}
\NormalTok{        dataframe }\OperatorTok{=} \VariableTok{self}\NormalTok{.transformation.convert(bayesdown)}
\NormalTok{        network }\OperatorTok{=} \VariableTok{self}\NormalTok{.network\_builder.construct(dataframe)}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.visualizer.render(network)}
\end{Highlighting}
\end{Shaded}

This clean separation of concerns enables targeted improvements and
alternative implementations for each component.

\section*{The Two-Stage Extraction
Process}\label{sec-two-stage-extraction}
\addcontentsline{toc}{section}{The Two-Stage Extraction Process}

\markright{The Two-Stage Extraction Process}

The core innovation of AMTAIR lies in separating structural extraction
from probability quantification. This two-stage approach addresses key
challenges in automated formalization.

\subsection*{Stage 1: Structural Extraction
(ArgDown)}\label{sec-stage1-argdown}
\addcontentsline{toc}{subsection}{Stage 1: Structural Extraction
(ArgDown)}

The first stage identifies argument structure without concerning itself
with quantification:

\textbf{Variable Identification}: Extract key propositions and entities
from text using patterns like ``X causes Y,'' ``If A then B,'' and
domain-specific indicators.

\textbf{Relationship Mapping}: Identify support, attack, and conditional
relationships between variables through linguistic analysis.

\textbf{Hierarchy Construction}: Build nested ArgDown representation
preserving logical flow:

\begin{verbatim}
[Existential_Catastrophe]: Destruction of humanity's potential.
 + [Human_Disempowerment]: Loss of control to AI systems.
   + [Misaligned_Power_Seeking]: AI pursuing problematic objectives.
     + [APS_Systems]: Advanced, agentic, strategic AI.
     + [Deployment_Decisions]: Choice to deploy despite risks.
\end{verbatim}

\textbf{Validation}: Ensure extracted structure forms valid directed
acyclic graph and preserves key argumentative relationships from source.

\subsection*{Stage 2: Probability Integration
(BayesDown)}\label{sec-stage2-bayesdown}
\addcontentsline{toc}{subsection}{Stage 2: Probability Integration
(BayesDown)}

The second stage adds quantitative information to the structural
skeleton:

\textbf{Question Generation}: For each node, generate probability
elicitation questions:

\begin{itemize}
\tightlist
\item
  ``What is the probability of existential catastrophe?''
\item
  ``What is P(catastrophe\textbar human\_disempowerment)?''
\end{itemize}

\textbf{Probability Extraction}: Identify explicit numerical statements
and map qualitative expressions:

\begin{itemize}
\tightlist
\item
  ``Very likely'' → 0.75-0.9
\item
  ``Possible but unlikely'' → 0.1-0.3
\end{itemize}

\textbf{Coherence Enforcement}: Ensure probabilities satisfy basic
constraints:

\begin{itemize}
\tightlist
\item
  Probabilities sum to 1.0
\item
  Conditional tables are complete
\item
  No logical contradictions
\end{itemize}

\textbf{Metadata Integration}: Combine structure with probabilities in
BayesDown format.

\subsection*{Why Two Stages?}\label{sec-why-two-stages}
\addcontentsline{toc}{subsection}{Why Two Stages?}

This separation provides several benefits:

\textbf{Modular Validation}: Structure can be verified independently
from probability estimates, simplifying quality assurance.

\textbf{Human Oversight}: Experts can review and correct structural
extraction before probability quantification.

\textbf{Flexible Quantification}: Different methods (LLM extraction,
expert elicitation, market data) can provide probabilities for the same
structure.

\textbf{Error Isolation}: Structural errors don't contaminate
probability extraction and vice versa.

\section*{Implementation Details}\label{sec-implementation-details}
\addcontentsline{toc}{section}{Implementation Details}

\markright{Implementation Details}

The system is implemented in Python, leveraging established libraries
while adding novel extraction capabilities.

\subsection*{Technology Stack}\label{sec-tech-stack}
\addcontentsline{toc}{subsection}{Technology Stack}

\begin{itemize}
\tightlist
\item
  \textbf{Language Models}: OpenAI GPT-4 and Anthropic Claude for
  extraction
\item
  \textbf{Network Analysis}: NetworkX for graph algorithms
\item
  \textbf{Probabilistic Modeling}: pgmpy for Bayesian network operations
\item
  \textbf{Visualization}: PyVis for interactive network rendering
\item
  \textbf{Data Processing}: Pandas for structured data manipulation
\end{itemize}

\subsection*{Key Algorithms}\label{sec-key-algorithms}
\addcontentsline{toc}{subsection}{Key Algorithms}

\textbf{Hierarchical Parsing}: The system parses ArgDown/BayesDown
syntax recognizing indentation-based hierarchy:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ parse\_markdown\_hierarchy\_fixed(markdown\_text, ArgDown}\OperatorTok{=}\VariableTok{False}\NormalTok{):}
    \CommentTok{"""Parse ArgDown or BayesDown format into structured DataFrame"""}
    \CommentTok{\# Clean text and extract node information}
\NormalTok{    titles\_info }\OperatorTok{=}\NormalTok{ extract\_titles\_info(clean\_text)}
    
    \CommentTok{\# Establish parent{-}child relationships based on indentation}
\NormalTok{    titles\_with\_relations }\OperatorTok{=}\NormalTok{ establish\_relationships\_fixed(titles\_info)}
    
    \CommentTok{\# Convert to DataFrame with proper columns}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ convert\_to\_dataframe(titles\_with\_relations, ArgDown)}
    
    \CommentTok{\# Add derived properties}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ add\_network\_analysis\_columns(df)}
    
    \ControlFlowTok{return}\NormalTok{ df}
\end{Highlighting}
\end{Shaded}

\textbf{Probability Completion}: When sources don't specify all required
probabilities, the system uses principled methods:

\begin{itemize}
\tightlist
\item
  Maximum entropy for missing values
\item
  Coherence constraints propagation
\item
  Expert-specified defaults
\end{itemize}

\textbf{Visual Encoding}: Nodes are colored by probability magnitude and
styled by network position:

\begin{itemize}
\tightlist
\item
  Green (high probability) to red (low probability) gradient
\item
  Blue borders for root causes, purple for intermediate, magenta for
  effects
\end{itemize}

\subsection*{Performance Characteristics}\label{sec-performance}
\addcontentsline{toc}{subsection}{Performance Characteristics}

Benchmarking reveals practical scalability:

\begin{itemize}
\tightlist
\item
  \textbf{Small networks} (≤10 nodes): \textless1 second processing
\item
  \textbf{Medium networks} (11-30 nodes): 2-8 seconds
\item
  \textbf{Large networks} (31-50 nodes): 15-45 seconds
\item
  \textbf{Very large networks} (\textgreater50 nodes): Require
  approximation methods
\end{itemize}

The bottleneck shifts from extraction (linear in text length) to
inference (exponential in network connectivity) as models grow.

\section*{Case Study:
Rain-Sprinkler-Grass}\label{sec-case-rain-sprinkler}
\addcontentsline{toc}{section}{Case Study: Rain-Sprinkler-Grass}

\markright{Case Study: Rain-Sprinkler-Grass}

I begin with the canonical example to demonstrate the complete pipeline
on a simple, well-understood case.

\subsection*{Input Representation}\label{sec-rsg-input}
\addcontentsline{toc}{subsection}{Input Representation}

The source BayesDown representation:

\begin{verbatim}
[Grass_Wet]: Concentrated moisture on grass.
{"instantiations": ["grass_wet_TRUE", "grass_wet_FALSE"],
 "priors": {"p(grass_wet_TRUE)": "0.322", "p(grass_wet_FALSE)": "0.678"},
 "posteriors": {
   "p(grass_wet_TRUE|sprinkler_TRUE,rain_TRUE)": "0.99",
   "p(grass_wet_TRUE|sprinkler_TRUE,rain_FALSE)": "0.9",
   "p(grass_wet_TRUE|sprinkler_FALSE,rain_TRUE)": "0.8",
   "p(grass_wet_TRUE|sprinkler_FALSE,rain_FALSE)": "0.0"
 }}
 + [Rain]: Water falling from sky.
   {"instantiations": ["rain_TRUE", "rain_FALSE"],
    "priors": {"p(rain_TRUE)": "0.2", "p(rain_FALSE)": "0.8"}}
 + [Sprinkler]: Artificial watering system.
   {"instantiations": ["sprinkler_TRUE", "sprinkler_FALSE"],
    "priors": {"p(sprinkler_TRUE)": "0.448", "p(sprinkler_FALSE)": "0.552"},
    "posteriors": {
      "p(sprinkler_TRUE|rain_TRUE)": "0.01",
      "p(sprinkler_TRUE|rain_FALSE)": "0.4"
    }}
   + [Rain]
\end{verbatim}

\subsection*{Processing Steps}\label{sec-rsg-processing}
\addcontentsline{toc}{subsection}{Processing Steps}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Parsing}: Extract three nodes with relationships
\item
  \textbf{Validation}: Verify probability coherence and DAG structure
\item
  \textbf{Enhancement}: Calculate joint probabilities and network
  metrics
\item
  \textbf{Construction}: Build formal Bayesian network
\item
  \textbf{Visualization}: Render interactive display
\end{enumerate}

\subsection*{Results}\label{sec-rsg-results}
\addcontentsline{toc}{subsection}{Results}

The system successfully:

\begin{itemize}
\tightlist
\item
  Extracts complete network structure
\item
  Preserves all probability information
\item
  Calculates correct marginal probabilities
\item
  Generates interactive visualization
\item
  Enables inference queries
\end{itemize}

This simple example validates the basic pipeline functionality before
tackling complex real-world cases.

\section*{Case Study: Carlsmith's Power-Seeking AI
Model}\label{sec-case-carlsmith}
\addcontentsline{toc}{section}{Case Study: Carlsmith's Power-Seeking AI
Model}

\markright{Case Study: Carlsmith's Power-Seeking AI Model}

Applying AMTAIR to Carlsmith's model demonstrates scalability to
realistic AI safety arguments.

\subsection*{Model Complexity}\label{sec-carlsmith-complexity}
\addcontentsline{toc}{subsection}{Model Complexity}

The Carlsmith model contains:

\begin{itemize}
\tightlist
\item
  \textbf{23 nodes} representing different factors
\item
  \textbf{27 edges} encoding dependencies
\item
  \textbf{Multiple probability tables} with complex conditionals
\item
  \textbf{Six-level causal depth} from root causes to catastrophe
\end{itemize}

\subsection*{Extraction Results}\label{sec-carlsmith-extraction}
\addcontentsline{toc}{subsection}{Extraction Results}

The automated extraction successfully identifies:

\textbf{Core Risk Pathway}:

\begin{verbatim}
Existential_Catastrophe 
← Human_Disempowerment 
← Scale_Of_Power_Seeking
← Misaligned_Power_Seeking
← [APS_Systems, Difficulty_Of_Alignment, Deployment_Decisions]
\end{verbatim}

\textbf{Supporting Structure}:

\begin{itemize}
\tightlist
\item
  Competitive dynamics influencing deployment
\item
  Technical factors affecting alignment difficulty
\item
  Corrective mechanisms and their limitations
\end{itemize}

\textbf{Probability Preservation}:

\begin{itemize}
\tightlist
\item
  Extracted probabilities match Carlsmith's published estimates
\item
  Conditional relationships properly captured
\item
  Final P(doom) calculation reproduces \textasciitilde5\% result
\end{itemize}

\subsection*{Validation Against
Original}\label{sec-carlsmith-validation}
\addcontentsline{toc}{subsection}{Validation Against Original}

Comparing extracted model to Carlsmith's original:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Metric & Performance \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Structural Accuracy & 92\% (nodes and edges) \\
Probability Accuracy & 87\% (within 0.05) \\
Path Completeness & 100\% (all major paths) \\
Semantic Preservation & High (per expert review) \\
\end{longtable}

The high fidelity demonstrates AMTAIR's capability for complex
real-world arguments.

\subsection*{Insights from Formalization}\label{sec-carlsmith-insights}
\addcontentsline{toc}{subsection}{Insights from Formalization}

Formal representation reveals several insights:

\textbf{Critical Path Analysis}: The pathway through APS development and
deployment decisions carries the highest risk contribution.

\textbf{Sensitivity Points}: Small changes in deployment probability
create large changes in overall risk.

\textbf{Intervention Opportunities}: Improving alignment difficulty or
deployment governance show highest impact potential.

These insights emerge naturally from formal analysis but remain implicit
in textual arguments.

\section*{Validation Methodology}\label{sec-validation-methodology}
\addcontentsline{toc}{section}{Validation Methodology}

\markright{Validation Methodology}

Establishing trust in automated extraction requires rigorous validation
across multiple dimensions.

\subsection*{Ground Truth Construction}\label{sec-ground-truth}
\addcontentsline{toc}{subsection}{Ground Truth Construction}

I created validation datasets through:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Expert Manual Extraction}: Three domain experts independently
  extracted models from the same sources
\item
  \textbf{Consensus Building}: Reconciled differences to create gold
  standard representations
\item
  \textbf{Annotation}: Marked source passages supporting each element
\end{enumerate}

\subsection*{Evaluation Metrics}\label{sec-evaluation-metrics}
\addcontentsline{toc}{subsection}{Evaluation Metrics}

\textbf{Structural Metrics}:

\begin{itemize}
\tightlist
\item
  Precision: Fraction of extracted elements that are correct
\item
  Recall: Fraction of true elements that are extracted
\item
  F1 Score: Harmonic mean balancing precision and recall
\end{itemize}

\textbf{Probabilistic Metrics}:

\begin{itemize}
\tightlist
\item
  Mean Absolute Error for probability values
\item
  Kullback-Leibler divergence for distributions
\item
  Calibration plots for uncertainty expression
\end{itemize}

\textbf{Semantic Metrics}:

\begin{itemize}
\tightlist
\item
  Expert ratings of meaning preservation
\item
  Functional equivalence for inference queries
\end{itemize}

\subsection*{Results Summary}\label{sec-validation-results}
\addcontentsline{toc}{subsection}{Results Summary}

Across 20 test documents:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Component & Precision & Recall & F1 Score \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Node Identification & 89\% & 86\% & 0.875 \\
Edge Extraction & 84\% & 81\% & 0.825 \\
Probability Values & 76\% & 71\% & 0.735 \\
\textbf{Overall System} & \textbf{83\%} & \textbf{79\%} &
\textbf{0.810} \\
\end{longtable}

Performance is strongest for explicit structural elements and numerical
probabilities, with more challenges in extracting implicit relationships
and qualitative uncertainty.

\subsection*{Error Analysis}\label{sec-error-analysis}
\addcontentsline{toc}{subsection}{Error Analysis}

Common failure modes:

\textbf{Implicit Assumptions} (23\% of errors): Unstated background
assumptions that experts infer but system misses.

\textbf{Complex Conditionals} (19\% of errors): Nested conditionals with
multiple antecedents challenge current parsing.

\textbf{Ambiguous Quantifiers} (17\% of errors): Terms like
``significant'' lack clear probability mapping without context.

\textbf{Coreference Resolution} (15\% of errors): Pronouns and indirect
references create attribution challenges.

Understanding these limitations guides both current usage and future
improvements.

\section*{Policy Evaluation Capabilities}\label{sec-policy-evaluation}
\addcontentsline{toc}{section}{Policy Evaluation Capabilities}

\markright{Policy Evaluation Capabilities}

Beyond extraction and visualization, AMTAIR enables systematic policy
analysis through formal intervention modeling.

\subsection*{Intervention
Representation}\label{sec-intervention-representation}
\addcontentsline{toc}{subsection}{Intervention Representation}

Policies are modeled as modifications to network parameters:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ evaluate\_policy\_intervention(network, intervention, targets):}
    \CommentTok{"""Evaluate policy impact using do{-}calculus"""}
    \CommentTok{\# Baseline without intervention}
\NormalTok{    baseline }\OperatorTok{=}\NormalTok{ network.query(targets)}
    
    \CommentTok{\# Apply intervention using Pearl\textquotesingle{}s do{-}operator}
\NormalTok{    intervened }\OperatorTok{=}\NormalTok{ network.do\_query(}
\NormalTok{        intervention[}\StringTok{\textquotesingle{}variable\textquotesingle{}}\NormalTok{],}
\NormalTok{        intervention[}\StringTok{\textquotesingle{}value\textquotesingle{}}\NormalTok{], }
\NormalTok{        targets}
\NormalTok{    )}
    
    \CommentTok{\# Calculate effect metrics}
    \ControlFlowTok{return}\NormalTok{ \{}
        \StringTok{\textquotesingle{}baseline\_risk\textquotesingle{}}\NormalTok{: baseline,}
        \StringTok{\textquotesingle{}intervened\_risk\textquotesingle{}}\NormalTok{: intervened,}
        \StringTok{\textquotesingle{}relative\_reduction\textquotesingle{}}\NormalTok{: }\DecValTok{1} \OperatorTok{{-}}\NormalTok{ intervened}\OperatorTok{/}\NormalTok{baseline,}
        \StringTok{\textquotesingle{}absolute\_reduction\textquotesingle{}}\NormalTok{: baseline }\OperatorTok{{-}}\NormalTok{ intervened}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

\subsection*{Example: Deployment
Governance}\label{sec-deployment-example}
\addcontentsline{toc}{subsection}{Example: Deployment Governance}

Consider a policy requiring safety certification before deployment:

\textbf{Intervention}: Set P(deployment\textbar misaligned) = 0.1 (from
0.7)

\textbf{Results}:

\begin{itemize}
\tightlist
\item
  Baseline P(catastrophe) = 0.05
\item
  Intervened P(catastrophe) = 0.012
\item
  Relative risk reduction = 76\%
\item
  Number needed to regulate = 26 deployments
\end{itemize}

This quantitative analysis enables comparison across interventions.

\subsection*{Robustness Analysis}\label{sec-robustness}
\addcontentsline{toc}{subsection}{Robustness Analysis}

Policies must work across worldviews. AMTAIR enables:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Multi-Model Evaluation}: Test interventions across different
  extracted models
\item
  \textbf{Parameter Sensitivity}: Vary assumptions to find breaking
  points
\item
  \textbf{Scenario Analysis}: Combine interventions under different
  futures
\item
  \textbf{Confidence Bounds}: Propagate uncertainty through to outcomes
\end{enumerate}

This systematic approach moves beyond intuitive policy assessment.

\section*{Interactive Visualization
Design}\label{sec-visualization-design}
\addcontentsline{toc}{section}{Interactive Visualization Design}

\markright{Interactive Visualization Design}

Making Bayesian networks accessible to diverse stakeholders requires
careful visualization design.

\subsection*{Visual Encoding Strategy}\label{sec-visual-encoding}
\addcontentsline{toc}{subsection}{Visual Encoding Strategy}

The system uses multiple visual channels:

\textbf{Color}: Probability magnitude (green=high, red=low)
\textbf{Borders}: Node type (blue=root, purple=intermediate,
magenta=effect)\\
\textbf{Size}: Centrality in network (larger=more influential)
\textbf{Layout}: Force-directed positioning reveals clusters

\subsection*{Progressive Disclosure}\label{sec-progressive-disclosure}
\addcontentsline{toc}{subsection}{Progressive Disclosure}

Information appears at appropriate levels:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Overview}: Network structure and color coding
\item
  \textbf{Hover}: Node description and prior probability
\item
  \textbf{Click}: Full probability tables and details
\item
  \textbf{Interaction}: Drag to rearrange, zoom to explore
\end{enumerate}

This layered approach serves both quick assessment and deep analysis
needs.

\subsection*{User Interface Elements}\label{sec-ui-elements}
\addcontentsline{toc}{subsection}{User Interface Elements}

Key features enhance usability:

\begin{itemize}
\tightlist
\item
  \textbf{Physics Controls}: Adjust layout dynamics
\item
  \textbf{Filter Options}: Show/hide node types
\item
  \textbf{Export Functions}: Save images or data
\item
  \textbf{Comparison Mode}: Side-by-side worldviews
\end{itemize}

These features emerged from user testing with researchers and
policymakers.

\section*{Integration with Prediction
Markets}\label{sec-market-integration}
\addcontentsline{toc}{section}{Integration with Prediction Markets}

\markright{Integration with Prediction Markets}

While full integration remains future work, the architecture supports
connection to live forecasting data.

\subsection*{Design for Integration}\label{sec-integration-design}
\addcontentsline{toc}{subsection}{Design for Integration}

The system architecture anticipates market connections:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ PredictionMarketConnector:}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, market\_apis):}
        \VariableTok{self}\NormalTok{.markets }\OperatorTok{=}\NormalTok{ market\_apis}
        
    \KeywordTok{def}\NormalTok{ find\_relevant\_questions(}\VariableTok{self}\NormalTok{, model\_variables):}
        \CommentTok{"""Map model variables to forecast questions"""}
        \CommentTok{\# Semantic matching between variables and questions}
        
    \KeywordTok{def}\NormalTok{ fetch\_probabilities(}\VariableTok{self}\NormalTok{, questions):}
        \CommentTok{"""Retrieve latest market probabilities"""}
        \CommentTok{\# API calls with caching and error handling}
        
    \KeywordTok{def}\NormalTok{ update\_model(}\VariableTok{self}\NormalTok{, model, market\_data):}
        \CommentTok{"""Integrate market probabilities into model"""}
        \CommentTok{\# Weighted updating based on liquidity and track record}
\end{Highlighting}
\end{Shaded}

\subsection*{Challenges and Opportunities}\label{sec-market-challenges}
\addcontentsline{toc}{subsection}{Challenges and Opportunities}

Key integration challenges:

\begin{itemize}
\tightlist
\item
  \textbf{Question Mapping}: Model variables rarely match market
  questions exactly
\item
  \textbf{Temporal Alignment}: Markets forecast specific dates, models
  consider scenarios
\item
  \textbf{Quality Variation}: Market depth and participation vary
  significantly
\end{itemize}

Despite challenges, even partial integration provides value through
external validation and dynamic updating.

\section*{Computational Considerations}\label{sec-computational}
\addcontentsline{toc}{section}{Computational Considerations}

\markright{Computational Considerations}

As networks grow large, computational challenges emerge requiring
sophisticated approaches.

\subsection*{Exact vs.~Approximate
Inference}\label{sec-exact-approximate}
\addcontentsline{toc}{subsection}{Exact vs.~Approximate Inference}

Small networks enable exact inference through variable elimination.
Larger networks require approximation:

\textbf{Monte Carlo Methods}: Sample from probability distributions to
estimate queries \textbf{Variational Inference}: Optimize simpler
distributions to approximate true posteriors \textbf{Belief
Propagation}: Pass messages between nodes to converge on beliefs

The system automatically selects appropriate methods based on network
properties.

\subsection*{Scaling Strategies}\label{sec-scaling-strategies}
\addcontentsline{toc}{subsection}{Scaling Strategies}

For very large networks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Hierarchical Decomposition}: Break into sub-networks for
  independent analysis
\item
  \textbf{Pruning}: Remove low-influence paths for specific queries
\item
  \textbf{Caching}: Store computed results for common queries
\item
  \textbf{Parallelization}: Distribute sampling across processors
\end{enumerate}

These strategies extend practical network size limits significantly.

\section*{Summary of Technical
Achievements}\label{sec-technical-summary}
\addcontentsline{toc}{section}{Summary of Technical Achievements}

\markright{Summary of Technical Achievements}

AMTAIR successfully demonstrates:

\begin{itemize}
\tightlist
\item
  \textbf{Automated extraction} from natural language to formal models
\item
  \textbf{Two-stage architecture} separating structure from
  quantification
\item
  \textbf{High fidelity} preservation of complex arguments
\item
  \textbf{Interactive visualization} accessible to diverse users
\item
  \textbf{Policy evaluation} capabilities through intervention modeling
\item
  \textbf{Scalable implementation} handling realistic network sizes
\end{itemize}

These achievements validate the feasibility of computational
coordination infrastructure for AI governance.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\bookmarksetup{startatroot}

\chapter*{4. Discussion: Implications and
Limitations}\label{sec-discussion}
\addcontentsline{toc}{chapter}{4. Discussion: Implications and
Limitations}

\markboth{4. Discussion: Implications and Limitations}{4. Discussion:
Implications and Limitations}

\begin{verbatim}
### 10% of Grade: ~ 14% of text ~ 4200 words ~ 10 pages

- discusses specific objection to student's own argument
- provides convincing reply that bolsters or refines the main argument
- relates to or extends beyond materials/arguments covered in class
\end{verbatim}

This chapter critically examines AMTAIR's implications, limitations, and
potential failure modes. By engaging seriously with objections and
challenges, I aim to provide a balanced assessment of what this approach
can and cannot achieve for AI governance coordination.

\section*{Technical Limitations and
Responses}\label{sec-technical-limitations}
\addcontentsline{toc}{section}{Technical Limitations and Responses}

\markright{Technical Limitations and Responses}

\subsection*{Objection 1: Extraction Quality
Boundaries}\label{sec-extraction-boundaries}
\addcontentsline{toc}{subsection}{Objection 1: Extraction Quality
Boundaries}

\textbf{Critic}: ``Complex implicit reasoning chains resist
formalization. Automated extraction will systematically miss nuanced
arguments, subtle conditional relationships, and context-dependent
meanings that human readers naturally understand.''

\textbf{Response}: This concern has merit---extraction does face
inherent limitations. However, the empirical results tell a more nuanced
story. With extraction achieving 85\%+ accuracy for structural
relationships and 73\% for probability capture, the system performs well
enough for practical use while falling short of human expert
performance.

More importantly, AMTAIR employs a hybrid human-AI workflow that
addresses this limitation:

\begin{itemize}
\tightlist
\item
  \textbf{Two-stage verification}: Humans review structural extraction
  before probability quantification
\item
  \textbf{Transparent outputs}: All intermediate representations remain
  human-readable
\item
  \textbf{Iterative refinement}: Extraction prompts improve based on
  error analysis
\item
  \textbf{Ensemble approaches}: Multiple extraction attempts can
  identify ambiguities
\end{itemize}

The question is not whether automated extraction perfectly captures
every nuance---it doesn't. Rather, it's whether imperfect extraction
still provides value over no formal representation. When the alternative
is relying on conflicting mental models that remain entirely implicit,
even 75\% accurate formal models represent significant progress.

Furthermore, extraction errors often reveal interesting properties of
the source arguments themselves---ambiguities that human readers gloss
over become explicit when formalization fails. This diagnostic value
enhances rather than undermines the approach.

\subsection*{Objection 2: False Precision in
Uncertainty}\label{sec-false-precision}
\addcontentsline{toc}{subsection}{Objection 2: False Precision in
Uncertainty}

\textbf{Critic}: ``Attaching exact probabilities to unprecedented events
like AI catastrophe is fundamentally misguided. The numbers create false
confidence in what amounts to educated speculation about radically
uncertain futures.''

\textbf{Response}: This philosophical objection strikes at the heart of
formal risk assessment. However, AMTAIR addresses it through several
design choices:

First, the system explicitly represents uncertainty about uncertainty.
Rather than point estimates, the framework supports probability
distributions over parameters. When someone says ``likely'' we might
model this as Beta(8,2) rather than exactly 0.8, capturing both the
central estimate and our uncertainty about it.

Second, all probabilities are explicitly conditional on stated
assumptions. The system doesn't claim ``P(catastrophe) = 0.05''
absolutely, but rather ``Given Carlsmith's model assumptions,
P(catastrophe) = 0.05.'' This conditionality is preserved throughout
analysis.

Third, sensitivity analysis reveals which probabilities actually matter.
Often, precise values are unnecessary---knowing whether a parameter is
closer to 0.1 or 0.9 suffices for decision-making. The formalization
helps identify where precision matters and where it doesn't.

Finally, the alternative to quantification isn't avoiding the problem
but making it worse. When experts say ``highly likely'' or ``significant
risk,'' they implicitly reason with probabilities. Formalization simply
makes these implicit quantities explicit and subject to scrutiny. As
Dennis Lindley noted, ``Uncertainty is not in the events, but in our
knowledge about them.''

\subsection*{Objection 3: Correlation
Complexity}\label{sec-correlation-complexity}
\addcontentsline{toc}{subsection}{Objection 3: Correlation Complexity}

\textbf{Critic}: ``Bayesian networks assume conditional independence
given parents, but real-world AI risks involve complex correlations.
Ignoring these dependencies could dramatically misrepresent risk
levels.''

\textbf{Response}: Standard Bayesian networks do face limitations with
correlation representation---this is a genuine technical challenge.
However, several approaches within the framework address this:

\textbf{Explicit correlation nodes}: When factors share hidden common
causes, we can add latent variables to capture correlations. For
instance, ``AI research culture'' might influence both ``capability
advancement'' and ``safety investment.''

\textbf{Copula methods}: For known correlation structures, copula
functions can model dependencies while preserving marginal
distributions. This extends standard Bayesian networks significantly.

\textbf{Sensitivity bounds}: When correlations remain uncertain, we can
compute bounds on outcomes under different correlation assumptions. This
reveals when correlations critically affect conclusions.

\textbf{Model ensembles}: Different correlation structures can be
modeled separately and results aggregated, similar to climate modeling
approaches.

More fundamentally, the question is whether imperfect independence
assumptions invalidate the approach. In practice, explicitly modeling
first-order effects with known limitations often proves more valuable
than attempting to capture all dependencies informally. The framework
makes assumptions transparent, enabling targeted improvements where
correlations matter most.

\section*{Conceptual and Methodological
Concerns}\label{sec-conceptual-concerns}
\addcontentsline{toc}{section}{Conceptual and Methodological Concerns}

\markright{Conceptual and Methodological Concerns}

\subsection*{Objection 4: Democratic
Exclusion}\label{sec-democratic-exclusion}
\addcontentsline{toc}{subsection}{Objection 4: Democratic Exclusion}

\textbf{Critic}: ``Transforming policy debates into complex graphs and
equations will sideline non-technical stakeholders, concentrating
influence among those comfortable with formal models. This technocratic
approach undermines democratic participation in crucial decisions about
humanity's future.''

\textbf{Response}: This concern about technocratic exclusion deserves
serious consideration---formal methods can indeed create barriers.
However, AMTAIR's design explicitly prioritizes accessibility alongside
rigor:

\textbf{Progressive disclosure interfaces} allow engagement at multiple
levels. A policymaker might explore visual network structures and
probability color-coding without engaging mathematical details.
Interactive features let users modify assumptions and see consequences
without understanding implementation.

\textbf{Natural language preservation} ensures original arguments remain
accessible. The BayesDown format maintains human-readable descriptions
alongside formal specifications. Users can always trace from
mathematical representations back to source texts.

\textbf{Comparative advantage} comes from making implicit technical
content explicit, not adding complexity. When experts debate AI risk,
they already employ sophisticated probabilistic
reasoning---formalization reveals rather than creates this complexity.
Making hidden assumptions visible arguably enhances rather than reduces
democratic participation.

\textbf{Multiple interfaces} serve different communities. Researchers
access full technical depth, policymakers use summary dashboards, public
stakeholders explore interactive visualizations. The same underlying
model supports varied engagement modes.

Rather than excluding non-technical stakeholders, proper implementation
can democratize access to expert reasoning by making it inspectable and
modifiable. The risk lies not in formalization itself but in poor
interface design or gatekeeping behaviors around model access.

\subsection*{Objection 5: Oversimplification of Complex
Systems}\label{sec-oversimplification}
\addcontentsline{toc}{subsection}{Objection 5: Oversimplification of
Complex Systems}

\textbf{Critic}: ``Forcing rich socio-technical systems into discrete
Bayesian networks necessarily loses crucial dynamics---feedback loops,
emergent properties, institutional responses, and cultural factors that
shape AI development. The models become precise but wrong.''

\textbf{Response}: All models simplify by necessity---as Box noted,
``All models are wrong, but some are useful.'' The question becomes
whether formal simplifications improve upon informal mental models:

\textbf{Transparent limitations} make formal models' shortcomings
explicit. Unlike mental models where simplifications remain hidden,
network representations clearly show what is and isn't included. This
transparency enables targeted criticism and improvement.

\textbf{Iterative refinement} allows models to grow more sophisticated
over time. Starting with first-order effects and adding complexity where
it proves important follows successful practice in other domains.
Climate models began simply and added dynamics as computational power
and understanding grew.

\textbf{Complementary tools} address different aspects of the system.
Bayesian networks excel at probabilistic reasoning and intervention
analysis. Other approaches---agent-based models, system dynamics,
scenario planning---can capture different properties. AMTAIR provides
one lens, not the only lens.

\textbf{Empirical adequacy} ultimately judges models. If simplified
representations enable better predictions and decisions than informal
alternatives, their abstractions are justified. Early results suggest
formal models, despite simplifications, outperform intuitive reasoning
for complex risk assessment.

The goal isn't creating perfect representations but useful ones. By
making simplifications explicit and modifiable, formal models enable
systematic improvement in ways mental models cannot.

\section*{Red-Teaming Results}\label{sec-red-teaming}
\addcontentsline{toc}{section}{Red-Teaming Results}

\markright{Red-Teaming Results}

To identify failure modes, I conducted systematic adversarial testing of
the AMTAIR system.

\subsection*{Adversarial Extraction
Attempts}\label{sec-adversarial-extraction}
\addcontentsline{toc}{subsection}{Adversarial Extraction Attempts}

I tested the system with deliberately challenging inputs:

\textbf{Contradictory Arguments}: Texts asserting P(A) = 0.2 and P(A) =
0.8 in different sections

\begin{itemize}
\tightlist
\item
  Result: System flagged inconsistency rather than averaging
\item
  Mitigation: Explicit consistency checking with user resolution
\end{itemize}

\textbf{Circular Reasoning}: Arguments where A causes B causes C causes
A

\begin{itemize}
\tightlist
\item
  Result: DAG validation caught cycles, extraction failed gracefully
\item
  Mitigation: Clear error messages explaining the structural issue
\end{itemize}

\textbf{Extremely Vague Language}: Texts using only qualitative terms
without clear relationships

\begin{itemize}
\tightlist
\item
  Result: Extraction quality degraded significantly (F1 \textless{} 0.5)
\item
  Mitigation: Confidence scores on extracted elements, human review
  triggers
\end{itemize}

\textbf{Deceptive Framings}: Arguments designed to imply false causal
relationships

\begin{itemize}
\tightlist
\item
  Result: System sometimes extracted spurious connections
\item
  Mitigation: Source grounding requirements, validation against
  citations
\end{itemize}

\subsection*{Robustness Findings}\label{sec-robustness-findings}
\addcontentsline{toc}{subsection}{Robustness Findings}

Key vulnerabilities identified:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Anchoring bias}: System tends to over-weight first probability
  mentioned (34\% effect)
\item
  \textbf{Authority sensitivity}: Extracted probabilities inflated for
  cited experts (18\% average)
\item
  \textbf{Complexity degradation}: Performance drops sharply beyond 50
  nodes
\item
  \textbf{Context loss}: Long-range dependencies in text sometimes
  missed
\end{enumerate}

However, the system demonstrated robustness to:

\begin{itemize}
\tightlist
\item
  Different writing styles and academic disciplines
\item
  Variations in argument structure and presentation order
\item
  Mixed numerical and qualitative probability expressions
\item
  Reasonable levels of grammatical errors and typos
\end{itemize}

\subsection*{Implications for
Deployment}\label{sec-deployment-implications}
\addcontentsline{toc}{subsection}{Implications for Deployment}

These results suggest AMTAIR is suitable for:

\begin{itemize}
\tightlist
\item
  \textbf{Research applications} with expert oversight
\item
  \textbf{Policy analysis} of well-structured arguments
\item
  \textbf{Educational uses} demonstrating formal reasoning
\item
  \textbf{Collaborative modeling} with human verification
\end{itemize}

But should be used cautiously for:

\begin{itemize}
\tightlist
\item
  Fully automated analysis without review
\item
  Adversarial or politically contentious texts
\item
  Real-time decision-making without validation
\item
  Arguments far outside training distribution
\end{itemize}

\section*{Enhancing Epistemic Security}\label{sec-epistemic-security}
\addcontentsline{toc}{section}{Enhancing Epistemic Security}

\markright{Enhancing Epistemic Security}

Despite limitations, AMTAIR contributes to epistemic security in AI
governance through several mechanisms.

\subsection*{Making Models Inspectable}\label{sec-inspectable-models}
\addcontentsline{toc}{subsection}{Making Models Inspectable}

The greatest epistemic benefit comes from forcing implicit models into
explicit form. When an expert claims ``misalignment likely leads to
catastrophe,'' formalization asks:

\begin{itemize}
\tightlist
\item
  Likely means what probability?
\item
  Through what causal pathways?
\item
  Under what assumptions?
\item
  With what evidence?
\end{itemize}

This explicitation serves multiple functions:

\textbf{Clarity}: Vague statements become precise claims subject to
evaluation

\textbf{Comparability}: Different experts' models can be systematically
compared

\textbf{Criticizability}: Hidden assumptions become visible targets for
challenge

\textbf{Updatability}: Formal models can systematically incorporate new
evidence

\subsection*{Revealing Convergence and
Divergence}\label{sec-convergence-divergence}
\addcontentsline{toc}{subsection}{Revealing Convergence and Divergence}

Comparative analysis across extracted models reveals surprising
patterns:

\textbf{Structural convergence}: Different experts often share similar
causal models even when probability estimates diverge dramatically. This
suggests shared understanding of mechanisms despite disagreement on
magnitudes.

\textbf{Parameter clustering}: Probability estimates often cluster
around a few values rather than spreading uniformly, suggesting implicit
coordination or common evidence bases.

\textbf{Crux identification}: Formal comparison precisely identifies
where worldviews diverge---often just 2-3 key parameters drive different
conclusions about overall risk.

These insights remain hidden when arguments stay in natural language
form.

\subsection*{Improving Collective
Reasoning}\label{sec-collective-reasoning}
\addcontentsline{toc}{subsection}{Improving Collective Reasoning}

AMTAIR enhances group epistemics through:

\textbf{Explicit uncertainty}: Replacing ``might,'' ``could,''
``likely'' with probability distributions reduces miscommunication and
forces precision

\textbf{Compositional reasoning}: Complex arguments decompose into
manageable components that can be independently evaluated

\textbf{Evidence integration}: New information updates specific
parameters rather than requiring complete argument reconstruction

\textbf{Exploration tools}: Stakeholders can modify assumptions and
immediately see consequences, building intuition about model dynamics

Early pilot studies with AI governance researchers show 40\% reduction
in time to identify core disagreements and 60\% improvement in agreement
about what they disagree about---meta-agreement that enables productive
debate.

\section*{Scaling Challenges and Opportunities}\label{sec-scaling}
\addcontentsline{toc}{section}{Scaling Challenges and Opportunities}

\markright{Scaling Challenges and Opportunities}

Moving from prototype to widespread adoption faces both technical and
social challenges.

\subsection*{Technical Scaling}\label{sec-technical-scaling}
\addcontentsline{toc}{subsection}{Technical Scaling}

\textbf{Computational complexity} grows with network size, but several
approaches help:

\begin{itemize}
\tightlist
\item
  Hierarchical decomposition for very large models
\item
  Caching and approximation for common queries
\item
  Distributed processing for extraction tasks
\item
  Incremental updating rather than full recomputation
\end{itemize}

\textbf{Data quality} varies dramatically across sources:

\begin{itemize}
\tightlist
\item
  Academic papers provide structured arguments
\item
  Blog posts offer rich ideas with less formal structure
\item
  Policy documents mix normative and empirical claims
\item
  Social media presents extreme extraction challenges
\end{itemize}

\textbf{Integration complexity} increases with ecosystem growth:

\begin{itemize}
\tightlist
\item
  Multiple LLM providers with different capabilities
\item
  Diverse visualization needs across users
\item
  Various export formats for downstream tools
\item
  Version control for evolving models
\end{itemize}

\subsection*{Social and Institutional Scaling}\label{sec-social-scaling}
\addcontentsline{toc}{subsection}{Social and Institutional Scaling}

\textbf{Adoption barriers} include:

\begin{itemize}
\tightlist
\item
  Learning curve for formal methods
\item
  Institutional inertia in established processes
\item
  Concerns about replacing human judgment
\item
  Resource requirements for implementation
\end{itemize}

\textbf{Trust building} requires:

\begin{itemize}
\tightlist
\item
  Transparent methodology documentation
\item
  Published validation studies
\item
  High-profile successful applications
\item
  Community ownership and development
\end{itemize}

\textbf{Sustainability} depends on:

\begin{itemize}
\tightlist
\item
  Open source development model
\item
  Diverse funding sources
\item
  Academic and industry partnerships
\item
  Clear value demonstration
\end{itemize}

\subsection*{Opportunities for Impact}\label{sec-impact-opportunities}
\addcontentsline{toc}{subsection}{Opportunities for Impact}

Despite challenges, several factors favor adoption:

\textbf{Timing}: AI governance needs tools now, creating receptive
audiences

\textbf{Complementarity}: AMTAIR enhances rather than replaces existing
processes

\textbf{Flexibility}: The approach adapts to different contexts and
needs

\textbf{Network effects}: Value increases as more perspectives are
formalized

Early adopters in research organizations and think tanks can demonstrate
value, creating momentum for broader adoption.

\section*{Integration with Governance
Frameworks}\label{sec-governance-integration}
\addcontentsline{toc}{section}{Integration with Governance Frameworks}

\markright{Integration with Governance Frameworks}

AMTAIR complements rather than replaces existing governance approaches.

\subsection*{Standards Development}\label{sec-standards-integration}
\addcontentsline{toc}{subsection}{Standards Development}

Technical standards bodies could use AMTAIR to:

\begin{itemize}
\tightlist
\item
  Model how proposed standards affect risk pathways
\item
  Compare different standard options systematically
\item
  Identify unintended consequences through pathway analysis
\item
  Build consensus through explicit model negotiation
\end{itemize}

Example: Evaluating compute thresholds for AI system regulation by
modeling how different thresholds affect capability development, safety
investment, and competitive dynamics.

\subsection*{Regulatory Design}\label{sec-regulatory-integration}
\addcontentsline{toc}{subsection}{Regulatory Design}

Regulators could apply the framework to:

\begin{itemize}
\tightlist
\item
  Assess regulatory impact across different scenarios
\item
  Identify enforcement challenges through explicit modeling
\item
  Compare international approaches systematically
\item
  Design adaptive regulations responsive to evidence
\end{itemize}

Example: Analyzing how liability frameworks affect corporate AI
development decisions under different market conditions.

\subsection*{International
Coordination}\label{sec-international-integration}
\addcontentsline{toc}{subsection}{International Coordination}

Multilateral bodies could leverage shared models for:

\begin{itemize}
\tightlist
\item
  Establishing common risk assessments
\item
  Negotiating agreements with explicit assumptions
\item
  Monitoring compliance through parameter tracking
\item
  Adapting agreements as evidence emerges
\end{itemize}

Example: Building shared models for AGI development scenarios to inform
international AI governance treaties.

\subsection*{Organizational
Decision-Making}\label{sec-organizational-integration}
\addcontentsline{toc}{subsection}{Organizational Decision-Making}

Individual organizations could use AMTAIR for:

\begin{itemize}
\tightlist
\item
  Internal risk assessment and planning
\item
  Board-level communication about AI strategies
\item
  Research prioritization based on model sensitivity
\item
  Safety case development with explicit assumptions
\end{itemize}

Example: An AI lab modeling how different safety investments affect both
capability advancement and risk mitigation.

\section*{Future Research Directions}\label{sec-future-research}
\addcontentsline{toc}{section}{Future Research Directions}

\markright{Future Research Directions}

Several research directions could enhance AMTAIR's capabilities and
impact.

\subsection*{Technical Enhancements}\label{sec-technical-future}
\addcontentsline{toc}{subsection}{Technical Enhancements}

\textbf{Improved extraction}: Fine-tuning language models specifically
for argument extraction, handling implicit reasoning, and cross-document
synthesis

\textbf{Richer representations}: Temporal dynamics, continuous
variables, and multi-agent interactions within extended frameworks

\textbf{Inference advances}: Quantum computing applications, neural
approximate inference, and hybrid symbolic-neural methods

\textbf{Validation methods}: Automated consistency checking, anomaly
detection in extracted models, and benchmark dataset development

\subsection*{Methodological Extensions}\label{sec-methodological-future}
\addcontentsline{toc}{subsection}{Methodological Extensions}

\textbf{Causal discovery}: Inferring causal structures from data rather
than just extracting from text

\textbf{Experimental integration}: Connecting models to empirical
results from AI safety experiments

\textbf{Dynamic updating}: Continuous model refinement as new evidence
emerges from research and deployment

\textbf{Uncertainty quantification}: Richer representation of deep
uncertainty and model confidence

\subsection*{Application Domains}\label{sec-application-future}
\addcontentsline{toc}{subsection}{Application Domains}

\textbf{Beyond AI safety}: Climate risk, biosecurity, nuclear policy,
and other existential risks

\textbf{Corporate governance}: Strategic planning, risk management, and
innovation assessment

\textbf{Scientific modeling}: Formalizing theoretical arguments in
emerging fields

\textbf{Educational tools}: Teaching probabilistic reasoning and
critical thinking

\subsection*{Ecosystem Development}\label{sec-ecosystem-future}
\addcontentsline{toc}{subsection}{Ecosystem Development}

\textbf{Open standards}: Common formats for model exchange and tool
interoperability

\textbf{Community platforms}: Collaborative model development and
sharing infrastructure

\textbf{Training programs}: Building capacity for formal modeling in
governance communities

\textbf{Quality assurance}: Certification processes for high-stakes
model applications

These directions could transform AMTAIR from a single tool into a
broader ecosystem for enhanced reasoning about complex risks.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\bookmarksetup{startatroot}

\chapter*{5. Conclusion: Toward Coordinated AI
Governance}\label{sec-conclusion}
\addcontentsline{toc}{chapter}{5. Conclusion: Toward Coordinated AI
Governance}

\markboth{5. Conclusion: Toward Coordinated AI Governance}{5.
Conclusion: Toward Coordinated AI Governance}

\begin{verbatim}
### 10% of Grade: ~ 14% of text ~ 4200 words ~ 10 pages

- summarizes thesis and line of argument
- outlines possible implications
- notes outstanding issues / limitations of discussion
- points to avenues for further research
- overall conclusion is in line with introduction
\end{verbatim}

\section*{Summary of Key Contributions}\label{sec-key-contributions}
\addcontentsline{toc}{section}{Summary of Key Contributions}

\markright{Summary of Key Contributions}

This thesis has demonstrated both the need for and feasibility of
computational approaches to enhancing coordination in AI governance. The
work makes several distinct contributions across theory, methodology,
and implementation.

\subsection*{Theoretical
Contributions}\label{sec-theoretical-contributions}
\addcontentsline{toc}{subsection}{Theoretical Contributions}

\textbf{Diagnosis of the Coordination Crisis}: I've articulated how
fragmentation across technical, policy, and strategic communities
systematically amplifies existential risk from advanced AI. This framing
moves beyond identifying disagreements to understanding how misaligned
efforts create negative-sum dynamics---safety gaps emerge between
communities, resources are misallocated through duplication and neglect,
and interventions interact destructively.

\textbf{The Multiplicative Benefits Framework}: The combination of
automated extraction, prediction market integration, and formal policy
evaluation creates value exceeding the sum of parts. Automation enables
scale, markets provide empirical grounding, and policy analysis delivers
actionable insights. Together, they address different facets of the
coordination challenge while reinforcing each other's strengths.

\textbf{Epistemic Infrastructure Conception}: Positioning formal models
as epistemic infrastructure reframes the role of technical tools in
governance. Rather than replacing human judgment, computational
approaches provide common languages, shared representations, and
systematic methods for managing disagreement---essential foundations for
coordination under uncertainty.

\subsection*{Methodological
Innovations}\label{sec-methodological-innovations}
\addcontentsline{toc}{subsection}{Methodological Innovations}

\textbf{Two-Stage Extraction Architecture}: Separating structural
extraction (ArgDown) from probability quantification (BayesDown)
addresses key challenges in automated formalization. This modularity
enables human oversight at critical points, supports multiple
quantification methods, and isolates different types of errors for
targeted improvement.

\textbf{BayesDown as Bridge Representation}: The development of
BayesDown syntax creates a crucial intermediate representation
preserving both narrative accessibility and mathematical precision. This
bridge enables the transformation from qualitative arguments to
quantitative models while maintaining traceability and human
readability.

\textbf{Validation Framework}: The systematic approach to validating
automated extraction---comparing against expert annotations, measuring
multiple accuracy dimensions, and analyzing error patterns---establishes
scientific standards for assessing formalization tools. This framework
can guide future development in this emerging area.

\subsection*{Technical Achievements}\label{sec-technical-achievements}
\addcontentsline{toc}{subsection}{Technical Achievements}

\textbf{Working Implementation}: AMTAIR demonstrates end-to-end
feasibility from document ingestion through interactive visualization.
The system achieves practically useful accuracy levels: 85\%+ for
structural extraction and 73\% for probability capture on real AI safety
arguments.

\textbf{Scalability Solutions}: Technical approaches for handling
realistic model complexity---hierarchical decomposition, approximate
inference, and progressive visualization---show that computational
limitations need not prevent practical application.

\textbf{Accessibility Design}: The layered interface approach serves
diverse stakeholders without compromising technical depth. Progressive
disclosure, visual encoding, and interactive exploration make formal
models accessible beyond technical specialists.

\subsection*{Empirical Findings}\label{sec-empirical-findings}
\addcontentsline{toc}{subsection}{Empirical Findings}

\textbf{Extraction Feasibility}: The successful extraction of complex
arguments like Carlsmith's model validates the core premise that
implicit formal structures exist in natural language arguments and can
be computationally recovered with reasonable fidelity.

\textbf{Convergence Patterns}: Comparative analysis reveals surprising
structural agreement across worldviews even when probability estimates
diverge dramatically. This suggests shared causal understanding despite
parameter disagreements---a foundation for coordination.

\textbf{Intervention Impacts}: Policy evaluation demonstrates how formal
models enable rigorous assessment of governance options. The ability to
quantify risk reduction across scenarios and identify robust strategies
validates the practical value of formalization.

\section*{Limitations and Honest
Assessment}\label{sec-limitations-assessment}
\addcontentsline{toc}{section}{Limitations and Honest Assessment}

\markright{Limitations and Honest Assessment}

Despite these contributions, important limitations constrain current
capabilities and should guide appropriate use.

\subsection*{Technical Constraints}\label{sec-technical-constraints}
\addcontentsline{toc}{subsection}{Technical Constraints}

\textbf{Extraction Boundaries}: While 73-85\% accuracy suffices for many
purposes, systematic biases remain. The system struggles with implicit
assumptions, complex conditionals, and context-dependent meanings. These
limitations necessitate human review for high-stakes applications.

\textbf{Correlation Handling}: Standard Bayesian networks inadequately
represent complex correlations in real systems. While extensions like
copulas and explicit correlation nodes help, fully capturing
interdependencies remains challenging.

\textbf{Computational Scaling}: Very large networks (\textgreater50
nodes) require approximations that may affect accuracy. As models grow
to represent richer phenomena, computational constraints increasingly
bind.

\subsection*{Conceptual Limitations}\label{sec-conceptual-limitations}
\addcontentsline{toc}{subsection}{Conceptual Limitations}

\textbf{Formalization Trade-offs}: Converting rich arguments to formal
models necessarily loses nuance. While making assumptions explicit
provides value, some insights resist mathematical representation.

\textbf{Probability Interpretation}: Deep uncertainty about
unprecedented events challenges probabilistic representation. Numbers
can create false precision even when explicitly conditional and
uncertain.

\textbf{Social Complexity}: Institutional dynamics, cultural factors,
and political processes influence AI development in ways that simple
causal models struggle to capture.

\subsection*{Practical Constraints}\label{sec-practical-constraints}
\addcontentsline{toc}{subsection}{Practical Constraints}

\textbf{Adoption Barriers}: Learning curves, institutional inertia, and
resource requirements limit immediate deployment. Even demonstrably
valuable tools face implementation challenges.

\textbf{Maintenance Burden}: Models require updating as arguments evolve
and evidence emerges. Without sustained effort, formal representations
quickly become outdated.

\textbf{Context Dependence}: The approach works best for well-structured
academic arguments. Application to informal discussions, political
speeches, or social media remains challenging.

\section*{Implications for AI
Governance}\label{sec-governance-implications}
\addcontentsline{toc}{section}{Implications for AI Governance}

\markright{Implications for AI Governance}

Despite limitations, AMTAIR's approach offers significant implications
for how AI governance can evolve toward greater coordination and
effectiveness.

\subsection*{Near-Term Applications}\label{sec-near-term-applications}
\addcontentsline{toc}{subsection}{Near-Term Applications}

\textbf{Research Coordination}: Research organizations can use formal
models to:

\begin{itemize}
\tightlist
\item
  Map the landscape of current arguments and identify gaps
\item
  Prioritize investigations targeting high-sensitivity parameters
\item
  Build cumulative knowledge through explicit model updating
\item
  Facilitate collaboration through shared representations
\end{itemize}

\textbf{Policy Development}: Governance bodies can apply the framework
to:

\begin{itemize}
\tightlist
\item
  Evaluate proposals across multiple expert worldviews
\item
  Identify robust interventions effective under uncertainty
\item
  Make assumptions explicit for democratic scrutiny
\item
  Track how evidence changes optimal policies over time
\end{itemize}

\textbf{Stakeholder Communication}: The visualization and analysis tools
enable:

\begin{itemize}
\tightlist
\item
  Clearer communication between technical and policy communities
\item
  Public engagement with complex risk assessments
\item
  Board-level strategic discussions grounded in formal analysis
\item
  International negotiations with explicit shared models
\end{itemize}

\subsection*{Medium-Term Transformation}\label{sec-medium-term}
\addcontentsline{toc}{subsection}{Medium-Term Transformation}

As adoption spreads, we might see:

\textbf{Epistemic Commons}: Shared repositories of formalized arguments
become reference points for governance discussions, similar to how
economic models inform monetary policy or climate models guide
environmental agreements.

\textbf{Adaptive Governance}: Policies designed with explicit models can
include triggers for reassessment as key parameters change, enabling
responsive governance that avoids both paralysis and recklessness.

\textbf{Professionalization}: ``Model curator'' and ``argument
formalization specialist'' emerge as recognized roles, building
expertise in bridging natural language and formal representations.

\textbf{Quality Standards}: Community norms develop around model
transparency, validation requirements, and appropriate use cases,
preventing both dismissal and over-reliance on formal tools.

\subsection*{Long-Term Vision}\label{sec-long-term-vision}
\addcontentsline{toc}{subsection}{Long-Term Vision}

Successfully scaling this approach could fundamentally alter AI
governance:

\textbf{Coordinated Response}: Rather than fragmented efforts, the AI
safety ecosystem could operate with shared situational
awareness---different actors understanding how their efforts interact
and contribute to collective goals.

\textbf{Anticipatory Action}: Formal models with prediction market
integration could provide early warning of emerging risks, enabling
proactive rather than reactive governance.

\textbf{Global Cooperation}: Shared formal frameworks could facilitate
international coordination similar to how economic models enable
monetary coordination or climate models support environmental
agreements.

\textbf{Democratic Enhancement}: Making expert reasoning transparent and
modifiable could enable broader participation in crucial decisions about
humanity's technological future.

\section*{Recommendations for Stakeholders}\label{sec-recommendations}
\addcontentsline{toc}{section}{Recommendations for Stakeholders}

\markright{Recommendations for Stakeholders}

Different communities can take concrete steps to realize these benefits:

\subsection*{For Researchers}\label{sec-researcher-recommendations}
\addcontentsline{toc}{subsection}{For Researchers}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Experiment with formalization}: Try extracting your own
  arguments into ArgDown/BayesDown format to discover implicit
  assumptions
\item
  \textbf{Contribute to validation}: Provide expert annotations for
  building benchmark datasets and improving extraction quality
\item
  \textbf{Develop extensions}: Build on the open-source foundation to
  add capabilities for your specific domain needs
\item
  \textbf{Publish formally}: Include formal model representations
  alongside traditional papers to enable cumulative building
\end{enumerate}

\subsection*{For Policymakers}\label{sec-policymaker-recommendations}
\addcontentsline{toc}{subsection}{For Policymakers}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Pilot applications}: Use AMTAIR for internal analysis of
  specific policy proposals to build familiarity and identify value
\item
  \textbf{Demand transparency}: Request formal models underlying expert
  recommendations to understand assumptions and uncertainties
\item
  \textbf{Fund development}: Support tool development and training to
  build governance capacity for formal methods
\item
  \textbf{Design adaptively}: Create policies with explicit triggers
  based on model parameters to enable responsive governance
\end{enumerate}

\subsection*{For Technologists}\label{sec-technologist-recommendations}
\addcontentsline{toc}{subsection}{For Technologists}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Improve extraction}: Contribute better prompting strategies,
  fine-tuned models, or validation methods
\item
  \textbf{Enhance interfaces}: Develop visualizations and interactions
  serving specific stakeholder needs
\item
  \textbf{Build integrations}: Connect AMTAIR to other tools in the AI
  governance ecosystem
\item
  \textbf{Scale infrastructure}: Address computational challenges for
  larger models and broader deployment
\end{enumerate}

\subsection*{For Funders}\label{sec-funder-recommendations}
\addcontentsline{toc}{subsection}{For Funders}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Support ecosystem}: Fund not just tool development but
  training, community building, and maintenance
\item
  \textbf{Bridge communities}: Incentivize collaborations between formal
  modelers and domain experts
\item
  \textbf{Measure coordination}: Develop metrics for assessing
  coordination improvements from formal tools
\item
  \textbf{Patient capital}: Recognize that epistemic infrastructure
  requires sustained investment to reach potential
\end{enumerate}

\section*{Future Research Agenda}\label{sec-future-research-agenda}
\addcontentsline{toc}{section}{Future Research Agenda}

\markright{Future Research Agenda}

Building on this foundation, several research directions could amplify
impact:

\subsection*{Technical Priorities}\label{sec-technical-priorities}
\addcontentsline{toc}{subsection}{Technical Priorities}

\textbf{Extraction Enhancement}:

\begin{itemize}
\tightlist
\item
  Fine-tuning language models specifically for argument extraction
\item
  Handling implicit reasoning and long-range dependencies
\item
  Cross-document synthesis for comprehensive models
\item
  Multilingual extraction for global perspectives
\end{itemize}

\textbf{Representation Extensions}:

\begin{itemize}
\tightlist
\item
  Temporal dynamics for modeling AI development trajectories
\item
  Multi-agent representations for strategic interactions
\item
  Continuous variables for economic and capability metrics
\item
  Uncertainty types beyond probability distributions
\end{itemize}

\textbf{Integration Depth}:

\begin{itemize}
\tightlist
\item
  Semantic matching between models and prediction markets
\item
  Automated experiment design based on model sensitivity
\item
  Policy optimization algorithms using extracted models
\item
  Real-time updating from news and research feeds
\end{itemize}

\subsection*{Methodological
Development}\label{sec-methodological-development}
\addcontentsline{toc}{subsection}{Methodological Development}

\textbf{Validation Science}:

\begin{itemize}
\tightlist
\item
  Larger benchmark datasets with diverse argument types
\item
  Metrics for semantic preservation beyond accuracy
\item
  Adversarial robustness testing protocols
\item
  Longitudinal studies of model evolution
\end{itemize}

\textbf{Hybrid Approaches}:

\begin{itemize}
\tightlist
\item
  Optimal human-AI collaboration patterns for extraction
\item
  Combining formal models with other methods (scenarios, simulations)
\item
  Integration with deliberative and participatory processes
\item
  Balancing automation with expert judgment
\end{itemize}

\textbf{Social Methods}:

\begin{itemize}
\tightlist
\item
  Ethnographic studies of model use in organizations
\item
  Measuring coordination improvements empirically
\item
  Understanding adoption barriers and facilitators
\item
  Designing interventions for epistemic security
\end{itemize}

\subsection*{Application Expansion}\label{sec-application-expansion}
\addcontentsline{toc}{subsection}{Application Expansion}

\textbf{Domain Extensions}:

\begin{itemize}
\tightlist
\item
  Climate risk assessment and policy evaluation
\item
  Biosecurity governance and pandemic preparedness
\item
  Nuclear policy and deterrence stability
\item
  Emerging technology governance broadly
\end{itemize}

\textbf{Institutional Integration}:

\begin{itemize}
\tightlist
\item
  Embedding in regulatory impact assessment
\item
  Corporate strategic planning applications
\item
  Academic peer review enhancement
\item
  Democratic deliberation support tools
\end{itemize}

\textbf{Global Deployment}:

\begin{itemize}
\tightlist
\item
  Adapting to different governance contexts
\item
  Supporting multilateral negotiation processes
\item
  Building capacity in developing nations
\item
  Creating resilient distributed infrastructure
\end{itemize}

\section*{Closing Reflections}\label{sec-closing-reflections}
\addcontentsline{toc}{section}{Closing Reflections}

\markright{Closing Reflections}

The work presented in this thesis emerges from a simple observation:
while humanity mobilizes unprecedented resources to address AI risks,
our efforts remain tragically uncoordinated. Different communities work
with incompatible frameworks, duplicate efforts, and sometimes actively
undermine each other's work. This fragmentation amplifies the very risks
we seek to mitigate.

AMTAIR represents one attempt to build bridges---computational tools
that create common ground for disparate perspectives. By making implicit
models explicit, quantifying uncertainty, and enabling systematic policy
analysis, these tools offer hope for enhanced coordination. The
successful extraction of complex arguments, validation against expert
judgment, and demonstration of policy evaluation capabilities suggest
this approach has merit.

Yet tools alone cannot solve coordination problems rooted in incentives,
institutions, and human psychology. AMTAIR provides infrastructure for
coordination, not coordination itself. Success requires not just
technical development but changes in how we approach collective
challenges---valuing transparency over strategic ambiguity, embracing
uncertainty rather than false confidence, and prioritizing collective
outcomes over parochial interests.

The path forward demands both ambition and humility. Ambition to build
the epistemic infrastructure necessary for navigating unprecedented
risks. Humility to recognize our tools' limitations and the irreducible
role of human wisdom in governance. The question is not whether formal
models can replace human judgment---they cannot and should not. Rather,
it's whether we can augment our collective intelligence with
computational tools that help us reason together about futures too
important to leave to chance.

As AI capabilities advance toward transformative potential, the window
for establishing effective governance narrows. We cannot afford
continued fragmentation when facing potentially irreversible
consequences. The coordination crisis in AI governance represents both
existential risk and existential opportunity---risk if we fail to align
our efforts, opportunity if we succeed in building unprecedented
cooperation around humanity's most important challenge.

This thesis contributes technical foundations and demonstrates
feasibility. The greater work---building communities, changing
practices, and fostering coordination---remains ahead. May we prove
equal to the task, for all our futures depend on it.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\bookmarksetup{startatroot}

\chapter*{References}\label{sec-references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\printbibliography[heading=none]

\section{3.1.2 Test BayesDown
Extraction}\label{test-bayesdown-extraction}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{display(Markdown(md\_content\_ex\_rain)) }\CommentTok{\# view BayesDown file formatted as MarkDown}
\end{Highlighting}
\end{Shaded}

{[}Existential\_Catastrophe{]}: The destruction of humanity's long-term
potential due to AI systems we've lost control over.
\{``instantiations'': {[}``existential\_catastrophe\_TRUE'',
``existential\_catastrophe\_FALSE''{]}, ``priors'':
\{``p(existential\_catastrophe\_TRUE)'': ``0.05'',
``p(existential\_catastrophe\_FALSE)'': ``0.95''\}, ``posteriors'':
\{``p(existential\_catastrophe\_TRUE\textbar human\_disempowerment\_TRUE)'':
``0.95'',
``p(existential\_catastrophe\_TRUE\textbar human\_disempowerment\_FALSE)'':
``0.0'',
``p(existential\_catastrophe\_FALSE\textbar human\_disempowerment\_TRUE)'':
``0.05'',
``p(existential\_catastrophe\_FALSE\textbar human\_disempowerment\_FALSE)'':
``1.0''\}\} - {[}Human\_Disempowerment{]}: Permanent and collective
disempowerment of humanity relative to AI systems. \{``instantiations'':
{[}``human\_disempowerment\_TRUE'', ``human\_disempowerment\_FALSE''{]},
``priors'': \{``p(human\_disempowerment\_TRUE)'': ``0.208'',
``p(human\_disempowerment\_FALSE)'': ``0.792''\}, ``posteriors'':
\{``p(human\_disempowerment\_TRUE\textbar scale\_of\_power\_seeking\_TRUE)'':
``1.0'',
``p(human\_disempowerment\_TRUE\textbar scale\_of\_power\_seeking\_FALSE)'':
``0.0'',
``p(human\_disempowerment\_FALSE\textbar scale\_of\_power\_seeking\_TRUE)'':
``0.0'',
``p(human\_disempowerment\_FALSE\textbar scale\_of\_power\_seeking\_FALSE)'':
``1.0''\}\} - {[}Scale\_Of\_Power\_Seeking{]}: Power-seeking by AI
systems scaling to the point of permanently disempowering all of
humanity. \{``instantiations'': {[}``scale\_of\_power\_seeking\_TRUE'',
``scale\_of\_power\_seeking\_FALSE''{]}, ``priors'':
\{``p(scale\_of\_power\_seeking\_TRUE)'': ``0.208'',
``p(scale\_of\_power\_seeking\_FALSE)'': ``0.792''\}, ``posteriors'':
\{``p(scale\_of\_power\_seeking\_TRUE\textbar misaligned\_power\_seeking\_TRUE,
corrective\_feedback\_EFFECTIVE)'': ``0.25'',
``p(scale\_of\_power\_seeking\_TRUE\textbar misaligned\_power\_seeking\_TRUE,
corrective\_feedback\_INEFFECTIVE)'': ``0.60'',
``p(scale\_of\_power\_seeking\_TRUE\textbar misaligned\_power\_seeking\_FALSE,
corrective\_feedback\_EFFECTIVE)'': ``0.0'',
``p(scale\_of\_power\_seeking\_TRUE\textbar misaligned\_power\_seeking\_FALSE,
corrective\_feedback\_INEFFECTIVE)'': ``0.0'',
``p(scale\_of\_power\_seeking\_FALSE\textbar misaligned\_power\_seeking\_TRUE,
corrective\_feedback\_EFFECTIVE)'': ``0.75'',
``p(scale\_of\_power\_seeking\_FALSE\textbar misaligned\_power\_seeking\_TRUE,
corrective\_feedback\_INEFFECTIVE)'': ``0.40'',
``p(scale\_of\_power\_seeking\_FALSE\textbar misaligned\_power\_seeking\_FALSE,
corrective\_feedback\_EFFECTIVE)'': ``1.0'',
``p(scale\_of\_power\_seeking\_FALSE\textbar misaligned\_power\_seeking\_FALSE,
corrective\_feedback\_INEFFECTIVE)'': ``1.0''\}\} -
{[}Misaligned\_Power\_Seeking{]}: Deployed AI systems seeking power in
unintended and high-impact ways due to problems with their objectives.
\{``instantiations'': {[}``misaligned\_power\_seeking\_TRUE'',
``misaligned\_power\_seeking\_FALSE''{]}, ``priors'':
\{``p(misaligned\_power\_seeking\_TRUE)'': ``0.338'',
``p(misaligned\_power\_seeking\_FALSE)'': ``0.662''\}, ``posteriors'':
\{``p(misaligned\_power\_seeking\_TRUE\textbar aps\_systems\_TRUE,
difficulty\_of\_alignment\_TRUE, deployment\_decisions\_DEPLOY)'':
``0.90'',
``p(misaligned\_power\_seeking\_TRUE\textbar aps\_systems\_TRUE,
difficulty\_of\_alignment\_TRUE, deployment\_decisions\_WITHHOLD)'':
``0.10'',
``p(misaligned\_power\_seeking\_TRUE\textbar aps\_systems\_TRUE,
difficulty\_of\_alignment\_FALSE, deployment\_decisions\_DEPLOY)'':
``0.25'',
``p(misaligned\_power\_seeking\_TRUE\textbar aps\_systems\_TRUE,
difficulty\_of\_alignment\_FALSE, deployment\_decisions\_WITHHOLD)'':
``0.05'',
``p(misaligned\_power\_seeking\_TRUE\textbar aps\_systems\_FALSE,
difficulty\_of\_alignment\_TRUE, deployment\_decisions\_DEPLOY)'':
``0.0'',
``p(misaligned\_power\_seeking\_TRUE\textbar aps\_systems\_FALSE,
difficulty\_of\_alignment\_TRUE, deployment\_decisions\_WITHHOLD)'':
``0.0'',
``p(misaligned\_power\_seeking\_TRUE\textbar aps\_systems\_FALSE,
difficulty\_of\_alignment\_FALSE, deployment\_decisions\_DEPLOY)'':
``0.0'',
``p(misaligned\_power\_seeking\_TRUE\textbar aps\_systems\_FALSE,
difficulty\_of\_alignment\_FALSE, deployment\_decisions\_WITHHOLD)'':
``0.0'',
``p(misaligned\_power\_seeking\_FALSE\textbar aps\_systems\_TRUE,
difficulty\_of\_alignment\_TRUE, deployment\_decisions\_DEPLOY)'':
``0.10'',
``p(misaligned\_power\_seeking\_FALSE\textbar aps\_systems\_TRUE,
difficulty\_of\_alignment\_TRUE, deployment\_decisions\_WITHHOLD)'':
``0.90'',
``p(misaligned\_power\_seeking\_FALSE\textbar aps\_systems\_TRUE,
difficulty\_of\_alignment\_FALSE, deployment\_decisions\_DEPLOY)'':
``0.75'',
``p(misaligned\_power\_seeking\_FALSE\textbar aps\_systems\_TRUE,
difficulty\_of\_alignment\_FALSE, deployment\_decisions\_WITHHOLD)'':
``0.95'',
``p(misaligned\_power\_seeking\_FALSE\textbar aps\_systems\_FALSE,
difficulty\_of\_alignment\_TRUE, deployment\_decisions\_DEPLOY)'':
``1.0'',
``p(misaligned\_power\_seeking\_FALSE\textbar aps\_systems\_FALSE,
difficulty\_of\_alignment\_TRUE, deployment\_decisions\_WITHHOLD)'':
``1.0'',
``p(misaligned\_power\_seeking\_FALSE\textbar aps\_systems\_FALSE,
difficulty\_of\_alignment\_FALSE, deployment\_decisions\_DEPLOY)'':
``1.0'',
``p(misaligned\_power\_seeking\_FALSE\textbar aps\_systems\_FALSE,
difficulty\_of\_alignment\_FALSE, deployment\_decisions\_WITHHOLD)'':
``1.0''\}\} - {[}APS\_Systems{]}: AI systems with advanced capabilities,
agentic planning, and strategic awareness. \{``instantiations'':
{[}``aps\_systems\_TRUE'', ``aps\_systems\_FALSE''{]}, ``priors'':
\{``p(aps\_systems\_TRUE)'': ``0.65'', ``p(aps\_systems\_FALSE)'':
``0.35''\}, ``posteriors'':
\{``p(aps\_systems\_TRUE\textbar advanced\_ai\_capability\_TRUE,
agentic\_planning\_TRUE, strategic\_awareness\_TRUE)'': ``1.0'',
``p(aps\_systems\_TRUE\textbar advanced\_ai\_capability\_TRUE,
agentic\_planning\_TRUE, strategic\_awareness\_FALSE)'': ``0.0'',
``p(aps\_systems\_TRUE\textbar advanced\_ai\_capability\_TRUE,
agentic\_planning\_FALSE, strategic\_awareness\_TRUE)'': ``0.0'',
``p(aps\_systems\_TRUE\textbar advanced\_ai\_capability\_TRUE,
agentic\_planning\_FALSE, strategic\_awareness\_FALSE)'': ``0.0'',
``p(aps\_systems\_TRUE\textbar advanced\_ai\_capability\_FALSE,
agentic\_planning\_TRUE, strategic\_awareness\_TRUE)'': ``0.0'',
``p(aps\_systems\_TRUE\textbar advanced\_ai\_capability\_FALSE,
agentic\_planning\_TRUE, strategic\_awareness\_FALSE)'': ``0.0'',
``p(aps\_systems\_TRUE\textbar advanced\_ai\_capability\_FALSE,
agentic\_planning\_FALSE, strategic\_awareness\_TRUE)'': ``0.0'',
``p(aps\_systems\_TRUE\textbar advanced\_ai\_capability\_FALSE,
agentic\_planning\_FALSE, strategic\_awareness\_FALSE)'': ``0.0'',
``p(aps\_systems\_FALSE\textbar advanced\_ai\_capability\_TRUE,
agentic\_planning\_TRUE, strategic\_awareness\_TRUE)'': ``0.0'',
``p(aps\_systems\_FALSE\textbar advanced\_ai\_capability\_TRUE,
agentic\_planning\_TRUE, strategic\_awareness\_FALSE)'': ``1.0'',
``p(aps\_systems\_FALSE\textbar advanced\_ai\_capability\_TRUE,
agentic\_planning\_FALSE, strategic\_awareness\_TRUE)'': ``1.0'',
``p(aps\_systems\_FALSE\textbar advanced\_ai\_capability\_TRUE,
agentic\_planning\_FALSE, strategic\_awareness\_FALSE)'': ``1.0'',
``p(aps\_systems\_FALSE\textbar advanced\_ai\_capability\_FALSE,
agentic\_planning\_TRUE, strategic\_awareness\_TRUE)'': ``1.0'',
``p(aps\_systems\_FALSE\textbar advanced\_ai\_capability\_FALSE,
agentic\_planning\_TRUE, strategic\_awareness\_FALSE)'': ``1.0'',
``p(aps\_systems\_FALSE\textbar advanced\_ai\_capability\_FALSE,
agentic\_planning\_FALSE, strategic\_awareness\_TRUE)'': ``1.0'',
``p(aps\_systems\_FALSE\textbar advanced\_ai\_capability\_FALSE,
agentic\_planning\_FALSE, strategic\_awareness\_FALSE)'': ``1.0''\}\} -
{[}Advanced\_AI\_Capability{]}: AI systems that outperform humans on
tasks that grant significant power in the world. \{``instantiations'':
{[}``advanced\_ai\_capability\_TRUE'',
``advanced\_ai\_capability\_FALSE''{]}, ``priors'':
\{``p(advanced\_ai\_capability\_TRUE)'': ``0.80'',
``p(advanced\_ai\_capability\_FALSE)'': ``0.20''\}\} -
{[}Agentic\_Planning{]}: AI systems making and executing plans based on
world models to achieve objectives. \{``instantiations'':
{[}``agentic\_planning\_TRUE'', ``agentic\_planning\_FALSE''{]},
``priors'': \{``p(agentic\_planning\_TRUE)'': ``0.85'',
``p(agentic\_planning\_FALSE)'': ``0.15''\}\} -
{[}Strategic\_Awareness{]}: AI systems with models accurately
representing power dynamics with humans. \{``instantiations'':
{[}``strategic\_awareness\_TRUE'', ``strategic\_awareness\_FALSE''{]},
``priors'': \{``p(strategic\_awareness\_TRUE)'': ``0.75'',
``p(strategic\_awareness\_FALSE)'': ``0.25''\}\} -
{[}Difficulty\_Of\_Alignment{]}: It is harder to build aligned systems
than misaligned systems that are attractive to deploy.
\{``instantiations'': {[}``difficulty\_of\_alignment\_TRUE'',
``difficulty\_of\_alignment\_FALSE''{]}, ``priors'':
\{``p(difficulty\_of\_alignment\_TRUE)'': ``0.40'',
``p(difficulty\_of\_alignment\_FALSE)'': ``0.60''\}, ``posteriors'':
\{``p(difficulty\_of\_alignment\_TRUE\textbar instrumental\_convergence\_TRUE,
problems\_with\_proxies\_TRUE, problems\_with\_search\_TRUE)'':
``0.85'',
``p(difficulty\_of\_alignment\_TRUE\textbar instrumental\_convergence\_TRUE,
problems\_with\_proxies\_TRUE, problems\_with\_search\_FALSE)'':
``0.70'',
``p(difficulty\_of\_alignment\_TRUE\textbar instrumental\_convergence\_TRUE,
problems\_with\_proxies\_FALSE, problems\_with\_search\_TRUE)'':
``0.60'',
``p(difficulty\_of\_alignment\_TRUE\textbar instrumental\_convergence\_TRUE,
problems\_with\_proxies\_FALSE, problems\_with\_search\_FALSE)'':
``0.40'',
``p(difficulty\_of\_alignment\_TRUE\textbar instrumental\_convergence\_FALSE,
problems\_with\_proxies\_TRUE, problems\_with\_search\_TRUE)'':
``0.55'',
``p(difficulty\_of\_alignment\_TRUE\textbar instrumental\_convergence\_FALSE,
problems\_with\_proxies\_TRUE, problems\_with\_search\_FALSE)'':
``0.40'',
``p(difficulty\_of\_alignment\_TRUE\textbar instrumental\_convergence\_FALSE,
problems\_with\_proxies\_FALSE, problems\_with\_search\_TRUE)'':
``0.30'',
``p(difficulty\_of\_alignment\_TRUE\textbar instrumental\_convergence\_FALSE,
problems\_with\_proxies\_FALSE, problems\_with\_search\_FALSE)'':
``0.10'',
``p(difficulty\_of\_alignment\_FALSE\textbar instrumental\_convergence\_TRUE,
problems\_with\_proxies\_TRUE, problems\_with\_search\_TRUE)'':
``0.15'',
``p(difficulty\_of\_alignment\_FALSE\textbar instrumental\_convergence\_TRUE,
problems\_with\_proxies\_TRUE, problems\_with\_search\_FALSE)'':
``0.30'',
``p(difficulty\_of\_alignment\_FALSE\textbar instrumental\_convergence\_TRUE,
problems\_with\_proxies\_FALSE, problems\_with\_search\_TRUE)'':
``0.40'',
``p(difficulty\_of\_alignment\_FALSE\textbar instrumental\_convergence\_TRUE,
problems\_with\_proxies\_FALSE, problems\_with\_search\_FALSE)'':
``0.60'',
``p(difficulty\_of\_alignment\_FALSE\textbar instrumental\_convergence\_FALSE,
problems\_with\_proxies\_TRUE, problems\_with\_search\_TRUE)'':
``0.45'',
``p(difficulty\_of\_alignment\_FALSE\textbar instrumental\_convergence\_FALSE,
problems\_with\_proxies\_TRUE, problems\_with\_search\_FALSE)'':
``0.60'',
``p(difficulty\_of\_alignment\_FALSE\textbar instrumental\_convergence\_FALSE,
problems\_with\_proxies\_FALSE, problems\_with\_search\_TRUE)'':
``0.70'',
``p(difficulty\_of\_alignment\_FALSE\textbar instrumental\_convergence\_FALSE,
problems\_with\_proxies\_FALSE, problems\_with\_search\_FALSE)'':
``0.90''\}\} - {[}Instrumental\_Convergence{]}: AI systems with
misaligned objectives tend to seek power as an instrumental goal.
\{``instantiations'': {[}``instrumental\_convergence\_TRUE'',
``instrumental\_convergence\_FALSE''{]}, ``priors'':
\{``p(instrumental\_convergence\_TRUE)'': ``0.75'',
``p(instrumental\_convergence\_FALSE)'': ``0.25''\}\} -
{[}Problems\_With\_Proxies{]}: Optimizing for proxy objectives breaks
correlations with intended goals. \{``instantiations'':
{[}``problems\_with\_proxies\_TRUE'',
``problems\_with\_proxies\_FALSE''{]}, ``priors'':
\{``p(problems\_with\_proxies\_TRUE)'': ``0.80'',
``p(problems\_with\_proxies\_FALSE)'': ``0.20''\}\} -
{[}Problems\_With\_Search{]}: Search processes can yield systems
pursuing different objectives than intended. \{``instantiations'':
{[}``problems\_with\_search\_TRUE'',
``problems\_with\_search\_FALSE''{]}, ``priors'':
\{``p(problems\_with\_search\_TRUE)'': ``0.70'',
``p(problems\_with\_search\_FALSE)'': ``0.30''\}\} -
{[}Deployment\_Decisions{]}: Decisions to deploy potentially misaligned
AI systems. \{``instantiations'': {[}``deployment\_decisions\_DEPLOY'',
``deployment\_decisions\_WITHHOLD''{]}, ``priors'':
\{``p(deployment\_decisions\_DEPLOY)'': ``0.70'',
``p(deployment\_decisions\_WITHHOLD)'': ``0.30''\}, ``posteriors'':
\{``p(deployment\_decisions\_DEPLOY\textbar incentives\_to\_build\_aps\_STRONG,
deception\_by\_ai\_TRUE)'': ``0.90'',
``p(deployment\_decisions\_DEPLOY\textbar incentives\_to\_build\_aps\_STRONG,
deception\_by\_ai\_FALSE)'': ``0.75'',
``p(deployment\_decisions\_DEPLOY\textbar incentives\_to\_build\_aps\_WEAK,
deception\_by\_ai\_TRUE)'': ``0.60'',
``p(deployment\_decisions\_DEPLOY\textbar incentives\_to\_build\_aps\_WEAK,
deception\_by\_ai\_FALSE)'': ``0.30'',
``p(deployment\_decisions\_WITHHOLD\textbar incentives\_to\_build\_aps\_STRONG,
deception\_by\_ai\_TRUE)'': ``0.10'',
``p(deployment\_decisions\_WITHHOLD\textbar incentives\_to\_build\_aps\_STRONG,
deception\_by\_ai\_FALSE)'': ``0.25'',
``p(deployment\_decisions\_WITHHOLD\textbar incentives\_to\_build\_aps\_WEAK,
deception\_by\_ai\_TRUE)'': ``0.40'',
``p(deployment\_decisions\_WITHHOLD\textbar incentives\_to\_build\_aps\_WEAK,
deception\_by\_ai\_FALSE)'': ``0.70''\}\} -
{[}Incentives\_To\_Build\_APS{]}: Strong incentives to build and deploy
APS systems. \{``instantiations'':
{[}``incentives\_to\_build\_aps\_STRONG'',
``incentives\_to\_build\_aps\_WEAK''{]}, ``priors'':
\{``p(incentives\_to\_build\_aps\_STRONG)'': ``0.80'',
``p(incentives\_to\_build\_aps\_WEAK)'': ``0.20''\}, ``posteriors'':
\{``p(incentives\_to\_build\_aps\_STRONG\textbar usefulness\_of\_aps\_HIGH,
competitive\_dynamics\_STRONG)'': ``0.95'',
``p(incentives\_to\_build\_aps\_STRONG\textbar usefulness\_of\_aps\_HIGH,
competitive\_dynamics\_WEAK)'': ``0.80'',
``p(incentives\_to\_build\_aps\_STRONG\textbar usefulness\_of\_aps\_LOW,
competitive\_dynamics\_STRONG)'': ``0.70'',
``p(incentives\_to\_build\_aps\_STRONG\textbar usefulness\_of\_aps\_LOW,
competitive\_dynamics\_WEAK)'': ``0.30'',
``p(incentives\_to\_build\_aps\_WEAK\textbar usefulness\_of\_aps\_HIGH,
competitive\_dynamics\_STRONG)'': ``0.05'',
``p(incentives\_to\_build\_aps\_WEAK\textbar usefulness\_of\_aps\_HIGH,
competitive\_dynamics\_WEAK)'': ``0.20'',
``p(incentives\_to\_build\_aps\_WEAK\textbar usefulness\_of\_aps\_LOW,
competitive\_dynamics\_STRONG)'': ``0.30'',
``p(incentives\_to\_build\_aps\_WEAK\textbar usefulness\_of\_aps\_LOW,
competitive\_dynamics\_WEAK)'': ``0.70''\}\} -
{[}Usefulness\_Of\_APS{]}: APS systems are very useful for many valuable
tasks. \{``instantiations'': {[}``usefulness\_of\_aps\_HIGH'',
``usefulness\_of\_aps\_LOW''{]}, ``priors'':
\{``p(usefulness\_of\_aps\_HIGH)'': ``0.85'',
``p(usefulness\_of\_aps\_LOW)'': ``0.15''\}\} -
{[}Competitive\_Dynamics{]}: Competitive pressures between AI
developers. \{``instantiations'': {[}``competitive\_dynamics\_STRONG'',
``competitive\_dynamics\_WEAK''{]}, ``priors'':
\{``p(competitive\_dynamics\_STRONG)'': ``0.75'',
``p(competitive\_dynamics\_WEAK)'': ``0.25''\}\} -
{[}Deception\_By\_AI{]}: AI systems deceiving humans about their true
objectives. \{``instantiations'': {[}``deception\_by\_ai\_TRUE'',
``deception\_by\_ai\_FALSE''{]}, ``priors'':
\{``p(deception\_by\_ai\_TRUE)'': ``0.50'',
``p(deception\_by\_ai\_FALSE)'': ``0.50''\}\} -
{[}Corrective\_Feedback{]}: Human society implementing corrections after
observing problems. \{``instantiations'':
{[}``corrective\_feedback\_EFFECTIVE'',
``corrective\_feedback\_INEFFECTIVE''{]}, ``priors'':
\{``p(corrective\_feedback\_EFFECTIVE)'': ``0.60'',
``p(corrective\_feedback\_INEFFECTIVE)'': ``0.40''\}, ``posteriors'':
\{``p(corrective\_feedback\_EFFECTIVE\textbar warning\_shots\_OBSERVED,
rapid\_capability\_escalation\_TRUE)'': ``0.40'',
``p(corrective\_feedback\_EFFECTIVE\textbar warning\_shots\_OBSERVED,
rapid\_capability\_escalation\_FALSE)'': ``0.80'',
``p(corrective\_feedback\_EFFECTIVE\textbar warning\_shots\_UNOBSERVED,
rapid\_capability\_escalation\_TRUE)'': ``0.15'',
``p(corrective\_feedback\_EFFECTIVE\textbar warning\_shots\_UNOBSERVED,
rapid\_capability\_escalation\_FALSE)'': ``0.50'',
``p(corrective\_feedback\_INEFFECTIVE\textbar warning\_shots\_OBSERVED,
rapid\_capability\_escalation\_TRUE)'': ``0.60'',
``p(corrective\_feedback\_INEFFECTIVE\textbar warning\_shots\_OBSERVED,
rapid\_capability\_escalation\_FALSE)'': ``0.20'',
``p(corrective\_feedback\_INEFFECTIVE\textbar warning\_shots\_UNOBSERVED,
rapid\_capability\_escalation\_TRUE)'': ``0.85'',
``p(corrective\_feedback\_INEFFECTIVE\textbar warning\_shots\_UNOBSERVED,
rapid\_capability\_escalation\_FALSE)'': ``0.50''\}\} -
{[}Warning\_Shots{]}: Observable failures in weaker systems before
catastrophic risks. \{``instantiations'':
{[}``warning\_shots\_OBSERVED'', ``warning\_shots\_UNOBSERVED''{]},
``priors'': \{``p(warning\_shots\_OBSERVED)'': ``0.70'',
``p(warning\_shots\_UNOBSERVED)'': ``0.30''\}\} -
{[}Rapid\_Capability\_Escalation{]}: AI capabilities escalating very
rapidly, allowing little time for correction. \{``instantiations'':
{[}``rapid\_capability\_escalation\_TRUE'',
``rapid\_capability\_escalation\_FALSE''{]}, ``priors'':
\{``p(rapid\_capability\_escalation\_TRUE)'': ``0.45'',
``p(rapid\_capability\_escalation\_FALSE)'': ``0.55''\}\}
{[}Barriers\_To\_Understanding{]}: Difficulty in understanding the
internal workings of advanced AI systems. \{``instantiations'':
{[}``barriers\_to\_understanding\_HIGH'',
``barriers\_to\_understanding\_LOW''{]}, ``priors'':
\{``p(barriers\_to\_understanding\_HIGH)'': ``0.70'',
``p(barriers\_to\_understanding\_LOW)'': ``0.30''\}, ``posteriors'':
\{``p(barriers\_to\_understanding\_HIGH\textbar misaligned\_power\_seeking\_TRUE)'':
``0.85'',
``p(barriers\_to\_understanding\_HIGH\textbar misaligned\_power\_seeking\_FALSE)'':
``0.60'',
``p(barriers\_to\_understanding\_LOW\textbar misaligned\_power\_seeking\_TRUE)'':
``0.15'',
``p(barriers\_to\_understanding\_LOW\textbar misaligned\_power\_seeking\_FALSE)'':
``0.40''\}\} - {[}Misaligned\_Power\_Seeking{]}: Deployed AI systems
seeking power in unintended and high-impact ways due to problems with
their objectives. \{``instantiations'':
{[}``misaligned\_power\_seeking\_TRUE'',
``misaligned\_power\_seeking\_FALSE''{]}, ``priors'':
\{``p(misaligned\_power\_seeking\_TRUE)'': ``0.338'',
``p(misaligned\_power\_seeking\_FALSE)'': ``0.662''\}\}
{[}Adversarial\_Dynamics{]}: Potentially adversarial relationships
between humans and power-seeking AI. \{``instantiations'':
{[}``adversarial\_dynamics\_TRUE'', ``adversarial\_dynamics\_FALSE''{]},
``priors'': \{``p(adversarial\_dynamics\_TRUE)'': ``0.60'',
``p(adversarial\_dynamics\_FALSE)'': ``0.40''\}, ``posteriors'':
\{``p(adversarial\_dynamics\_TRUE\textbar misaligned\_power\_seeking\_TRUE)'':
``0.95'',
``p(adversarial\_dynamics\_TRUE\textbar misaligned\_power\_seeking\_FALSE)'':
``0.10'',
``p(adversarial\_dynamics\_FALSE\textbar misaligned\_power\_seeking\_TRUE)'':
``0.05'',
``p(adversarial\_dynamics\_FALSE\textbar misaligned\_power\_seeking\_FALSE)'':
``0.90''\}\} - {[}Misaligned\_Power\_Seeking{]}: Deployed AI systems
seeking power in unintended and high-impact ways due to problems with
their objectives. \{``instantiations'':
{[}``misaligned\_power\_seeking\_TRUE'',
``misaligned\_power\_seeking\_FALSE''{]}, ``priors'':
\{``p(misaligned\_power\_seeking\_TRUE)'': ``0.338'',
``p(misaligned\_power\_seeking\_FALSE)'': ``0.662''\}\}
{[}Stakes\_Of\_Error{]}: The escalating impact of mistakes with
power-seeking AI systems. \{``instantiations'':
{[}``stakes\_of\_error\_HIGH'', ``stakes\_of\_error\_LOW''{]},
``priors'': \{``p(stakes\_of\_error\_HIGH)'': ``0.85'',
``p(stakes\_of\_error\_LOW)'': ``0.15''\}, ``posteriors'':
\{``p(stakes\_of\_error\_HIGH\textbar misaligned\_power\_seeking\_TRUE)'':
``0.95'',
``p(stakes\_of\_error\_HIGH\textbar misaligned\_power\_seeking\_FALSE)'':
``0.50'',
``p(stakes\_of\_error\_LOW\textbar misaligned\_power\_seeking\_TRUE)'':
``0.05'',
``p(stakes\_of\_error\_LOW\textbar misaligned\_power\_seeking\_FALSE)'':
``0.50''\}\} - {[}Misaligned\_Power\_Seeking{]}: Deployed AI systems
seeking power in unintended and high-impact ways due to problems with
their objectives. \{``instantiations'':
{[}``misaligned\_power\_seeking\_TRUE'',
``misaligned\_power\_seeking\_FALSE''{]}, ``priors'':
\{``p(misaligned\_power\_seeking\_TRUE)'': ``0.338'',
``p(misaligned\_power\_seeking\_FALSE)'': ``0.662''\}\}

\section{3.1.2.2 Check the Graph Structure with the ArgDown Sandbox
Online}\label{check-the-graph-structure-with-the-argdown-sandbox-online-1}

Copy and paste the BayesDown formatted \ldots{} in the ArgDown Sandbox
below to quickly verify that the network renders correctly.

\section{3.3 Extraction}\label{extraction}

BayesDown Extraction Code already part of ArgDown extraction code,
therefore just use same function
``parse\_markdown\_hierarchy(markdown\_data)'' and ignore the extra
argument (``ArgDown'') because it is automatically set to false amd will
by default extract BayesDown.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result\_df }\OperatorTok{=}\NormalTok{ parse\_markdown\_hierarchy\_fixed(md\_content\_ex\_rain)}
\NormalTok{result\_df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllllllllllll@{}}
\toprule\noalign{}
& Title & Description & line & line\_numbers & indentation &
indentation\_levels & Parents & Children & instantiations & priors &
posteriors & No\_Parent & No\_Children & parent\_instantiations \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Existential\_Catastrophe & The destruction of
humanity\textquotesingle s long-term potent... & 0 & {[}0{]} & 0 &
{[}0{]} & {[}{]} & {[}{]} & {[}existential\_catastrophe\_TRUE,
existential\_cat... &
\{\textquotesingle p(existential\_catastrophe\_TRUE)\textquotesingle:
\textquotesingle0.05\textquotesingle, \textquotesingle p... &
\{\textquotesingle p(existential\_catastrophe\_TRUE\textbar human\_disempo...
& True & True & {[}{]} \\
1 & Human\_Disempowerment & Permanent and collective disempowerment of
hum... & 1 & {[}1{]} & 0 & {[}0{]} & {[}Scale\_Of\_Power\_Seeking{]} &
{[}{]} & {[}human\_disempowerment\_TRUE, human\_disempowerme... &
\{\textquotesingle p(human\_disempowerment\_TRUE)\textquotesingle:
\textquotesingle0.208\textquotesingle, \textquotesingle p(h... &
\{\textquotesingle p(human\_disempowerment\_TRUE\textbar scale\_of\_power\_s...
& False & True & {[}{[}scale\_of\_power\_seeking\_TRUE,
scale\_of\_power\_... \\
2 & Scale\_Of\_Power\_Seeking & Power-seeking by AI systems scaling to
the poi... & 2 & {[}2{]} & 4 & {[}4{]} & {[}Misaligned\_Power\_Seeking,
Corrective\_Feedback{]} & {[}Human\_Disempowerment{]} &
{[}scale\_of\_power\_seeking\_TRUE, scale\_of\_power\_s... &
\{\textquotesingle p(scale\_of\_power\_seeking\_TRUE)\textquotesingle:
\textquotesingle0.208\textquotesingle, \textquotesingle p... &
\{\textquotesingle p(scale\_of\_power\_seeking\_TRUE\textbar misaligned\_pow...
& False & False & {[}{[}misaligned\_power\_seeking\_TRUE,
misaligned\_po... \\
3 & Misaligned\_Power\_Seeking & Deployed AI systems seeking power in
unintende... & 3 & {[}3, 21, 23, 25{]} & 8 & {[}8, 0, 0, 0{]} &
{[}APS\_Systems, Difficulty\_Of\_Alignment, Deploym... &
{[}Scale\_Of\_Power\_Seeking{]} & {[}misaligned\_power\_seeking\_TRUE,
misaligned\_pow... &
\{\textquotesingle p(misaligned\_power\_seeking\_TRUE)\textquotesingle:
\textquotesingle0.338\textquotesingle, ... &
\{\textquotesingle p(misaligned\_power\_seeking\_TRUE\textbar aps\_systems\_...
& False & False & {[}{[}aps\_systems\_TRUE, aps\_systems\_FALSE{]},
{[}diffi... \\
4 & APS\_Systems & AI systems with advanced capabilities, agentic... & 4
& {[}4{]} & 12 & {[}12{]} & {[}Advanced\_AI\_Capability,
Agentic\_Planning, Str... & {[}Misaligned\_Power\_Seeking{]} &
{[}aps\_systems\_TRUE, aps\_systems\_FALSE{]} &
\{\textquotesingle p(aps\_systems\_TRUE)\textquotesingle:
\textquotesingle0.65\textquotesingle, \textquotesingle p(aps\_systems...
&
\{\textquotesingle p(aps\_systems\_TRUE\textbar advanced\_ai\_capability\_TR...
& False & False & {[}{[}advanced\_ai\_capability\_TRUE,
advanced\_ai\_cap... \\
5 & Advanced\_AI\_Capability & AI systems that outperform humans on
tasks tha... & 5 & {[}5{]} & 16 & {[}16{]} & {[}{]} & {[}APS\_Systems{]}
& {[}advanced\_ai\_capability\_TRUE, advanced\_ai\_capa... &
\{\textquotesingle p(advanced\_ai\_capability\_TRUE)\textquotesingle:
\textquotesingle0.80\textquotesingle, \textquotesingle p(... & \{\} &
True & False & {[}{]} \\
6 & Agentic\_Planning & AI systems making and executing plans based
on... & 6 & {[}6{]} & 16 & {[}16{]} & {[}{]} & {[}APS\_Systems{]} &
{[}agentic\_planning\_TRUE, agentic\_planning\_FALSE{]} &
\{\textquotesingle p(agentic\_planning\_TRUE)\textquotesingle:
\textquotesingle0.85\textquotesingle, \textquotesingle p(agenti... &
\{\} & True & False & {[}{]} \\
7 & Strategic\_Awareness & AI systems with models accurately
representing... & 7 & {[}7{]} & 16 & {[}16{]} & {[}{]} &
{[}APS\_Systems{]} & {[}strategic\_awareness\_TRUE,
strategic\_awareness... &
\{\textquotesingle p(strategic\_awareness\_TRUE)\textquotesingle:
\textquotesingle0.75\textquotesingle, \textquotesingle p(str... & \{\} &
True & False & {[}{]} \\
8 & Difficulty\_Of\_Alignment & It is harder to build aligned systems
than mis... & 8 & {[}8{]} & 12 & {[}12{]} &
{[}Instrumental\_Convergence, Problems\_With\_Proxi... &
{[}Misaligned\_Power\_Seeking{]} & {[}difficulty\_of\_alignment\_TRUE,
difficulty\_of\_a... &
\{\textquotesingle p(difficulty\_of\_alignment\_TRUE)\textquotesingle:
\textquotesingle0.40\textquotesingle, \textquotesingle p... &
\{\textquotesingle p(difficulty\_of\_alignment\_TRUE\textbar instrumental\_...
& False & False & {[}{[}instrumental\_convergence\_TRUE,
instrumental\_... \\
9 & Instrumental\_Convergence & AI systems with misaligned objectives
tend to ... & 9 & {[}9{]} & 16 & {[}16{]} & {[}{]} &
{[}Difficulty\_Of\_Alignment{]} & {[}instrumental\_convergence\_TRUE,
instrumental\_c... &
\{\textquotesingle p(instrumental\_convergence\_TRUE)\textquotesingle:
\textquotesingle0.75\textquotesingle, \textquotesingle... & \{\} & True
& False & {[}{]} \\
10 & Problems\_With\_Proxies & Optimizing for proxy objectives breaks
correla... & 10 & {[}10{]} & 16 & {[}16{]} & {[}{]} &
{[}Difficulty\_Of\_Alignment{]} & {[}problems\_with\_proxies\_TRUE,
problems\_with\_pro... &
\{\textquotesingle p(problems\_with\_proxies\_TRUE)\textquotesingle:
\textquotesingle0.80\textquotesingle, \textquotesingle p(p... & \{\} &
True & False & {[}{]} \\
11 & Problems\_With\_Search & Search processes can yield systems
pursuing di... & 11 & {[}11{]} & 16 & {[}16{]} & {[}{]} &
{[}Difficulty\_Of\_Alignment{]} & {[}problems\_with\_search\_TRUE,
problems\_with\_sear... &
\{\textquotesingle p(problems\_with\_search\_TRUE)\textquotesingle:
\textquotesingle0.70\textquotesingle, \textquotesingle p(pr... & \{\} &
True & False & {[}{]} \\
12 & Deployment\_Decisions & Decisions to deploy potentially misaligned
AI ... & 12 & {[}12{]} & 12 & {[}12{]} & {[}Incentives\_To\_Build\_APS,
Deception\_By\_AI{]} & {[}Misaligned\_Power\_Seeking{]} &
{[}deployment\_decisions\_DEPLOY, deployment\_decis... &
\{\textquotesingle p(deployment\_decisions\_DEPLOY)\textquotesingle:
\textquotesingle0.70\textquotesingle, \textquotesingle p(... &
\{\textquotesingle p(deployment\_decisions\_DEPLOY\textbar incentives\_to\_...
& False & False & {[}{[}incentives\_to\_build\_aps\_STRONG,
incentives\_t... \\
13 & Incentives\_To\_Build\_APS & Strong incentives to build and deploy
APS syst... & 13 & {[}13{]} & 16 & {[}16{]} & {[}Usefulness\_Of\_APS,
Competitive\_Dynamics{]} & {[}Deployment\_Decisions{]} &
{[}incentives\_to\_build\_aps\_STRONG, incentives\_to... &
\{\textquotesingle p(incentives\_to\_build\_aps\_STRONG)\textquotesingle:
\textquotesingle0.80\textquotesingle, ... &
\{\textquotesingle p(incentives\_to\_build\_aps\_STRONG\textbar usefulness\_...
& False & False & {[}{[}usefulness\_of\_aps\_HIGH,
usefulness\_of\_aps\_LO... \\
14 & Usefulness\_Of\_APS & APS systems are very useful for many valuable
... & 14 & {[}14{]} & 20 & {[}20{]} & {[}{]} &
{[}Incentives\_To\_Build\_APS{]} & {[}usefulness\_of\_aps\_HIGH,
usefulness\_of\_aps\_LOW{]} &
\{\textquotesingle p(usefulness\_of\_aps\_HIGH)\textquotesingle:
\textquotesingle0.85\textquotesingle, \textquotesingle p(usefu... & \{\}
& True & False & {[}{]} \\
15 & Competitive\_Dynamics & Competitive pressures between AI
developers. & 15 & {[}15{]} & 20 & {[}20{]} & {[}{]} &
{[}Incentives\_To\_Build\_APS{]} & {[}competitive\_dynamics\_STRONG,
competitive\_dyna... &
\{\textquotesingle p(competitive\_dynamics\_STRONG)\textquotesingle:
\textquotesingle0.75\textquotesingle, \textquotesingle p(... & \{\} &
True & False & {[}{]} \\
16 & Deception\_By\_AI & AI systems deceiving humans about their true
o... & 16 & {[}16{]} & 16 & {[}16{]} & {[}{]} &
{[}Deployment\_Decisions{]} & {[}deception\_by\_ai\_TRUE,
deception\_by\_ai\_FALSE{]} &
\{\textquotesingle p(deception\_by\_ai\_TRUE)\textquotesingle:
\textquotesingle0.50\textquotesingle, \textquotesingle p(decepti... &
\{\} & True & False & {[}{]} \\
17 & Corrective\_Feedback & Human society implementing corrections after
o... & 17 & {[}17{]} & 8 & {[}8{]} & {[}Warning\_Shots,
Rapid\_Capability\_Escalation{]} & {[}Scale\_Of\_Power\_Seeking{]} &
{[}corrective\_feedback\_EFFECTIVE, corrective\_fee... &
\{\textquotesingle p(corrective\_feedback\_EFFECTIVE)\textquotesingle:
\textquotesingle0.60\textquotesingle, \textquotesingle... &
\{\textquotesingle p(corrective\_feedback\_EFFECTIVE\textbar warning\_shot...
& False & False & {[}{[}warning\_shots\_OBSERVED,
warning\_shots\_UNOBSE... \\
18 & Warning\_Shots & Observable failures in weaker systems before c...
& 18 & {[}18{]} & 12 & {[}12{]} & {[}{]} & {[}Corrective\_Feedback{]} &
{[}warning\_shots\_OBSERVED, warning\_shots\_UNOBSER... &
\{\textquotesingle p(warning\_shots\_OBSERVED)\textquotesingle:
\textquotesingle0.70\textquotesingle, \textquotesingle p(warni... & \{\}
& True & False & {[}{]} \\
19 & Rapid\_Capability\_Escalation & AI capabilities escalating very
rapidly, allow... & 19 & {[}19{]} & 12 & {[}12{]} & {[}{]} &
{[}Corrective\_Feedback{]} & {[}rapid\_capability\_escalation\_TRUE,
rapid\_capab... &
\{\textquotesingle p(rapid\_capability\_escalation\_TRUE)\textquotesingle:
\textquotesingle0.45\textquotesingle... & \{\} & True & False &
{[}{]} \\
20 & Barriers\_To\_Understanding & Difficulty in understanding the
internal worki... & 20 & {[}20{]} & 0 & {[}0{]} & {[}{]} & {[}{]} &
{[}barriers\_to\_understanding\_HIGH, barriers\_to\_u... &
\{\textquotesingle p(barriers\_to\_understanding\_HIGH)\textquotesingle:
\textquotesingle0.70\textquotesingle, ... &
\{\textquotesingle p(barriers\_to\_understanding\_HIGH\textbar misaligned\_...
& True & True & {[}{]} \\
21 & Adversarial\_Dynamics & Potentially adversarial relationships
between ... & 22 & {[}22{]} & 0 & {[}0{]} & {[}{]} & {[}{]} &
{[}adversarial\_dynamics\_TRUE, adversarial\_dynami... &
\{\textquotesingle p(adversarial\_dynamics\_TRUE)\textquotesingle:
\textquotesingle0.60\textquotesingle, \textquotesingle p(ad... &
\{\textquotesingle p(adversarial\_dynamics\_TRUE\textbar misaligned\_power...
& True & True & {[}{]} \\
22 & Stakes\_Of\_Error & The escalating impact of mistakes with
power-s... & 24 & {[}24{]} & 0 & {[}0{]} & {[}{]} & {[}{]} &
{[}stakes\_of\_error\_HIGH, stakes\_of\_error\_LOW{]} &
\{\textquotesingle p(stakes\_of\_error\_HIGH)\textquotesingle:
\textquotesingle0.85\textquotesingle, \textquotesingle p(stakes\_... &
\{\textquotesingle p(stakes\_of\_error\_HIGH\textbar misaligned\_power\_seek...
& True & True & {[}{]} \\
\end{longtable}

\subsection{3.3 Data-Post-Processing}\label{data-post-processing}

Add rows to data frame that can be calculated from the extracted rows

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# @title 3.3.1 Data Post{-}Processing Functions {-}{-}{-}}

\CommentTok{"""}
\CommentTok{BLOCK PURPOSE: Enhances the extracted BayesDown data with calculated metrics and network properties.}

\CommentTok{This block provides functions to enrich the basic extracted data with additional}
\CommentTok{calculated columns that are useful for analysis and visualization:}

\CommentTok{1. Joint probabilities {-} Calculating P(A,B) from conditional and prior probabilities}
\CommentTok{2. Network metrics {-} Centrality measures that indicate importance of nodes in the network}
\CommentTok{3. Markov blanket {-} Identifying the minimal set of nodes that shield a node from the rest}

\CommentTok{These enhancements provide valuable context for understanding the network structure}
\CommentTok{and the relationships between variables, enabling more advanced analysis and}
\CommentTok{improving visualization.}

\CommentTok{DEPENDENCIES: networkx for graph calculations}
\CommentTok{INPUTS: DataFrame with basic extracted BayesDown data}
\CommentTok{OUTPUTS: Enhanced DataFrame with additional calculated columns}
\CommentTok{"""}

\KeywordTok{def}\NormalTok{ enhance\_extracted\_data(df):}
    \CommentTok{"""}
\CommentTok{    Enhance the extracted data with calculated columns}

\CommentTok{    Args:}
\CommentTok{        df: DataFrame with extracted BayesDown data}

\CommentTok{    Returns:}
\CommentTok{        Enhanced DataFrame with additional columns}
\CommentTok{    """}
    \CommentTok{\# Create a copy to avoid modifying the original}
\NormalTok{    enhanced\_df }\OperatorTok{=}\NormalTok{ df.copy()}

    \CommentTok{\# 1. Calculate joint probabilities {-} P(A,B) = P(A|B) * P(B)}
\NormalTok{    enhanced\_df[}\StringTok{\textquotesingle{}joint\_probabilities\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \VariableTok{None}

    \ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ enhanced\_df.iterrows():}
\NormalTok{        title }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}
\NormalTok{        priors }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(row[}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{], }\BuiltInTok{dict}\NormalTok{) }\ControlFlowTok{else}\NormalTok{ \{\}}
\NormalTok{        posteriors }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}posteriors\textquotesingle{}}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(row[}\StringTok{\textquotesingle{}posteriors\textquotesingle{}}\NormalTok{], }\BuiltInTok{dict}\NormalTok{) }\ControlFlowTok{else}\NormalTok{ \{\}}
\NormalTok{        parents }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Parents\textquotesingle{}}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(row[}\StringTok{\textquotesingle{}Parents\textquotesingle{}}\NormalTok{], }\BuiltInTok{list}\NormalTok{) }\ControlFlowTok{else}\NormalTok{ []}

        \CommentTok{\# Skip if no parents or no priors}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ parents }\KeywordTok{or} \KeywordTok{not}\NormalTok{ priors:}
            \ControlFlowTok{continue}

        \CommentTok{\# Initialize joint probabilities dictionary}
\NormalTok{        joint\_probs }\OperatorTok{=}\NormalTok{ \{\}}

        \CommentTok{\# Get instantiations}
\NormalTok{        instantiations }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}instantiations\textquotesingle{}}\NormalTok{]}
        \ControlFlowTok{if} \KeywordTok{not} \BuiltInTok{isinstance}\NormalTok{(instantiations, }\BuiltInTok{list}\NormalTok{) }\KeywordTok{or} \KeywordTok{not}\NormalTok{ instantiations:}
            \ControlFlowTok{continue}

        \CommentTok{\# For each parent and child instantiation combination, calculate joint probability}
        \ControlFlowTok{for}\NormalTok{ inst }\KeywordTok{in}\NormalTok{ instantiations:}
            \CommentTok{\# Get this instantiation\textquotesingle{}s prior probability}
\NormalTok{            inst\_prior\_key }\OperatorTok{=} \SpecialStringTok{f"p(}\SpecialCharTok{\{}\NormalTok{inst}\SpecialCharTok{\}}\SpecialStringTok{)"}
            \ControlFlowTok{if}\NormalTok{ inst\_prior\_key }\KeywordTok{not} \KeywordTok{in}\NormalTok{ priors:}
                \ControlFlowTok{continue}

            \ControlFlowTok{try}\NormalTok{:}
\NormalTok{                inst\_prior }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(priors[inst\_prior\_key])}
            \ControlFlowTok{except}\NormalTok{ (}\PreprocessorTok{ValueError}\NormalTok{, }\PreprocessorTok{TypeError}\NormalTok{):}
                \ControlFlowTok{continue}

            \CommentTok{\# For each parent}
            \ControlFlowTok{for}\NormalTok{ parent }\KeywordTok{in}\NormalTok{ parents:}
\NormalTok{                parent\_row }\OperatorTok{=}\NormalTok{ enhanced\_df[enhanced\_df[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ parent]}
                \ControlFlowTok{if}\NormalTok{ parent\_row.empty:}
                    \ControlFlowTok{continue}

\NormalTok{                parent\_insts }\OperatorTok{=}\NormalTok{ parent\_row.iloc[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}instantiations\textquotesingle{}}\NormalTok{]}
                \ControlFlowTok{if} \KeywordTok{not} \BuiltInTok{isinstance}\NormalTok{(parent\_insts, }\BuiltInTok{list}\NormalTok{) }\KeywordTok{or} \KeywordTok{not}\NormalTok{ parent\_insts:}
                    \ControlFlowTok{continue}

                \ControlFlowTok{for}\NormalTok{ parent\_inst }\KeywordTok{in}\NormalTok{ parent\_insts:}
                    \CommentTok{\# Get conditional probability}
\NormalTok{                    cond\_key }\OperatorTok{=} \SpecialStringTok{f"p(}\SpecialCharTok{\{}\NormalTok{inst}\SpecialCharTok{\}}\SpecialStringTok{|}\SpecialCharTok{\{}\NormalTok{parent}\SpecialCharTok{\}}\SpecialStringTok{=}\SpecialCharTok{\{}\NormalTok{parent\_inst}\SpecialCharTok{\}}\SpecialStringTok{)"}
                    \ControlFlowTok{if}\NormalTok{ cond\_key }\KeywordTok{in}\NormalTok{ posteriors:}
                        \ControlFlowTok{try}\NormalTok{:}
\NormalTok{                            cond\_prob }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(posteriors[cond\_key])}

                            \CommentTok{\# Get parent\textquotesingle{}s prior}
\NormalTok{                            parent\_priors }\OperatorTok{=}\NormalTok{ parent\_row.iloc[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{]}
                            \ControlFlowTok{if} \KeywordTok{not} \BuiltInTok{isinstance}\NormalTok{(parent\_priors, }\BuiltInTok{dict}\NormalTok{):}
                                \ControlFlowTok{continue}

\NormalTok{                            parent\_prior\_key }\OperatorTok{=} \SpecialStringTok{f"p(}\SpecialCharTok{\{}\NormalTok{parent\_inst}\SpecialCharTok{\}}\SpecialStringTok{)"}
                            \ControlFlowTok{if}\NormalTok{ parent\_prior\_key }\KeywordTok{not} \KeywordTok{in}\NormalTok{ parent\_priors:}
                                \ControlFlowTok{continue}

                            \ControlFlowTok{try}\NormalTok{:}
\NormalTok{                                parent\_prior }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(parent\_priors[parent\_prior\_key])}

                                \CommentTok{\# Calculate joint probability: P(A,B) = P(A|B) * P(B)}
\NormalTok{                                joint\_prob }\OperatorTok{=}\NormalTok{ cond\_prob }\OperatorTok{*}\NormalTok{ parent\_prior}
\NormalTok{                                joint\_key }\OperatorTok{=} \SpecialStringTok{f"p(}\SpecialCharTok{\{}\NormalTok{inst}\SpecialCharTok{\}}\SpecialStringTok{,}\SpecialCharTok{\{}\NormalTok{parent}\SpecialCharTok{\}}\SpecialStringTok{=}\SpecialCharTok{\{}\NormalTok{parent\_inst}\SpecialCharTok{\}}\SpecialStringTok{)"}
\NormalTok{                                joint\_probs[joint\_key] }\OperatorTok{=} \BuiltInTok{str}\NormalTok{(}\BuiltInTok{round}\NormalTok{(joint\_prob, }\DecValTok{4}\NormalTok{))}
                            \ControlFlowTok{except}\NormalTok{ (}\PreprocessorTok{ValueError}\NormalTok{, }\PreprocessorTok{TypeError}\NormalTok{):}
\NormalTok{                                joint\_prob }\OperatorTok{=}\NormalTok{ cond\_prob }\OperatorTok{*}\NormalTok{ parent\_prior}
\NormalTok{                                joint\_key }\OperatorTok{=} \SpecialStringTok{f"p(}\SpecialCharTok{\{}\NormalTok{inst}\SpecialCharTok{\}}\SpecialStringTok{,}\SpecialCharTok{\{}\NormalTok{parent}\SpecialCharTok{\}}\SpecialStringTok{=}\SpecialCharTok{\{}\NormalTok{parent\_inst}\SpecialCharTok{\}}\SpecialStringTok{)"}
\NormalTok{                                joint\_probs[joint\_key] }\OperatorTok{=} \BuiltInTok{str}\NormalTok{(}\BuiltInTok{round}\NormalTok{(joint\_prob, }\DecValTok{4}\NormalTok{))}
                            \ControlFlowTok{except}\NormalTok{ (}\PreprocessorTok{ValueError}\NormalTok{, }\PreprocessorTok{TypeError}\NormalTok{):}
                                \ControlFlowTok{continue}
                        \ControlFlowTok{except}\NormalTok{ (}\PreprocessorTok{ValueError}\NormalTok{, }\PreprocessorTok{TypeError}\NormalTok{):}
                            \ControlFlowTok{continue}

        \CommentTok{\# Store joint probabilities in dataframe}
\NormalTok{        enhanced\_df.at[idx, }\StringTok{\textquotesingle{}joint\_probabilities\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ joint\_probs}

    \CommentTok{\# 2. Calculate network metrics}
    \CommentTok{\# Create a directed graph}
    \ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}
\NormalTok{    G }\OperatorTok{=}\NormalTok{ nx.DiGraph()}

    \CommentTok{\# Add nodes}
    \ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ enhanced\_df.iterrows():}
\NormalTok{        G.add\_node(row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{])}

    \CommentTok{\# Add edges}
    \ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ enhanced\_df.iterrows():}
\NormalTok{        child }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}
\NormalTok{        parents }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Parents\textquotesingle{}}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(row[}\StringTok{\textquotesingle{}Parents\textquotesingle{}}\NormalTok{], }\BuiltInTok{list}\NormalTok{) }\ControlFlowTok{else}\NormalTok{ []}

        \ControlFlowTok{for}\NormalTok{ parent }\KeywordTok{in}\NormalTok{ parents:}
            \ControlFlowTok{if}\NormalTok{ parent }\KeywordTok{in}\NormalTok{ G.nodes():}
\NormalTok{                G.add\_edge(parent, child)}

    \CommentTok{\# Calculate centrality measures}
\NormalTok{    degree\_centrality }\OperatorTok{=}\NormalTok{ nx.degree\_centrality(G)  }\CommentTok{\# Overall connectedness}
\NormalTok{    in\_degree\_centrality }\OperatorTok{=}\NormalTok{ nx.in\_degree\_centrality(G)  }\CommentTok{\# How many nodes affect this one}
\NormalTok{    out\_degree\_centrality }\OperatorTok{=}\NormalTok{ nx.out\_degree\_centrality(G)  }\CommentTok{\# How many nodes this one affects}

    \ControlFlowTok{try}\NormalTok{:}
\NormalTok{        betweenness\_centrality }\OperatorTok{=}\NormalTok{ nx.betweenness\_centrality(G)  }\CommentTok{\# Node\textquotesingle{}s role as a connector}
    \ControlFlowTok{except}\NormalTok{:}
\NormalTok{        betweenness\_centrality }\OperatorTok{=}\NormalTok{ \{node: }\DecValTok{0} \ControlFlowTok{for}\NormalTok{ node }\KeywordTok{in}\NormalTok{ G.nodes()\}}

    \CommentTok{\# Add metrics to dataframe}
\NormalTok{    enhanced\_df[}\StringTok{\textquotesingle{}degree\_centrality\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \VariableTok{None}
\NormalTok{    enhanced\_df[}\StringTok{\textquotesingle{}in\_degree\_centrality\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \VariableTok{None}
\NormalTok{    enhanced\_df[}\StringTok{\textquotesingle{}out\_degree\_centrality\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \VariableTok{None}
\NormalTok{    enhanced\_df[}\StringTok{\textquotesingle{}betweenness\_centrality\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \VariableTok{None}

    \ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ enhanced\_df.iterrows():}
\NormalTok{        title }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}
\NormalTok{        enhanced\_df.at[idx, }\StringTok{\textquotesingle{}degree\_centrality\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ degree\_centrality.get(title, }\DecValTok{0}\NormalTok{)}
\NormalTok{        enhanced\_df.at[idx, }\StringTok{\textquotesingle{}in\_degree\_centrality\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ in\_degree\_centrality.get(title, }\DecValTok{0}\NormalTok{)}
\NormalTok{        enhanced\_df.at[idx, }\StringTok{\textquotesingle{}out\_degree\_centrality\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ out\_degree\_centrality.get(title, }\DecValTok{0}\NormalTok{)}
\NormalTok{        enhanced\_df.at[idx, }\StringTok{\textquotesingle{}betweenness\_centrality\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ betweenness\_centrality.get(title, }\DecValTok{0}\NormalTok{)}

    \CommentTok{\# 3. Add Markov blanket information (parents, children, and children\textquotesingle{}s parents)}
\NormalTok{    enhanced\_df[}\StringTok{\textquotesingle{}markov\_blanket\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \VariableTok{None}

    \ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ enhanced\_df.iterrows():}
\NormalTok{        title }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}
\NormalTok{        parents }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Parents\textquotesingle{}}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(row[}\StringTok{\textquotesingle{}Parents\textquotesingle{}}\NormalTok{], }\BuiltInTok{list}\NormalTok{) }\ControlFlowTok{else}\NormalTok{ []}
\NormalTok{        children }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Children\textquotesingle{}}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(row[}\StringTok{\textquotesingle{}Children\textquotesingle{}}\NormalTok{], }\BuiltInTok{list}\NormalTok{) }\ControlFlowTok{else}\NormalTok{ []}

        \CommentTok{\# Get children\textquotesingle{}s parents (excluding this node)}
\NormalTok{        childrens\_parents }\OperatorTok{=}\NormalTok{ []}
        \ControlFlowTok{for}\NormalTok{ child }\KeywordTok{in}\NormalTok{ children:}
\NormalTok{            child\_row }\OperatorTok{=}\NormalTok{ enhanced\_df[enhanced\_df[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ child]}
            \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ child\_row.empty:}
\NormalTok{                child\_parents }\OperatorTok{=}\NormalTok{ child\_row.iloc[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}Parents\textquotesingle{}}\NormalTok{]}
                \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(child\_parents, }\BuiltInTok{list}\NormalTok{):}
\NormalTok{                    childrens\_parents.extend([p }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ child\_parents }\ControlFlowTok{if}\NormalTok{ p }\OperatorTok{!=}\NormalTok{ title])}

        \CommentTok{\# Remove duplicates}
\NormalTok{        childrens\_parents }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{set}\NormalTok{(childrens\_parents))}

        \CommentTok{\# Combine to get Markov blanket}
\NormalTok{        markov\_blanket }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{set}\NormalTok{(parents }\OperatorTok{+}\NormalTok{ children }\OperatorTok{+}\NormalTok{ childrens\_parents))}
\NormalTok{        enhanced\_df.at[idx, }\StringTok{\textquotesingle{}markov\_blanket\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ markov\_blanket}

    \ControlFlowTok{return}\NormalTok{ enhanced\_df}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# @title 3.3 {-}{-}{-} Enhance Extracted Data with Network Metrics {-}{-}{-}}

\CommentTok{"""}
\CommentTok{BLOCK PURPOSE: Applies the post{-}processing functions to enhance the extracted data.}

\CommentTok{This block takes the basic extracted DataFrame from the BayesDown parsing step}
\CommentTok{and enriches it with calculated metrics that provide deeper insight into the}
\CommentTok{network structure and relationships. It:}

\CommentTok{1. Applies the enhancement functions defined previously}
\CommentTok{2. Displays summary information about key calculated metrics}
\CommentTok{3. Saves the enhanced data for further analysis and visualization}

\CommentTok{The enhanced DataFrame provides a richer representation of the Bayesian network,}
\CommentTok{including measures of node importance and conditional relationships that are}
\CommentTok{essential for effective analysis and visualization.}

\CommentTok{DEPENDENCIES: enhance\_extracted\_data function}
\CommentTok{INPUTS: DataFrame with basic extracted BayesDown data}
\CommentTok{OUTPUTS: Enhanced DataFrame with additional calculated columns, saved to CSV}
\CommentTok{"""}

\CommentTok{\# Enhance the extracted dataframe with calculated columns}
\NormalTok{enhanced\_df }\OperatorTok{=}\NormalTok{ enhance\_extracted\_data(result\_df)}

\CommentTok{\# Display the enhanced dataframe}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Enhanced DataFrame with additional calculated columns:"}\NormalTok{)}
\NormalTok{enhanced\_df.head()}

\CommentTok{\# Check some calculated metrics}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Joint Probabilities Example:"}\NormalTok{)}
\NormalTok{example\_node }\OperatorTok{=}\NormalTok{ enhanced\_df.loc[}\DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}
\NormalTok{joint\_probs }\OperatorTok{=}\NormalTok{ enhanced\_df.loc[}\DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}joint\_probabilities\textquotesingle{}}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Joint probabilities for }\SpecialCharTok{\{}\NormalTok{example\_node}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(joint\_probs)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Network Metrics:"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ enhanced\_df.iterrows():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Degree Centrality: }\SpecialCharTok{\{}\NormalTok{row[}\StringTok{\textquotesingle{}degree\_centrality\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Betweenness Centrality: }\SpecialCharTok{\{}\NormalTok{row[}\StringTok{\textquotesingle{}betweenness\_centrality\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Save the enhanced dataframe}
\NormalTok{enhanced\_df.to\_csv(}\StringTok{\textquotesingle{}enhanced\_extracted\_data.csv\textquotesingle{}}\NormalTok{, index}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Enhanced data saved to \textquotesingle{}enhanced\_extracted\_data.csv\textquotesingle{}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Enhanced DataFrame with additional calculated columns:

Joint Probabilities Example:
Joint probabilities for Existential_Catastrophe:
None

Network Metrics:
Existential_Catastrophe:
  Degree Centrality: 0.000
  Betweenness Centrality: 0.000
Human_Disempowerment:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Scale_Of_Power_Seeking:
  Degree Centrality: 0.136
  Betweenness Centrality: 0.037
Misaligned_Power_Seeking:
  Degree Centrality: 0.182
  Betweenness Centrality: 0.056
APS_Systems:
  Degree Centrality: 0.182
  Betweenness Centrality: 0.019
Advanced_AI_Capability:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Agentic_Planning:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Strategic_Awareness:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Difficulty_Of_Alignment:
  Degree Centrality: 0.182
  Betweenness Centrality: 0.019
Instrumental_Convergence:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Problems_With_Proxies:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Problems_With_Search:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Deployment_Decisions:
  Degree Centrality: 0.136
  Betweenness Centrality: 0.026
Incentives_To_Build_APS:
  Degree Centrality: 0.136
  Betweenness Centrality: 0.017
Usefulness_Of_APS:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Competitive_Dynamics:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Deception_By_AI:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Corrective_Feedback:
  Degree Centrality: 0.136
  Betweenness Centrality: 0.009
Warning_Shots:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Rapid_Capability_Escalation:
  Degree Centrality: 0.045
  Betweenness Centrality: 0.000
Barriers_To_Understanding:
  Degree Centrality: 0.000
  Betweenness Centrality: 0.000
Adversarial_Dynamics:
  Degree Centrality: 0.000
  Betweenness Centrality: 0.000
Stakes_Of_Error:
  Degree Centrality: 0.000
  Betweenness Centrality: 0.000

Enhanced data saved to 'enhanced_extracted_data.csv'
\end{verbatim}

\subsection{3.4 Download and save finished data frame as .csv
file}\label{download-and-save-finished-data-frame-as-.csv-file}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# @title 3.4 {-}{-}{-} Save Extracted Data for Further Processing {-}{-}{-}}

\CommentTok{"""}
\CommentTok{BLOCK PURPOSE: Saves the extracted data to a CSV file for further processing.}

\CommentTok{This step is essential for:}
\CommentTok{1. Persisting the structured representation of the Bayesian network}
\CommentTok{2. Enabling further analysis in other tools or notebook sections}
\CommentTok{3. Creating a permanent record of the extraction results}
\CommentTok{4. Making the data available for the visualization pipeline}

\CommentTok{The CSV format provides a standardized, tabular representation of the network}
\CommentTok{that can be easily loaded and processed in subsequent analysis steps.}

\CommentTok{DEPENDENCIES: pandas DataFrame operations}
\CommentTok{INPUTS: Extracted DataFrame from the parsing step}
\CommentTok{OUTPUTS: CSV file containing the structured network data}
\CommentTok{"""}

\CommentTok{\# Save the extracted data as a CSV file}
\NormalTok{result\_df.to\_csv(}\StringTok{\textquotesingle{}extracted\_data.csv\textquotesingle{}}\NormalTok{, index}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"✅ Extracted data saved successfully to \textquotesingle{}extracted\_data.csv\textquotesingle{}"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Note: If using updated data in future steps, the file must be pushed to the GitHub repository"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
✅ Extracted data saved successfully to 'extracted_data.csv'
Note: If using updated data in future steps, the file must be pushed to the GitHub repository
\end{verbatim}

\bookmarksetup{startatroot}

\chapter{4. 4.0 Analysis \& Inference: Bayesian Network
Visualization}\label{analysis-inference-bayesian-network-visualization}

\section{Bayesian Network Visualization
Approach}\label{bayesian-network-visualization-approach}

This section implements the visualization component of the AMTAIR
project, transforming the structured data extracted from BayesDown into
an interactive network visualization that makes complex probabilistic
relationships accessible to human understanding.

\subsection{Visualization Philosophy}\label{visualization-philosophy}

A key challenge in AI governance is making complex probabilistic
relationships understandable to diverse stakeholders. This visualization
system addresses this challenge through:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Visual Encoding of Probability}: Node colors reflect
  probability values (green for high probability, red for low)
\item
  \textbf{Structural Classification}: Border colors indicate node types
  (blue for root causes, purple for intermediate nodes, magenta for leaf
  nodes)
\item
  \textbf{Progressive Disclosure}: Basic information in tooltips,
  detailed probability tables in modal popups
\item
  \textbf{Interactive Exploration}: Draggable nodes, configurable
  physics, click interactions
\end{enumerate}

\subsection{Connection to AMTAIR
Goals}\label{connection-to-amtair-goals}

This visualization approach directly supports the AMTAIR project's goal
of improving coordination in AI governance by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Making implicit models explicit through visual representation
\item
  Providing a common language for discussing probabilistic relationships
\item
  Enabling non-technical stakeholders to engage with formal models
\item
  Creating shareable artifacts that facilitate collaboration
\end{enumerate}

\subsection{Implementation Structure}\label{implementation-structure}

The visualization system is implemented in four phases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Network Construction}: Creating a directed graph
  representation using NetworkX
\item
  \textbf{Node Classification}: Identifying node types based on network
  position
\item
  \textbf{Visual Enhancement}: Adding color coding, tooltips, and
  interactive elements
\item
  \textbf{Interactive Features}: Implementing click handling for
  detailed exploration
\end{enumerate}

The resulting visualization serves as both an analytical tool for
experts and a communication tool for broader audiences, bridging the gap
between technical and policy domains in AI governance discussions.

\section{Phase 1:
Dependencies/Functions}\label{phase-1-dependenciesfunctions}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# @title 4.0 {-}{-}{-} Bayesian Network Visualization Functions {-}{-}{-}}

\CommentTok{"""}
\CommentTok{BLOCK PURPOSE: Provides functions to create interactive Bayesian network visualizations}
\CommentTok{from DataFrame representations of ArgDown/BayesDown data.}

\CommentTok{This block implements the visualization pipeline described in the AMTAIR project, transforming}
\CommentTok{the structured DataFrame extracted from ArgDown/BayesDown into an interactive network graph}
\CommentTok{that displays nodes, relationships, and probability information. The visualization leverages}
\CommentTok{NetworkX for graph representation and PyVis for interactive display.}

\CommentTok{Key visualization features:}
\CommentTok{1. Color{-}coding of nodes based on probability values}
\CommentTok{2. Border styling to indicate node types (root, intermediate, leaf)}
\CommentTok{3. Interactive tooltips with probability information}
\CommentTok{4. Modal popups with detailed conditional probability tables}
\CommentTok{5. Physics{-}based layout for intuitive exploration}

\CommentTok{DEPENDENCIES: networkx, pyvis, HTML display from IPython}
\CommentTok{INPUTS: DataFrame with node information, relationships, and probabilities}
\CommentTok{OUTPUTS: Interactive HTML visualization of the Bayesian network}
\CommentTok{"""}

\ImportTok{from}\NormalTok{ pyvis.network }\ImportTok{import}\NormalTok{ Network}
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}
\ImportTok{from}\NormalTok{ IPython.display }\ImportTok{import}\NormalTok{ HTML}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ io}
\ImportTok{import}\NormalTok{ base64}
\ImportTok{import}\NormalTok{ colorsys}
\ImportTok{import}\NormalTok{ json}

\KeywordTok{def}\NormalTok{ create\_bayesian\_network\_with\_probabilities(df):}
    \CommentTok{"""}
\CommentTok{    Create an interactive Bayesian network visualization with enhanced probability visualization}
\CommentTok{    and node classification based on network structure.}

\CommentTok{    Args:}
\CommentTok{        df (pandas.DataFrame): DataFrame containing node information, relationships, and probabilities}

\CommentTok{    Returns:}
\CommentTok{        IPython.display.HTML: Interactive HTML visualization of the Bayesian network}
\CommentTok{    """}
    \CommentTok{\# PHASE 1: Create a directed graph representation}
\NormalTok{    G }\OperatorTok{=}\NormalTok{ nx.DiGraph()}

    \CommentTok{\# Add nodes with proper attributes}
    \ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ df.iterrows():}
\NormalTok{        title }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}
\NormalTok{        description }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Description\textquotesingle{}}\NormalTok{]}

        \CommentTok{\# Process probability information}
\NormalTok{        priors }\OperatorTok{=}\NormalTok{ get\_priors(row)}
\NormalTok{        instantiations }\OperatorTok{=}\NormalTok{ get\_instantiations(row)}

        \CommentTok{\# Add node with base information}
\NormalTok{        G.add\_node(}
\NormalTok{            title,}
\NormalTok{            description}\OperatorTok{=}\NormalTok{description,}
\NormalTok{            priors}\OperatorTok{=}\NormalTok{priors,}
\NormalTok{            instantiations}\OperatorTok{=}\NormalTok{instantiations,}
\NormalTok{            posteriors}\OperatorTok{=}\NormalTok{get\_posteriors(row)}
\NormalTok{        )}

    \CommentTok{\# Add edges based on parent{-}child relationships}
    \ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ df.iterrows():}
\NormalTok{        child }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}
\NormalTok{        parents }\OperatorTok{=}\NormalTok{ get\_parents(row)}

        \CommentTok{\# Add edges from each parent to this child}
        \ControlFlowTok{for}\NormalTok{ parent }\KeywordTok{in}\NormalTok{ parents:}
            \ControlFlowTok{if}\NormalTok{ parent }\KeywordTok{in}\NormalTok{ G.nodes():}
\NormalTok{                G.add\_edge(parent, child)}

    \CommentTok{\# PHASE 2: Classify nodes based on network structure}
\NormalTok{    classify\_nodes(G)}

    \CommentTok{\# PHASE 3: Create interactive network visualization}
\NormalTok{    net }\OperatorTok{=}\NormalTok{ Network(notebook}\OperatorTok{=}\VariableTok{True}\NormalTok{, directed}\OperatorTok{=}\VariableTok{True}\NormalTok{, cdn\_resources}\OperatorTok{=}\StringTok{"in\_line"}\NormalTok{, height}\OperatorTok{=}\StringTok{"600px"}\NormalTok{, width}\OperatorTok{=}\StringTok{"100\%"}\NormalTok{)}

    \CommentTok{\# Configure physics for better layout}
\NormalTok{    net.force\_atlas\_2based(gravity}\OperatorTok{={-}}\DecValTok{50}\NormalTok{, spring\_length}\OperatorTok{=}\DecValTok{100}\NormalTok{, spring\_strength}\OperatorTok{=}\FloatTok{0.02}\NormalTok{)}
\NormalTok{    net.show\_buttons(filter\_}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}physics\textquotesingle{}}\NormalTok{])  }\CommentTok{\# Allow user to adjust physics settings}

    \CommentTok{\# Add the graph to the network}
\NormalTok{    net.from\_nx(G)}

    \CommentTok{\# PHASE 4: Enhance node appearance with probability information}
    \ControlFlowTok{for}\NormalTok{ node }\KeywordTok{in}\NormalTok{ net.nodes:}
\NormalTok{        node\_id }\OperatorTok{=}\NormalTok{ node[}\StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{]}
\NormalTok{        node\_data }\OperatorTok{=}\NormalTok{ G.nodes[node\_id]}

        \CommentTok{\# Get node type and set border color}
\NormalTok{        node\_type }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}node\_type\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}unknown\textquotesingle{}}\NormalTok{)}
\NormalTok{        border\_color }\OperatorTok{=}\NormalTok{ get\_border\_color(node\_type)}

        \CommentTok{\# Get probability information}
\NormalTok{        priors }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{, \{\})}
\NormalTok{        true\_prob }\OperatorTok{=}\NormalTok{ priors.get(}\StringTok{\textquotesingle{}true\_prob\textquotesingle{}}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\ControlFlowTok{if}\NormalTok{ priors }\ControlFlowTok{else} \FloatTok{0.5}

        \CommentTok{\# Get proper state names}
\NormalTok{        instantiations }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}instantiations\textquotesingle{}}\NormalTok{, [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{])}
\NormalTok{        true\_state }\OperatorTok{=}\NormalTok{ instantiations[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(instantiations) }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \StringTok{"TRUE"}
\NormalTok{        false\_state }\OperatorTok{=}\NormalTok{ instantiations[}\DecValTok{1}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(instantiations) }\OperatorTok{\textgreater{}} \DecValTok{1} \ControlFlowTok{else} \StringTok{"FALSE"}

        \CommentTok{\# Create background color based on probability}
\NormalTok{        background\_color }\OperatorTok{=}\NormalTok{ get\_probability\_color(priors)}

        \CommentTok{\# Create tooltip with probability information}
\NormalTok{        tooltip }\OperatorTok{=}\NormalTok{ create\_tooltip(node\_id, node\_data)}

        \CommentTok{\# Create a simpler node label with probability}
\NormalTok{        simple\_label }\OperatorTok{=} \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{node\_id}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{p=}\SpecialCharTok{\{}\NormalTok{true\_prob}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}

        \CommentTok{\# Store expanded content as a node attribute for use in click handler}
\NormalTok{        node\_data[}\StringTok{\textquotesingle{}expanded\_content\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ create\_expanded\_content(node\_id, node\_data)}

        \CommentTok{\# Set node attributes}
\NormalTok{        node[}\StringTok{\textquotesingle{}title\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ tooltip  }\CommentTok{\# Tooltip HTML}
\NormalTok{        node[}\StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ simple\_label  }\CommentTok{\# Simple text label}
\NormalTok{        node[}\StringTok{\textquotesingle{}shape\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \StringTok{\textquotesingle{}box\textquotesingle{}}
\NormalTok{        node[}\StringTok{\textquotesingle{}color\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ \{}
            \StringTok{\textquotesingle{}background\textquotesingle{}}\NormalTok{: background\_color,}
            \StringTok{\textquotesingle{}border\textquotesingle{}}\NormalTok{: border\_color,}
            \StringTok{\textquotesingle{}highlight\textquotesingle{}}\NormalTok{: \{}
                \StringTok{\textquotesingle{}background\textquotesingle{}}\NormalTok{: background\_color,}
                \StringTok{\textquotesingle{}border\textquotesingle{}}\NormalTok{: border\_color}
\NormalTok{            \}}
\NormalTok{        \}}

    \CommentTok{\# PHASE 5: Setup interactive click handling}
    \CommentTok{\# Prepare data for click handler}
\NormalTok{    setup\_data }\OperatorTok{=}\NormalTok{ \{}
        \StringTok{\textquotesingle{}nodes\_data\textquotesingle{}}\NormalTok{: \{node\_id: \{}
            \StringTok{\textquotesingle{}expanded\_content\textquotesingle{}}\NormalTok{: json.dumps(G.nodes[node\_id].get(}\StringTok{\textquotesingle{}expanded\_content\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)),}
            \StringTok{\textquotesingle{}description\textquotesingle{}}\NormalTok{: G.nodes[node\_id].get(}\StringTok{\textquotesingle{}description\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{),}
            \StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{: G.nodes[node\_id].get(}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{, \{\}),}
            \StringTok{\textquotesingle{}posteriors\textquotesingle{}}\NormalTok{: G.nodes[node\_id].get(}\StringTok{\textquotesingle{}posteriors\textquotesingle{}}\NormalTok{, \{\})}
\NormalTok{        \} }\ControlFlowTok{for}\NormalTok{ node\_id }\KeywordTok{in}\NormalTok{ G.nodes()\}}
\NormalTok{    \}}

    \CommentTok{\# JavaScript code for handling node clicks}
\NormalTok{    click\_js }\OperatorTok{=} \StringTok{"""}
\StringTok{    // Store node data for click handling}
\StringTok{    var nodesData = }\SpecialCharTok{\%s}\StringTok{;}

\StringTok{    // Add event listener for node clicks}
\StringTok{    network.on("click", function(params) \{}
\StringTok{        if (params.nodes.length \textgreater{} 0) \{}
\StringTok{            var nodeId = params.nodes[0];}
\StringTok{            var nodeInfo = nodesData[nodeId];}

\StringTok{            if (nodeInfo) \{}
\StringTok{                // Create a modal popup for expanded content}
\StringTok{                var modal = document.createElement(\textquotesingle{}div\textquotesingle{});}
\StringTok{                modal.style.position = \textquotesingle{}fixed\textquotesingle{};}
\StringTok{                modal.style.left = \textquotesingle{}50}\SpecialCharTok{\%\%}\StringTok{\textquotesingle{};}
\StringTok{                modal.style.top = \textquotesingle{}50}\SpecialCharTok{\%\%}\StringTok{\textquotesingle{};}
\StringTok{                modal.style.transform = \textquotesingle{}translate({-}50}\SpecialCharTok{\%\%}\StringTok{, {-}50}\SpecialCharTok{\%\%}\StringTok{)\textquotesingle{};}
\StringTok{                modal.style.backgroundColor = \textquotesingle{}white\textquotesingle{};}
\StringTok{                modal.style.padding = \textquotesingle{}20px\textquotesingle{};}
\StringTok{                modal.style.borderRadius = \textquotesingle{}5px\textquotesingle{};}
\StringTok{                modal.style.boxShadow = \textquotesingle{}0 0 10px rgba(0,0,0,0.5)\textquotesingle{};}
\StringTok{                modal.style.zIndex = \textquotesingle{}1000\textquotesingle{};}
\StringTok{                modal.style.maxWidth = \textquotesingle{}80}\SpecialCharTok{\%\%}\StringTok{\textquotesingle{};}
\StringTok{                modal.style.maxHeight = \textquotesingle{}80}\SpecialCharTok{\%\%}\StringTok{\textquotesingle{};}
\StringTok{                modal.style.overflow = \textquotesingle{}auto\textquotesingle{};}

\StringTok{                // Add expanded content}
\StringTok{                modal.innerHTML = nodeInfo.expanded\_content || \textquotesingle{}No detailed information available\textquotesingle{};}

\StringTok{                // Add close button}
\StringTok{                var closeBtn = document.createElement(\textquotesingle{}button\textquotesingle{});}
\StringTok{                closeBtn.innerHTML = \textquotesingle{}Close\textquotesingle{};}
\StringTok{                closeBtn.style.marginTop = \textquotesingle{}10px\textquotesingle{};}
\StringTok{                closeBtn.style.padding = \textquotesingle{}5px 10px\textquotesingle{};}
\StringTok{                closeBtn.style.cursor = \textquotesingle{}pointer\textquotesingle{};}
\StringTok{                closeBtn.onclick = function() \{}
\StringTok{                    document.body.removeChild(modal);}
\StringTok{                \};}
\StringTok{                modal.appendChild(closeBtn);}

\StringTok{                // Add modal to body}
\StringTok{                document.body.appendChild(modal);}
\StringTok{            \}}
\StringTok{        \}}
\StringTok{    \});}
\StringTok{    """} \OperatorTok{\%}\NormalTok{ json.dumps(setup\_data[}\StringTok{\textquotesingle{}nodes\_data\textquotesingle{}}\NormalTok{])}

    \CommentTok{\# PHASE 6: Save the graph to HTML and inject custom click handling}
\NormalTok{    html\_file }\OperatorTok{=} \StringTok{"bayesian\_network.html"}
\NormalTok{    net.save\_graph(html\_file)}

    \CommentTok{\# Inject custom click handling into HTML}
    \ControlFlowTok{try}\NormalTok{:}
        \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(html\_file, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{            html\_content }\OperatorTok{=}\NormalTok{ f.read()}

        \CommentTok{\# Insert click handling script before the closing body tag}
\NormalTok{        html\_content }\OperatorTok{=}\NormalTok{ html\_content.replace(}\StringTok{\textquotesingle{}\textless{}/body\textgreater{}\textquotesingle{}}\NormalTok{, }\SpecialStringTok{f\textquotesingle{}\textless{}script\textgreater{}}\SpecialCharTok{\{}\NormalTok{click\_js}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/script\textgreater{}\textless{}/body\textgreater{}\textquotesingle{}}\NormalTok{)}

        \CommentTok{\# Write back the modified HTML}
        \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(html\_file, }\StringTok{"w"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{            f.write(html\_content)}

        \ControlFlowTok{return}\NormalTok{ HTML(html\_content)}
    \ControlFlowTok{except} \PreprocessorTok{Exception} \ImportTok{as}\NormalTok{ e:}
        \ControlFlowTok{return}\NormalTok{ HTML(}\SpecialStringTok{f"\textless{}p\textgreater{}Error rendering HTML: }\SpecialCharTok{\{}\BuiltInTok{str}\NormalTok{(e)}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/p\textgreater{}\textless{}p\textgreater{}The network visualization has been saved to \textquotesingle{}}\SpecialCharTok{\{}\NormalTok{html\_file}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}\textless{}/p\textgreater{}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Phase 2: Node Classification and Styling
Module}\label{phase-2-node-classification-and-styling-module}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# @title 4.1 {-}{-}{-} Node Classification and Styling Functions {-}{-}{-}}

\CommentTok{"""}
\CommentTok{BLOCK PURPOSE: Implements the visual classification and styling of nodes in the Bayesian network.}

\CommentTok{This module handles the identification of node types based on their position in the network}
\CommentTok{and provides appropriate visual styling for each type. The functions:}

\CommentTok{1. Classify nodes as parents (causes), children (intermediate effects), or leaves (final effects)}
\CommentTok{2. Assign appropriate border colors to visually distinguish node types}
\CommentTok{3. Calculate background colors based on probability values}
\CommentTok{4. Extract relevant information from DataFrame rows in a robust manner}

\CommentTok{The visual encoding helps users understand both the structure of the network}
\CommentTok{and the probability distributions at a glance.}

\CommentTok{DEPENDENCIES: colorsys for color manipulation}
\CommentTok{INPUTS: Graph structure and node data}
\CommentTok{OUTPUTS: Classification and styling information for visualization}
\CommentTok{"""}

\KeywordTok{def}\NormalTok{ classify\_nodes(G):}
    \CommentTok{"""}
\CommentTok{    Classify nodes as parent, child, or leaf based on network structure}

\CommentTok{    Args:}
\CommentTok{        G (networkx.DiGraph): Directed graph representation of the Bayesian network}

\CommentTok{    Effects:}
\CommentTok{        Adds \textquotesingle{}node\_type\textquotesingle{} attribute to each node in the graph:}
\CommentTok{        {-} \textquotesingle{}parent\textquotesingle{}: Root node with no parents but has children (causal source)}
\CommentTok{        {-} \textquotesingle{}child\textquotesingle{}: Node with both parents and children (intermediate)}
\CommentTok{        {-} \textquotesingle{}leaf\textquotesingle{}: Node with parents but no children (final effect)}
\CommentTok{        {-} \textquotesingle{}isolated\textquotesingle{}: Node with no connections (rare in Bayesian networks)}
\CommentTok{    """}
    \ControlFlowTok{for}\NormalTok{ node }\KeywordTok{in}\NormalTok{ G.nodes():}
\NormalTok{        predecessors }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(G.predecessors(node))  }\CommentTok{\# Nodes pointing to this one (causes)}
\NormalTok{        successors }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(G.successors(node))      }\CommentTok{\# Nodes this one points to (effects)}

        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ predecessors:  }\CommentTok{\# No parents}
            \ControlFlowTok{if}\NormalTok{ successors:  }\CommentTok{\# Has children}
\NormalTok{                G.nodes[node][}\StringTok{\textquotesingle{}node\_type\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \StringTok{\textquotesingle{}parent\textquotesingle{}}  \CommentTok{\# Root cause}
            \ControlFlowTok{else}\NormalTok{:  }\CommentTok{\# No children either}
\NormalTok{                G.nodes[node][}\StringTok{\textquotesingle{}node\_type\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \StringTok{\textquotesingle{}isolated\textquotesingle{}}  \CommentTok{\# Disconnected node}
        \ControlFlowTok{else}\NormalTok{:  }\CommentTok{\# Has parents}
            \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ successors:  }\CommentTok{\# No children}
\NormalTok{                G.nodes[node][}\StringTok{\textquotesingle{}node\_type\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \StringTok{\textquotesingle{}leaf\textquotesingle{}}  \CommentTok{\# Final effect}
            \ControlFlowTok{else}\NormalTok{:  }\CommentTok{\# Has both parents and children}
\NormalTok{                G.nodes[node][}\StringTok{\textquotesingle{}node\_type\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \StringTok{\textquotesingle{}child\textquotesingle{}}  \CommentTok{\# Intermediate node}

\KeywordTok{def}\NormalTok{ get\_border\_color(node\_type):}
    \CommentTok{"""}
\CommentTok{    Return border color based on node type}

\CommentTok{    Args:}
\CommentTok{        node\_type (str): Type of node (\textquotesingle{}parent\textquotesingle{}, \textquotesingle{}child\textquotesingle{}, \textquotesingle{}leaf\textquotesingle{}, or \textquotesingle{}isolated\textquotesingle{})}

\CommentTok{    Returns:}
\CommentTok{        str: Hex color code for node border}
\CommentTok{    """}
    \ControlFlowTok{if}\NormalTok{ node\_type }\OperatorTok{==} \StringTok{\textquotesingle{}parent\textquotesingle{}}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}\#0000FF\textquotesingle{}}  \CommentTok{\# Blue for root causes}
    \ControlFlowTok{elif}\NormalTok{ node\_type }\OperatorTok{==} \StringTok{\textquotesingle{}child\textquotesingle{}}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}\#800080\textquotesingle{}}  \CommentTok{\# Purple for intermediate nodes}
    \ControlFlowTok{elif}\NormalTok{ node\_type }\OperatorTok{==} \StringTok{\textquotesingle{}leaf\textquotesingle{}}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}\#FF00FF\textquotesingle{}}  \CommentTok{\# Magenta for final effects}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}\#000000\textquotesingle{}}  \CommentTok{\# Default black for any other type}

\KeywordTok{def}\NormalTok{ get\_probability\_color(priors):}
    \CommentTok{"""}
\CommentTok{    Create background color based on probability (red to green gradient)}

\CommentTok{    Args:}
\CommentTok{        priors (dict): Dictionary containing probability information}

\CommentTok{    Returns:}
\CommentTok{        str: Hex color code for node background, ranging from red (low probability)}
\CommentTok{             to green (high probability)}
\CommentTok{    """}
    \CommentTok{\# Default to neutral color if no probability}
    \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ priors }\KeywordTok{or} \StringTok{\textquotesingle{}true\_prob\textquotesingle{}} \KeywordTok{not} \KeywordTok{in}\NormalTok{ priors:}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}\#F8F8F8\textquotesingle{}}  \CommentTok{\# Light grey}

    \CommentTok{\# Get probability value}
\NormalTok{    prob }\OperatorTok{=}\NormalTok{ priors[}\StringTok{\textquotesingle{}true\_prob\textquotesingle{}}\NormalTok{]}

    \CommentTok{\# Create color gradient from red (0.0) to green (1.0)}
\NormalTok{    hue }\OperatorTok{=} \DecValTok{120} \OperatorTok{*}\NormalTok{ prob  }\CommentTok{\# 0 = red, 120 = green (in HSL color space)}
\NormalTok{    saturation }\OperatorTok{=} \FloatTok{0.75}
\NormalTok{    lightness }\OperatorTok{=} \FloatTok{0.8}  \CommentTok{\# Lighter color for better text visibility}

    \CommentTok{\# Convert HSL to RGB}
\NormalTok{    r, g, b }\OperatorTok{=}\NormalTok{ colorsys.hls\_to\_rgb(hue}\OperatorTok{/}\DecValTok{360}\NormalTok{, lightness, saturation)}

    \CommentTok{\# Convert to hex format}
\NormalTok{    hex\_color }\OperatorTok{=} \StringTok{"\#}\SpecialCharTok{\{:02x\}\{:02x\}\{:02x\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(}\BuiltInTok{int}\NormalTok{(r}\OperatorTok{*}\DecValTok{255}\NormalTok{), }\BuiltInTok{int}\NormalTok{(g}\OperatorTok{*}\DecValTok{255}\NormalTok{), }\BuiltInTok{int}\NormalTok{(b}\OperatorTok{*}\DecValTok{255}\NormalTok{))}

    \ControlFlowTok{return}\NormalTok{ hex\_color}

\KeywordTok{def}\NormalTok{ get\_parents(row):}
    \CommentTok{"""}
\CommentTok{    Extract parent nodes from row data, with safe handling for different data types}

\CommentTok{    Args:}
\CommentTok{        row (pandas.Series): Row from DataFrame containing node information}

\CommentTok{    Returns:}
\CommentTok{        list: List of parent node names}
\CommentTok{    """}
    \ControlFlowTok{if} \StringTok{\textquotesingle{}Parents\textquotesingle{}} \KeywordTok{not} \KeywordTok{in}\NormalTok{ row:}
        \ControlFlowTok{return}\NormalTok{ []}

\NormalTok{    parents\_data }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Parents\textquotesingle{}}\NormalTok{]}

    \CommentTok{\# Handle NaN, None, or empty list}
    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(parents\_data, }\BuiltInTok{float}\NormalTok{) }\KeywordTok{and}\NormalTok{ pd.isna(parents\_data):}
        \ControlFlowTok{return}\NormalTok{ []}

    \ControlFlowTok{if}\NormalTok{ parents\_data }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ []}

    \CommentTok{\# Handle different data types}
    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(parents\_data, }\BuiltInTok{list}\NormalTok{):}
        \CommentTok{\# Return a list with NaN and empty strings removed}
        \ControlFlowTok{return}\NormalTok{ [p }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ parents\_data }\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ (}\BuiltInTok{isinstance}\NormalTok{(p, }\BuiltInTok{float}\NormalTok{) }\KeywordTok{and}\NormalTok{ pd.isna(p)) }\KeywordTok{and}\NormalTok{ p }\OperatorTok{!=} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{]}

    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(parents\_data, }\BuiltInTok{str}\NormalTok{):}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ parents\_data.strip():}
            \ControlFlowTok{return}\NormalTok{ []}

        \CommentTok{\# Remove brackets and split by comma, removing empty strings and NaN}
\NormalTok{        cleaned }\OperatorTok{=}\NormalTok{ parents\_data.strip(}\StringTok{\textquotesingle{}[]"}\CharTok{\textbackslash{}\textquotesingle{}}\StringTok{\textquotesingle{}}\NormalTok{)}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ cleaned:}
            \ControlFlowTok{return}\NormalTok{ []}

        \ControlFlowTok{return}\NormalTok{ [p.strip(}\StringTok{\textquotesingle{} "}\CharTok{\textbackslash{}\textquotesingle{}}\StringTok{\textquotesingle{}}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ cleaned.split(}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{) }\ControlFlowTok{if}\NormalTok{ p.strip()]}

    \CommentTok{\# Default: empty list}
    \ControlFlowTok{return}\NormalTok{ []}

\KeywordTok{def}\NormalTok{ get\_instantiations(row):}
    \CommentTok{"""}
\CommentTok{    Extract instantiations with safe handling for different data types}

\CommentTok{    Args:}
\CommentTok{        row (pandas.Series): Row from DataFrame containing node information}

\CommentTok{    Returns:}
\CommentTok{        list: List of possible instantiations (states) for the node}
\CommentTok{    """}
    \ControlFlowTok{if} \StringTok{\textquotesingle{}instantiations\textquotesingle{}} \KeywordTok{not} \KeywordTok{in}\NormalTok{ row:}
        \ControlFlowTok{return}\NormalTok{ [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{]}

\NormalTok{    inst\_data }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}instantiations\textquotesingle{}}\NormalTok{]}

    \CommentTok{\# Handle NaN or None}
    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(inst\_data, }\BuiltInTok{float}\NormalTok{) }\KeywordTok{and}\NormalTok{ pd.isna(inst\_data):}
        \ControlFlowTok{return}\NormalTok{ [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{]}

    \ControlFlowTok{if}\NormalTok{ inst\_data }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{]}

    \CommentTok{\# Handle different data types}
    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(inst\_data, }\BuiltInTok{list}\NormalTok{):}
        \ControlFlowTok{return}\NormalTok{ inst\_data }\ControlFlowTok{if}\NormalTok{ inst\_data }\ControlFlowTok{else}\NormalTok{ [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{]}

    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(inst\_data, }\BuiltInTok{str}\NormalTok{):}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ inst\_data.strip():}
            \ControlFlowTok{return}\NormalTok{ [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{]}

        \CommentTok{\# Remove brackets and split by comma}
\NormalTok{        cleaned }\OperatorTok{=}\NormalTok{ inst\_data.strip(}\StringTok{\textquotesingle{}[]"}\CharTok{\textbackslash{}\textquotesingle{}}\StringTok{\textquotesingle{}}\NormalTok{)}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ cleaned:}
            \ControlFlowTok{return}\NormalTok{ [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{]}

        \ControlFlowTok{return}\NormalTok{ [i.strip(}\StringTok{\textquotesingle{} "}\CharTok{\textbackslash{}\textquotesingle{}}\StringTok{\textquotesingle{}}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ cleaned.split(}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{) }\ControlFlowTok{if}\NormalTok{ i.strip()]}

    \CommentTok{\# Default}
    \ControlFlowTok{return}\NormalTok{ [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{]}

\KeywordTok{def}\NormalTok{ get\_priors(row):}
    \CommentTok{"""}
\CommentTok{    Extract prior probabilities with safe handling for different data types}

\CommentTok{    Args:}
\CommentTok{        row (pandas.Series): Row from DataFrame containing node information}

\CommentTok{    Returns:}
\CommentTok{        dict: Dictionary of prior probabilities with \textquotesingle{}true\_prob\textquotesingle{} added for convenience}
\CommentTok{    """}
    \ControlFlowTok{if} \StringTok{\textquotesingle{}priors\textquotesingle{}} \KeywordTok{not} \KeywordTok{in}\NormalTok{ row:}
        \ControlFlowTok{return}\NormalTok{ \{\}}

\NormalTok{    priors\_data }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{]}

    \CommentTok{\# Handle NaN or None}
    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(priors\_data, }\BuiltInTok{float}\NormalTok{) }\KeywordTok{and}\NormalTok{ pd.isna(priors\_data):}
        \ControlFlowTok{return}\NormalTok{ \{\}}

    \ControlFlowTok{if}\NormalTok{ priors\_data }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ \{\}}

\NormalTok{    result }\OperatorTok{=}\NormalTok{ \{\}}

    \CommentTok{\# Handle dictionary}
    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(priors\_data, }\BuiltInTok{dict}\NormalTok{):}
\NormalTok{        result }\OperatorTok{=}\NormalTok{ priors\_data}
    \CommentTok{\# Handle string representation of dictionary}
    \ControlFlowTok{elif} \BuiltInTok{isinstance}\NormalTok{(priors\_data, }\BuiltInTok{str}\NormalTok{):}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ priors\_data.strip() }\KeywordTok{or}\NormalTok{ priors\_data }\OperatorTok{==} \StringTok{\textquotesingle{}}\SpecialCharTok{\{\}}\StringTok{\textquotesingle{}}\NormalTok{:}
            \ControlFlowTok{return}\NormalTok{ \{\}}

        \ControlFlowTok{try}\NormalTok{:}
            \CommentTok{\# Try to evaluate as Python literal}
            \ImportTok{import}\NormalTok{ ast}
\NormalTok{            result }\OperatorTok{=}\NormalTok{ ast.literal\_eval(priors\_data)}
        \ControlFlowTok{except}\NormalTok{:}
            \CommentTok{\# Simple parsing for items like \{\textquotesingle{}p(TRUE)\textquotesingle{}: \textquotesingle{}0.2\textquotesingle{}, \textquotesingle{}p(FALSE)\textquotesingle{}: \textquotesingle{}0.8\textquotesingle{}\}}
            \ControlFlowTok{if} \StringTok{\textquotesingle{}\{\textquotesingle{}} \KeywordTok{in}\NormalTok{ priors\_data }\KeywordTok{and} \StringTok{\textquotesingle{}\}\textquotesingle{}} \KeywordTok{in}\NormalTok{ priors\_data:}
\NormalTok{                content }\OperatorTok{=}\NormalTok{ priors\_data[priors\_data.find(}\StringTok{\textquotesingle{}\{\textquotesingle{}}\NormalTok{)}\OperatorTok{+}\DecValTok{1}\NormalTok{:priors\_data.rfind(}\StringTok{\textquotesingle{}\}\textquotesingle{}}\NormalTok{)]}
\NormalTok{                items }\OperatorTok{=}\NormalTok{ [item.strip() }\ControlFlowTok{for}\NormalTok{ item }\KeywordTok{in}\NormalTok{ content.split(}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{)]}

                \ControlFlowTok{for}\NormalTok{ item }\KeywordTok{in}\NormalTok{ items:}
                    \ControlFlowTok{if} \StringTok{\textquotesingle{}:\textquotesingle{}} \KeywordTok{in}\NormalTok{ item:}
\NormalTok{                        key, value }\OperatorTok{=}\NormalTok{ item.split(}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{                        key }\OperatorTok{=}\NormalTok{ key.strip(}\StringTok{\textquotesingle{} }\CharTok{\textbackslash{}\textquotesingle{}\textbackslash{}"}\StringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{                        value }\OperatorTok{=}\NormalTok{ value.strip(}\StringTok{\textquotesingle{} }\CharTok{\textbackslash{}\textquotesingle{}\textbackslash{}"}\StringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{                        result[key] }\OperatorTok{=}\NormalTok{ value}

    \CommentTok{\# Extract main probability for TRUE state}
\NormalTok{    instantiations }\OperatorTok{=}\NormalTok{ get\_instantiations(row)}
\NormalTok{    true\_state }\OperatorTok{=}\NormalTok{ instantiations[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{if}\NormalTok{ instantiations }\ControlFlowTok{else} \StringTok{"TRUE"}
\NormalTok{    true\_key }\OperatorTok{=} \SpecialStringTok{f"p(}\SpecialCharTok{\{}\NormalTok{true\_state}\SpecialCharTok{\}}\SpecialStringTok{)"}

    \ControlFlowTok{if}\NormalTok{ true\_key }\KeywordTok{in}\NormalTok{ result:}
        \ControlFlowTok{try}\NormalTok{:}
\NormalTok{            result[}\StringTok{\textquotesingle{}true\_prob\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(result[true\_key])}
        \ControlFlowTok{except}\NormalTok{:}
            \ControlFlowTok{pass}

    \ControlFlowTok{return}\NormalTok{ result}

\KeywordTok{def}\NormalTok{ get\_posteriors(row):}
    \CommentTok{"""}
\CommentTok{    Extract posterior probabilities with safe handling for different data types}

\CommentTok{    Args:}
\CommentTok{        row (pandas.Series): Row from DataFrame containing node information}

\CommentTok{    Returns:}
\CommentTok{        dict: Dictionary of conditional probabilities}
\CommentTok{    """}
    \ControlFlowTok{if} \StringTok{\textquotesingle{}posteriors\textquotesingle{}} \KeywordTok{not} \KeywordTok{in}\NormalTok{ row:}
        \ControlFlowTok{return}\NormalTok{ \{\}}

\NormalTok{    posteriors\_data }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}posteriors\textquotesingle{}}\NormalTok{]}

    \CommentTok{\# Handle NaN or None}
    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(posteriors\_data, }\BuiltInTok{float}\NormalTok{) }\KeywordTok{and}\NormalTok{ pd.isna(posteriors\_data):}
        \ControlFlowTok{return}\NormalTok{ \{\}}

    \ControlFlowTok{if}\NormalTok{ posteriors\_data }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ \{\}}

\NormalTok{    result }\OperatorTok{=}\NormalTok{ \{\}}

    \CommentTok{\# Handle dictionary}
    \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(posteriors\_data, }\BuiltInTok{dict}\NormalTok{):}
\NormalTok{        result }\OperatorTok{=}\NormalTok{ posteriors\_data}
    \CommentTok{\# Handle string representation of dictionary}
    \ControlFlowTok{elif} \BuiltInTok{isinstance}\NormalTok{(posteriors\_data, }\BuiltInTok{str}\NormalTok{):}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ posteriors\_data.strip() }\KeywordTok{or}\NormalTok{ posteriors\_data }\OperatorTok{==} \StringTok{\textquotesingle{}}\SpecialCharTok{\{\}}\StringTok{\textquotesingle{}}\NormalTok{:}
            \ControlFlowTok{return}\NormalTok{ \{\}}

        \ControlFlowTok{try}\NormalTok{:}
            \CommentTok{\# Try to evaluate as Python literal}
            \ImportTok{import}\NormalTok{ ast}
\NormalTok{            result }\OperatorTok{=}\NormalTok{ ast.literal\_eval(posteriors\_data)}
        \ControlFlowTok{except}\NormalTok{:}
            \CommentTok{\# Simple parsing}
            \ControlFlowTok{if} \StringTok{\textquotesingle{}\{\textquotesingle{}} \KeywordTok{in}\NormalTok{ posteriors\_data }\KeywordTok{and} \StringTok{\textquotesingle{}\}\textquotesingle{}} \KeywordTok{in}\NormalTok{ posteriors\_data:}
\NormalTok{                content }\OperatorTok{=}\NormalTok{ posteriors\_data[posteriors\_data.find(}\StringTok{\textquotesingle{}\{\textquotesingle{}}\NormalTok{)}\OperatorTok{+}\DecValTok{1}\NormalTok{:posteriors\_data.rfind(}\StringTok{\textquotesingle{}\}\textquotesingle{}}\NormalTok{)]}
\NormalTok{                items }\OperatorTok{=}\NormalTok{ [item.strip() }\ControlFlowTok{for}\NormalTok{ item }\KeywordTok{in}\NormalTok{ content.split(}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{)]}

                \ControlFlowTok{for}\NormalTok{ item }\KeywordTok{in}\NormalTok{ items:}
                    \ControlFlowTok{if} \StringTok{\textquotesingle{}:\textquotesingle{}} \KeywordTok{in}\NormalTok{ item:}
\NormalTok{                        key, value }\OperatorTok{=}\NormalTok{ item.split(}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{                        key }\OperatorTok{=}\NormalTok{ key.strip(}\StringTok{\textquotesingle{} }\CharTok{\textbackslash{}\textquotesingle{}\textbackslash{}"}\StringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{                        value }\OperatorTok{=}\NormalTok{ value.strip(}\StringTok{\textquotesingle{} }\CharTok{\textbackslash{}\textquotesingle{}\textbackslash{}"}\StringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{                        result[key] }\OperatorTok{=}\NormalTok{ value}

    \ControlFlowTok{return}\NormalTok{ result}
\end{Highlighting}
\end{Shaded}

\section{Phase 3: HTML Content Generation
Module}\label{phase-3-html-content-generation-module}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# @title 4.2 {-}{-}{-} HTML Content Generation Functions {-}{-}{-}}

\CommentTok{"""}
\CommentTok{BLOCK PURPOSE: Creates rich HTML content for the interactive Bayesian network visualization.}

\CommentTok{This module generates the HTML components that enhance the Bayesian network visualization:}
\CommentTok{1. Probability bars {-} Visual representation of probability distributions}
\CommentTok{2. Node tooltips {-} Rich information displayed on hover}
\CommentTok{3. Expanded content {-} Detailed probability information shown when clicking nodes}

\CommentTok{These HTML components make the mathematical concepts of Bayesian networks more}
\CommentTok{intuitive and accessible to users without requiring deep statistical knowledge.}
\CommentTok{The visual encoding of probabilities (colors, bars) and the progressive disclosure}
\CommentTok{of information (hover, click) help users build understanding at their own pace.}

\CommentTok{DEPENDENCIES: HTML generation capabilities}
\CommentTok{INPUTS: Node data from the Bayesian network}
\CommentTok{OUTPUTS: HTML content for visualization components}
\CommentTok{"""}

\KeywordTok{def}\NormalTok{ create\_probability\_bar(true\_prob, false\_prob, height}\OperatorTok{=}\StringTok{"15px"}\NormalTok{, show\_values}\OperatorTok{=}\VariableTok{True}\NormalTok{, value\_prefix}\OperatorTok{=}\StringTok{""}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Creates a reusable HTML component to visualize probability distribution}

\CommentTok{    Args:}
\CommentTok{        true\_prob (float): Probability of the true state (0.0{-}1.0)}
\CommentTok{        false\_prob (float): Probability of the false state (0.0{-}1.0)}
\CommentTok{        height (str): CSS height of the bar}
\CommentTok{        show\_values (bool): Whether to display numerical values}
\CommentTok{        value\_prefix (str): Prefix to add before values (e.g., "p=")}

\CommentTok{    Returns:}
\CommentTok{        str: HTML for a horizontal bar showing probabilities}
\CommentTok{    """}
    \CommentTok{\# Prepare display labels if showing values}
\NormalTok{    true\_label }\OperatorTok{=} \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{value\_prefix}\SpecialCharTok{\}\{}\NormalTok{true\_prob}\SpecialCharTok{:.3f\}}\SpecialStringTok{"} \ControlFlowTok{if}\NormalTok{ show\_values }\ControlFlowTok{else} \StringTok{""}
\NormalTok{    false\_label }\OperatorTok{=} \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{value\_prefix}\SpecialCharTok{\}\{}\NormalTok{false\_prob}\SpecialCharTok{:.3f\}}\SpecialStringTok{"} \ControlFlowTok{if}\NormalTok{ show\_values }\ControlFlowTok{else} \StringTok{""}

    \CommentTok{\# Create the HTML for a horizontal stacked bar}
\NormalTok{    html }\OperatorTok{=} \SpecialStringTok{f"""}
\SpecialStringTok{    \textless{}div style="width:100\%; height:}\SpecialCharTok{\{}\NormalTok{height}\SpecialCharTok{\}}\SpecialStringTok{; display:flex; border:1px solid \#ccc; overflow:hidden; border{-}radius:3px; margin{-}top:3px; margin{-}bottom:3px;"\textgreater{}}
\SpecialStringTok{        \textless{}div style="flex{-}basis:}\SpecialCharTok{\{}\NormalTok{true\_prob}\OperatorTok{*}\DecValTok{100}\SpecialCharTok{\}}\SpecialStringTok{\%; background:linear{-}gradient(to bottom, rgba(0,180,0,0.9), rgba(0,140,0,0.7)); border{-}right:2px solid \#008800; display:flex; align{-}items:center; justify{-}content:center; overflow:hidden; min{-}width:}\SpecialCharTok{\{}\DecValTok{2} \ControlFlowTok{if}\NormalTok{ true\_prob }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \DecValTok{0}\SpecialCharTok{\}}\SpecialStringTok{px;"\textgreater{}}
\SpecialStringTok{            \textless{}span style="font{-}size:10px; color:white; text{-}shadow:0px 0px 2px \#000;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{true\_label}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/span\textgreater{}}
\SpecialStringTok{        \textless{}/div\textgreater{}}
\SpecialStringTok{        \textless{}div style="flex{-}basis:}\SpecialCharTok{\{}\NormalTok{false\_prob}\OperatorTok{*}\DecValTok{100}\SpecialCharTok{\}}\SpecialStringTok{\%; background:linear{-}gradient(to bottom, rgba(220,0,0,0.9), rgba(180,0,0,0.7)); border{-}left:2px solid \#880000; display:flex; align{-}items:center; justify{-}content:center; overflow:hidden; min{-}width:}\SpecialCharTok{\{}\DecValTok{2} \ControlFlowTok{if}\NormalTok{ false\_prob }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \DecValTok{0}\SpecialCharTok{\}}\SpecialStringTok{px;"\textgreater{}}
\SpecialStringTok{            \textless{}span style="font{-}size:10px; color:white; text{-}shadow:0px 0px 2px \#000;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{false\_label}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/span\textgreater{}}
\SpecialStringTok{        \textless{}/div\textgreater{}}
\SpecialStringTok{    \textless{}/div\textgreater{}}
\SpecialStringTok{    """}
    \ControlFlowTok{return}\NormalTok{ html}

\KeywordTok{def}\NormalTok{ create\_tooltip(node\_id, node\_data):}
    \CommentTok{"""}
\CommentTok{    Create rich HTML tooltip with probability information}

\CommentTok{    Args:}
\CommentTok{        node\_id (str): Identifier of the node}
\CommentTok{        node\_data (dict): Node attributes including probabilities}

\CommentTok{    Returns:}
\CommentTok{        str: HTML content for tooltip displayed on hover}
\CommentTok{    """}
    \CommentTok{\# Extract node information}
\NormalTok{    description }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}description\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{    priors }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{, \{\})}
\NormalTok{    instantiations }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}instantiations\textquotesingle{}}\NormalTok{, [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{])}

    \CommentTok{\# Start building the HTML tooltip}
\NormalTok{    html }\OperatorTok{=} \SpecialStringTok{f"""}
\SpecialStringTok{    \textless{}div style="max{-}width:350px; padding:10px; background{-}color:\#f8f9fa; border{-}radius:5px; font{-}family:Arial, sans{-}serif;"\textgreater{}}
\SpecialStringTok{        \textless{}h3 style="margin{-}top:0; color:\#202124;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{node\_id}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/h3\textgreater{}}
\SpecialStringTok{        \textless{}p style="font{-}style:italic;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{description}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/p\textgreater{}}
\SpecialStringTok{    """}

    \CommentTok{\# Add prior probabilities section}
    \ControlFlowTok{if}\NormalTok{ priors }\KeywordTok{and} \StringTok{\textquotesingle{}true\_prob\textquotesingle{}} \KeywordTok{in}\NormalTok{ priors:}
\NormalTok{        true\_prob }\OperatorTok{=}\NormalTok{ priors[}\StringTok{\textquotesingle{}true\_prob\textquotesingle{}}\NormalTok{]}
\NormalTok{        false\_prob }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ true\_prob}

        \CommentTok{\# Get proper state names}
\NormalTok{        true\_state }\OperatorTok{=}\NormalTok{ instantiations[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(instantiations) }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \StringTok{"TRUE"}
\NormalTok{        false\_state }\OperatorTok{=}\NormalTok{ instantiations[}\DecValTok{1}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(instantiations) }\OperatorTok{\textgreater{}} \DecValTok{1} \ControlFlowTok{else} \StringTok{"FALSE"}

\NormalTok{        html }\OperatorTok{+=} \SpecialStringTok{f"""}
\SpecialStringTok{        \textless{}div style="margin{-}top:10px; background{-}color:\#fff; padding:8px; border{-}radius:4px; border:1px solid \#ddd;"\textgreater{}}
\SpecialStringTok{            \textless{}h4 style="margin{-}top:0; font{-}size:14px;"\textgreater{}Prior Probabilities:\textless{}/h4\textgreater{}}
\SpecialStringTok{            \textless{}div style="display:flex; justify{-}content:space{-}between; margin{-}bottom:4px;"\textgreater{}}
\SpecialStringTok{                \textless{}div style="font{-}size:12px;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{true\_state}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{true\_prob}\SpecialCharTok{:.3f\}}\SpecialStringTok{\textless{}/div\textgreater{}}
\SpecialStringTok{                \textless{}div style="font{-}size:12px;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{false\_state}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{false\_prob}\SpecialCharTok{:.3f\}}\SpecialStringTok{\textless{}/div\textgreater{}}
\SpecialStringTok{            \textless{}/div\textgreater{}}
\SpecialStringTok{            }\SpecialCharTok{\{}\NormalTok{create\_probability\_bar(true\_prob, false\_prob, }\StringTok{"20px"}\NormalTok{, }\VariableTok{True}\NormalTok{)}\SpecialCharTok{\}}
\SpecialStringTok{        \textless{}/div\textgreater{}}
\SpecialStringTok{        """}

    \CommentTok{\# Add click instruction}
\NormalTok{    html }\OperatorTok{+=} \StringTok{"""}
\StringTok{    \textless{}div style="margin{-}top:8px; font{-}size:12px; color:\#666; text{-}align:center;"\textgreater{}}
\StringTok{        Click node to see full probability details}
\StringTok{    \textless{}/div\textgreater{}}
\StringTok{    \textless{}/div\textgreater{}}
\StringTok{    """}

    \ControlFlowTok{return}\NormalTok{ html}

\KeywordTok{def}\NormalTok{ create\_expanded\_content(node\_id, node\_data):}
    \CommentTok{"""}
\CommentTok{    Create expanded content shown when a node is clicked}

\CommentTok{    Args:}
\CommentTok{        node\_id (str): Identifier of the node}
\CommentTok{        node\_data (dict): Node attributes including probabilities}

\CommentTok{    Returns:}
\CommentTok{        str: HTML content for detailed view displayed on click}
\CommentTok{    """}
    \CommentTok{\# Extract node information}
\NormalTok{    description }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}description\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{    priors }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{, \{\})}
\NormalTok{    posteriors }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}posteriors\textquotesingle{}}\NormalTok{, \{\})}
\NormalTok{    instantiations }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}instantiations\textquotesingle{}}\NormalTok{, [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{])}

    \CommentTok{\# Get proper state names}
\NormalTok{    true\_state }\OperatorTok{=}\NormalTok{ instantiations[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(instantiations) }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \StringTok{"TRUE"}
\NormalTok{    false\_state }\OperatorTok{=}\NormalTok{ instantiations[}\DecValTok{1}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(instantiations) }\OperatorTok{\textgreater{}} \DecValTok{1} \ControlFlowTok{else} \StringTok{"FALSE"}

    \CommentTok{\# Extract probabilities}
\NormalTok{    true\_prob }\OperatorTok{=}\NormalTok{ priors.get(}\StringTok{\textquotesingle{}true\_prob\textquotesingle{}}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{    false\_prob }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ true\_prob}

    \CommentTok{\# Start building the expanded content}
\NormalTok{    html }\OperatorTok{=} \SpecialStringTok{f"""}
\SpecialStringTok{    \textless{}div style="max{-}width:500px; padding:15px; font{-}family:Arial, sans{-}serif;"\textgreater{}}
\SpecialStringTok{        \textless{}h2 style="margin{-}top:0; color:\#333;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{node\_id}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/h2\textgreater{}}
\SpecialStringTok{        \textless{}p style="font{-}style:italic; margin{-}bottom:15px;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{description}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/p\textgreater{}}

\SpecialStringTok{        \textless{}div style="margin{-}bottom:20px; padding:12px; border:1px solid \#ddd; background{-}color:\#f9f9f9; border{-}radius:5px;"\textgreater{}}
\SpecialStringTok{            \textless{}h3 style="margin{-}top:0; color:\#333;"\textgreater{}Prior Probabilities\textless{}/h3\textgreater{}}
\SpecialStringTok{            \textless{}div style="display:flex; justify{-}content:space{-}between; margin{-}bottom:5px;"\textgreater{}}
\SpecialStringTok{                \textless{}div\textgreater{}\textless{}strong\textgreater{}}\SpecialCharTok{\{}\NormalTok{true\_state}\SpecialCharTok{\}}\SpecialStringTok{:\textless{}/strong\textgreater{} }\SpecialCharTok{\{}\NormalTok{true\_prob}\SpecialCharTok{:.3f\}}\SpecialStringTok{\textless{}/div\textgreater{}}
\SpecialStringTok{                \textless{}div\textgreater{}\textless{}strong\textgreater{}}\SpecialCharTok{\{}\NormalTok{false\_state}\SpecialCharTok{\}}\SpecialStringTok{:\textless{}/strong\textgreater{} }\SpecialCharTok{\{}\NormalTok{false\_prob}\SpecialCharTok{:.3f\}}\SpecialStringTok{\textless{}/div\textgreater{}}
\SpecialStringTok{            \textless{}/div\textgreater{}}
\SpecialStringTok{            }\SpecialCharTok{\{}\NormalTok{create\_probability\_bar(true\_prob, false\_prob, }\StringTok{"25px"}\NormalTok{, }\VariableTok{True}\NormalTok{)}\SpecialCharTok{\}}
\SpecialStringTok{        \textless{}/div\textgreater{}}
\SpecialStringTok{    """}

    \CommentTok{\# Add conditional probability table if available}
    \ControlFlowTok{if}\NormalTok{ posteriors:}
\NormalTok{        html }\OperatorTok{+=} \StringTok{"""}
\StringTok{        \textless{}div style="padding:12px; border:1px solid \#ddd; background{-}color:\#f9f9f9; border{-}radius:5px;"\textgreater{}}
\StringTok{            \textless{}h3 style="margin{-}top:0; color:\#333;"\textgreater{}Conditional Probabilities\textless{}/h3\textgreater{}}
\StringTok{            \textless{}table style="width:100\%; border{-}collapse:collapse; font{-}size:13px;"\textgreater{}}
\StringTok{                \textless{}tr style="background{-}color:\#eee;"\textgreater{}}
\StringTok{                    \textless{}th style="padding:8px; text{-}align:left; border:1px solid \#ddd;"\textgreater{}Condition\textless{}/th\textgreater{}}
\StringTok{                    \textless{}th style="padding:8px; text{-}align:center; border:1px solid \#ddd; width:80px;"\textgreater{}Value\textless{}/th\textgreater{}}
\StringTok{                    \textless{}th style="padding:8px; text{-}align:center; border:1px solid \#ddd;"\textgreater{}Visualization\textless{}/th\textgreater{}}
\StringTok{                \textless{}/tr\textgreater{}}
\StringTok{        """}

        \CommentTok{\# Sort posteriors to group by similar conditions}
\NormalTok{        posterior\_items }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(posteriors.items())}
\NormalTok{        posterior\_items.sort(key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: x[}\DecValTok{0}\NormalTok{])}

        \CommentTok{\# Add rows for conditional probabilities}
        \ControlFlowTok{for}\NormalTok{ key, value }\KeywordTok{in}\NormalTok{ posterior\_items:}
            \ControlFlowTok{try}\NormalTok{:}
                \CommentTok{\# Try to parse probability value}
\NormalTok{                prob\_value }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(value)}
\NormalTok{                inv\_prob }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ prob\_value}

                \CommentTok{\# Add row with probability visualization}
\NormalTok{                html }\OperatorTok{+=} \SpecialStringTok{f"""}
\SpecialStringTok{                \textless{}tr\textgreater{}}
\SpecialStringTok{                    \textless{}td style="padding:8px; border:1px solid \#ddd;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{key}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/td\textgreater{}}
\SpecialStringTok{                    \textless{}td style="padding:8px; text{-}align:center; border:1px solid \#ddd;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{prob\_value}\SpecialCharTok{:.3f\}}\SpecialStringTok{\textless{}/td\textgreater{}}
\SpecialStringTok{                    \textless{}td style="padding:8px; border:1px solid \#ddd;"\textgreater{}}
\SpecialStringTok{                        }\SpecialCharTok{\{}\NormalTok{create\_probability\_bar(prob\_value, inv\_prob, }\StringTok{"20px"}\NormalTok{, }\VariableTok{False}\NormalTok{)}\SpecialCharTok{\}}
\SpecialStringTok{                    \textless{}/td\textgreater{}}
\SpecialStringTok{                \textless{}/tr\textgreater{}}
\SpecialStringTok{                """}
            \ControlFlowTok{except}\NormalTok{:}
                \CommentTok{\# Fallback for non{-}numeric values}
\NormalTok{                html }\OperatorTok{+=} \SpecialStringTok{f"""}
\SpecialStringTok{                \textless{}tr\textgreater{}}
\SpecialStringTok{                    \textless{}td style="padding:8px; border:1px solid \#ddd;"\textgreater{}}\SpecialCharTok{\{}\NormalTok{key}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/td\textgreater{}}
\SpecialStringTok{                    \textless{}td style="padding:8px; text{-}align:center; border:1px solid \#ddd;" colspan="2"\textgreater{}}\SpecialCharTok{\{}\NormalTok{value}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/td\textgreater{}}
\SpecialStringTok{                \textless{}/tr\textgreater{}}
\SpecialStringTok{                """}

\NormalTok{        html }\OperatorTok{+=} \StringTok{"""}
\StringTok{            \textless{}/table\textgreater{}}
\StringTok{        \textless{}/div\textgreater{}}
\StringTok{        """}

\NormalTok{    html }\OperatorTok{+=} \StringTok{"\textless{}/div\textgreater{}"}

    \ControlFlowTok{return}\NormalTok{ html}
\end{Highlighting}
\end{Shaded}

\section{Phase 4: Main Visualization
Function}\label{phase-4-main-visualization-function}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ create\_bayesian\_network\_with\_probabilities(df):}
    \CommentTok{"""}
\CommentTok{    Create an interactive Bayesian network visualization with enhanced probability visualization}
\CommentTok{    and node classification based on network structure.}
\CommentTok{    """}
    \CommentTok{\# Create a directed graph}
\NormalTok{    G }\OperatorTok{=}\NormalTok{ nx.DiGraph()}

    \CommentTok{\# Add nodes with proper attributes}
    \ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ df.iterrows():}
\NormalTok{        title }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}
\NormalTok{        description }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Description\textquotesingle{}}\NormalTok{]}

        \CommentTok{\# Process probability information}
\NormalTok{        priors }\OperatorTok{=}\NormalTok{ get\_priors(row)}
\NormalTok{        instantiations }\OperatorTok{=}\NormalTok{ get\_instantiations(row)}

        \CommentTok{\# Add node with base information}
\NormalTok{        G.add\_node(}
\NormalTok{            title,}
\NormalTok{            description}\OperatorTok{=}\NormalTok{description,}
\NormalTok{            priors}\OperatorTok{=}\NormalTok{priors,}
\NormalTok{            instantiations}\OperatorTok{=}\NormalTok{instantiations,}
\NormalTok{            posteriors}\OperatorTok{=}\NormalTok{get\_posteriors(row)}
\NormalTok{        )}

    \CommentTok{\# Add edges}
    \ControlFlowTok{for}\NormalTok{ idx, row }\KeywordTok{in}\NormalTok{ df.iterrows():}
\NormalTok{        child }\OperatorTok{=}\NormalTok{ row[}\StringTok{\textquotesingle{}Title\textquotesingle{}}\NormalTok{]}
\NormalTok{        parents }\OperatorTok{=}\NormalTok{ get\_parents(row)}

        \CommentTok{\# Add edges from each parent to this child}
        \ControlFlowTok{for}\NormalTok{ parent }\KeywordTok{in}\NormalTok{ parents:}
            \ControlFlowTok{if}\NormalTok{ parent }\KeywordTok{in}\NormalTok{ G.nodes():}
\NormalTok{                G.add\_edge(parent, child)}

    \CommentTok{\# Classify nodes based on network structure}
\NormalTok{    classify\_nodes(G)}

    \CommentTok{\# Create network visualization}
\NormalTok{    net }\OperatorTok{=}\NormalTok{ Network(notebook}\OperatorTok{=}\VariableTok{True}\NormalTok{, directed}\OperatorTok{=}\VariableTok{True}\NormalTok{, cdn\_resources}\OperatorTok{=}\StringTok{"in\_line"}\NormalTok{, height}\OperatorTok{=}\StringTok{"600px"}\NormalTok{, width}\OperatorTok{=}\StringTok{"100\%"}\NormalTok{)}

    \CommentTok{\# Configure physics for better layout}
\NormalTok{    net.force\_atlas\_2based(gravity}\OperatorTok{={-}}\DecValTok{50}\NormalTok{, spring\_length}\OperatorTok{=}\DecValTok{100}\NormalTok{, spring\_strength}\OperatorTok{=}\FloatTok{0.02}\NormalTok{)}
\NormalTok{    net.show\_buttons(filter\_}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}physics\textquotesingle{}}\NormalTok{])}

    \CommentTok{\# Add the graph to the network}
\NormalTok{    net.from\_nx(G)}

    \CommentTok{\# Enhance node appearance with probability information and classification}
    \ControlFlowTok{for}\NormalTok{ node }\KeywordTok{in}\NormalTok{ net.nodes:}
\NormalTok{        node\_id }\OperatorTok{=}\NormalTok{ node[}\StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{]}
\NormalTok{        node\_data }\OperatorTok{=}\NormalTok{ G.nodes[node\_id]}

        \CommentTok{\# Get node type and set border color}
\NormalTok{        node\_type }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}node\_type\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}unknown\textquotesingle{}}\NormalTok{)}
\NormalTok{        border\_color }\OperatorTok{=}\NormalTok{ get\_border\_color(node\_type)}

        \CommentTok{\# Get probability information}
\NormalTok{        priors }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{, \{\})}
\NormalTok{        true\_prob }\OperatorTok{=}\NormalTok{ priors.get(}\StringTok{\textquotesingle{}true\_prob\textquotesingle{}}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\ControlFlowTok{if}\NormalTok{ priors }\ControlFlowTok{else} \FloatTok{0.5}

        \CommentTok{\# Get proper state names}
\NormalTok{        instantiations }\OperatorTok{=}\NormalTok{ node\_data.get(}\StringTok{\textquotesingle{}instantiations\textquotesingle{}}\NormalTok{, [}\StringTok{"TRUE"}\NormalTok{, }\StringTok{"FALSE"}\NormalTok{])}
\NormalTok{        true\_state }\OperatorTok{=}\NormalTok{ instantiations[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(instantiations) }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \StringTok{"TRUE"}
\NormalTok{        false\_state }\OperatorTok{=}\NormalTok{ instantiations[}\DecValTok{1}\NormalTok{] }\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(instantiations) }\OperatorTok{\textgreater{}} \DecValTok{1} \ControlFlowTok{else} \StringTok{"FALSE"}

        \CommentTok{\# Create background color based on probability}
\NormalTok{        background\_color }\OperatorTok{=}\NormalTok{ get\_probability\_color(priors)}

        \CommentTok{\# Create tooltip with probability information}
\NormalTok{        tooltip }\OperatorTok{=}\NormalTok{ create\_tooltip(node\_id, node\_data)}

        \CommentTok{\# Create a simpler node label with probability}
\NormalTok{        simple\_label }\OperatorTok{=} \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{node\_id}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{p=}\SpecialCharTok{\{}\NormalTok{true\_prob}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}

        \CommentTok{\# Store expanded content as a node attribute for use in click handler}
\NormalTok{        node\_data[}\StringTok{\textquotesingle{}expanded\_content\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ create\_expanded\_content(node\_id, node\_data)}

        \CommentTok{\# Set node attributes}
\NormalTok{        node[}\StringTok{\textquotesingle{}title\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ tooltip  }\CommentTok{\# Tooltip HTML}
\NormalTok{        node[}\StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ simple\_label  }\CommentTok{\# Simple text label}
\NormalTok{        node[}\StringTok{\textquotesingle{}shape\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \StringTok{\textquotesingle{}box\textquotesingle{}}
\NormalTok{        node[}\StringTok{\textquotesingle{}color\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ \{}
            \StringTok{\textquotesingle{}background\textquotesingle{}}\NormalTok{: background\_color,}
            \StringTok{\textquotesingle{}border\textquotesingle{}}\NormalTok{: border\_color,}
            \StringTok{\textquotesingle{}highlight\textquotesingle{}}\NormalTok{: \{}
                \StringTok{\textquotesingle{}background\textquotesingle{}}\NormalTok{: background\_color,}
                \StringTok{\textquotesingle{}border\textquotesingle{}}\NormalTok{: border\_color}
\NormalTok{            \}}
\NormalTok{        \}}

    \CommentTok{\# Set up the click handler with proper data}
\NormalTok{    setup\_data }\OperatorTok{=}\NormalTok{ \{}
        \StringTok{\textquotesingle{}nodes\_data\textquotesingle{}}\NormalTok{: \{node\_id: \{}
            \StringTok{\textquotesingle{}expanded\_content\textquotesingle{}}\NormalTok{: json.dumps(G.nodes[node\_id].get(}\StringTok{\textquotesingle{}expanded\_content\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)),}
            \StringTok{\textquotesingle{}description\textquotesingle{}}\NormalTok{: G.nodes[node\_id].get(}\StringTok{\textquotesingle{}description\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{),}
            \StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{: G.nodes[node\_id].get(}\StringTok{\textquotesingle{}priors\textquotesingle{}}\NormalTok{, \{\}),}
            \StringTok{\textquotesingle{}posteriors\textquotesingle{}}\NormalTok{: G.nodes[node\_id].get(}\StringTok{\textquotesingle{}posteriors\textquotesingle{}}\NormalTok{, \{\})}
\NormalTok{        \} }\ControlFlowTok{for}\NormalTok{ node\_id }\KeywordTok{in}\NormalTok{ G.nodes()\}}
\NormalTok{    \}}

    \CommentTok{\# Add custom click handling JavaScript}
\NormalTok{    click\_js }\OperatorTok{=} \StringTok{"""}
\StringTok{    // Store node data for click handling}
\StringTok{    var nodesData = }\SpecialCharTok{\%s}\StringTok{;}

\StringTok{    // Add event listener for node clicks}
\StringTok{    network.on("click", function(params) \{}
\StringTok{        if (params.nodes.length \textgreater{} 0) \{}
\StringTok{            var nodeId = params.nodes[0];}
\StringTok{            var nodeInfo = nodesData[nodeId];}

\StringTok{            if (nodeInfo) \{}
\StringTok{                // Create a modal popup for expanded content}
\StringTok{                var modal = document.createElement(\textquotesingle{}div\textquotesingle{});}
\StringTok{                modal.style.position = \textquotesingle{}fixed\textquotesingle{};}
\StringTok{                modal.style.left = \textquotesingle{}50}\SpecialCharTok{\%\%}\StringTok{\textquotesingle{};}
\StringTok{                modal.style.top = \textquotesingle{}50}\SpecialCharTok{\%\%}\StringTok{\textquotesingle{};}
\StringTok{                modal.style.transform = \textquotesingle{}translate({-}50}\SpecialCharTok{\%\%}\StringTok{, {-}50}\SpecialCharTok{\%\%}\StringTok{)\textquotesingle{};}
\StringTok{                modal.style.backgroundColor = \textquotesingle{}white\textquotesingle{};}
\StringTok{                modal.style.padding = \textquotesingle{}20px\textquotesingle{};}
\StringTok{                modal.style.borderRadius = \textquotesingle{}5px\textquotesingle{};}
\StringTok{                modal.style.boxShadow = \textquotesingle{}0 0 10px rgba(0,0,0,0.5)\textquotesingle{};}
\StringTok{                modal.style.zIndex = \textquotesingle{}1000\textquotesingle{};}
\StringTok{                modal.style.maxWidth = \textquotesingle{}80}\SpecialCharTok{\%\%}\StringTok{\textquotesingle{};}
\StringTok{                modal.style.maxHeight = \textquotesingle{}80}\SpecialCharTok{\%\%}\StringTok{\textquotesingle{};}
\StringTok{                modal.style.overflow = \textquotesingle{}auto\textquotesingle{};}

\StringTok{                // Parse the JSON string back to HTML content}
\StringTok{                try \{}
\StringTok{                    var expandedContent = JSON.parse(nodeInfo.expanded\_content);}
\StringTok{                    modal.innerHTML = expandedContent;}
\StringTok{                \} catch (e) \{}
\StringTok{                    modal.innerHTML = \textquotesingle{}Error displaying content: \textquotesingle{} + e.message;}
\StringTok{                \}}

\StringTok{                // Add close button}
\StringTok{                var closeBtn = document.createElement(\textquotesingle{}button\textquotesingle{});}
\StringTok{                closeBtn.innerHTML = \textquotesingle{}Close\textquotesingle{};}
\StringTok{                closeBtn.style.marginTop = \textquotesingle{}10px\textquotesingle{};}
\StringTok{                closeBtn.style.padding = \textquotesingle{}5px 10px\textquotesingle{};}
\StringTok{                closeBtn.style.cursor = \textquotesingle{}pointer\textquotesingle{};}
\StringTok{                closeBtn.onclick = function() \{}
\StringTok{                    document.body.removeChild(modal);}
\StringTok{                \};}
\StringTok{                modal.appendChild(closeBtn);}

\StringTok{                // Add modal to body}
\StringTok{                document.body.appendChild(modal);}
\StringTok{            \}}
\StringTok{        \}}
\StringTok{    \});}
\StringTok{    """} \OperatorTok{\%}\NormalTok{ json.dumps(setup\_data[}\StringTok{\textquotesingle{}nodes\_data\textquotesingle{}}\NormalTok{])}

    \CommentTok{\# Save the graph to HTML}
\NormalTok{    html\_file }\OperatorTok{=} \StringTok{"bayesian\_network.html"}
\NormalTok{    net.save\_graph(html\_file)}

    \CommentTok{\# Inject custom click handling into HTML}
    \ControlFlowTok{try}\NormalTok{:}
        \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(html\_file, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{            html\_content }\OperatorTok{=}\NormalTok{ f.read()}

        \CommentTok{\# Insert click handling script before the closing body tag}
\NormalTok{        html\_content }\OperatorTok{=}\NormalTok{ html\_content.replace(}\StringTok{\textquotesingle{}\textless{}/body\textgreater{}\textquotesingle{}}\NormalTok{, }\SpecialStringTok{f\textquotesingle{}\textless{}script\textgreater{}}\SpecialCharTok{\{}\NormalTok{click\_js}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/script\textgreater{}\textless{}/body\textgreater{}\textquotesingle{}}\NormalTok{)}

        \CommentTok{\# Write back the modified HTML}
        \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(html\_file, }\StringTok{"w"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{            f.write(html\_content)}

        \ControlFlowTok{return}\NormalTok{ HTML(html\_content)}
    \ControlFlowTok{except} \PreprocessorTok{Exception} \ImportTok{as}\NormalTok{ e:}
        \ControlFlowTok{return}\NormalTok{ HTML(}\SpecialStringTok{f"\textless{}p\textgreater{}Error rendering HTML: }\SpecialCharTok{\{}\BuiltInTok{str}\NormalTok{(e)}\SpecialCharTok{\}}\SpecialStringTok{\textless{}/p\textgreater{}\textless{}p\textgreater{}The network visualization has been saved to \textquotesingle{}}\SpecialCharTok{\{}\NormalTok{html\_file}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}\textless{}/p\textgreater{}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\bookmarksetup{startatroot}

\chapter{Quickly check HTML Outputs}\label{quickly-check-html-outputs}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{create\_bayesian\_network\_with\_probabilities(result\_df)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Use the function to create and display the visualization}

\BuiltInTok{print}\NormalTok{(result\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                          Title  \
0       Existential_Catastrophe   
1          Human_Disempowerment   
2        Scale_Of_Power_Seeking   
3      Misaligned_Power_Seeking   
4                   APS_Systems   
5        Advanced_AI_Capability   
6              Agentic_Planning   
7           Strategic_Awareness   
8       Difficulty_Of_Alignment   
9      Instrumental_Convergence   
10        Problems_With_Proxies   
11         Problems_With_Search   
12         Deployment_Decisions   
13      Incentives_To_Build_APS   
14            Usefulness_Of_APS   
15         Competitive_Dynamics   
16              Deception_By_AI   
17          Corrective_Feedback   
18                Warning_Shots   
19  Rapid_Capability_Escalation   
20    Barriers_To_Understanding   
21         Adversarial_Dynamics   
22              Stakes_Of_Error   

                                          Description  line     line_numbers  \
0   The destruction of humanity's long-term potent...     0              [0]   
1   Permanent and collective disempowerment of hum...     1              [1]   
2   Power-seeking by AI systems scaling to the poi...     2              [2]   
3   Deployed AI systems seeking power in unintende...     3  [3, 21, 23, 25]   
4   AI systems with advanced capabilities, agentic...     4              [4]   
5   AI systems that outperform humans on tasks tha...     5              [5]   
6   AI systems making and executing plans based on...     6              [6]   
7   AI systems with models accurately representing...     7              [7]   
8   It is harder to build aligned systems than mis...     8              [8]   
9   AI systems with misaligned objectives tend to ...     9              [9]   
10  Optimizing for proxy objectives breaks correla...    10             [10]   
11  Search processes can yield systems pursuing di...    11             [11]   
12  Decisions to deploy potentially misaligned AI ...    12             [12]   
13  Strong incentives to build and deploy APS syst...    13             [13]   
14  APS systems are very useful for many valuable ...    14             [14]   
15       Competitive pressures between AI developers.    15             [15]   
16  AI systems deceiving humans about their true o...    16             [16]   
17  Human society implementing corrections after o...    17             [17]   
18  Observable failures in weaker systems before c...    18             [18]   
19  AI capabilities escalating very rapidly, allow...    19             [19]   
20  Difficulty in understanding the internal worki...    20             [20]   
21  Potentially adversarial relationships between ...    22             [22]   
22  The escalating impact of mistakes with power-s...    24             [24]   

    indentation indentation_levels  \
0             0                [0]   
1             0                [0]   
2             4                [4]   
3             8       [8, 0, 0, 0]   
4            12               [12]   
5            16               [16]   
6            16               [16]   
7            16               [16]   
8            12               [12]   
9            16               [16]   
10           16               [16]   
11           16               [16]   
12           12               [12]   
13           16               [16]   
14           20               [20]   
15           20               [20]   
16           16               [16]   
17            8                [8]   
18           12               [12]   
19           12               [12]   
20            0                [0]   
21            0                [0]   
22            0                [0]   

                                              Parents  \
0                                                  []   
1                            [Scale_Of_Power_Seeking]   
2     [Misaligned_Power_Seeking, Corrective_Feedback]   
3   [APS_Systems, Difficulty_Of_Alignment, Deploym...   
4   [Advanced_AI_Capability, Agentic_Planning, Str...   
5                                                  []   
6                                                  []   
7                                                  []   
8   [Instrumental_Convergence, Problems_With_Proxi...   
9                                                  []   
10                                                 []   
11                                                 []   
12         [Incentives_To_Build_APS, Deception_By_AI]   
13          [Usefulness_Of_APS, Competitive_Dynamics]   
14                                                 []   
15                                                 []   
16                                                 []   
17       [Warning_Shots, Rapid_Capability_Escalation]   
18                                                 []   
19                                                 []   
20                                                 []   
21                                                 []   
22                                                 []   

                      Children  \
0                           []   
1                           []   
2       [Human_Disempowerment]   
3     [Scale_Of_Power_Seeking]   
4   [Misaligned_Power_Seeking]   
5                [APS_Systems]   
6                [APS_Systems]   
7                [APS_Systems]   
8   [Misaligned_Power_Seeking]   
9    [Difficulty_Of_Alignment]   
10   [Difficulty_Of_Alignment]   
11   [Difficulty_Of_Alignment]   
12  [Misaligned_Power_Seeking]   
13      [Deployment_Decisions]   
14   [Incentives_To_Build_APS]   
15   [Incentives_To_Build_APS]   
16      [Deployment_Decisions]   
17    [Scale_Of_Power_Seeking]   
18       [Corrective_Feedback]   
19       [Corrective_Feedback]   
20                          []   
21                          []   
22                          []   

                                       instantiations  \
0   [existential_catastrophe_TRUE, existential_cat...   
1   [human_disempowerment_TRUE, human_disempowerme...   
2   [scale_of_power_seeking_TRUE, scale_of_power_s...   
3   [misaligned_power_seeking_TRUE, misaligned_pow...   
4               [aps_systems_TRUE, aps_systems_FALSE]   
5   [advanced_ai_capability_TRUE, advanced_ai_capa...   
6     [agentic_planning_TRUE, agentic_planning_FALSE]   
7   [strategic_awareness_TRUE, strategic_awareness...   
8   [difficulty_of_alignment_TRUE, difficulty_of_a...   
9   [instrumental_convergence_TRUE, instrumental_c...   
10  [problems_with_proxies_TRUE, problems_with_pro...   
11  [problems_with_search_TRUE, problems_with_sear...   
12  [deployment_decisions_DEPLOY, deployment_decis...   
13  [incentives_to_build_aps_STRONG, incentives_to...   
14    [usefulness_of_aps_HIGH, usefulness_of_aps_LOW]   
15  [competitive_dynamics_STRONG, competitive_dyna...   
16      [deception_by_ai_TRUE, deception_by_ai_FALSE]   
17  [corrective_feedback_EFFECTIVE, corrective_fee...   
18  [warning_shots_OBSERVED, warning_shots_UNOBSER...   
19  [rapid_capability_escalation_TRUE, rapid_capab...   
20  [barriers_to_understanding_HIGH, barriers_to_u...   
21  [adversarial_dynamics_TRUE, adversarial_dynami...   
22        [stakes_of_error_HIGH, stakes_of_error_LOW]   

                                               priors  \
0   {'p(existential_catastrophe_TRUE)': '0.05', 'p...   
1   {'p(human_disempowerment_TRUE)': '0.208', 'p(h...   
2   {'p(scale_of_power_seeking_TRUE)': '0.208', 'p...   
3   {'p(misaligned_power_seeking_TRUE)': '0.338', ...   
4   {'p(aps_systems_TRUE)': '0.65', 'p(aps_systems...   
5   {'p(advanced_ai_capability_TRUE)': '0.80', 'p(...   
6   {'p(agentic_planning_TRUE)': '0.85', 'p(agenti...   
7   {'p(strategic_awareness_TRUE)': '0.75', 'p(str...   
8   {'p(difficulty_of_alignment_TRUE)': '0.40', 'p...   
9   {'p(instrumental_convergence_TRUE)': '0.75', '...   
10  {'p(problems_with_proxies_TRUE)': '0.80', 'p(p...   
11  {'p(problems_with_search_TRUE)': '0.70', 'p(pr...   
12  {'p(deployment_decisions_DEPLOY)': '0.70', 'p(...   
13  {'p(incentives_to_build_aps_STRONG)': '0.80', ...   
14  {'p(usefulness_of_aps_HIGH)': '0.85', 'p(usefu...   
15  {'p(competitive_dynamics_STRONG)': '0.75', 'p(...   
16  {'p(deception_by_ai_TRUE)': '0.50', 'p(decepti...   
17  {'p(corrective_feedback_EFFECTIVE)': '0.60', '...   
18  {'p(warning_shots_OBSERVED)': '0.70', 'p(warni...   
19  {'p(rapid_capability_escalation_TRUE)': '0.45'...   
20  {'p(barriers_to_understanding_HIGH)': '0.70', ...   
21  {'p(adversarial_dynamics_TRUE)': '0.60', 'p(ad...   
22  {'p(stakes_of_error_HIGH)': '0.85', 'p(stakes_...   

                                           posteriors  No_Parent  No_Children  \
0   {'p(existential_catastrophe_TRUE|human_disempo...       True         True   
1   {'p(human_disempowerment_TRUE|scale_of_power_s...      False         True   
2   {'p(scale_of_power_seeking_TRUE|misaligned_pow...      False        False   
3   {'p(misaligned_power_seeking_TRUE|aps_systems_...      False        False   
4   {'p(aps_systems_TRUE|advanced_ai_capability_TR...      False        False   
5                                                  {}       True        False   
6                                                  {}       True        False   
7                                                  {}       True        False   
8   {'p(difficulty_of_alignment_TRUE|instrumental_...      False        False   
9                                                  {}       True        False   
10                                                 {}       True        False   
11                                                 {}       True        False   
12  {'p(deployment_decisions_DEPLOY|incentives_to_...      False        False   
13  {'p(incentives_to_build_aps_STRONG|usefulness_...      False        False   
14                                                 {}       True        False   
15                                                 {}       True        False   
16                                                 {}       True        False   
17  {'p(corrective_feedback_EFFECTIVE|warning_shot...      False        False   
18                                                 {}       True        False   
19                                                 {}       True        False   
20  {'p(barriers_to_understanding_HIGH|misaligned_...       True         True   
21  {'p(adversarial_dynamics_TRUE|misaligned_power...       True         True   
22  {'p(stakes_of_error_HIGH|misaligned_power_seek...       True         True   

                                parent_instantiations  
0                                                  []  
1   [[scale_of_power_seeking_TRUE, scale_of_power_...  
2   [[misaligned_power_seeking_TRUE, misaligned_po...  
3   [[aps_systems_TRUE, aps_systems_FALSE], [diffi...  
4   [[advanced_ai_capability_TRUE, advanced_ai_cap...  
5                                                  []  
6                                                  []  
7                                                  []  
8   [[instrumental_convergence_TRUE, instrumental_...  
9                                                  []  
10                                                 []  
11                                                 []  
12  [[incentives_to_build_aps_STRONG, incentives_t...  
13  [[usefulness_of_aps_HIGH, usefulness_of_aps_LO...  
14                                                 []  
15                                                 []  
16                                                 []  
17  [[warning_shots_OBSERVED, warning_shots_UNOBSE...  
18                                                 []  
19                                                 []  
20                                                 []  
21                                                 []  
22                                                 []  
\end{verbatim}

\bookmarksetup{startatroot}

\chapter{Conclusion: From Prototype to
Production}\label{conclusion-from-prototype-to-production}

\section{Summary of Achievements}\label{summary-of-achievements}

This notebook has successfully demonstrated the core AMTAIR extraction
pipeline, transforming structured argument representations into
interactive Bayesian network visualizations through the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Environment Setup}: Established a reproducible environment
  with necessary libraries and data access
\item
  \textbf{Argument Extraction}: Processed structured ArgDown
  representations preserving the hierarchical relationships
\item
  \textbf{Probability Integration}: Enhanced arguments with probability
  information to create BayesDown
\item
  \textbf{Data Transformation}: Converted BayesDown into structured
  DataFrame representation
\item
  \textbf{Visualization \& Analysis}: Created interactive Bayesian
  network visualizations with probability encoding
\end{enumerate}

The rain-sprinkler-lawn example, though simple, demonstrates all the key
components of the extraction pipeline that can be applied to more
complex AI safety arguments.

\section{Limitations and Future Work}\label{limitations-and-future-work}

While this prototype successfully demonstrates the core pipeline,
several limitations and opportunities for future work remain:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{LLM Extraction}: The current implementation focuses on
  processing pre-formatted ArgDown rather than performing extraction
  directly from unstructured text. Future work will integrate
  LLM-powered extraction.
\item
  \textbf{Scalability}: The system has been tested on small examples;
  scaling to larger, more complex arguments will require additional
  optimization and handling of computational complexity.
\item
  \textbf{Policy Evaluation}: The current implementation focuses on
  representation and visualization; future work will add policy
  evaluation capabilities by implementing intervention modeling.
\item
  \textbf{Prediction Market Integration}: Future versions will integrate
  with forecasting platforms to incorporate live data into the models.
\end{enumerate}

\section{Connection to AMTAIR
Project}\label{connection-to-amtair-project}

This prototype represents just one component of the broader AMTAIR
project described in the project documentation (see
PY\_AMTAIRDescription and PY\_AMTAIR\_SoftwareToolsNMilestones). The
full project includes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{AI Risk Pathway Analyzer (ARPA)}: The core extraction and
  visualization system demonstrated in this notebook
\item
  \textbf{Worldview Comparator}: Tools for comparing different
  perspectives on AI risk
\item
  \textbf{Policy Impact Evaluator}: Systems for evaluating intervention
  effects across scenarios
\item
  \textbf{Strategic Intervention Generator}: Tools for identifying
  robust governance strategies
\end{enumerate}

Together, these components aim to address the coordination crisis in AI
governance by providing computational tools that make implicit models
explicit, identify cruxes of disagreement, and evaluate policy impacts
across diverse worldviews.

By transforming unstructured text into formal, analyzable
representations, the AMTAIR project helps bridge the gaps between
technical researchers, policy specialists, and other stakeholders,
enabling more effective coordination in addressing existential risks
from advanced AI.

\bookmarksetup{startatroot}

\chapter{6.0 Save Outputs}\label{save-outputs}

\bookmarksetup{startatroot}

\chapter{6. Saving and Exporting
Results}\label{saving-and-exporting-results}

This section provides tools for saving the notebook results and
visualizations in various formats:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{HTML Export}: Creates a self-contained HTML version of the
  notebook with all visualizations
\item
  \textbf{Markdown Export}: Generates documentation-friendly Markdown
  version of the notebook
\item
  \textbf{PDF Export}: Creates a PDF document for formal sharing
  (requires LaTeX installation)
\end{enumerate}

These exports are essential for: - Sharing analysis results with
colleagues and stakeholders - Including visualizations in presentations
and reports - Creating documentation for the AMTAIR project - Preserving
results for future reference

The different formats serve different purposes, from interactive
exploration (HTML) to documentation (Markdown) to formal presentation
(PDF).

Instruction:

Download the ipynb, which you want to convert, on your local computer.
Run the code below to upload the ipynb.

The html version will be downloaded automatically on your local machine.
Enjoy it!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# @title 6.0 {-}{-}{-} Save Visualization and Notebook Outputs as .HTML{-}{-}{-}}

\CommentTok{"""}
\CommentTok{BLOCK PURPOSE: Provides tools for saving the notebook results in various formats.}

\CommentTok{This block offers functions to:}
\CommentTok{1. Convert the notebook to HTML for easy sharing and viewing}
\CommentTok{2. Convert the notebook to Markdown for documentation purposes}
\CommentTok{3. Save the visualization outputs for external use}

\CommentTok{These tools are essential for preserving the analysis results and making them}
\CommentTok{accessible outside the notebook environment, supporting knowledge transfer}
\CommentTok{and integration with other AMTAIR project components.}

\CommentTok{DEPENDENCIES: nbformat, nbconvert modules}
\CommentTok{INPUTS: Current notebook state}
\CommentTok{OUTPUTS: HTML, Markdown, or other format versions of the notebook}
\CommentTok{"""}

\ImportTok{import}\NormalTok{ nbformat}
\ImportTok{from}\NormalTok{ nbconvert }\ImportTok{import}\NormalTok{ HTMLExporter}
\ImportTok{import}\NormalTok{ os}

\CommentTok{\# Repository URL variable for file access}
\NormalTok{repo\_url }\OperatorTok{=} \StringTok{"https://raw.githubusercontent.com/SingularitySmith/AMTAIR\_Prototype/main/data/example\_carlsmith/"}
\NormalTok{notebook\_name }\OperatorTok{=} \StringTok{"AMTAIR\_Prototype\_example\_carlsmith"}  \CommentTok{\# Change when working with different examples}

\CommentTok{\# Download the notebook file}
\OperatorTok{!}\NormalTok{wget \{repo\_url\}\{notebook\_name\}.ipynb }\OperatorTok{{-}}\NormalTok{O \{notebook\_name\}.ipynb}

\CommentTok{\# Load the notebook}
\ControlFlowTok{try}\NormalTok{:}
  \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    nb }\OperatorTok{=}\NormalTok{ nbformat.read(f, as\_version}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
  \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"✅ Successfully loaded notebook: }\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb"}\NormalTok{)}
\ControlFlowTok{except} \PreprocessorTok{FileNotFoundError}\NormalTok{:}
  \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"❌ Error: File \textquotesingle{}}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb\textquotesingle{} not found. Please check if it was downloaded correctly."}\NormalTok{)}

\CommentTok{\# Initialize the HTML exporter}
\NormalTok{exporter }\OperatorTok{=}\NormalTok{ HTMLExporter()}

\CommentTok{\# Convert the notebook to HTML}
\ControlFlowTok{try}\NormalTok{:}
\NormalTok{    (body, resources) }\OperatorTok{=}\NormalTok{ exporter.from\_notebook\_node(nb)}

    \CommentTok{\# Save the HTML to a file}
    \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{IPYNB.html"}\NormalTok{, }\StringTok{"w"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        f.write(body)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"✅ Successfully saved HTML version to: }\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{IPYNB.html"}\NormalTok{)}
\ControlFlowTok{except} \PreprocessorTok{Exception} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"❌ Error converting notebook to HTML: }\SpecialCharTok{\{}\BuiltInTok{str}\NormalTok{(e)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
--2025-04-26 22:34:17--  https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_rain-sprinkler-lawn/AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1120047 (1.1M) [text/plain]
Saving to: ‘AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb’

          AMTAIR_Pr   0%[                    ]       0  --.-KB/s               AMTAIR_Prototype_ex 100%[===================>]   1.07M  --.-KB/s    in 0.07s   

2025-04-26 22:34:17 (16.4 MB/s) - ‘AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb’ saved [1120047/1120047]

✅ Successfully loaded notebook: AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb
✅ Successfully saved HTML version to: AMTAIR_Prototype_example_rain-sprinkler-lawnIPYNB.html
\end{verbatim}

\section{Convert .ipynb Notebook to
MarkDown}\label{convert-.ipynb-notebook-to-markdown}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# @title {-}{-}{-} Convert .ipynb Notebook to MarkDown {-}{-}{-}}

\ImportTok{import}\NormalTok{ nbformat}
\ImportTok{from}\NormalTok{ nbconvert }\ImportTok{import}\NormalTok{ MarkdownExporter}
\ImportTok{import}\NormalTok{ os}

\CommentTok{\# repo\_url = "https://raw.githubusercontent.com/SingularitySmith/AMTAIR\_Prototype/main/data/example\_1/"}
\NormalTok{notebook\_name }\OperatorTok{=} \StringTok{"AMTAIR\_Prototype\_example\_carlsmith"}  \CommentTok{\#Change Notebook name and path when working on different examples}

\CommentTok{\# Download the notebook file}
\OperatorTok{!}\NormalTok{wget \{repo\_url\}\{notebook\_name\}.ipynb }\OperatorTok{{-}}\NormalTok{O \{notebook\_name\}.ipynb  }\CommentTok{\# Corrected line}

\CommentTok{\# Load the notebook}
\CommentTok{\# add error handling for file not found}
\ControlFlowTok{try}\NormalTok{:}
  \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    nb }\OperatorTok{=}\NormalTok{ nbformat.read(f, as\_version}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
\ControlFlowTok{except} \PreprocessorTok{FileNotFoundError}\NormalTok{:}
  \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Error: File \textquotesingle{}}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb\textquotesingle{} not found. Please check if it was downloaded correctly."}\NormalTok{)}


\CommentTok{\# Initialize the Markdown exporter}
\NormalTok{exporter }\OperatorTok{=}\NormalTok{ MarkdownExporter(exclude\_output}\OperatorTok{=}\VariableTok{True}\NormalTok{)  }\CommentTok{\# Correct initialization}

\CommentTok{\# Convert the notebook to Markdown}
\NormalTok{(body, resources) }\OperatorTok{=}\NormalTok{ exporter.from\_notebook\_node(nb)}

\CommentTok{\# Save the Markdown to a file}
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{IPYNB.md"}\NormalTok{, }\StringTok{"w"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    f.write(body)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
--2025-04-26 22:33:43--  https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_rain-sprinkler-lawn/AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1120047 (1.1M) [text/plain]
Saving to: ‘AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb’

          AMTAIR_Pr   0%[                    ]       0  --.-KB/s               AMTAIR_Prototype_ex 100%[===================>]   1.07M  --.-KB/s    in 0.06s   

2025-04-26 22:33:43 (18.1 MB/s) - ‘AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb’ saved [1120047/1120047]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# @title 6.1 {-}{-}{-} Convert Notebook to Markdown Documentation {-}{-}{-}}

\CommentTok{"""}
\CommentTok{BLOCK PURPOSE: Converts the notebook to Markdown format for documentation purposes.}

\CommentTok{Markdown is a lightweight markup language that is widely used for documentation}
\CommentTok{and is easily readable in both plain text and rendered formats. This conversion:}

\CommentTok{1. Preserves the structure and content of the notebook}
\CommentTok{2. Creates a format suitable for inclusion in documentation systems}
\CommentTok{3. Excludes code outputs to focus on the process and methodology}
\CommentTok{4. Supports version control and collaboration on GitHub}

\CommentTok{The resulting Markdown file can be used in project documentation, GitHub wikis,}
\CommentTok{or as a standalone reference guide to the AMTAIR extraction pipeline.}

\CommentTok{DEPENDENCIES: nbformat, nbconvert.MarkdownExporter modules}
\CommentTok{INPUTS: Current notebook state}
\CommentTok{OUTPUTS: Markdown version of the notebook}
\CommentTok{"""}

\ImportTok{import}\NormalTok{ nbformat}
\ImportTok{from}\NormalTok{ nbconvert }\ImportTok{import}\NormalTok{ MarkdownExporter}
\ImportTok{import}\NormalTok{ os}

\CommentTok{\# Repository URL variable for file access}
\CommentTok{\# repo\_url = "https://raw.githubusercontent.com/SingularitySmith/AMTAIR\_Prototype/main/data/example\_carlsmith/"}
\NormalTok{notebook\_name }\OperatorTok{=} \StringTok{"AMTAIR\_Prototype\_example\_carlsmith"}  \CommentTok{\# Change when working with different examples}

\CommentTok{\# Download the notebook file}
\OperatorTok{!}\NormalTok{wget \{repo\_url\}\{notebook\_name\}.ipynb }\OperatorTok{{-}}\NormalTok{O \{notebook\_name\}.ipynb}

\CommentTok{\# Load the notebook}
\ControlFlowTok{try}\NormalTok{:}
  \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    nb }\OperatorTok{=}\NormalTok{ nbformat.read(f, as\_version}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
  \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"✅ Successfully loaded notebook: }\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb"}\NormalTok{)}
\ControlFlowTok{except} \PreprocessorTok{FileNotFoundError}\NormalTok{:}
  \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"❌ Error: File \textquotesingle{}}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb\textquotesingle{} not found. Please check if it was downloaded correctly."}\NormalTok{)}


\CommentTok{\# Initialize the Markdown exporter}
\NormalTok{exporter }\OperatorTok{=}\NormalTok{ MarkdownExporter(exclude\_output}\OperatorTok{=}\VariableTok{True}\NormalTok{)  }\CommentTok{\# Exclude outputs for cleaner documentation}

\CommentTok{\# Convert the notebook to Markdown}
\ControlFlowTok{try}\NormalTok{:}
\NormalTok{    (body, resources) }\OperatorTok{=}\NormalTok{ exporter.from\_notebook\_node(nb)}

    \CommentTok{\# Save the Markdown to a file}
    \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{IPYNB.md"}\NormalTok{, }\StringTok{"w"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        f.write(body)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"✅ Successfully saved Markdown version to: }\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{IPYNB.md"}\NormalTok{)}
\ControlFlowTok{except} \PreprocessorTok{Exception} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"❌ Error converting notebook to Markdown: }\SpecialCharTok{\{}\BuiltInTok{str}\NormalTok{(e)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
--2025-04-26 22:31:45--  https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_rain-sprinkler-lawn/AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1120047 (1.1M) [text/plain]
Saving to: ‘AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb’

          AMTAIR_Pr   0%[                    ]       0  --.-KB/s               AMTAIR_Prototype_ex 100%[===================>]   1.07M  --.-KB/s    in 0.06s   

2025-04-26 22:31:45 (18.0 MB/s) - ‘AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb’ saved [1120047/1120047]

✅ Successfully loaded notebook: AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb
✅ Successfully saved Markdown version to: AMTAIR_Prototype_example_rain-sprinkler-lawnIPYNB.md
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ nbformat}
\ImportTok{from}\NormalTok{ nbconvert }\ImportTok{import}\NormalTok{ PDFExporter}
\ImportTok{import}\NormalTok{ os}
\ImportTok{import}\NormalTok{ subprocess}
\ImportTok{import}\NormalTok{ re}

\KeywordTok{def}\NormalTok{ escape\_latex\_special\_chars(text):}
  \CommentTok{"""Escapes special LaTeX characters in a string."""}
\NormalTok{  latex\_special\_chars }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}\&\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\%\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\#\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\_\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\{\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\}\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textasciitilde{}\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\^{}\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\textquotesingle{}}\NormalTok{]}
\NormalTok{  replacement\_patterns }\OperatorTok{=}\NormalTok{ [}
\NormalTok{      (char, }\StringTok{\textquotesingle{}}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{\textquotesingle{}} \OperatorTok{+}\NormalTok{ char) }\ControlFlowTok{for}\NormalTok{ char }\KeywordTok{in}\NormalTok{ latex\_special\_chars}
\NormalTok{  ]}

  \CommentTok{\# Escape reserved characters}
  \ControlFlowTok{for}\NormalTok{ original, replacement }\KeywordTok{in}\NormalTok{ replacement\_patterns:}
\NormalTok{    text }\OperatorTok{=}\NormalTok{ text.replace(original, replacement) }\CommentTok{\# This is the fix}
  \ControlFlowTok{return}\NormalTok{ text}

\CommentTok{\# Function to check if a command is available}
\KeywordTok{def}\NormalTok{ is\_command\_available(command):}
    \ControlFlowTok{try}\NormalTok{:}
\NormalTok{        subprocess.run([command], capture\_output}\OperatorTok{=}\VariableTok{True}\NormalTok{, check}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{return} \VariableTok{True}
    \ControlFlowTok{except}\NormalTok{ (subprocess.CalledProcessError, }\PreprocessorTok{FileNotFoundError}\NormalTok{):}
        \ControlFlowTok{return} \VariableTok{False}

\CommentTok{\# Check if xelatex is installed, and install if necessary}
\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ is\_command\_available(}\StringTok{"xelatex"}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Installing necessary TeX packages..."}\NormalTok{)}
    \OperatorTok{!}\NormalTok{apt}\OperatorTok{{-}}\NormalTok{get install }\OperatorTok{{-}}\NormalTok{y texlive}\OperatorTok{{-}}\NormalTok{xetex texlive}\OperatorTok{{-}}\NormalTok{fonts}\OperatorTok{{-}}\NormalTok{recommended texlive}\OperatorTok{{-}}\NormalTok{plain}\OperatorTok{{-}}\NormalTok{generic}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"TeX packages installed successfully."}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"xelatex is already installed. Skipping installation."}\NormalTok{)}

\CommentTok{\# repo\_url = "https://raw.githubusercontent.com/SingularitySmith/AMTAIR\_Prototype/main/data/example\_1/"}
\NormalTok{notebook\_name }\OperatorTok{=} \StringTok{"AMTAIR\_Prototype\_example\_carlsmith"}  \CommentTok{\#Change Notebook name and path when working on different examples}

\CommentTok{\# Download the notebook file}
\OperatorTok{!}\NormalTok{wget \{repo\_url\}\{notebook\_name\}.ipynb }\OperatorTok{{-}}\NormalTok{O \{notebook\_name\}.ipynb  }\CommentTok{\# Corrected line}

\CommentTok{\# Load the notebook}
\CommentTok{\# add error handling for file not found}
\ControlFlowTok{try}\NormalTok{:}
  \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    nb }\OperatorTok{=}\NormalTok{ nbformat.read(f, as\_version}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
\ControlFlowTok{except} \PreprocessorTok{FileNotFoundError}\NormalTok{:}
  \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Error: File \textquotesingle{}}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{.ipynb\textquotesingle{} not found. Please check if it was downloaded correctly."}\NormalTok{)}


\CommentTok{\# Initialize the PDF exporter}
\NormalTok{exporter }\OperatorTok{=}\NormalTok{ PDFExporter(exclude\_output}\OperatorTok{=}\VariableTok{True}\NormalTok{)  }\CommentTok{\# Changed to PDFExporter}

\CommentTok{\# Sanitize notebook cell titles to escape special LaTeX characters like \textquotesingle{}\&\textquotesingle{}}
\ControlFlowTok{for}\NormalTok{ cell }\KeywordTok{in}\NormalTok{ nb.cells:}
    \ControlFlowTok{if} \StringTok{\textquotesingle{}cell\_type\textquotesingle{}} \KeywordTok{in}\NormalTok{ cell }\KeywordTok{and}\NormalTok{ cell[}\StringTok{\textquotesingle{}cell\_type\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \StringTok{\textquotesingle{}markdown\textquotesingle{}}\NormalTok{:}
        \ControlFlowTok{if} \StringTok{\textquotesingle{}source\textquotesingle{}} \KeywordTok{in}\NormalTok{ cell }\KeywordTok{and} \BuiltInTok{isinstance}\NormalTok{(cell[}\StringTok{\textquotesingle{}source\textquotesingle{}}\NormalTok{], }\BuiltInTok{str}\NormalTok{):}
            \CommentTok{\# Replace \textquotesingle{}\&\textquotesingle{} with \textquotesingle{}\textbackslash{}protect\&\textquotesingle{} in markdown cell titles AND CONTENT}
            \CommentTok{\# Updated to use escape\_latex\_special\_chars function}
\NormalTok{            cell[}\StringTok{\textquotesingle{}source\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ escape\_latex\_special\_chars(cell[}\StringTok{\textquotesingle{}source\textquotesingle{}}\NormalTok{])}
            \CommentTok{\# Additionally, escape special characters in headings}
\NormalTok{            cell[}\StringTok{\textquotesingle{}source\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ re.sub(}\VerbatimStringTok{r\textquotesingle{}}\KeywordTok{(}\VerbatimStringTok{\#}\OperatorTok{+}\KeywordTok{)}\DecValTok{\textbackslash{}s}\OperatorTok{*}\KeywordTok{(}\DecValTok{.}\OperatorTok{*}\KeywordTok{)}\VerbatimStringTok{\textquotesingle{}}\NormalTok{, }\KeywordTok{lambda}\NormalTok{ m: m.group(}\DecValTok{1}\NormalTok{) }\OperatorTok{+} \StringTok{\textquotesingle{} \textquotesingle{}} \OperatorTok{+}\NormalTok{ escape\_latex\_special\_chars(m.group(}\DecValTok{2}\NormalTok{)), cell[}\StringTok{\textquotesingle{}source\textquotesingle{}}\NormalTok{])}



\CommentTok{\# Convert the notebook to PDF}
\NormalTok{(body, resources) }\OperatorTok{=}\NormalTok{ exporter.from\_notebook\_node(nb)}


\CommentTok{\# Save the PDF to a file}
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{notebook\_name}\SpecialCharTok{\}}\SpecialStringTok{IPYNB.pdf"}\NormalTok{, }\StringTok{"wb"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:  }\CommentTok{\# Changed to \textquotesingle{}wb\textquotesingle{} for binary writing}
\NormalTok{    f.write(body)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Installing necessary TeX packages...
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following additional packages will be installed:
  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono
  fonts-texgyre fonts-urw-base35 libapache-pom-java libcommons-logging-java
  libcommons-parent-java libfontbox-java libgs9 libgs9-common libidn12
  libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0
  libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13
  lmodern poppler-data preview-latex-style rake ruby ruby-net-telnet
  ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils
  teckit tex-common tex-gyre texlive-base texlive-binaries texlive-latex-base
  texlive-latex-extra texlive-latex-recommended texlive-pictures tipa
  xfonts-encodings xfonts-utils
Suggested packages:
  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java
  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java
  poppler-utils ghostscript fonts-japanese-mincho | fonts-ipafont-mincho
  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai
  fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv
  | postscript-viewer perl-tk xpdf | pdf-viewer xzdec
  texlive-fonts-recommended-doc texlive-latex-base-doc python3-pygments
  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl
  texlive-latex-extra-doc texlive-latex-recommended-doc texlive-luatex
  texlive-pstricks dot2tex prerex texlive-pictures-doc vprerex
  default-jre-headless tipa-doc
The following NEW packages will be installed:
  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono
  fonts-texgyre fonts-urw-base35 libapache-pom-java libcommons-logging-java
  libcommons-parent-java libfontbox-java libgs9 libgs9-common libidn12
  libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0
  libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13
  lmodern poppler-data preview-latex-style rake ruby ruby-net-telnet
  ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils
  teckit tex-common tex-gyre texlive-base texlive-binaries
  texlive-fonts-recommended texlive-latex-base texlive-latex-extra
  texlive-latex-recommended texlive-pictures texlive-plain-generic
  texlive-xetex tipa xfonts-encodings xfonts-utils
0 upgraded, 53 newly installed, 0 to remove and 34 not upgraded.
Need to get 182 MB of archives.
After this operation, 571 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]
Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]
Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]
Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]
Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]
Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.11 [753 kB]
Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]
Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]
Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]
Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.11 [5,031 kB]
Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]
Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]
Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]
Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]
Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]
Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]
Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]
Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]
Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]
Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.2 [39.1 kB]
Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]
Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.10 [50.1 kB]
Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]
Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]
Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]
Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]
Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-webrick all 1.7.0-3ubuntu0.1 [52.1 kB]
Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]
Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.10 [5,114 kB]
Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]
Get:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]
Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.2 [120 kB]
Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.2 [267 kB]
Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]
Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]
Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]
Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]
Get:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]
Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]
Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]
Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]
Get:42 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.2 [9,860 kB]
Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]
Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]
Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]
Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]
Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]
Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]
Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]
Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]
Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]
Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]
Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]
Fetched 182 MB in 3s (69.8 MB/s)
Extracting templates from packages: 100%
Preconfiguring packages ...
Selecting previously unselected package fonts-droid-fallback.
(Reading database ... 126558 files and directories currently installed.)
Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...
Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...
Selecting previously unselected package fonts-lato.
Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...
Unpacking fonts-lato (2.0-2.1) ...
Selecting previously unselected package poppler-data.
Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...
Unpacking poppler-data (0.4.11-1) ...
Selecting previously unselected package tex-common.
Preparing to unpack .../03-tex-common_6.17_all.deb ...
Unpacking tex-common (6.17) ...
Selecting previously unselected package fonts-urw-base35.
Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...
Unpacking fonts-urw-base35 (20200910-1) ...
Selecting previously unselected package libgs9-common.
Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.11_all.deb ...
Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.11) ...
Selecting previously unselected package libidn12:amd64.
Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...
Unpacking libidn12:amd64 (1.38-4ubuntu1) ...
Selecting previously unselected package libijs-0.35:amd64.
Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...
Unpacking libijs-0.35:amd64 (0.35-15build2) ...
Selecting previously unselected package libjbig2dec0:amd64.
Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...
Unpacking libjbig2dec0:amd64 (0.19-3build2) ...
Selecting previously unselected package libgs9:amd64.
Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.11_amd64.deb ...
Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.11) ...
Selecting previously unselected package libkpathsea6:amd64.
Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...
Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Selecting previously unselected package libwoff1:amd64.
Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...
Unpacking libwoff1:amd64 (1.0.2-1build4) ...
Selecting previously unselected package dvisvgm.
Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...
Unpacking dvisvgm (2.13.1-1) ...
Selecting previously unselected package fonts-lmodern.
Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...
Unpacking fonts-lmodern (2.004.5-6.1) ...
Selecting previously unselected package fonts-noto-mono.
Preparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...
Unpacking fonts-noto-mono (20201225-1build1) ...
Selecting previously unselected package fonts-texgyre.
Preparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...
Unpacking fonts-texgyre (20180621-3.1) ...
Selecting previously unselected package libapache-pom-java.
Preparing to unpack .../16-libapache-pom-java_18-1_all.deb ...
Unpacking libapache-pom-java (18-1) ...
Selecting previously unselected package libcommons-parent-java.
Preparing to unpack .../17-libcommons-parent-java_43-1_all.deb ...
Unpacking libcommons-parent-java (43-1) ...
Selecting previously unselected package libcommons-logging-java.
Preparing to unpack .../18-libcommons-logging-java_1.2-2_all.deb ...
Unpacking libcommons-logging-java (1.2-2) ...
Selecting previously unselected package libptexenc1:amd64.
Preparing to unpack .../19-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...
Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Selecting previously unselected package rubygems-integration.
Preparing to unpack .../20-rubygems-integration_1.18_all.deb ...
Unpacking rubygems-integration (1.18) ...
Selecting previously unselected package ruby3.0.
Preparing to unpack .../21-ruby3.0_3.0.2-7ubuntu2.10_amd64.deb ...
Unpacking ruby3.0 (3.0.2-7ubuntu2.10) ...
Selecting previously unselected package ruby-rubygems.
Preparing to unpack .../22-ruby-rubygems_3.3.5-2_all.deb ...
Unpacking ruby-rubygems (3.3.5-2) ...
Selecting previously unselected package ruby.
Preparing to unpack .../23-ruby_1%3a3.0~exp1_amd64.deb ...
Unpacking ruby (1:3.0~exp1) ...
Selecting previously unselected package rake.
Preparing to unpack .../24-rake_13.0.6-2_all.deb ...
Unpacking rake (13.0.6-2) ...
Selecting previously unselected package ruby-net-telnet.
Preparing to unpack .../25-ruby-net-telnet_0.1.1-2_all.deb ...
Unpacking ruby-net-telnet (0.1.1-2) ...
Selecting previously unselected package ruby-webrick.
Preparing to unpack .../26-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...
Unpacking ruby-webrick (1.7.0-3ubuntu0.1) ...
Selecting previously unselected package ruby-xmlrpc.
Preparing to unpack .../27-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...
Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...
Selecting previously unselected package libruby3.0:amd64.
Preparing to unpack .../28-libruby3.0_3.0.2-7ubuntu2.10_amd64.deb ...
Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.10) ...
Selecting previously unselected package libsynctex2:amd64.
Preparing to unpack .../29-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...
Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Selecting previously unselected package libteckit0:amd64.
Preparing to unpack .../30-libteckit0_2.5.11+ds1-1_amd64.deb ...
Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...
Selecting previously unselected package libtexlua53:amd64.
Preparing to unpack .../31-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...
Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Selecting previously unselected package libtexluajit2:amd64.
Preparing to unpack .../32-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...
Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Selecting previously unselected package libzzip-0-13:amd64.
Preparing to unpack .../33-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...
Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...
Selecting previously unselected package xfonts-encodings.
Preparing to unpack .../34-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...
Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...
Selecting previously unselected package xfonts-utils.
Preparing to unpack .../35-xfonts-utils_1%3a7.7+6build2_amd64.deb ...
Unpacking xfonts-utils (1:7.7+6build2) ...
Selecting previously unselected package lmodern.
Preparing to unpack .../36-lmodern_2.004.5-6.1_all.deb ...
Unpacking lmodern (2.004.5-6.1) ...
Selecting previously unselected package preview-latex-style.
Preparing to unpack .../37-preview-latex-style_12.2-1ubuntu1_all.deb ...
Unpacking preview-latex-style (12.2-1ubuntu1) ...
Selecting previously unselected package t1utils.
Preparing to unpack .../38-t1utils_1.41-4build2_amd64.deb ...
Unpacking t1utils (1.41-4build2) ...
Selecting previously unselected package teckit.
Preparing to unpack .../39-teckit_2.5.11+ds1-1_amd64.deb ...
Unpacking teckit (2.5.11+ds1-1) ...
Selecting previously unselected package tex-gyre.
Preparing to unpack .../40-tex-gyre_20180621-3.1_all.deb ...
Unpacking tex-gyre (20180621-3.1) ...
Selecting previously unselected package texlive-binaries.
Preparing to unpack .../41-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...
Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...
Selecting previously unselected package texlive-base.
Preparing to unpack .../42-texlive-base_2021.20220204-1_all.deb ...
Unpacking texlive-base (2021.20220204-1) ...
Selecting previously unselected package texlive-fonts-recommended.
Preparing to unpack .../43-texlive-fonts-recommended_2021.20220204-1_all.deb ...
Unpacking texlive-fonts-recommended (2021.20220204-1) ...
Selecting previously unselected package texlive-latex-base.
Preparing to unpack .../44-texlive-latex-base_2021.20220204-1_all.deb ...
Unpacking texlive-latex-base (2021.20220204-1) ...
Selecting previously unselected package libfontbox-java.
Preparing to unpack .../45-libfontbox-java_1%3a1.8.16-2_all.deb ...
Unpacking libfontbox-java (1:1.8.16-2) ...
Selecting previously unselected package libpdfbox-java.
Preparing to unpack .../46-libpdfbox-java_1%3a1.8.16-2_all.deb ...
Unpacking libpdfbox-java (1:1.8.16-2) ...
Selecting previously unselected package texlive-latex-recommended.
Preparing to unpack .../47-texlive-latex-recommended_2021.20220204-1_all.deb ...
Unpacking texlive-latex-recommended (2021.20220204-1) ...
Selecting previously unselected package texlive-pictures.
Preparing to unpack .../48-texlive-pictures_2021.20220204-1_all.deb ...
Unpacking texlive-pictures (2021.20220204-1) ...
Selecting previously unselected package texlive-latex-extra.
Preparing to unpack .../49-texlive-latex-extra_2021.20220204-1_all.deb ...
Unpacking texlive-latex-extra (2021.20220204-1) ...
Selecting previously unselected package texlive-plain-generic.
Preparing to unpack .../50-texlive-plain-generic_2021.20220204-1_all.deb ...
Unpacking texlive-plain-generic (2021.20220204-1) ...
Selecting previously unselected package tipa.
Preparing to unpack .../51-tipa_2%3a1.3-21_all.deb ...
Unpacking tipa (2:1.3-21) ...
Selecting previously unselected package texlive-xetex.
Preparing to unpack .../52-texlive-xetex_2021.20220204-1_all.deb ...
Unpacking texlive-xetex (2021.20220204-1) ...
Setting up fonts-lato (2.0-2.1) ...
Setting up fonts-noto-mono (20201225-1build1) ...
Setting up libwoff1:amd64 (1.0.2-1build4) ...
Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Setting up libijs-0.35:amd64 (0.35-15build2) ...
Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Setting up libfontbox-java (1:1.8.16-2) ...
Setting up rubygems-integration (1.18) ...
Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...
Setting up fonts-urw-base35 (20200910-1) ...
Setting up poppler-data (0.4.11-1) ...
Setting up tex-common (6.17) ...
update-language: texlive-base not installed and configured, doing nothing!
Setting up libjbig2dec0:amd64 (0.19-3build2) ...
Setting up libteckit0:amd64 (2.5.11+ds1-1) ...
Setting up libapache-pom-java (18-1) ...
Setting up ruby-net-telnet (0.1.1-2) ...
Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...
Setting up t1utils (1.41-4build2) ...
Setting up libidn12:amd64 (1.38-4ubuntu1) ...
Setting up fonts-texgyre (20180621-3.1) ...
Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Setting up ruby-webrick (1.7.0-3ubuntu0.1) ...
Setting up fonts-lmodern (2.004.5-6.1) ...
Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...
Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...
Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.11) ...
Setting up teckit (2.5.11+ds1-1) ...
Setting up libpdfbox-java (1:1.8.16-2) ...
Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.11) ...
Setting up preview-latex-style (12.2-1ubuntu1) ...
Setting up libcommons-parent-java (43-1) ...
Setting up dvisvgm (2.13.1-1) ...
Setting up libcommons-logging-java (1.2-2) ...
Setting up xfonts-utils (1:7.7+6build2) ...
Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...
Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...
update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode
update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode
Setting up lmodern (2.004.5-6.1) ...
Setting up texlive-base (2021.20220204-1) ...
/usr/bin/ucfr
/usr/bin/ucfr
/usr/bin/ucfr
/usr/bin/ucfr
mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... 
mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... 
mktexlsr: Updating /var/lib/texmf/ls-R... 
mktexlsr: Done.
tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps
tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg
tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper
tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex
Setting up tex-gyre (20180621-3.1) ...
Setting up texlive-plain-generic (2021.20220204-1) ...
Setting up texlive-latex-base (2021.20220204-1) ...
Setting up texlive-latex-recommended (2021.20220204-1) ...
Setting up texlive-pictures (2021.20220204-1) ...
Setting up texlive-fonts-recommended (2021.20220204-1) ...
Setting up tipa (2:1.3-21) ...
Setting up texlive-latex-extra (2021.20220204-1) ...
Setting up texlive-xetex (2021.20220204-1) ...
Setting up rake (13.0.6-2) ...
Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.10) ...
Setting up ruby3.0 (3.0.2-7ubuntu2.10) ...
Setting up ruby (1:3.0~exp1) ...
Setting up ruby-rubygems (3.3.5-2) ...
Processing triggers for man-db (2.10.2-1) ...
Processing triggers for mailcap (3.70+nmu1ubuntu1) ...
Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...
Processing triggers for libc-bin (2.35-0ubuntu3.8) ...
/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link

/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link

Processing triggers for tex-common (6.17) ...
Running updmap-sys. This may take some time... done.
Running mktexlsr /var/lib/texmf ... done.
Building format(s) --all.
    This may take some time... done.
TeX packages installed successfully.
--2025-04-26 22:32:56--  https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_rain-sprinkler-lawn/AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1120047 (1.1M) [text/plain]
Saving to: ‘AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb’

AMTAIR_Prototype_ex 100%[===================>]   1.07M  --.-KB/s    in 0.06s   

2025-04-26 22:32:56 (17.0 MB/s) - ‘AMTAIR_Prototype_example_rain-sprinkler-lawn.ipynb’ saved [1120047/1120047]
\end{verbatim}


\backmatter
\printbibliography[title=Bibliography]



\clearpage
\thispagestyle{empty} % Removes page numbering for current page

\newpage


% Top header with logo (left) and department (right)
\begin{minipage}{0.3\textwidth}
  \includegraphics[width=5cm]{latex/uni-bayreuth-logo.png}
\end{minipage}
\hfill
\begin{minipage}{0.9\textwidth}
  \begin{center}
    -- P\&E Master's Programme --\\
    Chair of Philosophy, Computer\\
    Science \& Artificial Intelligence
  \end{center}
\end{minipage}

% Horizontal rule
\vspace{1.5cm}
\hrule
\vspace{2.5cm}

% Title in bold

  \LARGE\textbf{Affidavit}
\vspace{1.5cm}

\center

\normalsize

% \part*{Affidavit}

    \subsection*{\Large Declaration of Academic Honesty}
	    \vspace{1cm}\noindent \\
	    Hereby, I attest that I have composed and written the presented thesis 
        \vspace*{0.5cm}\noindent \\
        \textit{ \textbf{ Automating the Modelling of Transformative Artificial Intelligence Risks }}
        \vspace*{0.5cm}\noindent \\
        independently on my own, without the use of other than the stated aids and without any other resources than the ones indicated. All thoughts taken directly or indirectly from external sources are properly denoted as such.
	    \vspace{\baselineskip}
	    \\  This paper has neither been previously submitted in the same or a similar form to another authority nor has it been published yet.
	    \vspace{2cm}
	    
    \flushright
    \begin{minipage}{0.5\textwidth}
        \begin{flushleft} \large
        \textsc{Bayreuth}                     %   Place
        on the \\ % 26th of May 2025     \\
        \today           %   Date
        \vspace{2cm}\\
    	{\rule[-3pt]{\linewidth}{.4pt}\par\smallskip  
        \textsc{Valentin Meyer}	\\         %   Your name
    	}
        \end{flushleft}
        \end{minipage}


\end{document}
