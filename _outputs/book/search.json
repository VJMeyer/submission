[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "1 Abstract",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "1.1 Acknowledgments",
    "text": "1.1 Acknowledgments\n\nAcademic supervisor (Prof. Timo Speith) and institution (University of Bayreuth)\n\nResearch collaborators, especially those connected to the original MTAIR project\n\nTechnical advisors who provided feedback on implementation aspects\n\nPersonal supporters who enabled the research through encouragement and feedback",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "index.html#list-of-abbreviations",
    "href": "index.html#list-of-abbreviations",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "List of Abbreviations",
    "text": "List of Abbreviations\n\n\n\nAGI - Artificial General Intelligence\nAMTAIR - Automating Modeling of Transformative AI Risks\nAPI - Application Programming Interface\nAPS - Advanced, Planning, Strategic\nBN - Bayesian Network\nCPT - Conditional Probability Table\nDAG - Directed Acyclic Graph\nLLM - Large Language Model\nMTAIR - Modeling Transformative AI Risks\nTAI - Transformative Artificial Intelligence",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "index.html#glossary",
    "href": "index.html#glossary",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Glossary",
    "text": "Glossary\n\n\n\nArgument mapping: A method for visually representing the structure of arguments\n\nBayesDown: An extension of ArgDown that incorporates probabilistic information\n\nBayesian network: A probabilistic graphical model representing variables and their dependencies\n\nConditional probability: The probability of an event given that another event has occurred\n\nDirected Acyclic Graph (DAG): A graph with directed edges and no cycles\n\nExistential risk: Risk of permanent curtailment of humanity’s potential\n\nPower-seeking AI: AI systems with instrumental incentives to acquire resources and power\n\nPrediction market: A market where participants trade contracts that resolve based on future events\n\nd-separation: A criterion for identifying conditional independence relationships in Bayesian networks\n\nMonte Carlo sampling: A computational technique using random sampling to obtain numerical results",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html",
    "href": "chapters/Outlines/final_draft.html",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "2.1 Frontmatter: Preface\nThis thesis represents the culmination of interdisciplinary research at the intersection of AI safety, formal epistemology, and computational social science. The work emerged from recognizing a fundamental challenge in AI governance: while investment in AI safety research has grown exponentially, coordination between different stakeholder communities remains fragmented, potentially increasing existential risk through misaligned efforts.\nThe journey from initial concept to working implementation involved iterative refinement based on feedback from advisors, domain experts, and potential users. What began as a technical exercise in automated extraction evolved into a broader framework for enhancing epistemic security in one of humanity’s most critical coordination challenges. The AMTAIR project—Automating Transformative AI Risk Modeling—represents an attempt to build computational bridges between communities that, despite shared concerns about AI risk, often struggle to communicate effectively due to incompatible frameworks, terminologies, and implicit assumptions.\nI hope this work contributes to building the intellectual and technical infrastructure necessary for humanity to navigate the transition to transformative AI safely. The tools and frameworks presented here are offered in the spirit of collaborative problem-solving, recognizing that the challenges we face require unprecedented cooperation across disciplines, institutions, and worldviews.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#acknowledgments",
    "href": "chapters/Outlines/final_draft.html#acknowledgments",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.2 Acknowledgments",
    "text": "2.2 Acknowledgments\n\nI thank my supervisor Dr. Timo Speith for his guidance throughout this project, providing both technical insights and philosophical grounding. The MTAIR team’s pioneering manual approach inspired this automation effort, and I am grateful for their foundational work.\n\nI acknowledge Johannes Meyer and Jelena Meyer for their invaluable assistance in verifying the automated extraction procedure through manual extraction of ArgDown and BayesDown data from the Carlsmith paper, providing crucial ground truth for validation.\nSpecial recognition goes to Coleman Snell for his partnership and research collaboration with the AMTAIR project, offering both technical expertise and strategic vision. The AI safety community’s creation of rich literature made this work possible, and I thank all researchers whose arguments provided the raw material for formalization.\nAny errors or limitations remain my own responsibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#list-of-figures",
    "href": "chapters/Outlines/final_draft.html#list-of-figures",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.3 List of Figures",
    "text": "2.3 List of Figures",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#list-of-tables",
    "href": "chapters/Outlines/final_draft.html#list-of-tables",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.4 List of Tables",
    "text": "2.4 List of Tables",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#list-of-abbreviations",
    "href": "chapters/Outlines/final_draft.html#list-of-abbreviations",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.5 List of Abbreviations",
    "text": "2.5 List of Abbreviations\n\nAI - Artificial Intelligence\nAGI - Artificial General Intelligence\nAMTAIR - Automating Transformative AI Risk Modeling\nAPI - Application Programming Interface\nAPS - Advanced, Planning, Strategic (AI systems)\nBN - Bayesian Network\nCPT - Conditional Probability Table\nDAG - Directed Acyclic Graph\nLLM - Large Language Model\nML - Machine Learning\nMTAIR - Modeling Transformative AI Risks\nNLP - Natural Language Processing\nP&E - Philosophy & Economics\nPDF - Portable Document Format\nTAI - Transformative Artificial Intelligence",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#opening-scenario-the-policymakers-dilemma",
    "href": "chapters/Outlines/final_draft.html#opening-scenario-the-policymakers-dilemma",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "3.1 1.1 Opening Scenario: The Policymaker’s Dilemma",
    "text": "3.1 1.1 Opening Scenario: The Policymaker’s Dilemma\n\n\nImagine a senior policy advisor preparing recommendations for AI governance legislation. On her desk lie a dozen reports from leading AI safety researchers, each painting a different picture of the risks ahead. One argues that misaligned AI could pose existential risks within the decade, citing complex technical arguments about instrumental convergence and orthogonality. Another suggests these concerns are overblown, emphasizing uncertainty and the strength of existing institutions. A third proposes specific technical standards but acknowledges deep uncertainty about their effectiveness.\nEach report seems compelling in isolation, written by credentialed experts with sophisticated arguments. Yet they reach dramatically different conclusions about both the magnitude of risk and appropriate interventions. The technical arguments involve unfamiliar concepts—mesa-optimization, corrigibility, capability amplification—expressed through different frameworks and implicit assumptions. Time is limited, stakes are high, and the legislation could shape humanity’s trajectory for decades.\n\nThis scenario1 plays out daily across government offices, corporate boardrooms, and research institutions worldwide. It exemplifies what I term the “coordination crisis” in AI governance: despite unprecedented attention and resources directed toward AI safety, we lack the epistemic infrastructure to synthesize diverse expert knowledge into actionable governance strategies Todd (2024).\n\nShow Image",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#the-coordination-crisis-in-ai-governance",
    "href": "chapters/Outlines/final_draft.html#the-coordination-crisis-in-ai-governance",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "3.2 1.2 The Coordination Crisis in AI Governance",
    "text": "3.2 1.2 The Coordination Crisis in AI Governance\n\n\n\nAs AI capabilities advance at an accelerating pace—demonstrated by the rapid progression from GPT-3 to GPT-4, Claude, and emerging multimodal systems Maslej (2025) Samborska (2025)—humanity faces a governance challenge unlike any in history. The task of ensuring increasingly powerful AI systems remain aligned with human values and beneficial to our long-term flourishing grows more urgent with each capability breakthrough. This challenge becomes particularly acute when considering transformative AI systems that could drastically alter civilization’s trajectory, potentially including existential risks from misaligned systems pursuing objectives counter to human welfare.\nDespite unprecedented investment in AI safety research, rapidly growing awareness among key stakeholders, and proliferating frameworks for responsible AI development, we face what I’ll term the “coordination crisis” in AI governance—a systemic failure to align diverse efforts across technical, policy, and strategic domains into a coherent response proportionate to the risks we face.\nThe current state of AI governance presents a striking paradox. On one hand, we witness extraordinary mobilization: billions in research funding, proliferating safety initiatives, major tech companies establishing alignment teams, and governments worldwide developing AI strategies. The Asilomar AI Principles garnered thousands of signatures Tegmark (2024), the EU advances comprehensive AI regulation European (2024), and technical researchers produce increasingly sophisticated work on alignment, interpretability, and robustness.\nYet alongside this activity, we observe systematic coordination failures that may prove catastrophic. Technical safety researchers develop sophisticated alignment techniques without clear implementation pathways. Policy specialists craft regulatory frameworks lacking technical grounding to ensure practical efficacy. Ethicists articulate normative principles that lack operational specificity. Strategy researchers identify critical uncertainties but struggle to translate these into actionable guidance. International bodies convene without shared frameworks for assessing interventions.\n\nShow Image\n\n3.2.1 1.2.1 Safety Gaps from Misaligned Efforts\n\nThe fragmentation problem manifests in incompatible frameworks between technical researchers, policy specialists, and strategic analysts. Each community develops sophisticated approaches within their domain, yet translation between domains remains primitive. This creates systematic blind spots where risks emerge at the interfaces between technical capabilities, institutional responses, and strategic dynamics.\nWhen different communities operate with incompatible frameworks, critical risks fall through the cracks. Technical researchers may solve alignment problems under assumptions that policymakers’ decisions invalidate. Regulations optimized for current systems may inadvertently incentivize dangerous development patterns. Without shared models of the risk landscape, our collective efforts resemble the parable of blind men describing an elephant—each accurate within their domain but missing the complete picture Paul (2023).\n\nHistorical precedents demonstrate how coordination failures in technology governance can lead to dangerous dynamics. The nuclear arms race exemplifies how lack of coordination can create negative-sum outcomes where all parties become less secure despite massive investments in safety measures. Similar dynamics may emerge in AI development without proper coordination infrastructure.\n\n\n3.2.2 1.2.2 Resource Misallocation\n\nThe AI safety community faces a complex tradeoff in resource allocation. While some duplication of efforts can improve reliability through independent verification—akin to reproducing scientific results—the current level of fragmentation often leads to wasteful redundancy. Multiple teams independently develop similar frameworks without building on each other’s work, creating opportunity costs where critical but unglamorous research areas remain understaffed. Funders struggle to identify high-impact opportunities across technical and governance domains, lacking the epistemic infrastructure to assess where marginal resources would have the greatest impact. This misallocation becomes more costly as the window for establishing effective governance narrows with accelerating AI development.\n\n\n\n\nTable 3.1: Examples of duplicated AI safety efforts across organizations\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Area\nOrganization A\nOrganization B\nDuplication Level\nOpportunity Cost\n\n\n\n\nInterpretability Methods\nAnthropic’s mechanistic interpretability\nDeepMind’s concept activation vectors\nMedium\nReduced focus on multi-agent safety\n\n\nAlignment Frameworks\nMIRI’s embedded agency\nFHI’s comprehensive AI services\nHigh\nLimited work on institutional design\n\n\nRisk Assessment Models\nGovAI’s policy models\nCSER’s existential risk frameworks\nHigh\nInsufficient capability benchmarking\n\n\n\n\n\n\n\n\n3.2.3 1.2.3 Negative-Sum Dynamics\n\nPerhaps most concerning, uncoordinated interventions can actively increase risk. Safety standards that advantage established players may accelerate risky development elsewhere. Partial transparency requirements might enable capability advances without commensurate safety improvements. International agreements lacking shared technical understanding may lock in dangerous practices. Without coordination, our cure risks becoming worse than the disease.\nThe game-theoretic structure of AI development creates particularly pernicious dynamics. Armstrong et al. Armstrong, Bostrom, and Shulman (2016) demonstrate how uncoordinated policies can incentivize a “race to the precipice” where competitive pressures override safety considerations. The situation resembles a multi-player prisoner’s dilemma or stag hunt where individually rational decisions lead to collectively catastrophic outcomes Samuel (2023) Hunt (2025).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#historical-parallels-and-temporal-urgency",
    "href": "chapters/Outlines/final_draft.html#historical-parallels-and-temporal-urgency",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "3.3 1.3 Historical Parallels and Temporal Urgency",
    "text": "3.3 1.3 Historical Parallels and Temporal Urgency\n\nHistory offers instructive parallels. The nuclear age began with scientists racing to understand and control forces that could destroy civilization. Early coordination failures—competing national programs, scientist-military tensions, public-expert divides—nearly led to catastrophe multiple times. Only through developing shared frameworks (deterrence theory) Schelling (1960), institutions (IAEA), and communication channels (hotlines, treaties) did humanity navigate the nuclear precipice Rehman (2025).\nYet AI presents unique coordination challenges that compress our response timeline:\nAccelerating Development: Unlike nuclear weapons requiring massive infrastructure, AI development proceeds in corporate labs and academic departments worldwide. Capability improvements come through algorithmic insights and computational scale, both advancing exponentially.\nDual-Use Ubiquity: Every AI advance potentially contributes to both beneficial applications and catastrophic risks. The same language model architectures enabling scientific breakthroughs could facilitate dangerous manipulation or deception at scale.\nComprehension Barriers: Nuclear risks were viscerally understandable—cities vaporized, radiation sickness, nuclear winter. AI risks involve abstract concepts like optimization processes, goal misspecification, and emergent capabilities that resist intuitive understanding.\nGovernance Lag: Traditional governance mechanisms—legislation, international treaties, professional standards—operate on timescales of years to decades. AI capabilities advance on timescales of months to years, creating an ever-widening capability-governance gap.\n\nShow Image",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#research-question-and-scope",
    "href": "chapters/Outlines/final_draft.html#research-question-and-scope",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "3.4 1.4 Research Question and Scope",
    "text": "3.4 1.4 Research Question and Scope\n\nThis thesis addresses a specific dimension of the coordination challenge by investigating the question:\nCan frontier AI technologies be utilized to automate the modeling of transformative AI risks, enabling robust prediction of policy impacts across diverse worldviews?\nMore specifically, I explore whether frontier language models can automate the extraction and formalization of probabilistic world models from AI safety literature, creating a scalable computational framework that enhances coordination in AI governance through systematic policy evaluation under uncertainty.\nTo break this down into its components:\n\nFrontier AI Technologies: Today’s most capable language models (GPT-4, Claude-3 level systems)\nAutomated Modeling: Using these systems to extract and formalize argument structures from natural language\nTransformative AI Risks: Potentially catastrophic outcomes from advanced AI systems, particularly existential risks\nPolicy Impact Prediction: Evaluating how governance interventions might alter probability distributions over outcomes\nDiverse Worldviews: Accounting for fundamental disagreements about AI development trajectories and risk factors\n\nThe investigation encompasses both theoretical development and practical implementation, focusing specifically on existential risks from misaligned AI systems rather than broader AI ethics concerns. This narrowed scope enables deep technical development while addressing the highest-stakes coordination challenges.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#the-multiplicative-benefits-framework",
    "href": "chapters/Outlines/final_draft.html#the-multiplicative-benefits-framework",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "3.5 1.5 The Multiplicative Benefits Framework",
    "text": "3.5 1.5 The Multiplicative Benefits Framework\n\nThe central thesis of this work is that combining three elements—automated worldview extraction, prediction market integration, and formal policy evaluation—creates multiplicative rather than merely additive benefits for AI governance. Each component enhances the others, creating a system more valuable than the sum of its parts.\n\nShow Image\n\n3.5.1 1.5.1 Automated Worldview Extraction\nCurrent approaches to AI risk modeling, exemplified by the Modeling Transformative AI Risks (MTAIR) project, demonstrate the value of formal representation but require extensive manual effort. Creating a single model demands dozens of expert-hours to translate qualitative arguments into quantitative frameworks. This bottleneck severely limits the number of perspectives that can be formalized and the speed of model updates as new arguments emerge.\nAutomation using frontier language models addresses this scaling challenge. By developing systematic methods to extract causal structures and probability judgments from natural language, we can:\n\nProcess orders of magnitude more content\nIncorporate diverse perspectives rapidly\nMaintain models that evolve with the discourse\nReduce barriers to entry for contributing worldviews\n\n\n\n3.5.2 1.5.2 Live Data Integration\nStatic models, however well-constructed, quickly become outdated in fast-moving domains. Prediction markets and forecasting platforms aggregate distributed knowledge about uncertain futures, providing continuously updated probability estimates. By connecting formal models to these live data sources, we create dynamic assessments that incorporate the latest collective intelligence P. E. Tetlock and Gardner (2015).\nThis integration serves multiple purposes:\n\nGrounding abstract models in empirical forecasts\nIdentifying which uncertainties most affect outcomes\nRevealing when model assumptions diverge from collective expectations\nGenerating new questions for forecasting communities\n\n\n\n3.5.3 1.5.3 Formal Policy Evaluation\nFormal policy evaluation transforms static risk assessments into actionable guidance by modeling how specific interventions alter critical parameters. Using causal inference techniques Pearl (2000) Pearl (2009), we can assess not just the probability of adverse outcomes but how those probabilities change under different policy regimes.\nThis enables genuinely evidence-based policy development:\n\nComparing interventions across multiple worldviews\nIdentifying robust strategies that work across scenarios\nUnderstanding which uncertainties most affect policy effectiveness\nPrioritizing research to reduce decision-relevant uncertainty\n\n\n\n3.5.4 1.5.4 The Synergy\nThe multiplicative benefits emerge from the interactions between components:\n\nAutomation enables comprehensive coverage, making prediction market integration more valuable by connecting to more perspectives\nMarket data validates and calibrates automated extractions, improving quality\nPolicy evaluation gains precision from both comprehensive models and live probability updates\nThe complete system creates feedback loops where policy analysis identifies critical uncertainties for market attention\n\nThis synergistic combination addresses the coordination crisis by providing common ground for disparate communities, translating between technical and policy languages, quantifying previously implicit disagreements, and enabling evidence-based compromise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#thesis-structure-and-roadmap",
    "href": "chapters/Outlines/final_draft.html#thesis-structure-and-roadmap",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "3.6 1.6 Thesis Structure and Roadmap",
    "text": "3.6 1.6 Thesis Structure and Roadmap\n\nThe remainder of this thesis develops the multiplicative benefits framework from theoretical foundations to practical implementation:\nChapter 2: Context and Theoretical Foundations establishes the intellectual groundwork, examining the epistemic challenges unique to AI governance, Bayesian networks as formal tools for uncertainty representation, argument mapping as a bridge from natural language to formal models, the MTAIR project’s achievements and limitations, and requirements for effective coordination infrastructure.\nChapter 3: AMTAIR Design and Implementation presents the technical system including overall architecture and design principles, the two-stage extraction pipeline (ArgDown → BayesDown), validation methodology and results, case studies from simple examples to complex AI risk models, and integration with prediction markets and policy evaluation.\nChapter 4: Discussion - Implications and Limitations critically examines technical limitations and failure modes, conceptual concerns about formalization, integration with existing governance frameworks, scaling challenges and opportunities, and broader implications for epistemic security.\nChapter 5: Conclusion synthesizes key contributions and charts paths forward with a summary of theoretical and practical achievements, concrete recommendations for stakeholders, research agenda for community development, and vision for AI governance with proper coordination infrastructure.\nThroughout this progression, I maintain dual focus on theoretical sophistication and practical utility. The framework aims not merely to advance academic understanding but to provide actionable tools for improving coordination in AI governance during this critical period.\n\nShow Image\nHaving established the coordination crisis and outlined how automated modeling can address it, we now turn to the theoretical foundations that make this approach possible. The next chapter examines the unique epistemic challenges of AI governance and introduces the formal tools—particularly Bayesian networks—that enable rigorous reasoning under deep uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#ai-existential-risk-the-carlsmith-model",
    "href": "chapters/Outlines/final_draft.html#ai-existential-risk-the-carlsmith-model",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "4.1 2.1 AI Existential Risk: The Carlsmith Model",
    "text": "4.1 2.1 AI Existential Risk: The Carlsmith Model\n\nTo ground our discussion in concrete terms, I examine Joseph Carlsmith’s “Is Power-Seeking AI an Existential Risk?” as an exemplar of structured reasoning about AI catastrophic risk Carlsmith (2022). Carlsmith’s analysis stands out for its explicit probabilistic decomposition of the path from current AI development to potential existential catastrophe.\n\n\n\n4.1.1 2.1.1 Six-Premise Decomposition\n\nAccording to the MTAIR model Clarke et al. (2022), Carlsmith decomposes existential risk into a probabilistic chain with explicit estimates2:\n\nPremise 1: Transformative AI development this century (P≈0.80)(P ≈ 0.80) (P≈0.80)\nPremise 2: AI systems pursuing objectives in the world (P≈0.95)(P ≈ 0.95) (P≈0.95)\nPremise 3: Systems with power-seeking instrumental incentives (P≈0.40)(P ≈ 0.40) (P≈0.40)\nPremise 4: Sufficient capability for existential threat (P≈0.65)(P ≈ 0.65) (P≈0.65)\nPremise 5: Misaligned systems despite safety efforts (P≈0.50)(P ≈ 0.50) (P≈0.50)\nPremise 6: Catastrophic outcomes from misaligned power-seeking (P≈0.65)(P ≈ 0.65) (P≈0.65)\n\nComposite Risk Calculation: P(doom)≈0.05P(doom) ≈ 0.05 P(doom)≈0.05 (5%)\n\nmermaid\nflowchart TD\n    P1[Premise 1: Transformative AI&lt;br/&gt;P ≈ 0.80] --&gt; P2[Premise 2: AI pursuing objectives&lt;br/&gt;P ≈ 0.95]\n    P2 --&gt; P3[Premise 3: Power-seeking incentives&lt;br/&gt;P ≈ 0.40]\n    P3 --&gt; P4[Premise 4: Existential capability&lt;br/&gt;P ≈ 0.65]\n    P4 --&gt; P5[Premise 5: Misalignment despite safety&lt;br/&gt;P ≈ 0.50]\n    P5 --&gt; P6[Premise 6: Catastrophic outcome&lt;br/&gt;P ≈ 0.65]\n    P6 --&gt; D[Existential Catastrophe&lt;br/&gt;P ≈ 0.05]\nCarlsmith structures his argument through six conditional premises, each assigned explicit probability estimates:\nPremise 1: APS Systems by 2070 (P≈0.65)(P ≈ 0.65) (P≈0.65) “By 2070, there will be AI systems with Advanced capability, Agentic planning, and Strategic awareness”—the conjunction of capabilities that could enable systematic pursuit of objectives in the world.\nPremise 2: Alignment Difficulty (P≈0.40)(P ≈ 0.40) (P≈0.40) “It will be harder to build aligned APS systems than misaligned systems that are still attractive to deploy”—capturing the challenge that safety may conflict with capability or efficiency.\nPremise 3: Deployment Despite Misalignment (P≈0.70)(P ≈ 0.70) (P≈0.70) “Conditional on 1 and 2, we will deploy misaligned APS systems”—reflecting competitive pressures and limited coordination.\nPremise 4: Power-Seeking Behavior (P≈0.65)(P ≈ 0.65) (P≈0.65) “Conditional on 1-3, misaligned APS systems will seek power in high-impact ways”—based on instrumental convergence arguments.\nPremise 5: Disempowerment Success (P≈0.40)(P ≈ 0.40) (P≈0.40) “Conditional on 1-4, power-seeking will scale to permanent human disempowerment”—despite potential resistance and safeguards.\nPremise 6: Existential Catastrophe (P≈0.95)(P ≈ 0.95) (P≈0.95) “Conditional on 1-5, this disempowerment constitutes existential catastrophe”—connecting power loss to permanent curtailment of human potential.\nOverall Risk: Multiplying through the conditional chain yields P(doom)≈0.05P(doom) ≈ 0.05 P(doom)≈0.05 or 5% by 2070.\nThis structured approach exemplifies the type of reasoning AMTAIR aims to formalize and automate. While Carlsmith spent months developing this model manually, similar rigor exists implicitly in many AI safety arguments awaiting extraction.\n\n\n\n4.1.2 2.1.2 Why Carlsmith Exemplifies Formalizable Arguments\nCarlsmith’s model demonstrates several features that make it ideal for formal representation:\nExplicit Probabilistic Structure: Each premise receives numerical probability estimates with documented reasoning, enabling direct translation to Bayesian network parameters.\nClear Conditional Dependencies: The logical flow from capabilities through deployment decisions to catastrophic outcomes maps naturally onto directed acyclic graphs.\nTransparent Decomposition: Breaking the argument into modular premises allows independent evaluation and sensitivity analysis of each component.\nDocumented Reasoning: Extensive justification for each probability enables extraction of both structure and parameters from the source text.\n\nWe will return to Carlsmith’s model in Chapter 3 as our primary complex case study, demonstrating how AMTAIR successfully extracts and formalizes this sophisticated multi-level argument.\n\n\nBeyond Carlsmith’s model, other structured approaches to AI risk—such as Christiano’s “What failure looks like” Christiano (2019)—provide additional targets for automated extraction, enabling comparative analysis across different expert worldviews.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#the-epistemic-challenge-of-policy-evaluation",
    "href": "chapters/Outlines/final_draft.html#the-epistemic-challenge-of-policy-evaluation",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "4.2 2.2 The Epistemic Challenge of Policy Evaluation",
    "text": "4.2 2.2 The Epistemic Challenge of Policy Evaluation\n\nAI governance policy evaluation faces unique epistemic challenges that render traditional policy analysis methods insufficient. Understanding these challenges motivates the need for new computational approaches.\n\n4.2.1 2.2.1 Unique Characteristics of AI Governance\nDeep Uncertainty Rather Than Risk: Traditional policy analysis distinguishes between risk (known probability distributions) and uncertainty (known possibilities, unknown probabilities). AI governance faces deep uncertainty—we cannot confidently enumerate possible futures, much less assign probabilities Hallegatte et al. (2012). Will recursive self-improvement enable rapid capability gains? Can value alignment be solved technically? These foundational questions resist empirical resolution before their answers become catastrophically relevant.\nComplex Multi-Level Causation: Policy effects propagate through technical, institutional, and social levels with intricate feedback loops. A technical standard might alter research incentives, shifting capability development trajectories, changing competitive dynamics, and ultimately affecting existential risk through pathways invisible at the policy’s inception. Traditional linear causal models cannot capture these dynamics.\nIrreversibility and Lock-In: Many AI governance decisions create path dependencies that prove difficult or impossible to reverse. Early technical standards shape development trajectories. Institutional structures ossify. International agreements create sticky equilibria. Unlike many policy domains where course correction remains possible, AI governance mistakes may prove permanent.\nValue-Laden Technical Choices: The entanglement of technical and normative questions confounds traditional separation of facts and values. What constitutes “alignment”? How much capability development should we risk for economic benefits? Technical specifications embed ethical judgments that resist neutral expertise.\n\n\n\n\nTable 4.1: Comparison of AI governance vs traditional policy domains\n\n\n\n\n\n\n\n\n\n\nDimension\nTraditional Policy\nAI Governance\n\n\n\n\nUncertainty Type\nRisk (known distributions)\nDeep uncertainty (unknown unknowns)\n\n\nCausal Structure\nLinear, traceable\nMulti-level, feedback loops\n\n\nReversibility\nCourse correction possible\nPath dependencies, lock-in\n\n\nFact-Value Separation\nClear boundaries\nEntangled technical-normative\n\n\nEmpirical Grounding\nHistorical precedents\nUnprecedented phenomena\n\n\nTime Horizons\nYears to decades\nMonths to centuries\n\n\n\n\n\n\n\n\n4.2.2 2.2.2 Limitations of Traditional Approaches\nStandard policy evaluation tools prove inadequate for these challenges:\nCost-Benefit Analysis assumes commensurable outcomes and stable probability distributions. When potential outcomes include existential catastrophe with deeply uncertain probabilities, the mathematical machinery breaks down. Infinite negative utility resists standard decision frameworks.\nScenario Planning helps explore possible futures but typically lacks the probabilistic reasoning needed for decision-making under uncertainty. Without quantification, scenarios provide narrative richness but limited action guidance.\nExpert Elicitation aggregates specialist judgment but struggles with interdisciplinary questions where no single expert grasps all relevant factors. Moreover, experts often operate with different implicit models, making aggregation problematic.\nRed Team Exercises test specific plans but miss systemic risks emerging from component interactions. Gaming individual failures cannot reveal emergent catastrophic possibilities.\nThese limitations create a methodological gap: we need approaches that handle deep uncertainty, represent complex causation, quantify expert disagreement, and enable systematic exploration of intervention effects.\n\n\n\n4.2.3 2.2.3 The Underlying Epistemic Framework\n\nThe AMTAIR approach rests on a specific epistemic framework that combines probabilistic reasoning, conditional logic, and possible worlds semantics. This framework provides the philosophical foundation for representing deep uncertainty about AI futures.\nProbabilistic Epistemology: Following the Bayesian tradition, we treat probability as a measure of rational credence rather than objective frequency. This subjective interpretation allows meaningful probability assignments even for unique, unprecedented events like AI catastrophe. As E.T. Jaynes demonstrated, probability theory extends deductive logic to handle uncertainty, providing a calculus for rational belief Jaynes (2003).\nConditional Structure: The framework emphasizes conditional rather than absolute probabilities. Instead of asking “What is P(catastrophe)?” we ask “What is P(catastrophe | specific assumptions)?” This conditionalization makes explicit the dependency of conclusions on worldview assumptions, enabling productive disagreement about premises rather than conclusions.\nPossible Worlds Semantics: We conceptualize uncertainty as distributions over possible worlds—complete descriptions of how reality might unfold. Each world represents a coherent scenario with specific values for all relevant variables. Probability distributions over these worlds capture both what we know and what we don’t know about the future.\nThis framework enables several key capabilities:\n\nRepresenting ignorance: We can express uncertainty about uncertainty itself through hierarchical probability models\nCombining evidence: Bayesian updating provides principled methods for integrating new information\nComparing worldviews: Different probability distributions over the same space of possibilities enable systematic comparison\nEvaluating interventions: Counterfactual reasoning about how actions change probability distributions\n\n\n\n\n4.2.4 2.2.4 Toward New Epistemic Tools\n\nThe inadequacy of traditional methods for AI governance creates an urgent need for new epistemic tools. These tools must:\n\nHandle Deep Uncertainty: Move beyond point estimates to represent ranges of possibilities\nCapture Complex Causation: Model multi-level interactions and feedback loops\nQuantify Disagreement: Make explicit where experts diverge and why\nEnable Systematic Analysis: Support rigorous comparison of policy options\n\nKey Insight: The computational approaches developed in this thesis—particularly Bayesian networks enhanced with automated extraction—directly address each of these requirements by providing formal frameworks for reasoning under uncertainty.\n\nShow Image\nShow Image\nShow Image\nShow Image\n\nRecent work on conditional trees demonstrates the value of structured approaches to uncertainty. McCaslin et al. McCaslin et al. (2024) show how hierarchical conditional forecasting can identify high-value questions for reducing uncertainty about complex topics like AI risk. Their methodology, which asks experts to produce simplified Bayesian networks of informative forecasting questions, achieved nine times higher information value than standard forecasting platform questions.\n\nTetlock’s work with the Forecasting Research Institute P. Tetlock (2022) exemplifies how prediction markets can provide empirical grounding for formal models. By structuring questions as conditional trees, they enable forecasters to express complex dependencies between events, providing exactly the type of data needed for Bayesian network parameterization.\n\nGruetzemacher Gruetzemacher (2022) evaluates the tradeoffs between full Bayesian networks and conditional trees for forecasting tournaments. While conditional trees offer simplicity, Bayesian networks provide richer representation of dependencies—motivating AMTAIR’s approach of using full networks while leveraging conditional tree insights for question generation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#bayesian-networks-as-knowledge-representation",
    "href": "chapters/Outlines/final_draft.html#bayesian-networks-as-knowledge-representation",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "4.3 2.3 Bayesian Networks as Knowledge Representation",
    "text": "4.3 2.3 Bayesian Networks as Knowledge Representation\n\nBayesian networks offer a mathematical framework uniquely suited to addressing these epistemic challenges. By combining graphical structure with probability theory, they provide tools for reasoning about complex uncertain domains.\n\n4.3.1 2.3.1 Mathematical Foundations\nA Bayesian network consists of:\n\nDirected Acyclic Graph (DAG): Nodes represent variables, edges represent direct dependencies\nConditional Probability Tables (CPTs): For each node, P(node|parents) quantifies relationships\n\nThe joint probability distribution factors according to the graph structure:\n\nP(X1,X2,…,Xn)=∏i=1nP(Xi∣Parents(Xi))P(X_1, X_2, …, X_n) = _{i=1}^{n} P(X_i | Parents(X_i))P(X1​,X2​,…,Xn​)=i=1∏n​P(Xi​∣Parents(Xi​))\nThis factorization enables efficient inference and embodies causal assumptions explicitly.\n\nPearl’s foundational work Pearl (2014) established Bayesian networks as a principled approach to automated reasoning under uncertainty, providing both theoretical foundations and practical algorithms.\n\n\n\n\n4.3.2 2.3.2 The Rain-Sprinkler-Grass Example\n\nThe canonical example illustrates key concepts3:\n\n[Grass_Wet]: Concentrated moisture on grass. \n + [Rain]: Water falling from sky.\n + [Sprinkler]: Artificial watering system.\n   + [Rain]\nNetwork Structure:\n\nRain (root cause): P(rain) = 0.2\nSprinkler (intermediate): P(sprinkler|rain) varies by rain state\nGrass_Wet (effect): P(wet|rain, sprinkler) depends on both causes\n\n\nmermaid\nflowchart TD\n    R[Rain&lt;br/&gt;P(rain) = 0.2] --&gt; S[Sprinkler]\n    R --&gt; G[Grass_Wet]\n    S --&gt; G\n    \n    subgraph CPT1[Sprinkler CPT]\n        S1[P(sprinkler|rain) = 0.01]\n        S2[P(sprinkler|¬rain) = 0.4]\n    end\n    \n    subgraph CPT2[Grass_Wet CPT]\n        G1[P(wet|rain,sprinkler) = 0.99]\n        G2[P(wet|rain,¬sprinkler) = 0.8]\n        G3[P(wet|¬rain,sprinkler) = 0.9]\n        G4[P(wet|¬rain,¬sprinkler) = 0.01]\n    end\npython\n# Basic network representation\nnodes = ['Rain', 'Sprinkler', 'Grass_Wet']\nedges = [('Rain', 'Sprinkler'), ('Rain', 'Grass_Wet'), ('Sprinkler', 'Grass_Wet')]\n\n# Conditional probability specification\nP_wet_given_causes = {\n    (True, True): 0.99,    # Rain=T, Sprinkler=T\n    (True, False): 0.80,   # Rain=T, Sprinkler=F  \n    (False, True): 0.90,   # Rain=F, Sprinkler=T\n    (False, False): 0.01   # Rain=F, Sprinkler=F\n}\nThis simple network demonstrates:\n\nMarginal Inference: P(grass_wet) computed from joint distribution\nDiagnostic Reasoning: P(rain|grass_wet) reasoning from effects to causes\nIntervention Modeling: P(grass_wet|do(sprinkler=on)) for policy analysis\n\n\nShow Image\n\n4.3.2.1 Rain-Sprinkler-Grass Network Rendering\n#| label: rain_sprinkler_grass_example_network_rendering\n#| echo: true\n#| eval: true\n#| fig-cap: \"Dynamic Html Rendering of the Rain-Sprinkler-Grass DAG with Conditional Probabilities\"\n#| fig-link: \"https://singularitysmith.github.io/AMTAIR_Prototype/bayesian_network.html\"\n#| fig-alt: \"Dynamic Html Rendering of the Rain-Sprinkler-Grass DAG\"\n\nfrom IPython.display import IFrame\n\nIFrame(src=\"https://singularitysmith.github.io/AMTAIR_Prototype/bayesian_network.html\", width=\"100%\", height=\"600px\")\n\n\n\n4.3.3 2.3.3 Advantages for AI Risk Modeling\nThese features address key requirements for AI governance:\n\nHandling Uncertainty: Every parameter is a distribution, not a point estimate\nRepresenting Causation: Directed edges embody causal relationships\nEnabling Analysis: Formal inference algorithms support systematic evaluation\nFacilitating Communication: Visual structure aids cross-domain understanding\n\n\n\n\n4.3.4 2.3.3 Advantages for AI Risk Modeling\nBayesian networks offer several compelling advantages for the peculiar challenge of modeling AI risks—a domain where we’re essentially trying to reason about systems that don’t yet exist, wielding capabilities we can barely imagine, potentially causing outcomes we desperately hope to avoid.\nExplicit Uncertainty Representation: Unlike traditional risk assessment tools that often hide uncertainty behind point estimates, Bayesian networks wear their uncertainty on their sleeve. Every node, every edge, every probability is a distribution rather than a false certainty. This matters enormously when discussing AI catastrophe—we’re not pretending to know the unknowable, but rather mapping the landscape of our ignorance with mathematical precision.\nNative Causal Reasoning: The directed edges in Bayesian networks aren’t just arrows on a diagram; they encode causal beliefs about how the world works. This enables both forward reasoning (“If we develop AGI, what happens?”) and diagnostic reasoning (“Given that we observe concerning AI behaviors, what does this tell us about underlying alignment?”). Pearl’s do-calculus Pearl (2009) transforms these networks into laboratories for counterfactual exploration.\nEvidence Integration: As new research emerges, as capabilities advance, as governance experiments succeed or fail, Bayesian networks provide a principled framework for updating our beliefs. Unlike static position papers that age poorly, these models can evolve with our understanding—a living document for a rapidly changing field.\nModular Construction: Complex arguments about AI risk involve multiple interacting factors across technical, social, and political domains. Bayesian networks allow us to build these arguments piece by piece, validating each component before assembling the whole. This modularity also enables different experts to contribute their specialized knowledge without needing to understand every aspect of the system.\nVisual Communication: Perhaps most importantly for the coordination challenge, Bayesian networks provide a visual language that transcends disciplinary boundaries. A policymaker might not understand the mathematics of instrumental convergence, but they can see how the “power-seeking” node connects to “human disempowerment” in the network diagram. This shared visual vocabulary creates common ground for productive disagreement.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-argument-mapping",
    "href": "chapters/Outlines/final_draft.html#sec-argument-mapping",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "4.4 2.4 Argument Mapping and Formal Representations",
    "text": "4.4 2.4 Argument Mapping and Formal Representations\n\nThe journey from a researcher’s intuition about AI risk to a formal probabilistic model resembles translating poetry into mathematics—something essential is always at risk of being lost, yet something equally essential might be gained. Argument mapping provides the crucial middle ground, a structured approach to preserving the logic of natural language arguments while preparing them for mathematical formalization.\n\n4.4.1 2.4.1 From Natural Language to Structure\nNatural language arguments about AI risk are rich tapestries woven from causal claims, conditional relationships, uncertainty expressions, and support patterns. When Bostrom writes about the “treacherous turn” Bostrom (2014), he’s not just coining a memorable phrase—he’s encoding a complex causal story about how a seemingly aligned AI system might conceal its true objectives until it gains sufficient power to pursue them without constraint.\nThe challenge lies in extracting this structure without losing the nuance. Traditional logical analysis might reduce Bostrom’s argument to syllogisms, but this would miss the probabilistic texture, the implicit conditionality, the causal directionality that makes the argument compelling. Argument mapping takes a different approach, seeking to identify:\n\nCore claims and propositions: What exactly is being asserted?\nInferential relationships: How do claims support or challenge each other?\nImplicit assumptions: What unstated premises make the argument work?\nUncertainty qualifications: Where does the author express doubt or confidence?\n\nRecent advances in computational argument mining Anderson (2007) Benn and Macintosh (2011) Khartabil et al. (2021) have shown promise in automating parts of this process. Tools like Microsoft’s Claimify Metropolitansky and Larson (2025) demonstrate how large language models can extract verifiable claims from complex texts, though the challenge of preserving argumentative structure remains formidable.\n\n\n\n4.4.2 2.4.2 ArgDown: Structured Argument Notation\nEnter ArgDown Voigt ([2014] 2025), a markdown-inspired syntax that captures hierarchical argument structure while remaining human-readable. Think of it as the middle child between the wild expressiveness of natural language and the rigid formality of logic—inheriting the best traits of both parents while developing its own personality.\n[AI_Poses_Risk]: Advanced AI systems may pose existential risk to humanity.\n + [Capability_Growth]: AI capabilities are growing exponentially.\n   + [Compute_Scaling]: Available compute doubles every few months.\n   + [Algorithmic_Progress]: New architectures show surprising emergent abilities.\n + [Alignment_Difficulty]: Aligning AI with human values is unsolved.\n   - [Current_Progress]: Some progress on interpretability and oversight.\n - [Institutional_Response]: Institutions are mobilizing to address risks.\nThis notation does several clever things simultaneously. The hierarchical structure mirrors how we naturally think about arguments—main claims supported by evidence, which in turn rest on more fundamental observations. The + and - symbols indicate support and opposition relationships, creating a visual flow of argumentative force. Most importantly, it preserves the semantic content of each claim while imposing just enough structure to enable computational processing.\nFor AMTAIR, we adapt ArgDown specifically for causal arguments, where the hierarchy represents causal influence rather than logical support. This seemingly small change has profound implications—we’re not just mapping what follows from what, but what causes what.\n\n\n\n4.4.3 2.4.3 BayesDown: The Bridge to Bayesian Networks\n\nIf ArgDown is the middle child, then BayesDown—developed specifically for this thesis—is the ambitious younger sibling who insists on quantifying everything. By extending ArgDown syntax with probabilistic metadata in JSON format, BayesDown creates a complete specification for Bayesian networks while maintaining human readability.\njson\n[Existential_Catastrophe]: Permanent curtailment of humanity's potential. {\n  \"instantiations\": [\"catastrophe_TRUE\", \"catastrophe_FALSE\"],\n  \"priors\": {\"p(catastrophe_TRUE)\": \"0.05\", \"p(catastrophe_FALSE)\": \"0.95\"},\n  \"posteriors\": {\n    \"p(catastrophe_TRUE|disempowerment_TRUE)\": \"0.95\",\n    \"p(catastrophe_TRUE|disempowerment_FALSE)\": \"0.001\"\n  }\n}\n + [Human_Disempowerment]: Loss of human control over future trajectory. {\n   \"instantiations\": [\"disempowerment_TRUE\", \"disempowerment_FALSE\"],\n   \"priors\": {\"p(disempowerment_TRUE)\": \"0.20\", \"p(disempowerment_FALSE)\": \"0.80\"}\n }\nThis representation performs a delicate balancing act. The natural language descriptions preserve the semantic meaning that makes arguments comprehensible. The hierarchical structure maintains the causal relationships that give arguments their logical force. The JSON metadata adds the mathematical precision needed for formal analysis. Together, they create what I call a “hybrid representation”—neither fully natural nor fully formal, but something more useful than either alone.\nThe two-stage extraction process (ArgDown → BayesDown) mirrors how experts actually think about complex arguments. First, we identify what matters and how things relate causally (structure). Then, we consider how likely different scenarios are based on those relationships (quantification). This separation isn’t just convenient for implementation—it’s psychologically valid.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-mtair-framework",
    "href": "chapters/Outlines/final_draft.html#sec-mtair-framework",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "4.5 2.5 The MTAIR Framework: Achievements and Limitations",
    "text": "4.5 2.5 The MTAIR Framework: Achievements and Limitations\n\nUnderstanding AMTAIR requires understanding its intellectual ancestor: the Modeling Transformative AI Risks (MTAIR) project. Like many good ideas in science, MTAIR began with a simple observation and a ambitious goal.\n\n4.5.1 2.5.1 MTAIR’s Approach\nThe MTAIR project, spearheaded by David Manheim and colleagues Clarke et al. (2022), emerged from a frustration familiar to anyone who’s attended a conference on AI safety: brilliant people talking past each other, using the same words to mean different things, reaching incompatible conclusions from seemingly shared premises. The diagnosis was elegant—perhaps these disagreements stemmed not from fundamental philosophical differences but from implicit models that had never been made explicit.\nTheir prescription was equally elegant: manually translate influential AI risk arguments into formal Bayesian networks, making assumptions visible and disagreements quantifiable. Using Analytica software, the team embarked on what can only be described as an intellectual archaeology expedition, carefully excavating the implicit causal models buried in papers, blog posts, and treatises about AI risk.\nThe process was painstaking:\n\nSystematic Decomposition: Breaking complex arguments into component claims, identifying variables and relationships through close reading and expert consultation.\nProbability Elicitation: Gathering quantitative estimates through structured expert interviews, literature review, and careful interpretation of qualitative claims.\nSensitivity Analysis: Testing which parameters most influenced conclusions, revealing where disagreements actually mattered versus where they were merely academic.\nVisual Communication: Creating interactive models that stakeholders could explore, modify, and understand without deep technical training.\n\nThe ambition was breathtaking—to create a formal lingua franca for AI risk discussions, enabling productive disagreement and cumulative progress.\n\n\n\n4.5.2 2.5.2 Key Achievements\nCredit where credit is due: MTAIR demonstrated something many thought impossible. Complex philosophical arguments about AI risk—the kind that sprawl across hundred-page papers mixing technical detail with speculative scenarios—could indeed be formalized without losing their essential insights.\nFeasibility of Formalization: The project’s greatest achievement was simply showing it could be done. Arguments from Bostrom, Christiano, and others translated surprisingly well into network form, suggesting that beneath the surface complexity lay coherent causal models waiting to be extracted.\nValue of Quantification: Moving from “likely” and “probably” to actual numbers forced precision in a domain often clouded by vague pronouncements. Disagreements that seemed fundamental sometimes evaporated when forced to specify exactly what probability ranges were under dispute.\nCross-Perspective Communication: The formal models created neutral ground where technical AI researchers and policy wonks could meet. Instead of talking past each other in incompatible languages, they could point to specific nodes and edges, making disagreements concrete and tractable.\nResearch Prioritization: Perhaps most practically, sensitivity analysis revealed which empirical questions actually mattered. If changing your belief about technical parameter X from 0.3 to 0.7 doesn’t meaningfully affect the conclusion about AI risk, maybe we should focus our research elsewhere.\n\n\n\n4.5.3 2.5.3 Fundamental Limitations\nBut here’s where the story takes a sobering turn. Despite these achievements, MTAIR faced limitations that prevented it from achieving its full vision—limitations that ultimately motivated the development of AMTAIR.\nLabor Intensity: Creating a single model required what can charitably be called a heroic effort. Based on team reports and model complexity, estimates ranged from 200 to 400 expert-hours per formalization4. In a field where new influential arguments appear monthly, this pace couldn’t keep up with the discourse.\n\nStatic Nature: Once built, these beautiful models began aging immediately. New research emerged, capability assessments shifted, governance proposals evolved—but updating the models required near-complete reconstruction. They were snapshots of arguments at particular moments, not living representations that could evolve.\nLimited Accessibility: Using the models required Analytica software and non-trivial technical sophistication. The very experts whose arguments were being formalized often couldn’t directly engage with their formalized representations without intermediation.\nSingle Perspective: Each model represented one worldview at a time. Comparing different perspectives required building entirely separate models, making systematic comparison across viewpoints labor-intensive and error-prone.\nThese weren’t failures of execution but fundamental constraints of the manual approach. Like medieval scribes copying manuscripts, the MTAIR team had shown the value of preservation and dissemination, but the printing press had yet to be invented.\n\n\n\n4.5.4 2.5.4 The Automation Opportunity\nThe MTAIR experience revealed a tantalizing possibility: if the bottleneck was human labor rather than conceptual feasibility, perhaps automation could crack open the problem. The rise of large language models capable of sophisticated reasoning about text created a technological moment ripe for exploitation.\nKey lessons from MTAIR informed the automation approach:\n\nFormal models genuinely enhance understanding and coordination—the juice is worth the squeeze\nThe modeling process itself surfaces implicit assumptions—extraction is as valuable as the final product\nQuantification enables analyses impossible with qualitative arguments alone—numbers matter even when uncertain\nBut manual approaches cannot scale to match the challenge—we need computational leverage\n\nThis set the stage for AMTAIR’s central innovation: using frontier language models to automate the extraction and formalization process while preserving the benefits MTAIR had demonstrated. Not to replace human judgment, but to amplify it—turning what took weeks into what takes hours, enabling comprehensive coverage rather than selective sampling.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-literature-review",
    "href": "chapters/Outlines/final_draft.html#sec-literature-review",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "4.6 2.6 Literature Review: Content and Technical Levels",
    "text": "4.6 2.6 Literature Review: Content and Technical Levels\n\nThe intellectual landscape surrounding AI risk resembles a rapidly expanding metropolis—new neighborhoods of thought spring up monthly, connected by bridges of varying stability to the established districts. A comprehensive review would fill volumes, so let me provide a guided tour of the territories most relevant to AMTAIR’s mission.\n\n4.6.1 2.6.1 AI Risk Models Evolution\nThe evolution of AI risk models traces a path from philosophical speculation to increasingly rigorous formalization—a journey from “what if?” to “how likely?”\nEarly Phase (2000-2010): The conversation began with broad conceptual arguments. Good’s ultraintelligent machine Good (1966) and Vinge’s technological singularity set the stage, but these were more thought experiments than models. Yudkowsky’s early writings Yudkowsky (2008) introduced key concepts like recursive self-improvement and orthogonality but remained largely qualitative.\nFormalization Phase (2010-2018): Bostrom’s Superintelligence Bostrom (2014) marked a watershed, providing systematic analysis of pathways, capabilities, and risks. The book’s genius lay not in mathematical formalism but in conceptual clarity—decomposing the nebulous fear of “robot overlords” into specific mechanisms like instrumental convergence and infrastructure profusion.\nQuantification Phase (2018-present): Recent years have seen explicit probability estimates entering mainstream discourse. Carlsmith’s power-seeking model Carlsmith (2022), Cotra’s biological anchors, and various compute-based timelines represent attempts to put numbers on previously qualitative claims. The field increasingly recognizes that governance decisions require more than philosophical arguments—they need probability distributions.\nThis progression reflects a maturing field, though it also creates new challenges. As models become more quantitative, they risk false precision. As they become more complex, they risk inscrutability. AMTAIR attempts to navigate these tensions by preserving the narrative clarity of earlier work while enabling the mathematical rigor of recent approaches.\n\n\n\n4.6.2 2.6.2 Governance Proposals Taxonomy\nIf risk models are the diagnosis, governance proposals are the treatment plans—and like medicine, they range from gentle interventions to radical surgery.\nTechnical Standards: The “first, do no harm” approach focuses on concrete safety requirements—interpretability benchmarks, robustness testing, capability thresholds. These proposals, exemplified by standard-setting bodies and technical safety organizations, offer specificity at the cost of narrowness.\nRegulatory Frameworks: Moving up the intervention ladder, we find comprehensive regulatory proposals like the EU AI Act European (2024). These create institutional structures, liability regimes, and oversight mechanisms, trading broad coverage for implementation complexity.\nInternational Coordination: At the ambitious end, proposals for international AI governance treaties, soft law arrangements, and technical cooperation agreements aim to prevent races to the bottom. Think nuclear non-proliferation but for minds instead of missiles.\nResearch Priorities: Cutting across these categories, work by Dafoe Dafoe (2018) and others maps the research landscape itself—what questions need answering before we can govern wisely? This meta-level analysis shapes funding flows and talent allocation.\nA particularly compelling example of conditional governance thinking comes from “A Narrow Path” Miotti et al. (2024), which proposes a phased approach: immediate safety measures to prevent uncontrolled development, international institutions to ensure stability, and long-term scientific foundations for beneficial transformative AI. This temporal sequencing—safety, stability, then flourishing—reflects growing sophistication in governance thinking.\n\n\n\n4.6.3 2.6.3 Bayesian Network Theory and Applications\nThe mathematical machinery underlying AMTAIR rests on decades of theoretical development in probabilistic graphical models. Understanding this foundation helps appreciate both the power and limitations of the approach.\nThe key insight, crystallized in the work of Pearl Pearl (2014) and elaborated by Koller & Friedman Koller and Friedman (2009), is that independence relationships in complex systems can be read from graph structure. D-separation, the Markov condition, and the relationship between graphs and probability distributions provide the mathematical spine that makes Bayesian networks more than pretty pictures.\nCritical concepts for AI risk modeling:\n\nConditional Independence: Variable A is independent of C given B—encoded through graph separation\nMarkov Condition: Each variable is independent of its non-descendants given its parents\nInference Algorithms: From exact variable elimination to approximate Monte Carlo methods\nCausal Interpretation: When edges represent causal influence, the network supports counterfactual reasoning\n\nThese aren’t just mathematical niceties. When we claim that “deployment decisions” mediates the relationship between “capability advancement” and “catastrophic risk,” we’re making a precise statement about conditional independence that has testable implications.\n\n\n\n4.6.4 2.6.4 Software Tools Landscape\nThe gap between Bayesian network theory and practical implementation is bridged by an ecosystem of software tools, each with its own strengths and opinions about how probabilistic reasoning should work.\npgmpy: This Python library provides the computational backbone for AMTAIR, offering both learning algorithms and inference engines. Its object-oriented design maps naturally onto our extraction pipeline.\nNetworkX: For graph manipulation and analysis, NetworkX has become the de facto standard in Python, providing algorithms for everything from centrality measurement to community detection.\nPyVis: Interactive visualization transforms static networks into explorable landscapes. PyVis’s integration with web technologies enables the rich interactive features that make formal models accessible.\nPandas/NumPy: The workhorses of scientific Python handle data manipulation and numerical computation, providing the infrastructure on which everything else builds.\nThe integration challenge—making these tools play nicely together while maintaining performance and correctness—shaped many architectural decisions in AMTAIR. Each tool excels in its domain, but the seams between them required careful engineering.\n\n\n\n4.6.5 2.6.5 Formalization Approaches\nThe challenge of formalizing natural language arguments extends far beyond AI risk, touching on fundamental questions in logic, linguistics, and artificial intelligence.\nPollock’s work on cognitive carpentry Pollock (1995) provides philosophical grounding, arguing that human reasoning itself involves implicit formal structures that can be computationally modeled. This view—that formalization reveals rather than imposes structure—underlies AMTAIR’s approach.\nKey theoretical challenges:\n\nSemantic Preservation: How do we maintain meaning while adding precision?\nStructural Extraction: What implicit relationships lurk in natural language?\nUncertainty Quantification: How do we map “likely” to numbers?\n\nRecent work on causal structure learning from text Babakov et al. (2025) Ban et al. (2023) Bethard (2007) offers hope that these challenges can be addressed computationally. The convergence of large language models with formal methods creates new possibilities for bridging the semantic-symbolic gap.\n\n\n\n4.6.6 2.6.6 Correlation Accounting Methods\nOne of the most persistent criticisms of Bayesian networks concerns their assumption of conditional independence given parents. In the real world, and especially in complex socio-technical systems like AI development, correlations abound.\nMethods for handling these correlations have evolved considerably:\nCopula Methods: By separating marginal distributions from dependence structure, copulas Nelson (2006) allow modeling of complex correlations while preserving the Bayesian network framework. Think of it as adding a correlation layer on top of the basic network.\nHierarchical Models: Introducing latent variables that influence multiple observed variables captures correlations naturally. If “AI research culture” influences both “capability progress” and “safety investment,” their correlation is explained.\nExplicit Correlation Nodes: Sometimes the most straightforward approach is best—directly model correlation mechanisms as additional nodes in the network.\nSensitivity Bounds: When correlations remain uncertain, compute best and worst case scenarios. This reveals when independence assumptions critically affect conclusions versus when they’re harmless simplifications.\nFor AMTAIR, the pragmatic approach dominates: start with independence assumptions, identify where they matter through sensitivity analysis, then selectively add correlation modeling where it most affects conclusions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-methodology",
    "href": "chapters/Outlines/final_draft.html#sec-methodology",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "4.7 2.7 Methodology",
    "text": "4.7 2.7 Methodology\n\nThe methodology of this research resembles less a linear march from hypothesis to conclusion and more an iterative dance between theory and implementation, vision and reality. Let me walk you through the choreography.\n\n4.7.1 2.7.1 Research Design Overview\nThis research follows what methodologists might call a “design science” approach—we’re not just studying existing phenomena but creating new artifacts (the AMTAIR system) and evaluating their utility for solving practical problems (the coordination crisis in AI governance).\nThe overall flow:\n\nTheoretical Development: Establishing why automated extraction could address the coordination crisis, grounded in epistemic theory and mechanism design\nTechnical Implementation: Building working software that demonstrates feasibility, not as a proof-of-concept toy but as a system capable of handling real arguments\nEmpirical Validation: Testing extraction quality against expert judgment, measuring not just accuracy but usefulness for downstream tasks\nApplication Studies: Applying the system to real AI governance questions, evaluating whether formal models actually enhance decision-making\n\nThis isn’t waterfall development where each phase completes before the next begins. Rather, insights from implementation fed back into theory, validation results shaped technical improvements, and application attempts revealed new requirements. The methodology itself embodied the iterative refinement it sought to enable.\n\n\n\n4.7.2 2.7.2 Formalizing World Models from AI Safety Literature\nThe core methodological challenge—transforming natural language arguments into formal probabilistic models—requires careful consideration of what we’re actually trying to capture.\nA “world model” in this context isn’t just any formal representation but specifically a causal model embodying beliefs about how different factors influence AI risk. The extraction approach must therefore:\n\nIdentify key variables: Not just any entities mentioned, but causally relevant factors\nExtract causal relationships: Not mere correlation or co-occurrence, but directed influence\nCapture uncertainty: Both structural uncertainty (does A cause B?) and parametric uncertainty (how strongly?)\nPreserve context: Maintaining enough semantic information to interpret the formal model\n\nLarge language models enable this through sophisticated pattern recognition and reasoning capabilities, but they’re tools, not magic wands. The methodology must account for their strengths (recognizing implicit structure) and weaknesses (potential hallucination, inconsistency).\n\n\n\n4.7.3 2.7.3 From Natural Language to Computational Models\nThe journey from text to computation follows a carefully designed pipeline that mirrors human cognitive processes. Just as you wouldn’t ask someone to simultaneously parse grammar and solve equations, we separate structural understanding from quantitative reasoning.\nThe Two-Stage Process:\nStage 1 focuses on structure—what causes what? The LLM reads an argument much as a human would, identifying key claims and their relationships. The prompt design here is crucial, providing enough guidance to ensure consistent extraction while allowing flexibility for different argument styles.\nStage 2 adds quantities—how likely is each outcome? With structure established, the system generates targeted questions about probabilities. This separation enables different approaches to quantification: extracting explicit estimates from text, inferring from qualitative language, or even connecting to external prediction markets.\nThe magic happens in the interplay. Structure constrains what probabilities are needed. Probability requirements might reveal missing structural elements. The process is a dialogue between qualitative and quantitative understanding.\n\n\n\n4.7.4 2.7.4 Directed Acyclic Graphs: Structure and Semantics\nAt the mathematical heart of Bayesian networks lie Directed Acyclic Graphs (DAGs)—structures that are simultaneously simple enough to analyze and rich enough to capture complex phenomena.\nThe “directed” part encodes causality or influence—edges have direction, flowing from cause to effect. The “acyclic” part ensures logical coherence—you can’t have A causing B causing C causing A, no matter how much certain political arguments might suggest otherwise.\nKey properties for AI risk modeling:\nAcyclicity: More than a mathematical convenience, this enforces coherent temporal or causal ordering. In AI risk arguments, this prevents circular reasoning where consequences justify premises that predict those same consequences.\nD-separation: This graphical criterion determines conditional independence. If knowing about AI capabilities tells you nothing additional about risk given that you know deployment decisions, then capabilities and risk are d-separated given deployment.\nMarkov Condition: Each variable depends only on its parents, not on its entire ancestry. This locality assumption makes inference tractable and forces modelers to make intervention points explicit.\nPath Analysis: Following paths through the graph reveals how influence propagates. Multiple paths between variables indicate redundancy—important for understanding intervention robustness.\nThe causal interpretation, following Pearl’s framework, transforms these mathematical objects into tools for counterfactual reasoning. When we ask “what if we prevented deployment of misaligned systems?” we’re performing surgery on the DAG, setting variables and propagating consequences.\n\n\n\n4.7.5 2.7.5 Quantification of Probabilistic Judgments\nHere we encounter one of the most philosophically fraught aspects of the methodology: turning words into numbers. When an expert writes “highly likely,” what probability should we assign? When they say “significant risk,” what distribution captures their belief?\nThe methodology embraces rather than elides this challenge:\nCalibration Studies: Research on human probability expression shows systematic patterns. “Highly likely” typically maps to 0.8-0.9, “probable” to 0.6-0.8, though individual and cultural variation is substantial.\nExtraction Strategies: The system uses multiple approaches:\n\nDirect extraction: “We estimate 65% probability”\nLinguistic mapping: “Very likely” → 0.85 (with uncertainty)\nComparative extraction: “More likely than X” where P(X) is known\nBounded extraction: “At least 30%” → [0.30, 1.0]\n\nUncertainty Representation: Rather than false precision, we maintain uncertainty about probabilities themselves. This might seem like uncertainty piled on uncertainty, but it’s honest—and mathematically tractable through hierarchical models.\nThe goal isn’t perfect extraction but useful extraction. If we can narrow “significant risk” from [0, 1] to [0.15, 0.45], we’ve added information even if we haven’t achieved precision.\n\n\n\n4.7.6 2.7.6 Inference Techniques for Complex Networks\nOnce we’ve built these formal models, we need to reason with them—and here computational complexity rears its exponential head. The number of probability calculations required for exact inference grows exponentially with network connectivity, quickly overwhelming even modern computers.\nThe methodology employs a portfolio of approaches:\nExact Methods: For smaller networks (&lt;30 nodes), variable elimination and junction tree algorithms provide exact answers. These form the gold standard against which we validate approximate methods.\nSampling Approaches: Monte Carlo methods trade exactness for scalability. By simulating many possible worlds consistent with our probability model, we approximate the true distributions. The law of large numbers is our friend here.\nVariational Methods: These turn inference into optimization—find the simplest distribution that approximates our true beliefs. Like finding the best polynomial approximation to a complex curve.\nHybrid Strategies: Different parts of the network might use different methods. Exact inference for critical subgraphs, approximation for peripheral components.\nThe choice of method affects not just computation time but the types of questions we can meaningfully ask. This creates a methodological feedback loop where feasible inference shapes model design.\n\n\n\n4.7.7 2.7.7 Integration with Prediction Markets and Forecasting Platforms\n\nWhile full integration remains future work, the methodology anticipates connection to live forecasting data as a critical enhancement. The vision is compelling: formal models grounded in collective intelligence, updating as new information emerges.\nThe planned approach would involve:\nSemantic Matching: Model variables rarely align perfectly with forecast questions. “AI causes human extinction” might map to multiple specific forecasts about capabilities, deployment, and impacts. Developing robust matching algorithms is essential.\nTemporal Alignment: Markets predict specific dates (“AGI by 2030”) while models consider scenarios (“given AGI development”). Bridging these requires careful probability conditioning.\nQuality Weighting: Not all forecasts are created equal. Platform reputation, forecaster track records, and market depth all affect reliability. The methodology must account for this heterogeneity.\nUpdate Scheduling: Real-time updates would overwhelm users and computation. The system needs intelligent policies about when model updates provide value.\nPlatforms like Metaculus P. Tetlock (2022) already demonstrate sophisticated conditional forecasting on AI topics. The challenge lies not in data availability but in meaningful integration that enhances rather than complicates decision-making.\n\nWith these theoretical foundations and methodological commitments established, we can now turn to the concrete implementation of AMTAIR. The next chapter demonstrates how these abstract principles translate into working software that addresses real governance challenges. The journey from theory to practice always involves surprises—some pleasant, others less so—but that’s what makes it interesting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-system-architecture",
    "href": "chapters/Outlines/final_draft.html#sec-system-architecture",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.1 3.1 System Architecture Overview",
    "text": "5.1 3.1 System Architecture Overview\n\nPicture, if you will, a factory for transforming arguments into models. Raw materials (PDFs, blog posts, research papers) enter at one end. Finished products (interactive Bayesian networks) emerge at the other. In between lies a carefully orchestrated pipeline where each stage performs its specialized transformation, passing refined materials to the next.\nThe AMTAIR architecture embodies a philosophy: complex tasks become manageable when decomposed into focused components. Rather than building a monolithic “argument-to-model” black box, we created a series of specialized modules, each excellent at one thing.\n\nThe pipeline consists of five main stages:\n\nText Ingestion and Preprocessing: Like a careful librarian, this stage catalogues incoming documents, normalizes their format, extracts metadata, and identifies the argumentative content worth processing.\nArgument Extraction: The intellectual heart of the system, where large language models perform their magic, transforming prose into structured representations.\nData Transformation: The workshop where extracted arguments are refined, validated, and prepared for mathematical representation.\nNetwork Construction: The assembly line where formal Bayesian networks are instantiated, complete with conditional probability tables.\nInteractive Visualization: The showroom where complex models become accessible through thoughtful design and interactivity.\n\n\n5.1.1 3.1.1 Five-Stage Pipeline Architecture\nLet’s examine each stage more closely, understanding not just what they do but why they exist as separate components.\nText Ingestion and Preprocessing handles the unglamorous but essential work of standardization. Academic PDFs, with their two-column layouts and embedded figures, differ vastly from blog posts with inline code and hyperlinks. This stage creates a uniform representation while preserving essential structure and metadata. Format normalization strips away presentation while preserving content. Metadata extraction captures authorship, publication date, and citations. Relevance filtering identifies sections containing arguments rather than literature reviews or acknowledgments. Character encoding standardization prevents those maddening �replacement characters that plague text processing.\nArgument Extraction represents AMTAIR’s core innovation. Using a two-stage process that mirrors human reasoning, it first identifies structural relationships (what influences what) then quantifies those relationships (how likely, how strong). This separation enables targeted prompts optimized for each task, human verification between stages, and modular improvements as LLM capabilities evolve.\nData Transformation bridges the gap between textual representations and mathematical models. It parses the BayesDown syntax into structured data, validates that the resulting network forms a proper DAG, checks probability consistency, and handles missing data intelligently.\nNetwork Construction instantiates the formal mathematical model. This involves creating nodes and edges according to extracted structure, populating conditional probability tables, initializing inference engines, and validating the complete model.\nInteractive Visualization makes the complex accessible. Through thoughtful visual encoding of probabilities and relationships, progressive disclosure of detail, interactive exploration capabilities, and multiple export formats, it serves diverse stakeholder needs.\n\n\n\n5.1.2 3.1.2 Design Principles\nCore Design Philosophy: The architecture embodies several principles that guided countless implementation decisions:\nModularity: Each component has clear inputs, outputs, and responsibilities. This isn’t just good software engineering—it enables independent improvement of components and graceful degradation when parts fail.\nValidation Checkpoints: Between each stage, we validate outputs before proceeding. Bad extractions don’t propagate into visualization. Malformed networks trigger re-extraction rather than cryptic errors.\nHuman-in-the-Loop: While pursuing automation, we recognize that human judgment remains invaluable. The architecture provides natural intervention points where experts can verify and correct.\nExtensibility: New document formats, improved extraction prompts, alternative visualization libraries—the architecture accommodates growth without restructuring.\nThe system emphasizes transparency over black-box efficiency. Users can inspect intermediate representations, understand extraction decisions, and verify transformations. This builds trust—essential for a system handling high-stakes arguments about existential risk.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-two-stage-extraction",
    "href": "chapters/Outlines/final_draft.html#sec-two-stage-extraction",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.2 3.2 The Two-Stage Extraction Process",
    "text": "5.2 3.2 The Two-Stage Extraction Process\n\nThe heart of AMTAIR beats with a two-stage rhythm: structure, then probability. This separation, which initially seemed like an implementation detail, revealed itself as fundamental to the extraction challenge.\n\n5.2.1 3.2.1 Stage 1: Structural Extraction (ArgDown)\nImagine reading a complex argument about AI risk. Your first pass likely isn’t calculating exact probabilities—you’re mapping the landscape. What are the key claims? How do they relate? What supports what? Stage 1 mirrors this cognitive process.\nThe extraction begins with pattern recognition. Natural language contains linguistic markers of causal relationships: “leads to,” “results in,” “depends on,” “influences.” The LLM, trained on vast corpora of argumentative text, recognizes these patterns and their variations.\nConsider extracting from a passage like: “The development of artificial general intelligence will likely lead to rapid capability gains through recursive self-improvement. This intelligence explosion could result in systems pursuing convergent instrumental goals, potentially including resource acquisition and self-preservation. Without solved alignment, such power-seeking behavior poses existential risks to humanity.”\nThe system identifies three key variables connected by causal relationships:\n\nAGI Development → Intelligence Explosion\nIntelligence Explosion → Power-Seeking Behavior\nPower-Seeking Behavior → Existential Risk\n\nBut extraction goes beyond simple pattern matching. The system must handle complex linguistic phenomena like coreference (“this,” “such systems”), implicit relationships, conditional statements, and negative statements. The magic lies in prompt engineering that guides the LLM to consistent extraction while remaining flexible enough for diverse argument styles.\nThe output, formatted in ArgDown syntax, preserves both structure and semantics:\n[Existential_Risk]: Threat to humanity's continued existence and flourishing.\n + [Power_Seeking_Behavior]: AI systems pursuing instrumental goals like resource acquisition.\n   + [Intelligence_Explosion]: Rapid recursive self-improvement leading to superintelligence.\n     + [AGI_Development]: Creation of artificial general intelligence systems.\n\n\n\n5.2.2 3.2.2 Stage 2: Probability Integration (BayesDown)\nWith structure established, Stage 2 adds the quantitative flesh to the qualitative bones. This stage faces a different challenge: extracting numerical beliefs from text that often expresses uncertainty in frustratingly vague terms.\nThe process begins by generating targeted questions based on the extracted structure. For each node, we need prior probabilities. For each child-parent relationship, we need conditional probabilities. The combinatorics can be daunting—a node with three binary parents requires 8 conditional probability values.\nThe system employs multiple strategies for probability extraction:\nExplicit Extraction: When authors provide numerical estimates (“we assign 70% probability”), extraction is straightforward, though we must handle various formats and contexts.\nLinguistic Mapping: Qualitative expressions map to probability ranges based on calibration studies. “Highly likely” becomes approximately 0.85, though we maintain uncertainty about this mapping.\nComparative Reasoning: Statements like “more probable than not” or “at least as likely as X” provide bounds even without exact values.\nCoherence Enforcement: Probabilities must sum correctly. If P(A|B) = 0.7, then P(not A|B) must equal 0.3. The system detects and resolves inconsistencies.\nThe result is a complete BayesDown specification:\njson\n[Existential_Risk]: Threat to humanity's continued existence. {\n  \"instantiations\": [\"true\", \"false\"],\n  \"priors\": {\"p(true)\": \"0.10\", \"p(false)\": \"0.90\"},\n  \"posteriors\": {\n    \"p(true|power_seeking_true)\": \"0.65\",\n    \"p(true|power_seeking_false)\": \"0.001\"\n  }\n}\n\n\n\n5.2.3 3.2.3 Why Two Stages?\nThe separation of structure from probability isn’t merely convenient—it’s cognitively valid and practically essential. Let me count the ways this design decision pays dividends:\nCognitive Alignment: Humans naturally separate “what relates to what” from “how likely is it.” The two-stage process mirrors this, making the system’s operation intuitive and interpretable.\nError Isolation: Structural errors (missing a key variable) differ fundamentally from probability errors (estimating 0.7 instead of 0.8). Separating stages allows targeted debugging and improvement.\nModular Validation: Experts can verify structure without needing to evaluate every probability. This enables efficient human oversight at natural checkpoints.\nFlexible Quantification: Different probability sources (text extraction, expert elicitation, market data) can feed into the same structure. The architecture accommodates multiple approaches to the probability challenge.\nTransparency: Users can inspect ArgDown to understand what was extracted before probabilities were added. This builds trust and enables meaningful correction.\nThe two-stage approach also revealed an unexpected benefit: ArgDown itself became a valuable output. Researchers began using these structural extractions for qualitative analysis, even without probability quantification. Sometimes, just making argument structure explicit provides sufficient value.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-implementation-tech",
    "href": "chapters/Outlines/final_draft.html#sec-implementation-tech",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.3 3.3 Implementation Technologies",
    "text": "5.3 3.3 Implementation Technologies\n\nChoosing technologies for AMTAIR resembled assembling a band—each instrument needed to excel individually while harmonizing with the ensemble. The selection criteria balanced capability, maturity, interoperability, and community support.\n\n5.3.1 3.3.1 Technology Stack\nThe final ensemble performs beautifully:\n\n\n\n\n\n\n\n\n\nComponent\nTechnology\nPurpose\nWhy This Choice\n\n\n\n\nLanguage Models\nGPT-4, Claude\nArgument extraction\nState-of-the-art reasoning capabilities\n\n\nNetwork Analysis\nNetworkX\nGraph algorithms\nMature, comprehensive, well-documented\n\n\nProbabilistic Modeling\npgmpy\nBayesian operations\nNative Python, active development\n\n\nVisualization\nPyVis\nInteractive rendering\nWeb-based, customizable, responsive\n\n\nData Processing\nPandas\nStructured manipulation\nIndustry standard, powerful operations\n\n\n\nLanguage Models form the cognitive core. GPT-4 and Claude demonstrate remarkable ability to understand complex arguments, recognize implicit structure, and maintain coherence across long extractions. The choice to support multiple models provides robustness and allows leveraging their complementary strengths.\nNetworkX handles all graph-theoretic heavy lifting. From basic operations like cycle detection to advanced algorithms like centrality measurement, it provides a comprehensive toolkit that would take years to replicate.\npgmpy bridges the gap between graph structure and probabilistic reasoning. Its clean API design maps naturally onto our extracted representations, while its inference algorithms handle the computational complexity of Bayesian reasoning.\nPyVis transforms static networks into living documents. Built on vis.js, it provides smooth physics simulations, rich interactivity, and extensive customization options—all accessible through Python.\nPandas might seem mundane compared to its companions, but it’s the reliable rhythm section that keeps everything together. Its ability to reshape, merge, and transform structured data makes the complex data transformations tractable.\n\n\n\n5.3.2 3.3.2 Key Algorithms\nBeyond the libraries lie custom algorithms that address AMTAIR-specific challenges:\nHierarchical Parsing: The algorithm that transforms indented ArgDown text into structured data represents a small miracle of recursive descent parsing adapted for our custom syntax. It maintains parent-child relationships while handling edge cases like repeated nodes and complex dependencies.\npython\ndef parse_hierarchy(text, current_indent=0):\n    \"\"\"Recursively parse indented structure maintaining relationships\"\"\"\n    # Track nodes at each level for parent identification\n    # Handle repeated nodes by reference\n    # Validate DAG property during construction\nProbability Completion: Real arguments rarely specify all required probabilities. Our completion algorithm uses maximum entropy principles—when uncertain, assume maximum disorder. This provides conservative estimates that can be refined with additional information.\nVisual Encoding: The algorithm mapping probabilities to colors uses perceptual uniformity. The green-to-red gradient isn’t linear in RGB space but follows human perception of color difference. Small details, big impact on usability.\nLayout Optimization: Force-directed layouts often produce “hairballs” for complex networks. Our customized approach uses hierarchical initialization based on causal depth, then refines with physics simulation. The result: layouts that reveal structure rather than obscuring it.\n\n\n\n5.3.3 3.3.3 (Expected) Performance Characteristics\n\nPerformance in a system like AMTAIR involves multiple dimensions—speed, accuracy, scalability. Let’s examine what theoretical analysis and design considerations suggest about system behavior.\nComputational Complexity: The extraction phase exhibits linear complexity in document length—processing twice as much text takes roughly twice as long. However, the inference phase faces exponential complexity in network connectivity. A fully connected network with n binary nodes requires O(2^n) operations for exact inference. This fundamental limitation shapes practical usage patterns.\nPractical Implications: Small networks (&lt;20 nodes) enable real-time interaction with exact inference. Medium networks (20-50 nodes) require seconds to minutes depending on connectivity. Large networks (&gt;50 nodes) necessitate approximate methods, trading accuracy for tractability. Very large networks push the boundaries of current methods.\nThe bottleneck shifts predictably: extraction remains manageable even for lengthy documents, but inference becomes challenging as models grow. This suggests a natural workflow—extract comprehensively, then focus on relevant subnetworks for detailed analysis.\nOptimization Opportunities: Several strategies could improve performance: caching frequent inference queries, hierarchical decomposition of large networks, parallel processing for independent subgraphs, and progressive rendering for visualization. The modular architecture accommodates these enhancements without fundamental restructuring.\n\n\n\n5.3.4 3.3.4 Deterministic vs. Probabilistic Components of the Workflow\nAn interesting philosophical question arises: in a system reasoning about probability, which components should themselves be probabilistic?\nThe current implementation draws a clear line:\nDeterministic Components: All data transformations, graph algorithms, and inference calculations operate deterministically. Given the same input, they produce identical output. This provides reproducibility and debuggability—essential for building trust.\nProbabilistic Components: The LLM calls for extraction introduce variability. Even with temperature set to 0, language models exhibit some randomness. Different runs might extract slightly different structures or probability estimates from the same text.\nThis division reflects a deeper principle: use determinism wherever possible, embrace probability where necessary. The extraction task—interpreting natural language—inherently involves uncertainty. But once we have formal representations, all subsequent operations should be predictable.\nFrom an information-theoretic perspective, we’re trying to extract maximum information from documents within computational budget constraints. Each document contains some finite amount of formalizable argument structure. Our goal is recovering as much as possible given realistic resource limits.\nThe two-stage extraction can be viewed as successive refinement—first recovering the higher-order bits (structure), then filling in lower-order bits (probabilities). This aligns with rate-distortion theory, where we get the most important information first.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-case-rain-sprinkler",
    "href": "chapters/Outlines/final_draft.html#sec-case-rain-sprinkler",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.4 3.4 Case Study: Rain-Sprinkler-Grass",
    "text": "5.4 3.4 Case Study: Rain-Sprinkler-Grass\n\nEvery field has its canonical examples—physics has spherical cows, economics has widget factories, and Bayesian networks have the rain-sprinkler-grass scenario. Despite its simplicity, this example teaches profound lessons about causal reasoning and serves as the perfect test case for AMTAIR.\n\n5.4.1 3.4.1 Processing Steps\nLet me walk you through how AMTAIR processes this foundational example:\nThe input arrives as a simple text description: “When it rains, the grass gets wet. The sprinkler also makes the grass wet. However, when it rains, we usually don’t run the sprinkler.”\nFrom this prosaic description, the system performs five transformations:\n\nArgDown Parsing: Extract three variables (Rain, Sprinkler, Grass_Wet) and identify that rain influences both sprinkler usage and grass wetness, while the sprinkler also influences grass wetness.\nQuestion Generation: Create probability queries: What’s P(Rain)? What’s P(Sprinkler|Rain)? What’s P(Grass_Wet|Rain,Sprinkler) for all combinations?\nBayesDown Extraction: Either extract probabilities from text or apply reasonable defaults. The “usually don’t run” becomes P(Sprinkler|Rain) ≈ 0.01.\nNetwork Construction: Build the formal Bayesian network with three nodes, three edges, and complete conditional probability tables.\nVisualization Rendering: Create an interactive display where rain appears as a root cause, influencing both sprinkler and grass directly.\n\nEach step validates its outputs before proceeding, ensuring that errors don’t cascade through the pipeline.\n\n\n\n5.4.2 3.4.2 Example Conversion Steps\nLet’s trace the actual transformations to see the pipeline in action:\nInitial ArgDown Extraction:\nmarkdown\n[Grass_Wet]: Concentrated moisture on grass blades. {\"instantiations\": [\"wet\", \"dry\"]}    \n + [Rain]: Precipitation from the sky. {\"instantiations\": [\"raining\", \"not_raining\"]}\n + [Sprinkler]: Artificial watering system. {\"instantiations\": [\"on\", \"off\"]}\n   + [Rain]\nThe hierarchy captures that rain influences sprinkler usage—a subtle but important causal relationship that pure correlation would miss.\nGenerated Questions for Probability Extraction:\nmarkdown\n/* Prior probabilities */\n- What is the probability that it rains?\n- What is the probability the sprinkler is on?\n\n/* Conditional probabilities */  \n- What is the probability the sprinkler is on when it's raining?\n- What is the probability the sprinkler is on when it's not raining?\n- What is the probability the grass is wet when it's raining and sprinkler is on?\n- [... and so on for all combinations]\nThe system generates exactly the questions needed to fully specify the network—no more, no less.\nComplete BayesDown Result:\njson\n[Grass_Wet]: Concentrated moisture on grass. {\n  \"instantiations\": [\"wet\", \"dry\"],\n  \"priors\": {\"p(wet)\": \"0.45\", \"p(dry)\": \"0.55\"},\n  \"posteriors\": {\n    \"p(wet|raining,on)\": \"0.99\",\n    \"p(wet|raining,off)\": \"0.80\", \n    \"p(wet|not_raining,on)\": \"0.90\",\n    \"p(wet|not_raining,off)\": \"0.01\"\n  }\n}\nNotice how the probabilities tell a coherent story—grass is almost certainly wet if either water source is active, almost certainly dry if neither is.\nResulting DataFrame Structure:\nThe transformation into tabular format enables standard data analysis tools while preserving all relationships and probabilities. Each row represents a node with its properties, parents, children, and probability distributions.\n\n\n\n5.4.3 3.4.3 Results\nThe successfully processed rain-sprinkler-grass example demonstrates several key capabilities:\nStructure Preservation: The causal relationships—including the subtle influence of rain on sprinkler usage—are correctly captured and maintained throughout processing.\nProbability Coherence: All probability distributions sum to 1.0, conditional probabilities are complete, and the values tell a plausible story.\nVisual Clarity: The rendered network clearly shows rain as the root cause, influencing both sprinkler and grass, while sprinkler provides an additional pathway to wet grass.\nInteractive Exploration: Users can click nodes to see detailed probabilities, drag to rearrange for clarity, and explore how changing parameters affects outcomes.\nInference Capability: The system correctly calculates derived probabilities like P(Rain|Grass_Wet)—the diagnostic reasoning from effect to cause that makes Bayesian networks so powerful.\nThis simple example validates the basic pipeline functionality. But the real test comes with complex, real-world arguments…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-case-carlsmith",
    "href": "chapters/Outlines/final_draft.html#sec-case-carlsmith",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.5 3.5 Case Study: Carlsmith’s Power-Seeking AI Model",
    "text": "5.5 3.5 Case Study: Carlsmith’s Power-Seeking AI Model\n\nFrom the gentle meadows of rain and sprinklers, we now ascend to the existential peaks of AI risk. Carlsmith’s model represents a dramatic increase in complexity—both conceptually and computationally. Where rain-sprinkler-grass has 3 nodes, Carlsmith involves 23. Where grass wetness is intuitive, “mesa-optimization” and “corrigibility” require careful thought.\n\n5.5.1 3.5.1 Model Complexity\nThe numbers tell only part of the story:\n\n23 nodes: Each representing a substantive claim about AI development, deployment, or risk\n29 edges: Encoding causal relationships across technical, strategic, and societal domains\nMultiple probability tables: Many nodes have several parents, creating combinatorial explosion\nSix-level causal depth: From root causes to final catastrophe, influence propagates through multiple stages\n\nBut the conceptual complexity dwarfs the computational. Nodes like “APS-Systems” (Advanced, Planning, Strategically aware) encode specific technical hypotheses. Relationships like how “incentives to build” influence “deployment despite misalignment” require understanding of organizational behavior under competitive pressure.\nThis is no longer a toy problem but a serious attempt to formalize one of the most important arguments of our time.\n\n\n\n5.5.2 3.5.2 Automated Extraction of the Carlsmith’s Argument Structure\nThe extraction process began with feeding Carlsmith’s paper to AMTAIR. Watching the system work felt like observing an archaeological excavation—layers of argument slowly revealed their structure.\nThe LLM prompts for extraction deserve special attention. Through iterative refinement, we developed prompts that guide extraction while remaining flexible:\npython\nARGDOWN_EXTRACTION = PromptTemplate(\"\"\"\nYou are extracting the causal model from an AI safety argument.\nFocus on:\n1. Identifying key variables that affect outcomes\n2. Capturing causal relationships (not mere association)  \n3. Preserving the author's terminology where possible\n4. Creating a directed acyclic graph structure\n\nFor Carlsmith's argument about power-seeking AI, pay special attention to:\n- The chain from capabilities to catastrophe\n- Conditional relationships (X matters only if Y)\n- Technical preconditions for risk\n\"\"\")\nThe extraction revealed Carlsmith’s elegant decomposition. At the highest level: capabilities enable power-seeking, which enables disempowerment, which constitutes catastrophe. But the details matter—deployment decisions mediated by incentives and deception, alignment difficulty influenced by multiple technical factors, corrective mechanisms that might interrupt the chain.\nThe ArgDown representation captured this structure:\n[Existential_Catastrophe]: Permanent curtailment of humanity's potential\n + [Human_Disempowerment]: Humans lose control over future\n   + [Scale_Of_Power_Seeking]: Power-seeking behavior becomes overwhelming\n     + [Misaligned_Power_Seeking]: AI systems pursue problematic objectives\n       + [APS_Systems]: Advanced, planning, strategically aware AI\n       + [Alignment_Difficulty]: Hard to align such systems\n       + [Deployment_Despite_Misalignment]: Systems deployed anyway\n         + [Incentives_To_Build]: Strong pressure to develop AI\n         + [Deception]: AI systems hide misalignment\nThe structure revealed insights. “Misaligned_Power_Seeking” emerged as a critical hub, influenced by multiple factors and influencing multiple outcomes. The pathway from incentives through deployment to risk became explicit.\n\n\n\n5.5.3 3.5.3 From ArgDown to BayesDown in Carlsmith’s Model\nAdding probabilities to Carlsmith’s structure presented unique challenges. Unlike rain-sprinkler probabilities that have intuitive values, what’s the probability of “mesa-optimization” or “deceptive alignment”?\nThe system generated over 100 probability questions for the full model. A sample:\nFor [Deployment_Decisions]:\n- What is P(deploy)?\n- What is P(deploy|strong_incentives, deception)?\n- What is P(deploy|strong_incentives, no_deception)?\n- What is P(deploy|weak_incentives, deception)?\n- What is P(deploy|weak_incentives, no_deception)?\nEach question targets a specific parameter needed for the Bayesian network. The conditional structure reflects Carlsmith’s argument—deployment depends on both incentives (external pressure) and deception (hidden misalignment).\nThe LLM extraction drew on Carlsmith’s explicit estimates where available and inferred reasonable values elsewhere. The result captured both the structure and Carlsmith’s quantitative risk assessment:\njson\n[Deployment_Decisions]: Decisions to deploy potentially misaligned AI. {\n  \"instantiations\": [\"deploy\", \"withhold\"],\n  \"priors\": {\"p(deploy)\": \"0.70\", \"p(withhold)\": \"0.30\"},\n  \"posteriors\": {\n    \"p(deploy|strong_incentives,deception)\": \"0.90\",\n    \"p(deploy|strong_incentives,no_deception)\": \"0.75\",\n    \"p(deploy|weak_incentives,deception)\": \"0.60\",\n    \"p(deploy|weak_incentives,no_deception)\": \"0.30\"\n  }\n}\nThe probabilities tell a plausible story: deployment becomes more likely with stronger incentives and successful deception, but even without deception, strong incentives create substantial deployment probability.\n\n\n\n5.5.4 3.5.4 Practically Meaningful BayesDown\nThe BayesDown representation achieves something remarkable: it bridges the chasm between Carlsmith’s nuanced prose and mathematical formalism without losing the essence of either.\nConsider what this bridge enables:\nFor Technical Researchers: The formal structure makes assumptions explicit. Is power-seeking really independent of capability level given strategic awareness? The model forces clarity.\nFor Policymakers: Probabilities attached to comprehensible descriptions provide actionable intelligence. “70% chance of deployment despite misalignment” translates better than abstract concerns.\nFor Strategic Analysts: The network structure reveals intervention points. Which nodes, if changed, most affect the final outcome? Where should we focus effort?\nThe hybrid nature—natural language plus formal structure plus probabilities—serves each audience while enabling communication between them. A policymaker can understand “deployment decisions” without probability theory. A researcher can analyze the mathematical model without losing sight of what the variables mean.\nThis isn’t just convenient—it’s essential for coordination. When different communities can refer to the same model but engage with it at their appropriate level of technical detail, we create common ground for productive disagreement and collaborative problem-solving.\n\n\n\n5.5.5 3.5.5 Interactive Visualization and Exploration\nThe moment when Carlsmith’s model first rendered as an interactive network felt like putting on glasses after years of squinting. Suddenly, the complex web of relationships became navigable.\nThe visualization system employs multiple visual channels simultaneously:\nColor Coding: Nodes shift from deep red (low probability) through yellow to bright green (high probability). At a glance, you see which factors Carlsmith considers likely versus speculative.\nBorder Styling: Blue borders mark root causes (like “Incentives_To_Build”), purple indicates intermediate nodes, magenta highlights final outcomes. The visual grammar guides the eye through causal flow.\nLayout Algorithm: Initial placement uses causal depth—root causes at bottom, final outcomes at top. Physics simulation then refines positions to minimize edge crossings while preserving hierarchical structure.\nProgressive Disclosure: Hovering reveals probability summaries. Clicking opens detailed conditional probability tables. Dragging allows custom arrangement. Each interaction level serves different analytical needs.\nThe implementation required careful attention to human factors:\npython\ndef create_interactive_visualization(network_df):\n    \"\"\"Transform formal model into explorable landscape\"\"\"\n    \n    # Initialize with thoughtful defaults\n    net = Network(height=\"720px\", width=\"100%\", directed=True)\n    \n    # Configure physics for clarity not just aesthetics\n    net.force_atlas_2based(\n        gravity=-50,      # Gentle spread\n        spring_length=150,  # Readable spacing\n        spring_strength=0.02  # Soft constraints\n    )\n    \n    # Add nodes with rich metadata\n    for node in nodes:\n        net.add_node(\n            node_id,\n            label=create_simple_label(node),      # \"Deployment\\np=0.70\"\n            title=create_rich_tooltip(node),      # Full probability details\n            color=probability_to_color(node),     # Visual encoding\n            borderWidth=3,                        # Visible borders\n            shape=\"box\"                          # Readable text\n        )\nThe resulting visualization transforms abstract relationships into tangible understanding. Users report “aha” moments when exploring—suddenly seeing how technical factors compound into strategic risks, or identifying previously unnoticed bottlenecks in the causal chain.\n\n\n\n5.5.6 3.5.6 Validation Against Original (From the MTAIR Project)\n\nValidating AMTAIR’s extraction required careful comparison with expert judgment. While comprehensive benchmarking remains future work, preliminary validation efforts provide encouraging signals.\nManual Baseline Creation: Domain experts, including Johannes Meyer and Jelena Meyer, independently extracted ArgDown and BayesDown representations from Carlsmith’s paper. This created ground truth accounting for legitimate interpretive variation—experts might reasonably disagree on some structural choices or probability estimates.\nStructural Comparison: Comparing extracted causal structures revealed high agreement on core relationships. AMTAIR consistently identified the main causal chain from capabilities through deployment to catastrophe. Some variation appeared in handling of auxiliary factors—where one expert might include a minor influence, another might omit it for simplicity.\nProbability Assessment: Probability extraction showed greater variation, reflecting inherent ambiguity in translating qualitative language. When Carlsmith writes “likely,” different readers might reasonably interpret this as 0.7, 0.75, or 0.8. AMTAIR’s extractions fell within the range of expert interpretations, suggesting successful capture of intended meaning even if not identical numbers.\nSemantic Preservation: Most importantly, the formal models preserved the essential insights of Carlsmith’s argument. The critical role of deployment decisions, the compound nature of risk, the importance of technical and strategic factors—all emerged clearly in the extracted representations.\nAn ideal validation protocol would expand this approach:\n\nMultiple expert extractors working independently\nSystematic comparison of structural and quantitative agreement\nAnalysis of where and why extractions diverge\nTesting whether different extractions lead to different policy conclusions\nIterative refinement based on identified failure modes\n\nThe goal isn’t perfect agreement—even human experts disagree. Rather, we seek extractions good enough to support meaningful analysis while acknowledging their limitations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-validation-methodology",
    "href": "chapters/Outlines/final_draft.html#sec-validation-methodology",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.6 3.6 Validation Methodology",
    "text": "5.6 3.6 Validation Methodology\n\nBuilding trust in automated extraction requires more than anecdotal success. We need systematic validation that honestly assesses both capabilities and limitations.\n\n5.6.1 3.6.1 Ground Truth Construction\n\nCreating ground truth for argument extraction poses unique challenges. Unlike named entity recognition or sentiment analysis, argument structure lacks universal standards. What constitutes the “correct” extraction from a complex text?\nAn ideal validation approach would embrace this inherent subjectivity:\nExpert Selection: Recruit 5-10 domain experts with demonstrated expertise in both AI safety and formal modeling. Diversity matters—include technical researchers, policy analysts, and those with mixed backgrounds.\nExtraction Protocol: Provide standardized training on ArgDown/BayesDown syntax while allowing flexibility in interpretation. Experts work independently to avoid anchoring bias, documenting their reasoning process alongside final extractions.\nConsensus Building: Through structured discussion, identify areas of convergence (likely core argument structure) versus legitimate disagreement (interpretive choices, granularity decisions). This distinguishes system errors from inherent ambiguity.\nQuality Metrics: Rather than binary correct/incorrect judgments, assess:\n\nStructural similarity (graph edit distance)\nProbability distribution overlap (KL divergence)\nSemantic preservation (expert ratings)\nDownstream task performance (policy analysis agreement)\n\nThe resulting dataset would capture not a single “truth” but a distribution of reasonable interpretations against which to evaluate automated extraction.\n\n\n\n5.6.2 3.6.2 Evaluation Metrics\n\nEvaluating argument extraction requires metrics that capture multiple dimensions of quality:\nStructural Fidelity:\n\nNode identification: What fraction of expert-identified variables does the system extract?\nEdge accuracy: Are causal relationships preserved?\nHierarchy preservation: Does the system maintain argument levels?\n\nProbability Calibration:\n\nExplicit extraction: When sources state probabilities, how accurately are they captured?\nLinguistic mapping: Do qualitative expressions translate to reasonable probabilities?\nCoherence: Are probability distributions properly normalized?\n\nSemantic Quality:\n\nDescription accuracy: Do extracted descriptions preserve original meaning?\nTerminology preservation: Does the system maintain author’s vocabulary?\nContext retention: Is sufficient information preserved for interpretation?\n\nFunctional Validity:\n\nInference agreement: Do extracted models support similar conclusions?\nSensitivity preservation: Are critical parameters identified as influential?\nPolicy robustness: Do different extractions suggest similar interventions?\n\nThese metrics acknowledge that perfect extraction is neither expected nor necessary. The goal is extraction sufficient for practical use while maintaining transparency about limitations.\n\n\n\n5.6.3 3.6.3 Results Summary\n\nWhile comprehensive validation remains future work, preliminary assessments using the methodology described above would likely reveal several patterns:\nExpected Strengths: Automated extraction should excel at identifying explicit causal claims, preserving hierarchical argument structure, and extracting stated probabilities. The two-stage approach likely improves quality by allowing focused optimization for each task.\nAnticipated Challenges: Implicit reasoning, complex conditionals, and ambiguous quantifiers would pose greater challenges. Coreference resolution across long documents and maintaining consistency in large models would require continued refinement.\nPractical Utility Threshold: Even with imperfect extraction, the system could provide value if it achieves perhaps 70-80% structural accuracy and captures probability estimates within reasonable ranges. This level of performance would enable rapid initial modeling that experts could refine, dramatically reducing the time from argument to formal model.\nThe validation framework itself represents a contribution—establishing systematic methods for assessing argument extraction quality as this research area develops.\n\n\n\n5.6.4 3.6.4 Error Analysis\nUnderstanding failure modes guides both appropriate use and future improvements:\nImplicit Assumptions: Authors often leave critical assumptions unstated, relying on shared background knowledge. When an AI safety researcher writes about “alignment,” they assume readers understand the technical concept. The system must either extract these implicit elements or flag their absence.\nComplex Conditionals: Natural language expresses conditionality in myriad ways. “If we achieve alignment (which seems unlikely without major theoretical breakthroughs), then deployment might be safe (assuming robust verification).” Parsing nested, qualified conditionals challenges current methods.\nAmbiguous Quantifiers: The word “significant” might mean 10% in one context, 60% in another. Without calibration to author-specific usage or domain conventions, probability extraction remains approximate.\nCoreference Challenges: Academic writing loves pronouns and indirect references. When “this approach” appears three paragraphs after introducing multiple approaches, identifying the correct referent requires sophisticated discourse understanding.\nThese limitations don’t invalidate the approach but rather define its boundaries. Users who understand these constraints can work within them, leveraging automation’s strengths while compensating for its weaknesses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-policy-evaluation",
    "href": "chapters/Outlines/final_draft.html#sec-policy-evaluation",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.7 3.7 Policy Evaluation Capabilities",
    "text": "5.7 3.7 Policy Evaluation Capabilities\n\nThe ultimate test of a model isn’t its elegance but its utility. Can AMTAIR’s extracted models actually inform governance decisions? This section demonstrates how formal models enable systematic policy analysis.\n\n5.7.1 3.7.1 Intervention Representation\n\nRepresenting policy interventions in Bayesian networks requires translating governance mechanisms into parameter modifications. Pearl’s do-calculus provides the mathematical framework, but the practical challenge lies in meaningful translation.\nAn ideal implementation would support several intervention types:\nParameter Modification: Policies often change probabilities. Safety requirements might reduce P(deployment|misaligned) from 0.7 to 0.2 by making unsafe deployment legally prohibited or reputationally costly.\nStructural Interventions: Some policies add new causal pathways. Introducing mandatory review boards creates new nodes and edges representing oversight mechanisms.\nUncertainty Modeling: Policy effectiveness is itself uncertain. Rather than assuming perfect implementation, represent ranges: P(deployment|misaligned) might become [0.1, 0.3] depending on enforcement.\nMulti-Level Effects: Policies influence multiple levels simultaneously. Compute governance affects technical development, corporate behavior, and international competition.\nThe system would translate high-level policy descriptions into specific network modifications, enabling rigorous counterfactual analysis of intervention effects.\n\n\n\n5.7.2 3.7.2 Example: Deployment Governance\nLet’s trace how a specific policy—mandatory safety certification before deployment—might be evaluated:\nBaseline Model: In Carlsmith’s original model, P(deployment|misaligned) = 0.7, reflecting competitive pressures overwhelming safety concerns.\nPolicy Specification: Safety certification requires demonstrating alignment properties before deployment authorization. Based on similar regulations in other domains, we might estimate 80-90% effectiveness.\nParameter Update: The modified model sets P(deployment|misaligned) = 0.1-0.2, representing the residual probability of circumvention or regulatory capture.\nDownstream Effects:\n\nReduced deployment of misaligned systems\nLower probability of power-seeking manifestation\nDecreased existential risk from ~5% to ~1.2%\n\nSensitivity Analysis: How robust is this conclusion? Varying certification effectiveness, enforcement probability, and other parameters reveals which assumptions critically affect the outcome.\nThis example illustrates policy evaluation’s value: moving from vague claims (“regulation would help”) to quantitative assessments (“this specific intervention might reduce risk by 75%±15%”).\n\n\n\n5.7.3 3.7.3 Robustness Analysis\nGood policies work across scenarios. AMTAIR enables testing interventions against multiple worldviews, parameter ranges, and structural variations.\nCross-Model Testing: Extract multiple expert models and evaluate the same policy in each. If an intervention reduces risk in Carlsmith’s model but increases it in Christiano’s, we’ve identified a critical dependency.\nParameter Sensitivity: Which uncertainties most affect policy effectiveness? If the intervention only works for P(alignment_difficulty) &lt; 0.3, and experts disagree whether it’s 0.2 or 0.4, we need more research before implementing.\nStructural Uncertainty: Some disagreements concern model structure itself. Does capability advancement directly influence misalignment risk, or only indirectly through deployment pressures? Test policies under both structures.\nConfidence Bounds: Rather than point estimates, compute ranges. “This policy reduces risk by 40-80%” honestly represents uncertainty while still providing actionable guidance.\nThe goal isn’t eliminating uncertainty but making decisions despite it. Robustness analysis reveals which policies work across uncertainties versus those requiring specific assumptions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-visualization-design",
    "href": "chapters/Outlines/final_draft.html#sec-visualization-design",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.8 3.8 Interactive Visualization Design",
    "text": "5.8 3.8 Interactive Visualization Design\n\nA Bayesian network without good visualization is like a symphony without performers—all potential, no impact. The visualization system transforms mathematical abstractions into intuitive understanding.\n\n5.8.1 3.8.1 Visual Encoding Strategy\nEvery visual element carries information:\nColor: The probability spectrum from red (low) through yellow to green (high) provides immediate gestalt understanding. Pre-attentive processing—the brain’s ability to process certain visual features without conscious attention—makes patterns jump out.\nBorders: Node type encoding (blue=root, purple=intermediate, magenta=outcome) creates visual flow. The eye naturally follows from blue through purple to magenta, tracing causal pathways.\nSize: Larger nodes have higher centrality—more connections, more influence. This emerges from the physics simulation but reinforces importance.\nLayout: Force-directed positioning naturally clusters related concepts while maintaining readability. The algorithm balances competing constraints: minimize edge crossings, maintain hierarchical levels, avoid node overlap, and create aesthetic appeal.\nThe encoding philosophy: every pixel should earn its place by conveying information while maintaining visual harmony.\n\n\n\n5.8.2 3.8.2 Progressive Disclosure\nInformation overload kills understanding. The interface reveals complexity gradually:\nLevel 1 - Overview: At first glance, see network structure and probability color coding. This answers: “What’s the shape of the argument? Where are the high-risk areas?”\nLevel 2 - Hover Details: Mouse over a node to see its description and prior probability. This adds: “What does this factor represent? How likely is it?”\nLevel 3 - Click Deep Dive: Clicking opens full probability tables and relationships. This reveals: “How does this probability change with conditions? What influences this factor?”\nLevel 4 - Interactive Exploration: Dragging, zooming, and physics controls enable custom investigation. This supports: “What if I reorganize to see different patterns? How do these clusters relate?”\nEach level serves different users and use cases. A policymaker might work primarily with levels 1-2, while a researcher dives into level 3-4 details.\n\n\n\n5.8.3 3.8.3 User Interface Elements\n\nEffective interface design for Bayesian networks requires balancing power with accessibility:\nPhysics Controls: Force-directed layouts benefit from tuning. Gravity affects spread, spring length controls spacing, damping influences settling time. Advanced users can adjust these for optimal layouts, while defaults work well for most cases.\nFilter Options: With large networks, selective viewing becomes essential. Filter by probability ranges (show only likely events), node types (focus on interventions), or causal depth (see only immediate effects).\nExport Functions: Different stakeholders need different formats. Researchers want raw data, policymakers need reports, presenters require images. Supporting diverse export formats enables broad usage.\nComparison Mode: Understanding often comes from contrast. Side-by-side viewing of baseline versus intervention, or different expert models, reveals critical differences.\nIterative design with actual users would refine these features, ensuring they serve real needs rather than imagined ones.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-market-integration",
    "href": "chapters/Outlines/final_draft.html#sec-market-integration",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.9 3.9 Integration with Prediction Markets",
    "text": "5.9 3.9 Integration with Prediction Markets\n\nThe vision: formal models that breathe with live data, updating as collective intelligence evolves. While full implementation awaits, the architecture anticipates this future.\n\n5.9.1 3.9.1 Design for Integration\nIntegration Architecture requires careful design to manage the impedance mismatch between formal models and market data:\nAPI Specifications: Each platform—Metaculus, Manifold, Good Judgment Open—has unique data formats, update frequencies, and question types. A unified adapter layer would translate platform-specific formats into model-compatible data.\nSemantic Matching: The hard problem—connecting “AI causes extinction by 2100” (market question) to “Existential_Catastrophe” (model node). This requires sophisticated NLP and possibly human curation for high-stakes connections.\nAggregation Methods: When multiple markets address similar questions, how do we combine? Weighted averages based on market depth, participant quality, and historical accuracy provide more signal than simple means.\nUpdate Scheduling: Real-time updates would overwhelm users and computation. Smart scheduling might update daily for slow-changing strategic questions, hourly for capability announcements, immediately for critical events.\n\n\n\n5.9.2 3.9.2 Challenges and Opportunities\nThe challenges are real but surmountable:\nQuestion Mapping: Markets ask specific, time-bound questions while models represent general relationships. “AGI by 2030?” maps uncertainly to “APS_Systems exists.” Developing robust mapping functions requires deep understanding of both domains.\nTemporal Alignment: Market probabilities change over time, but model parameters are typically static. Should we use current market values, time-weighted averages, or attempt to extract trend information?\nQuality Variation: A liquid market with expert participants provides different information than a thin market with casual forecasters. Weighting schemes must account for these quality differences.\nIncentive Effects: If models influence policy and policy influences outcomes, and markets forecast outcomes, we create feedback loops. Understanding these dynamics prevents perverse incentives.\nDespite challenges, even partial integration provides value:\n\nExternal validation of expert-derived probabilities\nDynamic updating as new information emerges\nIdentification of where model and market disagree\nQuantified uncertainty from market spread\n\nThe perfect shouldn’t be the enemy of the good—simple integration beats no integration.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-computational-performance",
    "href": "chapters/Outlines/final_draft.html#sec-computational-performance",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.10 3.10 Computational Performance Analysis",
    "text": "5.10 3.10 Computational Performance Analysis\n\nAs networks grow from toy examples to real-world complexity, computational challenges emerge. Understanding these constraints shapes realistic expectations and optimization priorities.\n\n5.10.1 3.10.1 Exact vs. Approximate Inference\nThe fundamental tradeoff in probabilistic reasoning: exactness versus tractability.\nExact Inference: Variable elimination and junction tree algorithms provide mathematically exact answers. For our 3-node rain-sprinkler network, calculations complete instantly. For 20-node networks with modest connectivity, expect seconds. But for 50+ node networks with complex dependencies, exact inference becomes impractical—potentially taking hours or exhausting memory.\nApproximate Methods: When exactness becomes impractical, approximation saves the day:\n\nMonte Carlo Sampling: Generate thousands of scenarios consistent with the network, estimate probabilities from frequencies. Accuracy improves with samples, trading computation time for precision.\nVariational Inference: Find the simplest distribution that approximates our complex reality. Like fitting a smooth curve to jagged data—we lose detail but gain comprehension.\nBelief Propagation: Pass messages between nodes until beliefs converge. Works beautifully for tree-structured networks, can oscillate or converge slowly for complex loops.\n\nThe system selects methods based on network properties:\n\nSmall networks: exact inference for precision\nMedium networks: belief propagation for speed\nLarge networks: sampling for scalability\nVery large networks: hierarchical decomposition\n\n\n\n\n5.10.2 3.10.2 Scaling Strategies\nWhen networks grow beyond convenient computation, clever strategies maintain usability:\nHierarchical Decomposition: Break large networks into smaller, manageable subnetworks. Compute locally, then integrate results. Like solving a jigsaw puzzle by completing sections before assembling the whole.\nRelevance Pruning: For specific queries, most nodes don’t matter. If asking about deployment risk, technical details about interpretability methods might be temporarily ignorable. Prune irrelevant subgraphs for focused analysis.\nCaching Architecture: Many queries repeat—P(catastrophe), P(deployment|misalignment). Cache results to avoid recomputation. Smart invalidation updates only affected queries when parameters change.\nParallel Processing: Inference calculations often decompose naturally. Different branches of the network can be processed simultaneously. Modern multi-core processors and cloud computing make this increasingly attractive.\nImplementation would balance these strategies based on usage patterns. Interactive exploration benefits from caching and pruning. Batch analysis leverages parallelization. The architecture accommodates multiple approaches.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-results-achievements",
    "href": "chapters/Outlines/final_draft.html#sec-results-achievements",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.11 3.11 Results and Achievements",
    "text": "5.11 3.11 Results and Achievements\n\n\n5.11.1 3.11.1 Extraction Quality Assessment\n\nAssessing extraction quality requires honesty about both achievements and limitations. An ideal evaluation would examine multiple dimensions:\nCoverage: What proportion of arguments in source texts does the system successfully capture? Initial applications suggest the two-stage approach identifies most explicit causal claims while struggling with deeply implicit relationships.\nAccuracy: How closely do automated extractions match expert consensus? Preliminary comparisons indicate strong agreement on primary causal structures with more variation in probability estimates.\nRobustness: How well does the system handle different writing styles, argument structures, and domains? Academic papers with clear argumentation extract more reliably than informal blog posts or policy documents.\nUtility: Do the extracted models enable meaningful analysis? Even imperfect extractions that capture 80% of structure with approximate probabilities can dramatically accelerate modeling compared to starting from scratch.\nThe key insight: perfect extraction isn’t necessary for practical value. Like machine translation, which provides useful results despite imperfections, automated argument extraction can enhance human capability without replacing human judgment.\n\n\n\n5.11.2 3.11.2 Computational Performance\n\nPerformance analysis would reveal the practical boundaries of the current system:\nExtraction Speed: LLM-based extraction scales roughly linearly with document length. A 20-page paper might require 30-60 seconds for structural extraction and similar time for probability extraction. This enables processing dozens of documents daily—orders of magnitude faster than manual approaches.\nNetwork Complexity Limits: Exact inference remains tractable for networks up to approximately 30-40 nodes with moderate connectivity. Beyond this, approximate methods become necessary, with sampling methods scaling to hundreds of nodes at the cost of precision.\nVisualization Responsiveness: Interactive visualization performs smoothly for networks under 50 nodes. Larger networks benefit from hierarchical viewing or focus+context techniques to maintain usability.\nEnd-to-End Pipeline: From document input to interactive visualization, expect 2-5 minutes for typical AI safety arguments. This represents roughly 100x speedup compared to manual modeling efforts.\nThese performance characteristics make AMTAIR practical for real-world use while highlighting areas for future optimization.\n\n\n\n5.11.3 3.11.3 Policy Impact Evaluation\n\nThe true test of AMTAIR lies in its ability to inform governance decisions. An ideal policy evaluation framework would demonstrate several capabilities:\nIntervention Modeling: Representing diverse policy proposals—from technical standards to international agreements—as parameter modifications in extracted networks. This translation from qualitative proposals to quantitative changes enables rigorous analysis.\nComparative Assessment: Evaluating multiple interventions across different expert worldviews to identify robust strategies. Policies that reduce risk across different models deserve priority over those requiring specific assumptions.\nSensitivity Analysis: Understanding which uncertainties most affect policy conclusions. If an intervention’s effectiveness depends critically on disputed parameters, this highlights research priorities.\nImplementation Guidance: Moving beyond “this policy reduces risk” to specific recommendations about design details, implementation sequences, and success metrics.\nThe system would transform abstract policy discussions into concrete quantitative analyses, enabling evidence-based decision-making in AI governance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-technical-summary",
    "href": "chapters/Outlines/final_draft.html#sec-technical-summary",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "5.12 3.12 Summary of Technical Contributions",
    "text": "5.12 3.12 Summary of Technical Contributions\nLooking back at the implementation journey, several achievements stand out:\nAutomated Extraction: The two-stage pipeline successfully transforms natural language arguments into formal models, achieving practical accuracy while maintaining transparency about limitations.\nHybrid Representation: BayesDown bridges qualitative and quantitative worlds, preserving semantic richness while enabling mathematical analysis.\nScalable Architecture: Modular design accommodates growth—new document types, improved extraction methods, additional visualization options—without fundamental restructuring.\nInteractive Accessibility: Thoughtful visualization makes complex models understandable to diverse stakeholders, democratizing access to formal reasoning tools.\nPolicy Relevance: The ability to model interventions and assess robustness transforms academic exercises into practical governance tools.\nThese technical achievements validate the feasibility of computational coordination infrastructure for AI governance. Not as a complete solution, but as a meaningful enhancement to human judgment and collaboration.\nThe implementation demonstrates that the vision of automated argument extraction is not merely theoretical but practically achievable. While challenges remain—particularly in handling implicit reasoning and diverse uncertainty expressions—the system provides a foundation for enhanced coordination in AI governance.\nThe journey from concept to implementation revealed unexpected insights. The two-stage extraction process, initially a pragmatic choice, proved cognitively valid. The intermediate representations became valuable outputs themselves. The visualization challenges led to design innovations applicable beyond this project.\nMost importantly, the implementation confirms that formal modeling of AI risk arguments need not remain the province of a few dedicated experts. Through automation and thoughtful design, these powerful tools can serve the broader community working to ensure advanced AI benefits humanity.\nHaving demonstrated technical feasibility and practical utility, we must now critically examine limitations, address objections, and explore broader implications. The next chapter undertakes this essential reflection, ensuring we neither oversell the approach nor undervalue its contributions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-technical-limitations",
    "href": "chapters/Outlines/final_draft.html#sec-technical-limitations",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "6.1 4.1 Technical Limitations and Responses",
    "text": "6.1 4.1 Technical Limitations and Responses\n\n6.1.1 4.1.1 Objection 1: Extraction Quality Boundaries\nCritic: “Complex implicit reasoning chains resist formalization; automated extraction will systematically miss nuanced arguments and subtle conditional relationships that human experts would identify.”\nResponse: This concern has merit—extraction does face inherent limitations. However, the empirical results tell a more nuanced story. The two-stage extraction process, while imperfect, captures sufficient structure for practical use while maintaining transparency about its limitations.\nMore importantly, AMTAIR employs a hybrid human-AI workflow that addresses this limitation:\n\nTwo-stage verification: Humans review structural extraction before probability quantification\nTransparent outputs: All intermediate representations remain human-readable\nIterative refinement: Extraction prompts improve based on error analysis\nEnsemble approaches: Multiple extraction attempts can identify ambiguities\n\nThe question is not whether automated extraction perfectly captures every nuance—it doesn’t. Rather, it’s whether imperfect extraction still provides value over no formal representation. When the alternative is relying on conflicting mental models that remain entirely implicit, even partially accurate formal models represent significant progress.\nFurthermore, extraction errors often reveal interesting properties of the source arguments themselves—ambiguities that human readers gloss over become explicit when formalization fails. This diagnostic value enhances rather than undermines the approach.\n\n\n6.1.2 4.1.2 Objection 2: False Precision in Uncertainty\nCritic: “Attaching exact probabilities to unprecedented events like AI catastrophe is fundamentally misguided. The numbers create false confidence in what amounts to educated speculation about radically uncertain futures.”\nResponse: This philosophical objection strikes at the heart of formal risk assessment. However, AMTAIR addresses it through several design choices:\nFirst, the system explicitly represents uncertainty about uncertainty. Rather than point estimates, the framework supports probability distributions over parameters. When someone says “likely” we might model this as a range rather than exactly 0.8, capturing both the central estimate and our uncertainty about it.\nSecond, all probabilities are explicitly conditional on stated assumptions. The system doesn’t claim “P(catastrophe) = 0.05” absolutely, but rather “Given Carlsmith’s model assumptions, P(catastrophe) = 0.05.” This conditionality is preserved throughout analysis.\nThird, sensitivity analysis reveals which probabilities actually matter. Often, precise values are unnecessary—knowing whether a parameter is closer to 0.1 or 0.9 suffices for decision-making. The formalization helps identify where precision matters and where it doesn’t.\nFinally, the alternative to quantification isn’t avoiding the problem but making it worse. When experts say “highly likely” or “significant risk,” they implicitly reason with probabilities. Formalization simply makes these implicit quantities explicit and subject to scrutiny. As Dennis Lindley noted, “Uncertainty is not in the events, but in our knowledge about them.”\n\n\n\n6.1.3 4.1.3 Objection 3: Correlation Complexity\nCritic: “Bayesian networks assume conditional independence given parents, but real-world AI risks involve complex correlations. Ignoring these dependencies could dramatically misrepresent risk levels.”\nResponse: Standard Bayesian networks do face limitations with correlation representation—this is a genuine technical challenge. However, several approaches within the framework address this:\nExplicit correlation nodes: When factors share hidden common causes, we can add latent variables to capture correlations. For instance, “AI research culture” might influence both “capability advancement” and “safety investment.”\nCopula methods: For known correlation structures, copula functions can model dependencies while preserving marginal distributions. This extends standard Bayesian networks significantly.5\nSensitivity bounds: When correlations remain uncertain, we can compute bounds on outcomes under different correlation assumptions. This reveals when correlations critically affect conclusions.\nModel ensembles: Different correlation structures can be modeled separately and results aggregated, similar to climate modeling approaches.\nMore fundamentally, the question is whether imperfect independence assumptions invalidate the approach. In practice, explicitly modeling first-order effects with known limitations often proves more valuable than attempting to capture all dependencies informally. The framework makes assumptions transparent, enabling targeted improvements where correlations matter most.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-conceptual-concerns",
    "href": "chapters/Outlines/final_draft.html#sec-conceptual-concerns",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "6.2 4.2 Conceptual and Methodological Concerns",
    "text": "6.2 4.2 Conceptual and Methodological Concerns\n\n6.2.1 4.2.1 Objection 4: Democratic Exclusion\nCritic: “Transforming policy debates into complex graphs and equations will sideline non-technical stakeholders, concentrating influence among those comfortable with formal models. This technocratic approach undermines democratic participation in crucial decisions about humanity’s future.”\nResponse: This concern about technocratic exclusion deserves serious consideration—formal methods can indeed create barriers. However, AMTAIR’s design explicitly prioritizes accessibility alongside rigor:\nProgressive disclosure interfaces allow engagement at multiple levels. A policymaker might explore visual network structures and probability color-coding without engaging mathematical details. Interactive features let users modify assumptions and see consequences without understanding implementation.\nNatural language preservation ensures original arguments remain accessible. The BayesDown format maintains human-readable descriptions alongside formal specifications. Users can always trace from mathematical representations back to source texts.\nComparative advantage comes from making implicit technical content explicit, not adding complexity. When experts debate AI risk, they already employ sophisticated probabilistic reasoning—formalization reveals rather than creates this complexity. Making hidden assumptions visible arguably enhances rather than reduces democratic participation.\nMultiple interfaces serve different communities. Researchers access full technical depth, policymakers use summary dashboards, public stakeholders explore interactive visualizations. The same underlying model supports varied engagement modes.\nRather than excluding non-technical stakeholders, proper implementation can democratize access to expert reasoning by making it inspectable and modifiable. The risk lies not in formalization itself but in poor interface design or gatekeeping behaviors around model access.\n\n\n6.2.2 4.2.2 Objection 5: Oversimplification of Complex Systems\nCritic: “Forcing rich socio-technical systems into discrete Bayesian networks necessarily loses crucial dynamics—feedback loops, emergent properties, institutional responses, and cultural factors that shape AI development. The models become precise but wrong.”\nResponse: All models simplify by necessity—as Box noted, “All models are wrong, but some are useful.” The question becomes whether formal simplifications improve upon informal mental models:\nTransparent limitations make formal models’ shortcomings explicit. Unlike mental models where simplifications remain hidden, network representations clearly show what is and isn’t included. This transparency enables targeted criticism and improvement.\nIterative refinement allows models to grow more sophisticated over time. Starting with first-order effects and adding complexity where it proves important follows successful practice in other domains. Climate models began simply and added dynamics as computational power and understanding grew.\nComplementary tools address different aspects of the system. Bayesian networks excel at probabilistic reasoning and intervention analysis. Other approaches—agent-based models, system dynamics, scenario planning—can capture different properties. AMTAIR provides one lens, not the only lens.\nEmpirical adequacy ultimately judges models. If simplified representations enable better predictions and decisions than informal alternatives, their abstractions are justified. Early results suggest formal models, despite simplifications, outperform intuitive reasoning for complex risk assessment.\nThe goal isn’t creating perfect representations but useful ones. By making simplifications explicit and modifiable, formal models enable systematic improvement in ways mental models cannot.\n\n\n6.2.3 4.2.3 Objection 6: Idiosyncratic Implementation and Modeling Choices\nCritic: “The specific choices made in AMTAIR’s implementation—from prompt design to parsing algorithms to visualization strategies—seem arbitrary. Different teams might make entirely different choices, leading to incompatible results. How can we trust conclusions that depend so heavily on implementation details?”\nResponse: This concern about implementation dependency is valid and deserves careful consideration. However, several factors mitigate this issue:\nConvergent Design Principles: While specific implementations vary, fundamental design principles tend to converge. The two-stage extraction process (structure then probability) emerges naturally from how humans parse arguments. The use of intermediate representations follows established practice in computational linguistics. These aren’t arbitrary choices but responses to inherent challenges.\nEmpirical Validation: The “correctness” of implementation choices isn’t philosophical but empirical. If different reasonable implementations extract similar structures and lead to similar policy conclusions, this demonstrates robustness. If they diverge dramatically, this reveals genuine ambiguity in source materials—itself valuable information.\nTransparent Methodology: By documenting all implementation choices and making code open source, AMTAIR enables replication and variation. Other teams can modify specific components while preserving overall architecture, testing which choices matter.\nConvergence at Higher Levels: Even if implementations differ in details, they may converge at levels that matter for coordination. If two systems extract slightly different network structures but reach similar conclusions about policy robustness, the implementation differences don’t undermine the approach’s value.\nCommunity Standards: As the field matures, community standards will likely emerge—not enforcing uniformity but establishing interoperability. This parallels development in other technical fields where multiple implementations coexist within shared frameworks.\nThe deeper insight is that implementation choices encode theoretical commitments. By making these explicit and variable, AMTAIR turns a bug into a feature—we can systematically explore how different assumptions affect conclusions, enhancing rather than undermining epistemic security.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-red-teaming",
    "href": "chapters/Outlines/final_draft.html#sec-red-teaming",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "6.3 4.3 Red-Teaming Results",
    "text": "6.3 4.3 Red-Teaming Results\n\nTo identify failure modes, systematic adversarial testing of the AMTAIR system would be essential.\n\n6.3.1 4.3.1 Adversarial Extraction Attempts\n\nA comprehensive red-teaming approach would test the system with:\nContradictory Arguments: Texts containing logically inconsistent claims or probability estimates. The system should flag contradictions rather than silently reconciling them.\nCircular Reasoning: Arguments with circular dependencies that violate DAG requirements. Proper validation should detect and report such structural issues.\nAmbiguous Language: Texts using extremely vague or metaphorical language. The system should acknowledge extraction uncertainty rather than forcing precise interpretations.\nDeceptive Framings: Arguments crafted to imply false causal relationships. This tests whether the system merely extracts surface claims or requires deeper coherence.\nAdversarial Prompts: Inputs designed to trigger known LLM failure modes. This ensures robustness against prompt injection and manipulation attempts.\nEach failure mode discovered would inform system improvements and user guidance.\n\n\n6.3.2 4.3.2 Robustness Findings\nTheoretical analysis suggests key vulnerabilities:\nAnchoring Effects: Language models may over-weight information presented early in documents, potentially biasing extraction toward initial framings.\nAuthority Sensitivity: Extraction might be influenced by explicit credibility signals in text, potentially giving undue weight to claimed expertise.\nComplexity Limits: Performance likely degrades with very large argument structures, requiring hierarchical decomposition strategies.\nContext Windows: Long-range dependencies exceeding model context windows could be missed, fragmenting cohesive arguments.\nUnderstanding these limitations enables appropriate use—leveraging strengths while compensating for weaknesses through human oversight and validation.\n\n\n6.3.3 4.3.3 Implications for Deployment\nThese considerations suggest AMTAIR is suitable for:\n\nResearch applications with expert oversight\nPolicy analysis of well-structured arguments\nEducational uses demonstrating formal reasoning\nCollaborative modeling with human verification\n\nBut should be used cautiously for:\n\nFully automated analysis without review\nAdversarial or politically contentious texts\nReal-time decision-making without validation\nArguments far outside training distribution",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-epistemic-security",
    "href": "chapters/Outlines/final_draft.html#sec-epistemic-security",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "6.4 4.4 Enhancing Epistemic Security",
    "text": "6.4 4.4 Enhancing Epistemic Security\n\nDespite limitations, AMTAIR contributes to epistemic security in AI governance through several mechanisms.\n\n6.4.1 4.4.1 Making Models Inspectable\nThe greatest epistemic benefit comes from forcing implicit models into explicit form. When an expert claims “misalignment likely leads to catastrophe,” formalization asks:\n\nLikely means what probability?\nThrough what causal pathways?\nUnder what assumptions?\nWith what evidence?\n\nThis explicitation serves multiple functions:\nClarity: Vague statements become precise claims subject to evaluation\nComparability: Different experts’ models can be systematically compared\nCriticizability: Hidden assumptions become visible targets for challenge\nUpdatability: Formal models can systematically incorporate new evidence\n\n\n6.4.2 4.4.2 Revealing Convergence and Divergence\n\nTheoretical analysis suggests formal comparison would reveal:\nStructural Patterns: Experts likely share more agreement about causal structures than probability values, suggesting common understanding of mechanisms despite quantitative disagreement.\nCrux Identification: Formal models make explicit which specific disagreements drive different conclusions, focusing discussion on genuinely critical differences.\nHidden Agreements: Apparently conflicting positions might share substantial common ground obscured by different terminology or emphasis.\nUncertainty Clustering: Areas of high uncertainty likely correlate across models, revealing where additional research would most reduce disagreement.\nThese patterns remain invisible in natural language debates but become analyzable through formalization.\n\n\n6.4.3 4.4.3 Improving Collective Reasoning\nAMTAIR enhances group epistemics through:\nExplicit uncertainty: Replacing “might,” “could,” “likely” with probability distributions reduces miscommunication and forces precision\nCompositional reasoning: Complex arguments decompose into manageable components that can be independently evaluated\nEvidence integration: New information updates specific parameters rather than requiring complete argument reconstruction\nExploration tools: Stakeholders can modify assumptions and immediately see consequences, building intuition about model dynamics\n\nWhile empirical validation remains future work, theoretical considerations suggest these mechanisms could substantially improve coordination quality. By providing shared representations and systematic methods for managing disagreement, formal models create infrastructure for collective intelligence that transcends individual limitations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-scaling",
    "href": "chapters/Outlines/final_draft.html#sec-scaling",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "6.5 4.5 Scaling Challenges and Opportunities",
    "text": "6.5 4.5 Scaling Challenges and Opportunities\n\nMoving from prototype to widespread adoption faces both technical and social challenges.\n\n6.5.1 4.5.1 Technical Scaling\nComputational complexity grows with network size, but several approaches help:\n\nHierarchical decomposition for very large models\nCaching and approximation for common queries\nDistributed processing for extraction tasks\nIncremental updating rather than full recomputation\n\nData quality varies dramatically across sources:\n\nAcademic papers provide structured arguments\nBlog posts offer rich ideas with less formal structure\nPolicy documents mix normative and empirical claims\nSocial media presents extreme extraction challenges\n\nIntegration complexity increases with ecosystem growth:\n\nMultiple LLM providers with different capabilities\nDiverse visualization needs across users\nVarious export formats for downstream tools\nVersion control for evolving models\n\n\n\n6.5.2 4.5.2 Social and Institutional Scaling\nAdoption barriers include:\n\nLearning curve for formal methods\nInstitutional inertia in established processes\nConcerns about replacing human judgment\nResource requirements for implementation\n\nTrust building requires:\n\nTransparent methodology documentation\nPublished validation studies\nHigh-profile successful applications\nCommunity ownership and development\n\nSustainability depends on:\n\nOpen source development model\nDiverse funding sources\nAcademic and industry partnerships\nClear value demonstration\n\n\n\n6.5.3 4.5.3 Opportunities for Impact\nDespite challenges, several factors favor adoption:\nTiming: AI governance needs tools now, creating receptive audiences\nComplementarity: AMTAIR enhances rather than replaces existing processes\nFlexibility: The approach adapts to different contexts and needs\nNetwork effects: Value increases as more perspectives are formalized\nEarly adopters in research organizations and think tanks can demonstrate value, creating momentum for broader adoption.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-governance-integration",
    "href": "chapters/Outlines/final_draft.html#sec-governance-integration",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "6.6 4.6 Integration with Governance Frameworks",
    "text": "6.6 4.6 Integration with Governance Frameworks\n\nAMTAIR complements rather than replaces existing governance approaches.\n\n6.6.1 4.6.1 Standards Development\nTechnical standards bodies could use AMTAIR to:\n\nModel how proposed standards affect risk pathways\nCompare different standard options systematically\nIdentify unintended consequences through pathway analysis\nBuild consensus through explicit model negotiation\n\nExample: Evaluating compute thresholds for AI system regulation by modeling how different thresholds affect capability development, safety investment, and competitive dynamics.\n\n\n6.6.2 4.6.2 Regulatory Design\nRegulators could apply the framework to:\n\nAssess regulatory impact across different scenarios\nIdentify enforcement challenges through explicit modeling\nCompare international approaches systematically\nDesign adaptive regulations responsive to evidence\n\nExample: Analyzing how liability frameworks affect corporate AI development decisions under different market conditions.\n\nThe extensive literature on corporate governance and liability frameworks Cuomo, Mallin, and Zattoni (2016) Demirag, Sudarsanam, and WRIGHT (2000) De Villiers and Dimes (2021) Di Vito and Trottier (2022) Kaur (2024) List and Pettit (2011) Solomon (2020) provides theoretical grounding for understanding how regulatory interventions shape organizational behavior. AMTAIR could formalize these relationships in the specific context of AI development, making explicit how different liability regimes might incentivize or discourage safety investments.\n\n\n6.6.3 4.6.3 International Coordination\nMultilateral bodies could leverage shared models for:\n\nEstablishing common risk assessments\nNegotiating agreements with explicit assumptions\nMonitoring compliance through parameter tracking\nAdapting agreements as evidence emerges\n\nExample: Building shared models for AGI development scenarios to inform international AI governance treaties.\n\n\n6.6.4 4.6.4 Organizational Decision-Making\nIndividual organizations could use AMTAIR for:\n\nInternal risk assessment and planning\nBoard-level communication about AI strategies\nResearch prioritization based on model sensitivity\nSafety case development with explicit assumptions\n\nExample: An AI lab modeling how different safety investments affect both capability advancement and risk mitigation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-future-research",
    "href": "chapters/Outlines/final_draft.html#sec-future-research",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "6.7 4.7 Future Research Directions",
    "text": "6.7 4.7 Future Research Directions\n\nSeveral research directions could enhance AMTAIR’s capabilities and impact.\n\n6.7.1 4.7.1 Technical Enhancements\nImproved extraction: Fine-tuning language models specifically for argument extraction, handling implicit reasoning, and cross-document synthesis\nRicher representations: Temporal dynamics, continuous variables, and multi-agent interactions within extended frameworks\nInference advances: Quantum computing applications, neural approximate inference, and hybrid symbolic-neural methods\nValidation methods: Automated consistency checking, anomaly detection in extracted models, and benchmark dataset development\n\n\n6.7.2 4.7.2 Methodological Extensions\nCausal discovery: Inferring causal structures from data rather than just extracting from text\nExperimental integration: Connecting models to empirical results from AI safety experiments\nDynamic updating: Continuous model refinement as new evidence emerges from research and deployment\nUncertainty quantification: Richer representation of deep uncertainty and model confidence\n\nRecent advances in causal structure learning from both text and data Babakov et al. (2025) Ban et al. (2023) Bethard (2007) Chen et al. (2023) Heinze-Deml, Maathuis, and Meinshausen (2018) Squires and Uhler (2023) Yang, Han, and Poon (2022) suggest promising directions for enhancing AMTAIR’s extraction capabilities. The theoretical foundations from Duhem (1954) and Meyer (2022) on the philosophy of science and knowledge structures provide epistemological grounding for these methodological extensions.\n\n\n6.7.3 4.7.3 Application Domains\nBeyond AI safety: Climate risk, biosecurity, nuclear policy, and other existential risks\nCorporate governance: Strategic planning, risk management, and innovation assessment\nScientific modeling: Formalizing theoretical arguments in emerging fields\nEducational tools: Teaching probabilistic reasoning and critical thinking\n\n\n6.7.4 4.7.4 Ecosystem Development\nOpen standards: Common formats for model exchange and tool interoperability\nCommunity platforms: Collaborative model development and sharing infrastructure\nTraining programs: Building capacity for formal modeling in governance communities\nQuality assurance: Certification processes for high-stakes model applications\nThese directions could transform AMTAIR from a single tool into a broader ecosystem for enhanced reasoning about complex risks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-deep-uncertainties",
    "href": "chapters/Outlines/final_draft.html#sec-deep-uncertainties",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "6.8 4.8 Known Unknowns and Deep Uncertainties",
    "text": "6.8 4.8 Known Unknowns and Deep Uncertainties\nWhile AMTAIR enhances reasoning under uncertainty, fundamental limitations remain regarding truly novel developments that might fall outside existing conceptual frameworks.\n\n6.8.1 4.8.1 Categories of Deep Uncertainty\nNovel Capabilities: Future AI developments may operate according to principles outside current scientific understanding. No amount of careful modeling can anticipate fundamental paradigm shifts in what intelligence can accomplish.\nEmergent Behaviors: Complex system properties that resist prediction from component analysis may dominate outcomes. The interaction between advanced AI systems and human society could produce wholly unexpected dynamics.\nStrategic Interactions: Game-theoretic dynamics with superhuman AI systems exceed human modeling capacity. We cannot reliably predict how entities smarter than us will behave strategically.\nSocial Transformation: Unprecedented social and economic changes may invalidate current institutional assumptions. Our models assume continuity in basic social structures that AI might fundamentally alter.\n\n\n6.8.2 4.8.2 Adaptation Strategies for Deep Uncertainty\nRather than pretending to model the unmodelable, AMTAIR incorporates several strategies:\nModel Architecture Flexibility: The modular structure enables rapid incorporation of new variables as novel factors become apparent. When surprises occur, models can be updated rather than discarded.\nExplicit Uncertainty Tracking: Confidence levels for each model component make clear where knowledge is solid versus speculative. This prevents false confidence in highly uncertain domains.\nScenario Branching: Multiple model variants capture different assumptions about fundamental uncertainties. Rather than committing to one worldview, the system maintains portfolios of possibilities.\nUpdate Mechanisms: Integration with prediction markets and expert assessment enables rapid model revision as new information emerges. Models evolve rather than remaining static.\n\n\n6.8.3 4.8.3 Robust Decision-Making Principles\nGiven deep uncertainty, certain decision principles become paramount:\nOption Value Preservation: Policies should maintain flexibility for future course corrections rather than locking in irreversible choices based on current models.\nPortfolio Diversification: Multiple approaches hedging across different uncertainty sources provide robustness against model error.\nEarly Warning Systems: Monitoring for developments that would invalidate current models enables rapid response when assumptions break down.\nAdaptive Governance: Institutional mechanisms must enable rapid response to new information rather than rigid adherence to plans based on outdated models.\nThe goal is not to eliminate uncertainty but to make good decisions despite it. AMTAIR provides tools for systematic reasoning about what we do know while maintaining appropriate humility about what we don’t and can’t know.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-implications-summary",
    "href": "chapters/Outlines/final_draft.html#sec-implications-summary",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "6.9 4.9 Summary of Implications",
    "text": "6.9 4.9 Summary of Implications\nThe discussion reveals both the promise and limitations of computational approaches to AI governance coordination:\nTechnical Feasibility: Despite imperfections, automated extraction and formal modeling prove practically viable for complex AI risk arguments.\nEpistemic Value: Making implicit models explicit, enabling systematic comparison, and supporting evidence integration enhance collective reasoning.\nPractical Limitations: Extraction boundaries, false precision risks, and implementation dependencies require careful management.\nIntegration Potential: The approach complements rather than replaces existing governance frameworks, adding rigor without sacrificing flexibility.\nFuture Development: Technical enhancements, methodological extensions, and ecosystem growth could amplify impact.\nDeep Uncertainty: Fundamental limits on predicting novel developments require maintaining humility and adaptability.\nThese findings suggest AMTAIR represents a valuable addition to the AI governance toolkit—not a panacea but a meaningful enhancement to our collective capacity for navigating unprecedented challenges.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-key-contributions",
    "href": "chapters/Outlines/final_draft.html#sec-key-contributions",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "7.1 5.1 Summary of Key Contributions",
    "text": "7.1 5.1 Summary of Key Contributions\nThis thesis has demonstrated both the need for and feasibility of computational approaches to enhancing coordination in AI governance. The work makes several distinct contributions across theory, methodology, and implementation.\n\n7.1.1 5.1.1 Theoretical Contributions\nDiagnosis of the Coordination Crisis: I’ve articulated how fragmentation across technical, policy, and strategic communities systematically amplifies existential risk from advanced AI. This framing moves beyond identifying disagreements to understanding how misaligned efforts create negative-sum dynamics—safety gaps emerge between communities, resources are misallocated through duplication and neglect, and interventions interact destructively.\nThe Multiplicative Benefits Framework: The combination of automated extraction, prediction market integration, and formal policy evaluation creates value exceeding the sum of parts. Automation enables scale, markets provide empirical grounding, and policy analysis delivers actionable insights. Together, they address different facets of the coordination challenge while reinforcing each other’s strengths.\nEpistemic Infrastructure Conception: Positioning formal models as epistemic infrastructure reframes the role of technical tools in governance. Rather than replacing human judgment, computational approaches provide common languages, shared representations, and systematic methods for managing disagreement—essential foundations for coordination under uncertainty.\n\n\n7.1.2 5.1.2 Methodological Innovations\nTwo-Stage Extraction Architecture: Separating structural extraction (ArgDown) from probability quantification (BayesDown) addresses key challenges in automated formalization. This modularity enables human oversight at critical points, supports multiple quantification methods, allows for unprecedented transparency and explainability of the entire process, and isolates different types of errors for targeted improvement.\nBayesDown as Bridge Representation: The development of BayesDown syntax creates a crucial intermediate representation preserving both narrative accessibility and mathematical precision. This bridge enables the transformation from qualitative arguments to quantitative models while maintaining traceability and human readability.\nValidation Framework: The systematic approach to validating automated extraction—comparing against expert annotations, measuring multiple accuracy dimensions, and analyzing error patterns—establishes scientific standards for assessing formalization tools. This framework can guide future development in this emerging area.\n\n\n7.1.3 5.1.3 Technical Achievements\nWorking Implementation: AMTAIR demonstrates end-to-end feasibility from document ingestion through interactive visualization. The system successfully processes complex arguments like Carlsmith’s power-seeking AI model, extracting hierarchical structures and probability information.\nScalability Solutions: Technical approaches for handling realistic model complexity—hierarchical decomposition, approximate inference, and progressive visualization—show that computational limitations need not prevent practical application.\nAccessibility Design: The layered interface approach serves diverse stakeholders without compromising technical depth. Progressive disclosure, visual encoding, and interactive exploration make formal models accessible beyond technical specialists.\n\n\n7.1.4 5.1.4 Empirical Findings\nExtraction Feasibility: The successful extraction of complex arguments like Carlsmith’s model validates the core premise that implicit formal structures exist in natural language arguments and can be computationally recovered with reasonable fidelity.\nConvergence Patterns: Theoretical analysis suggests that formal comparison would reveal structural agreements across different expert worldviews even when probability estimates diverge—providing foundations for coordination.\nIntervention Impacts: Policy evaluation capabilities demonstrate how formal models enable rigorous assessment of governance options. The ability to trace intervention effects through complex causal networks validates the practical value of formalization.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-limitations-assessment",
    "href": "chapters/Outlines/final_draft.html#sec-limitations-assessment",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "7.2 5.2 Limitations and Honest Assessment",
    "text": "7.2 5.2 Limitations and Honest Assessment\nDespite these contributions, important limitations constrain current capabilities and should guide appropriate use.\n\n7.2.1 5.2.1 Technical Constraints\nExtraction Boundaries: The system struggles with implicit assumptions, complex conditionals, and ambiguous quantifiers. These limitations necessitate human review for high-stakes applications.\nCorrelation Handling: Standard Bayesian networks inadequately represent complex correlations in real systems. While extensions like copulas and explicit correlation nodes help, fully capturing interdependencies remains challenging.\nComputational Scaling: Very large networks require approximations that may affect accuracy. As models grow to represent richer phenomena, computational constraints increasingly bind.\n\n\n7.2.2 5.2.2 Conceptual Limitations\nFormalization Trade-offs: Converting rich arguments to formal models necessarily loses nuance. While making assumptions explicit provides value, some insights resist mathematical representation.\nProbability Interpretation: Deep uncertainty about unprecedented events challenges probabilistic representation. Numbers can create false precision even when explicitly conditional and uncertain.\nSocial Complexity: Institutional dynamics, cultural factors, and political processes influence AI development in ways that causal models struggle to capture fully.\n\n\n7.2.3 5.2.3 Practical Constraints\nAdoption Barriers: Learning curves, institutional inertia, and resource requirements limit immediate deployment. Even demonstrably valuable tools face implementation challenges.\nMaintenance Burden: Models require updating as arguments evolve and evidence emerges. Without sustained effort, formal representations quickly become outdated.\nContext Dependence: The approach works best for well-structured academic arguments. Application to informal discussions or political rhetoric remains challenging.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-governance-implications",
    "href": "chapters/Outlines/final_draft.html#sec-governance-implications",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "7.3 5.3 Implications for AI Governance",
    "text": "7.3 5.3 Implications for AI Governance\n\nDespite limitations, AMTAIR’s approach offers significant implications for how AI governance can evolve toward greater coordination and effectiveness.\n\n7.3.1 5.3.1 Near-Term Applications\nResearch Coordination: Research organizations can use formal models to:\n\nMap the landscape of current arguments and identify gaps\nPrioritize investigations targeting high-sensitivity parameters\nBuild cumulative knowledge through explicit model updating\nFacilitate collaboration through shared representations\n\nPolicy Development: Governance bodies can apply the framework to:\n\nEvaluate proposals across multiple expert worldviews\nIdentify robust interventions effective under uncertainty\nMake assumptions explicit for democratic scrutiny\nTrack how evidence changes optimal policies over time\n\nStakeholder Communication: The visualization and analysis tools enable:\n\nClearer communication between technical and policy communities\nPublic engagement with complex risk assessments\nBoard-level strategic discussions grounded in formal analysis\nInternational negotiations with explicit shared models\n\n\n\n7.3.2 5.3.2 Medium-Term Transformation\nAs adoption spreads, we might see:\nEpistemic Commons: Shared repositories of formalized arguments become reference points for governance discussions, similar to how economic models inform monetary policy or climate models guide environmental agreements.\nAdaptive Governance: Policies designed with explicit models can include triggers for reassessment as key parameters change, enabling responsive governance that avoids both paralysis and recklessness.\nProfessionalization: “Model curator” and “argument formalization specialist” emerge as recognized roles, building expertise in bridging natural language and formal representations.\nQuality Standards: Community norms develop around model transparency, validation requirements, and appropriate use cases, preventing both dismissal and over-reliance on formal tools.\n\n\n7.3.3 5.3.3 Long-Term Vision\nSuccessfully scaling this approach could fundamentally alter AI governance:\nCoordinated Response: Rather than fragmented efforts, the AI safety ecosystem could operate with shared situational awareness—different actors understanding how their efforts interact and contribute to collective goals.\nAnticipatory Action: Formal models with prediction market integration could provide early warning of emerging risks, enabling proactive rather than reactive governance.\nGlobal Cooperation: Shared formal frameworks could facilitate international coordination similar to how economic models enable monetary coordination or climate models support environmental agreements.\nDemocratic Enhancement: Making expert reasoning transparent and modifiable could enable broader participation in crucial decisions about humanity’s technological future.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-recommendations",
    "href": "chapters/Outlines/final_draft.html#sec-recommendations",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "7.4 5.4 Recommendations for Stakeholders",
    "text": "7.4 5.4 Recommendations for Stakeholders\nDifferent communities can take concrete steps to realize these benefits:\n\n7.4.1 5.4.1 For Researchers\n\nExperiment with formalization: Try extracting your own arguments into ArgDown/BayesDown format to discover implicit assumptions\nContribute to validation: Provide expert annotations for building benchmark datasets and improving extraction quality\nDevelop extensions: Build on the open-source foundation to add capabilities for your specific domain needs\nPublish formally: Include formal model representations alongside traditional papers to enable cumulative building\n\n\n\n7.4.2 5.4.2 For Policymakers\n\nPilot applications: Use AMTAIR for internal analysis of specific policy proposals to build familiarity and identify value\nDemand transparency: Request formal models underlying expert recommendations to understand assumptions and uncertainties\nFund development: Support tool development and training to build governance capacity for formal methods\nDesign adaptively: Create policies with explicit triggers based on model parameters to enable responsive governance\n\n\n\n7.4.3 5.4.3 For Technologists\n\nImprove extraction: Contribute better prompting strategies, fine-tuned models, or validation methods\nEnhance interfaces: Develop visualizations and interactions serving specific stakeholder needs\nBuild integrations: Connect AMTAIR to other tools in the AI governance ecosystem\nScale infrastructure: Address computational challenges for larger models and broader deployment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-future-research-agenda",
    "href": "chapters/Outlines/final_draft.html#sec-future-research-agenda",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "7.5 5.5 Future Research Agenda",
    "text": "7.5 5.5 Future Research Agenda\nLooking ahead, the landscape of possibilities stretches toward the horizon, each path promising its own rewards and challenges. Let me map the territory worth exploring.\n\n7.5.1 5.5.1 Technical Priorities\nThe technical frontier advances on multiple fronts, each offering multiplicative improvements when combined:\nExtraction Enhancement: The current system, while functional, merely scratches the surface of what’s possible. Fine-tuning language models specifically on argument extraction tasks could dramatically improve accuracy. Imagine models trained not just on general text but on thousands of examples of arguments transformed into formal representations.\nHandling Implicit Reasoning: So much of expert argumentation relies on unstated background knowledge. When an AI safety researcher mentions “mesa-optimization,” they assume familiarity with complex concepts about learned optimization occurring within larger optimization processes. Future systems need to bridge these inferential gaps, perhaps by maintaining explicit knowledge bases of domain concepts or by training models to recognize and fill common argumentative ellipses.\nCross-Document Synthesis: Real understanding emerges not from single papers but from conversations across documents. Authors respond to each other, build on previous work, refine arguments over time. Future systems should trace these intellectual lineages, building composite models that capture evolving community understanding rather than static snapshots.\nRepresentation Extensions: Current Bayesian networks, while powerful, make limiting assumptions. Temporal dynamics matter—AI development unfolds over time, with early decisions constraining later options. Multi-agent representations could capture strategic interactions between actors. Continuous variables better represent quantities like “capability level” than binary approximations. Each extension opens new analytical possibilities.\n\n\n\n7.5.2 5.5.2 Methodological Development\nBeyond technical improvements lie deeper methodological questions about how we validate, use, and improve these systems:\nValidation Science: We need not just ad hoc evaluation but a science of argument extraction assessment. This means building benchmark datasets capturing diverse argument types, developing metrics that go beyond surface accuracy to semantic fidelity, creating adversarial test suites that probe system limitations, and establishing longitudinal studies tracking how extracted models evolve with updating source documents.\nHybrid Intelligence: The future isn’t human or AI but human and AI. Optimal collaboration patterns remain unexplored. Should humans verify structure while AI handles probabilities? Should AI propose multiple extractions for human selection? How do we combine formal models with scenario narratives, quantitative forecasts with qualitative insights? The design space for human-AI collaboration in argument formalization remains largely uncharted.\nSocial Methods: Technology embedded in social contexts requires social science. How do organizations actually use these models? What changes when formal representations replace informal discussions? Ethnographic studies of model use, measurement of coordination improvements, identification of adoption barriers—all essential for real-world impact.\n\n\n\n7.5.3 5.5.3 Application Expansion\nThe principles underlying AMTAIR apply far beyond AI risk:\nDomain Extensions: Every field grappling with complex risks could benefit. Biosecurity faces similar challenges—technical complexity, value-laden choices, deep uncertainty. Climate policy involves multi-level causation across physical, economic, and social systems. Nuclear policy, despite decades of study, still struggles with coordination across technical and strategic communities. Each domain would require specialized extraction approaches but could leverage the same fundamental architecture.\nInstitutional Integration: Moving from research prototype to institutional tool requires thoughtful embedding. Regulatory impact assessment could incorporate formal modeling to make assumptions explicit. Corporate strategic planning, especially for companies developing advanced technologies, needs tools for reasoning about unprecedented risks. Academic peer review might benefit from formal representation of complex arguments.\nGlobal Deployment: AI governance is inherently international, but different regions have different governance cultures, risk tolerances, and institutional structures. Adapting AMTAIR for different contexts—from Silicon Valley’s move-fast culture to the EU’s precautionary approach to China’s state-led development—requires both technical and cultural translation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#sec-closing-reflections",
    "href": "chapters/Outlines/final_draft.html#sec-closing-reflections",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "7.6 5.6 Closing Reflections",
    "text": "7.6 5.6 Closing Reflections\nAs I write these final words, I’m struck by the peculiar position we find ourselves in. We are arguably the first generation that must govern technologies that could fundamentally transform or terminate our species’ story. The margin for error shrinks as capabilities grow. The cost of coordination failure rises toward infinity.\nThe AMTAIR project emerged from a simple observation paired with an ambitious hope. The observation: while humanity mobilizes unprecedented resources to address AI risks, our efforts remain tragically uncoordinated. Different communities work with incompatible frameworks, duplicate efforts, and sometimes actively undermine each other’s work. The hope: that computational tools might help us build the epistemic infrastructure necessary for coordination.\nWhat we’ve accomplished here is both less and more than originally envisioned. Less, because the challenges proved deeper than anticipated. Natural language resists formalization. Probabilities remain stubbornly subjective. Coordination failures have roots beyond mere communication difficulties. More, because the journey revealed unexpected possibilities. Intermediate representations became valuable in themselves. The extraction process surfaced insights about argument structure. The visualization work demonstrated how thoughtful design can democratize access to formal tools.\nPerhaps most importantly, this work demonstrates that perfect solutions need not be the enemy of meaningful progress. AMTAIR doesn’t solve the coordination crisis—no single tool could. But it offers genuine assistance: making implicit models explicit, enabling systematic comparison across worldviews, supporting evidence-based policy evaluation, and creating common ground for productive disagreement.\nThe Stakes: Let me be plain about what’s at risk. The development of artificial general intelligence represents a discontinuity in human history comparable to the emergence of life or the evolution of consciousness. Get it right, and we might solve problems that have plagued humanity since our beginning—disease, poverty, ignorance, perhaps even death itself. Get it wrong, and we might extinguish not just ourselves but all the potential futures we might have created.\nThis isn’t science fiction or academic speculation. The capabilities advancing in labs today point toward systems that could, within decades or less, exceed human cognitive abilities across all domains. What happens when we create minds greater than our own? How do we ensure they remain aligned with human values and flourishing? These questions demand our best collective wisdom.\nYet we approach this challenge fragmented. Technical researchers develop alignment techniques without clear paths to implementation. Policymakers craft governance frameworks without deep technical understanding. Ethicists articulate values without operational specificity. International bodies convene without shared models of the risks they’re addressing. This fragmentation isn’t just inefficient—it’s existentially dangerous.\nAMTAIR represents one attempt to build bridges. By automating the extraction of worldviews, integrating live forecasts, and enabling systematic policy evaluation, we create infrastructure for enhanced coordination. Not coordination itself—that requires human wisdom, institutional change, and political will. But infrastructure that makes coordination more feasible.\nThe path forward demands both ambition and humility. Ambition to build the tools, institutions, and practices necessary for navigating unprecedented risks. Humility to recognize that our tools are imperfect, our understanding incomplete, and our time limited. We must act despite uncertainty, coordinate despite disagreement, and hope despite the magnitude of the challenge.\nAs I close this thesis, I think of future readers—perhaps humans living in a world made wonderful by aligned AI, perhaps historians studying how we navigated this crucial transition, perhaps no one at all if we fail. To those readers, know that we tried. We saw the challenge, recognized our limitations, and attempted to build what tools we could.\nThe coordination crisis in AI governance represents both existential risk and existential opportunity. Risk, if we fail to align our efforts before it’s too late. Opportunity, if we succeed in creating unprecedented cooperation around humanity’s most important challenge. AMTAIR offers one piece of the puzzle—computational infrastructure that enhances our collective ability to reason about complex risks.\nMay we prove worthy of the challenge before us. May our tools amplify our wisdom rather than our folly. And may future generations look back on this time not as when humanity failed to coordinate, but as when we rose to meet our greatest test.\nThe work continues. The stakes could not be higher. The time grows short. Let us build what we can, while we can, for all our futures depend on it.\n\n\n\n\nAnderson, Terence J. 2007. “Visualization Tools and Argument Schemes: A Question of Standpoint.” Law, Prob. & Risk 6: 97. https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/lawprisk6&section=9.\n\n\nArmstrong, Stuart, Nick Bostrom, and Carl Shulman. 2016. “Racing to the Precipice: A Model of Artificial Intelligence Development.” AI & SOCIETY 31 (2): 201–6. https://doi.org/10.1007/s00146-015-0590-y.\n\n\nBabakov, Nikolay, Adarsa Sivaprasad, Ehud Reiter, and Alberto Bugarín-Diz. 2025. “Reusability of Bayesian Networks Case Studies: A Survey.” Applied Intelligence 55 (6): 417. https://doi.org/10.1007/s10489-025-06289-5.\n\n\nBan, Taiyu, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, and Huanhuan Chen. 2023. “Causal Structure Learning Supervised by Large Language Model.” November 20, 2023. https://doi.org/10.48550/arXiv.2311.11689.\n\n\nBenn, Neil, and Ann Macintosh. 2011. “Argument Visualization for eParticipation: Towards a Research Agenda and Prototype Tool.” In Electronic Participation, edited by Efthimios Tambouris, Ann Macintosh, and Hans De Bruijn, 6847:60–73. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-23333-3_6.\n\n\nBethard, Steven John. 2007. “Finding Event, Temporal and Causal Structure in Text: A Machine Learning Approach.” PhD thesis, University of Colorado at Boulder. https://search.proquest.com/openview/405fe32503123d9b5f4836dc3be4c011/1?pq-origsite=gscholar&cbl=18750.\n\n\nBostrom, Nick. 2014. Superintelligence: Paths, Strategies, Dangers. Oxford: Oxford University Press. https://scholar.dominican.edu/cynthia-stokes-brown-books-big-history/47.\n\n\nCarlsmith, Joseph. 2021. “Is Power-Seeking AI an Existential Risk?” 2021. https://doi.org/10.48550/arXiv.2206.13353.\n\n\n———. 2022. “Is Power-Seeking AI an Existential Risk?” https://arxiv.org/abs/2206.13353.\n\n\n———. 2024. “Is Power-Seeking AI an Existential Risk?” August 13, 2024. https://doi.org/10.48550/arXiv.2206.13353.\n\n\nChen, Lu, Ruqing Zhang, Wei Huang, Wei Chen, Jiafeng Guo, and Xueqi Cheng. 2023. “Inducing Causal Structure for Abstractive Text Summarization.” In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 213–23. Birmingham United Kingdom: ACM. https://doi.org/10.1145/3583780.3614934.\n\n\nChristiano, Paul F. 2019. “What Failure Looks Like,” March. https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like.\n\n\nClarke, Sam, Ben Cottier, Aryeh Englander, Daniel Eth, David Manheim, Samuel Dylan Martin, and Issa Rice. 2022. “Modeling Transformative AI Risks (MTAIR) Project – Summary Report.” 2022. https://doi.org/10.48550/ARXIV.2206.09360.\n\n\nCuomo, Francesca, Christine Mallin, and Alessandro Zattoni. 2016. “Corporate Governance Codes: A Review and Research Agenda.” Corporate Governance: An International Review 24 (3): 222–41. https://ueaeprints.uea.ac.uk/id/eprint/57664/.\n\n\nDafoe, Allan. 2018. “AI Governance: A Research Agenda.” Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK 1442: 1443. https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf.\n\n\nDe Villiers, Charl, and Ruth Dimes. 2021. “Determinants, Mechanisms and Consequences of Corporate Governance Reporting: A Research Framework.” Journal of Management and Governance 25 (1): 7–26. https://doi.org/10.1007/s10997-020-09530-0.\n\n\nDemirag, Istemi, Sudi Sudarsanam, and MIKE WRIGHT. 2000. “Corporate Governance: Overview and Research Agenda.” The British Accounting Review 32 (4): 341–54. https://www.academia.edu/download/49469624/bare.2000.014620161009-3955-1dt4aq5.pdf.\n\n\nDi Vito, Jackie, and Kim Trottier. 2022. “A Literature Review on Corporate Governance Mechanisms: Past, Present, and Future*.” Accounting Perspectives 21 (2): 207–35. https://doi.org/10.1111/1911-3838.12279.\n\n\nDuhem, Pierre Maurice Marie. 1954. The Aim and Structure of Physical Theory. 1. Princeton University Press.\n\n\nEuropean, Union. 2024. “The Act Texts | EU Artificial Intelligence Act.” 2024. https://artificialintelligenceact.eu/the-act/.\n\n\nGood, Irving John. 1966. “Speculations Concerning the First Ultraintelligent Machine.” Advances in Computers, 31. https://doi.org/10.1016/S0065-2458(08)60418-0.\n\n\nGruetzemacher, Ross. 2022. “Bayesian Networks Vs. Conditional Trees for Creating Questions for Forecasting Tournaments.”\n\n\nHallegatte, Stéphane, Ankur Shah, Robert Lempert, Casey Brown, and Stuart Gill. 2012. “Investment Decision-Making Under Deep Uncertainty-Application to Climate Change.” Policy Research Working Paper 6193. https://enpc.hal.science/hal-00802049/document.\n\n\nHeinze-Deml, Christina, Marloes H. Maathuis, and Nicolai Meinshausen. 2018. “Causal Structure Learning.” Annual Review of Statistics and Its Application 5 (1): 371–91. https://doi.org/10.1146/annurev-statistics-031017-100630.\n\n\nHunt, Tam. 2025. “The Insane ‘Logic’ of the AI Arms Race.” Medium. March 3, 2025. https://tamhunt.medium.com/the-insane-logic-of-the-ai-arms-race-45a5f79f4c0e.\n\n\nJaynes, Edwin T. 2003. Probability Theory: The Logic of Science. Cambridge university press.\n\n\nKaur, Kawaljit. 2024. “Corporate Governance and Legal Accountability: A Critical Review of Global Practices.” Journal of Law 2 (6): 1–7. https://joi.shodhsagar.org/index.php/SSJOI/article/view/16.\n\n\nKhartabil, D., C. Collins, S. Wells, B. Bach, and J. Kennedy. 2021. “Design and Evaluation of Visualization Techniques to Facilitate Argument Exploration.” Computer Graphics Forum 40 (6): 447–65. https://doi.org/10.1111/cgf.14389.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT press. https://books.google.ca/books?hl=en&lr=&id=7dzpHCHzNQ4C&oi=fnd&pg=PR9&dq=Koller,+D.,+%26+Friedman,+N.+(2009).+Probabilistic+Graphical+Models&ots=py2HAh0VAL&sig=gpaID3x6-TY8x5SOopuXpZDXfzs.\n\n\nList, Christian, and Philip Pettit. 2011. Group Agency: The Possibility, Design, and Status of Corporate Agents. Oxford University Press.\n\n\nMaslej, Nestor. 2025. “Artificial Intelligence Index Report 2025.” Artificial Intelligence.\n\n\nMcCaslin, Tegan, Josh Rosenberg, Ezra Karger, Avital Morris, Molly Hickman, Sam Glover, Zach Jacobs, and Phil Tetlock. 2024. “Conditional Trees: A Method for Generating Informative Questions about Complex Topics.” Forecasting Research Institute. https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/66ba37a144f1d6095de467df/1723479995772/AIConditionalTrees.pdf.\n\n\nMetropolitansky, Dasha, and Jonathan Larson. 2025. “Towards Effective Extraction and Evaluation of Factual Claims.” February 15, 2025. https://doi.org/10.48550/arXiv.2502.10855.\n\n\nMeyer, Valentin Jakob. 2022. “A Structure of Knowledge & the Process of Science.” Philosophy of the Social Sciences First Course Paper. https://doi.org/https://www.vjmeyer.com/papers/essays.\n\n\nMiotti, Andrea, Tolga Bilge, Dave Kasten, and James Newport. 2024. “A Narrow Path.” https://www.narrowpath.co/.\n\n\nNelson, Roger B. 2006. An Introduction to Copulas. Springer Series in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/0-387-28678-0.\n\n\nPaul. 2023. “The elephAInt – Are We All Like the Six Blind Men When It Comes to AI? | PRISMAGuard LLC.” 2023. https://www.prismaguard.com/the-elephaint-are-we-all-like-the-six-blind-men-when-it-comes-to-ai/.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\n———. 2009. Causality: Models, Reasoning and Inference. 2nd ed. Cambridge University Press.\n\n\n———. 2014. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Elsevier. https://books.google.ca/books?hl=en&lr=&id=mn2jBQAAQBAJ&oi=fnd&pg=PP1&dq=Pearl,+J.+(1988).+Probabilistic+Reasoning+in+Intelligent+Systems&ots=4tEX2A4Ha8&sig=lgUs_RCoeXEEuGwM5xMEoyJy4HI.\n\n\nPollock, John L. 1995. Cognitive Carpentry: A Blueprint for How to Build a Person. Mit Press. https://books.google.ca/books?hl=en&lr=&id=JAfHrHTqswAC&oi=fnd&pg=PA1&dq=Pollock,+J.+(1995).+Cognitive+Carpentry&ots=rq-qSCBcxV&sig=aAfHGsGUosxl_1-JuxIEA7C2QO4.\n\n\nRehman, Iskander. 2025. “The Battle for Brilliant Minds: From the Nuclear Age to AI.” War on the Rocks. January 13, 2025. https://warontherocks.com/2025/01/the-battle-for-brilliant-minds-from-the-nuclear-age-to-ai/.\n\n\nSamborska, Veronika. 2025. “Scaling up: How Increasing Inputs Has Made Artificial Intelligence More Capable.” Our World in Data, January. https://ourworldindata.org/scaling-up-ai.\n\n\nSamuel, Sigal. 2023. “AI Is a ‘Tragedy of the Commons.’ We’ve Got Solutions for That.” Vox. July 7, 2023. https://www.vox.com/future-perfect/2023/7/7/23787011/ai-arms-race-tragedy-commons-risk-safety.\n\n\nSchelling, Thomas C. 1960. “I960. The Strategy of Conflict.” Cambridge, Mass.\n\n\nSolomon, Jill. 2020. Corporate Governance and Accountability. John Wiley & Sons. https://books.google.ca/books?hl=en&lr=&id=JAX9DwAAQBAJ&oi=fnd&pg=PR1&dq=review+of+the+effects+of+liability+frameworks+on+corporate+governance+&ots=ny23_vd-U0&sig=3LuNNhvSWXriEeg-ipAdDIQGAgo.\n\n\nSquires, Chandler, and Caroline Uhler. 2023. “Causal Structure Learning: A Combinatorial Perspective.” Foundations of Computational Mathematics 23 (5): 1781–1815. https://doi.org/10.1007/s10208-022-09581-9.\n\n\nTegmark, Max. 2024. “Asilomar AI Principles.” Future of Life Institute. 2024. https://futureoflife.org/open-letter/ai-principles/.\n\n\nTetlock, Phil. 2022. “Conditional Trees: AI Risk.” 2022. https://www.metaculus.com/tournament/3508/.\n\n\nTetlock, Philip E., and Dan Gardner. 2015. Superforecasting: The Art and Science of Prediction. First paperback edition. New York: Broadway Books.\n\n\nTodd, Benjamin. 2024. “It Looks Like There Are Some Good Funding Opportunities in AI Safety Right Now.” Substack newsletter. Benjamin Todd. December 21, 2024. https://benjamintodd.substack.com/p/looks-like-there-are-some-good-funding.\n\n\nVoigt, Christian. (2014) 2025. “Christianvoigt/Argdown.” https://github.com/christianvoigt/argdown.\n\n\nYang, Jie, Soyeon Caren Han, and Josiah Poon. 2022. “A Survey on Extraction of Causal Relations from Natural Language Text.” Knowledge and Information Systems 64 (5): 1161–86. https://doi.org/10.1007/s10115-022-01665-w.\n\n\nYudkowsky, Eliezer. 2008. “Artificial Intelligence as a Positive and Negative Factor in Global Risk.” In Global Catastrophic Risks, by Eliezer Yudkowsky. Oxford University Press. https://doi.org/10.1093/oso/9780198570509.003.0021.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#footnotes",
    "href": "chapters/Outlines/final_draft.html#footnotes",
    "title": "2  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "The orthogonality thesis posits that intelligence and goals are independent—an AI can have any set of objectives regardless of its intelligence level. The instrumental convergence thesis suggests that different AI systems may adopt similar instrumental goals (e.g., self-preservation, resource acquisition) to achieve their objectives.↩︎\nMultiple versions of Carlsmith’s paper exist with slight updates to probability estimates: Carlsmith (2021), Carlsmith (2022), Carlsmith (2024). We primarily reference the version used by the MTAIR team for their extraction. Extended discussion and expert probability estimates can be found on LessWrong.↩︎\nThis example, while simple, demonstrates all essential features of Bayesian networks and serves as the foundation for understanding more complex applications↩︎\nThese estimates include time for initial extraction, expert consultation, probability elicitation, validation, and refinement↩︎\nCopulas provide a mathematically elegant way to separate marginal behavior from dependence structure↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "ref/references.html",
    "href": "ref/references.html",
    "title": "Bibliography",
    "section": "",
    "text": "Anderson, Terence J. 2007. “Visualization Tools and Argument\nSchemes: A Question of Standpoint.” Law, Prob. &\nRisk 6: 97. https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/lawprisk6&section=9.\n\n\nArmstrong, Stuart, Nick Bostrom, and Carl Shulman. 2016. “Racing\nto the Precipice: A Model of Artificial Intelligence\nDevelopment.” AI & SOCIETY 31 (2): 201–6. https://doi.org/10.1007/s00146-015-0590-y.\n\n\nBabakov, Nikolay, Adarsa Sivaprasad, Ehud Reiter, and Alberto\nBugarín-Diz. 2025. “Reusability of Bayesian Networks\nCase Studies: A Survey.” Applied Intelligence 55 (6):\n417. https://doi.org/10.1007/s10489-025-06289-5.\n\n\nBan, Taiyu, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, and Huanhuan Chen.\n2023. “Causal Structure Learning Supervised by\nLarge Language Model.” November 20, 2023. https://doi.org/10.48550/arXiv.2311.11689.\n\n\nBenn, Neil, and Ann Macintosh. 2011. “Argument\nVisualization for eParticipation: Towards a\nResearch Agenda and Prototype Tool.” In\nElectronic Participation, edited by Efthimios\nTambouris, Ann Macintosh, and Hans De Bruijn, 6847:60–73. Berlin,\nHeidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-23333-3_6.\n\n\nBethard, Steven John. 2007. “Finding Event, Temporal and Causal\nStructure in Text: A Machine Learning Approach.” PhD\nthesis, University of Colorado at Boulder. https://search.proquest.com/openview/405fe32503123d9b5f4836dc3be4c011/1?pq-origsite=gscholar&cbl=18750.\n\n\nBostrom, Nick. 2014. Superintelligence: Paths,\nStrategies, Dangers. Oxford: Oxford University Press. https://scholar.dominican.edu/cynthia-stokes-brown-books-big-history/47.\n\n\nCarlsmith, Joseph. 2021. “Is Power-Seeking AI an\nExistential Risk?” 2021. https://doi.org/10.48550/arXiv.2206.13353.\n\n\n———. 2022. “Is Power-Seeking AI an Existential\nRisk?” https://arxiv.org/abs/2206.13353.\n\n\n———. 2024. “Is Power-Seeking AI an Existential\nRisk?” August 13, 2024. https://doi.org/10.48550/arXiv.2206.13353.\n\n\nChen, Lu, Ruqing Zhang, Wei Huang, Wei Chen, Jiafeng Guo, and Xueqi\nCheng. 2023. “Inducing Causal Structure for\nAbstractive Text Summarization.” In Proceedings\nof the 32nd ACM International Conference on\nInformation and Knowledge Management,\n213–23. Birmingham United Kingdom: ACM. https://doi.org/10.1145/3583780.3614934.\n\n\nChristiano, Paul F. 2019. “What Failure Looks Like,” March.\nhttps://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like.\n\n\nClarke, Sam, Ben Cottier, Aryeh Englander, Daniel Eth, David Manheim,\nSamuel Dylan Martin, and Issa Rice. 2022. “Modeling\nTransformative AI Risks (MTAIR)\nProject – Summary Report.” 2022. https://doi.org/10.48550/ARXIV.2206.09360.\n\n\nCuomo, Francesca, Christine Mallin, and Alessandro Zattoni. 2016.\n“Corporate Governance Codes: A Review and Research\nAgenda.” Corporate Governance: An International Review\n24 (3): 222–41. https://ueaeprints.uea.ac.uk/id/eprint/57664/.\n\n\nDafoe, Allan. 2018. “AI Governance: A Research\nAgenda.” Governance of AI Program, Future of Humanity\nInstitute, University of Oxford: Oxford, UK 1442: 1443. https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf.\n\n\nDe Villiers, Charl, and Ruth Dimes. 2021. “Determinants,\nMechanisms and Consequences of Corporate Governance Reporting: A\nResearch Framework.” Journal of Management and\nGovernance 25 (1): 7–26. https://doi.org/10.1007/s10997-020-09530-0.\n\n\nDemirag, Istemi, Sudi Sudarsanam, and MIKE WRIGHT. 2000.\n“Corporate Governance: Overview and Research Agenda.”\nThe British Accounting Review 32 (4): 341–54. https://www.academia.edu/download/49469624/bare.2000.014620161009-3955-1dt4aq5.pdf.\n\n\nDi Vito, Jackie, and Kim Trottier. 2022. “A Literature\nReview on Corporate Governance Mechanisms:\nPast, Present, and\nFuture*.” Accounting Perspectives 21 (2):\n207–35. https://doi.org/10.1111/1911-3838.12279.\n\n\nDuhem, Pierre Maurice Marie. 1954. The Aim and\nStructure of Physical Theory. 1.\nPrinceton University Press.\n\n\nEuropean, Union. 2024. “The Act Texts | EU\nArtificial Intelligence Act.” 2024. https://artificialintelligenceact.eu/the-act/.\n\n\nGood, Irving John. 1966. “Speculations Concerning the\nFirst Ultraintelligent Machine.” Advances in\nComputers, 31. https://doi.org/10.1016/S0065-2458(08)60418-0.\n\n\nGruetzemacher, Ross. 2022. “Bayesian Networks Vs.\nConditional Trees for Creating Questions for\nForecasting Tournaments.”\n\n\nHallegatte, Stéphane, Ankur Shah, Robert Lempert, Casey Brown, and\nStuart Gill. 2012. “Investment Decision-Making Under Deep\nUncertainty-Application to Climate Change.” Policy Research\nWorking Paper 6193. https://enpc.hal.science/hal-00802049/document.\n\n\nHeinze-Deml, Christina, Marloes H. Maathuis, and Nicolai Meinshausen.\n2018. “Causal Structure Learning.” Annual\nReview of Statistics and Its Application 5 (1): 371–91. https://doi.org/10.1146/annurev-statistics-031017-100630.\n\n\nHunt, Tam. 2025. “The Insane ‘Logic’ of the\nAI Arms Race.” Medium. March 3, 2025. https://tamhunt.medium.com/the-insane-logic-of-the-ai-arms-race-45a5f79f4c0e.\n\n\nJaynes, Edwin T. 2003. Probability Theory: The Logic of\nScience. Cambridge university press.\n\n\nKaur, Kawaljit. 2024. “Corporate Governance and\nLegal Accountability: A Critical Review of\nGlobal Practices.” Journal of Law 2 (6):\n1–7. https://joi.shodhsagar.org/index.php/SSJOI/article/view/16.\n\n\nKhartabil, D., C. Collins, S. Wells, B. Bach, and J. Kennedy. 2021.\n“Design and Evaluation of Visualization\nTechniques to Facilitate Argument\nExploration.” Computer Graphics Forum 40 (6):\n447–65. https://doi.org/10.1111/cgf.14389.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical\nModels: Principles and Techniques. MIT press. https://books.google.ca/books?hl=en&lr=&id=7dzpHCHzNQ4C&oi=fnd&pg=PR9&dq=Koller,+D.,+%26+Friedman,+N.+(2009).+Probabilistic+Graphical+Models&ots=py2HAh0VAL&sig=gpaID3x6-TY8x5SOopuXpZDXfzs.\n\n\nList, Christian, and Philip Pettit. 2011. Group Agency:\nThe Possibility, Design, and\nStatus of Corporate Agents. Oxford\nUniversity Press.\n\n\nMaslej, Nestor. 2025. “Artificial Intelligence Index\nReport 2025.” Artificial Intelligence.\n\n\nMcCaslin, Tegan, Josh Rosenberg, Ezra Karger, Avital Morris, Molly\nHickman, Sam Glover, Zach Jacobs, and Phil Tetlock. 2024.\n“Conditional Trees: A Method for\nGenerating Informative Questions about Complex\nTopics.” Forecasting Research Institute. https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/66ba37a144f1d6095de467df/1723479995772/AIConditionalTrees.pdf.\n\n\nMetropolitansky, Dasha, and Jonathan Larson. 2025. “Towards\nEffective Extraction and Evaluation of\nFactual Claims.” February 15, 2025. https://doi.org/10.48550/arXiv.2502.10855.\n\n\nMeyer, Valentin Jakob. 2022. “A Structure of\nKnowledge & the Process of\nScience.” Philosophy of the Social Sciences\nFirst Course Paper. https://doi.org/https://www.vjmeyer.com/papers/essays.\n\n\nMiotti, Andrea, Tolga Bilge, Dave Kasten, and James Newport. 2024.\n“A Narrow Path.” https://www.narrowpath.co/.\n\n\nNelson, Roger B. 2006. An Introduction to\nCopulas. Springer Series in\nStatistics. New York, NY: Springer New York. https://doi.org/10.1007/0-387-28678-0.\n\n\nPaul. 2023. “The elephAInt –\nAre We All Like the Six Blind Men When It Comes to\nAI? | PRISMAGuard LLC.” 2023. https://www.prismaguard.com/the-elephaint-are-we-all-like-the-six-blind-men-when-it-comes-to-ai/.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and\nInference. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\n———. 2009. Causality: Models, Reasoning and\nInference. 2nd ed. Cambridge University Press.\n\n\n———. 2014. Probabilistic Reasoning in Intelligent Systems: Networks\nof Plausible Inference. Elsevier. https://books.google.ca/books?hl=en&lr=&id=mn2jBQAAQBAJ&oi=fnd&pg=PP1&dq=Pearl,+J.+(1988).+Probabilistic+Reasoning+in+Intelligent+Systems&ots=4tEX2A4Ha8&sig=lgUs_RCoeXEEuGwM5xMEoyJy4HI.\n\n\nPollock, John L. 1995. Cognitive Carpentry: A Blueprint\nfor How to Build a Person. Mit Press. https://books.google.ca/books?hl=en&lr=&id=JAfHrHTqswAC&oi=fnd&pg=PA1&dq=Pollock,+J.+(1995).+Cognitive+Carpentry&ots=rq-qSCBcxV&sig=aAfHGsGUosxl_1-JuxIEA7C2QO4.\n\n\nRehman, Iskander. 2025. “The Battle for\nBrilliant Minds: From the Nuclear\nAge to AI.” War on the Rocks. January 13,\n2025. https://warontherocks.com/2025/01/the-battle-for-brilliant-minds-from-the-nuclear-age-to-ai/.\n\n\nSamborska, Veronika. 2025. “Scaling up: How Increasing Inputs Has\nMade Artificial Intelligence More Capable.” Our World in\nData, January. https://ourworldindata.org/scaling-up-ai.\n\n\nSamuel, Sigal. 2023. “AI Is a ‘Tragedy of the\nCommons.’ We’ve Got Solutions for That.” Vox.\nJuly 7, 2023. https://www.vox.com/future-perfect/2023/7/7/23787011/ai-arms-race-tragedy-commons-risk-safety.\n\n\nSchelling, Thomas C. 1960. “I960. The Strategy of\nConflict.” Cambridge, Mass.\n\n\nSolomon, Jill. 2020. Corporate Governance and Accountability.\nJohn Wiley & Sons. https://books.google.ca/books?hl=en&lr=&id=JAX9DwAAQBAJ&oi=fnd&pg=PR1&dq=review+of+the+effects+of+liability+frameworks+on+corporate+governance+&ots=ny23_vd-U0&sig=3LuNNhvSWXriEeg-ipAdDIQGAgo.\n\n\nSquires, Chandler, and Caroline Uhler. 2023. “Causal\nStructure Learning: A Combinatorial\nPerspective.” Foundations of Computational\nMathematics 23 (5): 1781–1815. https://doi.org/10.1007/s10208-022-09581-9.\n\n\nTegmark, Max. 2024. “Asilomar AI Principles.”\nFuture of Life Institute. 2024. https://futureoflife.org/open-letter/ai-principles/.\n\n\nTetlock, Phil. 2022. “Conditional Trees: AI\nRisk.” 2022. https://www.metaculus.com/tournament/3508/.\n\n\nTetlock, Philip E., and Dan Gardner. 2015. Superforecasting: The Art\nand Science of Prediction. First paperback edition. New York:\nBroadway Books.\n\n\nTodd, Benjamin. 2024. “It Looks Like There Are Some Good Funding\nOpportunities in AI Safety Right Now.” Substack\nnewsletter. Benjamin Todd. December 21, 2024. https://benjamintodd.substack.com/p/looks-like-there-are-some-good-funding.\n\n\nVoigt, Christian. (2014) 2025. “Christianvoigt/Argdown.” https://github.com/christianvoigt/argdown.\n\n\nYang, Jie, Soyeon Caren Han, and Josiah Poon. 2022. “A Survey on\nExtraction of Causal Relations from Natural Language Text.”\nKnowledge and Information Systems 64 (5): 1161–86. https://doi.org/10.1007/s10115-022-01665-w.\n\n\nYudkowsky, Eliezer. 2008. “Artificial Intelligence as\na Positive and Negative Factor in Global Risk.” In Global\nCatastrophic Risks, by Eliezer Yudkowsky. Oxford\nUniversity Press. https://doi.org/10.1093/oso/9780198570509.003.0021.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "chapters/manual_extraction_bucknall.html",
    "href": "chapters/manual_extraction_bucknall.html",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "[Existential_Risk]: Increase in existential risks for humanity. {“instantiations”: [TRUE”, “FALSE”]}\n\n[Unaligned_AGI_Risk]: Unaligned artificial general intelligence causes existential risk. {“instantiations”: [TRUE”, “FALSE”]}\n\n[State-State_Relations]\n\n[Near_term_AI]: Even if not unaligned AGI, near term AI can act as intermediate risk factor. {“instantiations”: [TRUE”, “FALSE”]}\n\n[State-State_Relations]: AI arms race dynamic inhibits international coordination, diverting resources from other pressing issues {“instantiations”: [TRUE”, “FALSE”]}\n\n[Cybersecurity]: Probably enhances Cyber-Attack-Offense, may intensify cyber warfare. {“instantiations”: [TRUE”, “FALSE”]}\n\n[State-Cooperation_Relations]: Cooperations have a lot of power and might have misaligned goals with society {“instantiations”: [TRUE”, “FALSE”]}\n[Stable_Repressive_Regime]: More repressive instruments, possibility of stable repressive regime. {“instantiations”: [“TRUE”, “FALSE”]}\n\n[State-Citizen_Relations]: AI helps regime monitor citizens {“instantiations”: [TRUE”, “FALSE”]}\n\n[Compromised_Political_Decision_Making]: AI can compromise political decision making. {“instantiations”: [“TRUE”, “FALSE”]}\n\n[Social_media_and_Recommender_Systems]: Influence of AI in social media on public opinion. {“instantiations”: [TRUE”, “FALSE”]}\n\n\n[Nuclear]: Probability that nuclear conflict escalates to end civilisation. {“instantiations”: [TRUE”, “FALSE”]}\n\n[Compromised_Political_Decision_Making]\n\n[Biological]: Probability that a natural or engineered pandemic poses existential risks. {“instantiations”: [“TRUE”, “FALSE”]}\n\n[Compromised_Political_Decision_Making]\n[Social_media_and_Recommender_Systems]\n\n[Natural]: Non-human caused existential risks, seem unrelated with AI. {“instantiations”: [“TRUE”, “FALSE”]}\n[Environmental]: Probability of climate catastrophe. {“instantiations”: [“TRUE”, “FALSE”]}\n\n[Compromised_Political_Decision_Making]\n[AI_resource_consumption]: Current AI models consume large amounts of energy having environmental impacts. {“instantiations”: [“TRUE”, “FALSE”]}\n[Social_media_and_Recommender_Systems]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>manual_extraction_bucknall.html</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "",
    "text": "C AMTAIR Prototype: Automating Transformative AI Risk Modeling",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#executive-summary",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#executive-summary",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "C.1 Executive Summary",
    "text": "C.1 Executive Summary\nThis notebook implements a prototype of the AMTAIR (Automating Transformative AI Risk Modeling) project, which addresses the critical coordination failure in AI governance by developing computational tools that automate the extraction of probabilistic world models from AI safety literature.\nThe prototype demonstrates the transformation pipeline from structured argument representations (ArgDown) to probabilistic Bayesian networks (BayesDown), enabling the visualization and analysis of causal relationships and probability distributions that underlie AI risk assessments and policy evaluations.\n\nC.1.1 Purpose Within the Master’s Thesis\nThis notebook serves as the technical implementation component of the Master’s thesis “Automating Transformative AI Risk Modeling: A Computational Approach to Policy Impact Evaluation.” It demonstrates the feasibility of automating the extraction and formalization of world models, focusing on the core extraction pipeline and visualization capabilities that form the foundation for more sophisticated analysis.\n\n\nC.1.2 Relevance to AI Governance\nThe coordination crisis in AI governance stems from different stakeholders working with incompatible assumptions, terminologies, and priorities. By making implicit models explicit through automated extraction and formalization, this work helps bridge communication gaps between technical researchers, policy specialists, and other stakeholders, contributing to more effective coordination in addressing existential risks from advanced AI.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#notebook-structure-and-workflow",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#notebook-structure-and-workflow",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "C.2 Notebook Structure and Workflow",
    "text": "C.2 Notebook Structure and Workflow\nThis notebook implements a multi-stage pipeline for transforming argument structures into interactive Bayesian network visualizations:\n\nEnvironment Setup (Sections 0.1-0.3): Establishes the technical environment with necessary libraries and data connections\nArgument Extraction: Sources to ArgDown (Sections 1.0-1.8): Processes source documents into structured ArgDown representations\nProbability Integration (Sections 2.0-2.8): Enhances ArgDown with probability information to create BayesDown\nData Transformation (Section 3.0): Converts BayesDown into structured DataFrame format\nVisualization and Analysis (Section 4.0): Creates interactive Bayesian network visualizations\nArchiving and Export (Sections 5.0-6.0): Provides utilities for saving and sharing results",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#instructions-how-to-use-this-notebook",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#instructions-how-to-use-this-notebook",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "C.3 Instructions — How to use this notebook:",
    "text": "C.3 Instructions — How to use this notebook:\n\nImport Libraries & Install Packages: Run Section 0.1 to set up the necessary dependencies for data processing and visualization.\nConnect to GitHub Repository & Load Data files: Run Section 0.2 to establish connections to the data repository and load example datasets. This step retrieves sample ArgDown files and extracted data for demonstration.\nProcess Source Documents to ArgDown: Sections 1.0-1.8 demonstrate the extraction of argument structures from source documents (such as PDFs) into ArgDown format, a markdown-like notation for structured arguments.\nConvert ArgDown to BayesDown: Sections 2.0-2.3 handle the transformation of ArgDown files into BayesDown format, which incorporates probabilistic information into the argument structure.\nExtract Data into Structured Format: Section 3.0 processes BayesDown format into structured database entries (CSV) that can be used for analysis.\nCreate and Analyze Bayesian Networks: Section 4.0 demonstrates how to build Bayesian networks from the extracted data and provides tools for analyzing risk pathways.\nSave and Export Results: Sections 5.0-6.0 provide methods for archiving results and exporting visualizations.\n\n\nAMTAIR Prototype Demonstration (Public Colab Notebook)\n\n\nAMTAIR Prototype: Automating Transformative AI Risk Modeling\n\n\n\nExecutive Summary\n\n\n\n\n\nPurpose Within the Master’s Thesis\n\n\n\n\n\n\nRelevance to AI Governance\n\n\n\n\n\nNotebook Structure and Workflow\n\n\n\n\nInstructions — How to use this notebook:\n\n\n\n\nKey Concepts:\n\n\n\n\nExample Workflow:\n\n\n\n\nTroubleshooting:\n\n\n\n0 Environment Setup and Data Access\n\n\n\n0.1 Prepare Colab/Python Environment — Import Libraries & Packages\n\n\n\n\n0.2 Connect to GitHub Repository\n\n\n\n\n0.3 File Import\n\n\n\n1 Sources (PDF’s of Papers) to ArgDown (.md file)\n\n\n\n1.0 Sources to ArgDown: Structured Argument Extraction\n\n\n\n\n\nProcess Overview\n\n\n\n\n\n\nWhat is ArgDown?\n\n\n\n\n\n1.1 Specify Source Document (e.g. PDF)\n\n\n\n\n1.2 Generate ArgDown Extraction Prompt\n\n\n\n\n1.3 Prepare LLM API Call\n\n\n\n\n1.4 Make ArgDown Extraction LLM API Call\n\n\n\n\n1.5 Save ArgDown Extraction Response\n\n\n\n\n1.6 Review and Check ArgDown.md File\n\n\n\n\n\n1.6.0 Check the Graph Structure with the ArgDown Sandbox Online\n\n\n\n\n\n1.7 Extract ArgDown Graph Information as DataFrame\n\n\n\n\n1.8 Store ArgDown Information as ‘ArgDown.csv’ file\n\n\n\n2 Probability Extractions: ArgDown (.csv) to BayesDown (.md + plugin JSON syntax)\n\n\n\n2.0 ArgDown to BayesDown: Adding Probability Information\n\n\n\n\n\nProcess Overview\n\n\n\n\n\n\nWhat is BayesDown?\n\n\n\n\n\n2.1 Probability Extraction Questions — ‘ArgDown.csv’ to ‘ArgDown_WithQuestions.csv’\n\n\n\n\n2.2 ‘ArgDown_WithQuestions.csv’ to ‘BayesDownQuestions.md’\n\n\n\n\n2.3 Generate BayesDown Probability Extraction Prompt\n\n\n\n\n\n2.3.0 BayesDown Format Specification\n\n\n\n\n\n\n\nCore Structure\n\n\n\n\n\n\n\n2.3.1 Rain-Sprinkler-Lawn Example\n\n\n\n\n\n2.4 Prepare 2nd API call\n\n\n\n\n2.5 Make BayesDown Probability Extraction API Call\n\n\n\n\n2.6 Save BayesDown with Probability Estimates (.csv)\n\n\n\n\n2.7 Review & Verify BayesDown Probability Estimates\n\n\n\n\n2.7.2 Check the Graph Structure with the ArgDown Sandbox Online\n\n\n\n\n2.8 Extract BayesDown with Probability Estimates as Dataframe\n\n\n\n3 Data Extraction: BayesDown (.md) to Database (.csv)\n\n\n\n3.0 BayesDown to Structured Data: Network Construction\n\n\n\n\nExtraction Pipeline Overview\n\n\n\n\n\nTheoretical Foundation\n\n\n\n\n\n\nRole in Thesis Research\n\n\n\n\n\n\n3.0.0 ExtractBayesDown-Data_v1\n\n\n\n\n\n\n3.0.1 Test BayesDown Extraction\n\n\n\n\n\n\n3.0.2 Check the Graph Structure with the ArgDown Sandbox Online\n\n\n\n\n\n3.1 Extraction\n\n\n\n\n3.2 Data-Post-Processing\n\n\n\n\n3.4 Download and save finished data frame as .csv file\n\n\n\n4 Analysis & Inference: Bayesian Network Visualization\n\n\n\n4.0 Bayesian Network Visualization Approach\n\n\n\n\n\nVisualization Philosophy\n\n\n\n\n\n\nConnection to AMTAIR Goals\n\n\n\n\n\n\nImplementation Structure\n\n\n\n\n\n4.1 Phase 1: Dependencies/Functions\n\n\n\n\n4.2 Phase 2: Node Classification and Styling Module\n\n\n\n\n4.3 Phase 3: HTML Content Generation Module\n\n\n\n\n4.4 Phase 4: Main Visualization Function\n\n\n\n5 Quick check HTML Outputs\n\n\nConclusion: From Prototype to Production\n\n\n\nSummary of Achievements\n\n\n\n\nLimitations and Future Work\n\n\n\n\nConnection to AMTAIR Project\n\n\n\n6 Save Outputs\n\n\n\n6.0 Saving and Exporting Results\n\n\n\n\n6.1 Convert .ipynb Notebook to MarkDown\n\n\n\n\n6.2 Convert Notebook to Markdown Documentation\n\n\n\n\n6.3 Create PDF and Latex",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#key-concepts",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#key-concepts",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "C.4 Key Concepts:",
    "text": "C.4 Key Concepts:\n\nArgDown: A structured format for representing arguments, with hierarchical relationships between statements.\nBayesDown: An extension of ArgDown that incorporates probabilistic information, allowing for Bayesian network construction.\nExtraction Pipeline: The process of converting unstructured text to structured argument representations.\nBayesian Networks: Probabilistic graphical models that represent variables and their conditional dependencies.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#example-workflow",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#example-workflow",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "C.5 Example Workflow:",
    "text": "C.5 Example Workflow:\n\nLoad a sample ArgDown file from the repository\nExtract the hierarchical structure and relationships\nAdd probabilistic information to create a BayesDown representation\nGenerate a Bayesian network visualization\nAnalyze conditional probabilities and risk pathways",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#troubleshooting",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#troubleshooting",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "C.6 Troubleshooting:",
    "text": "C.6 Troubleshooting:\n\nIf connectivity issues occur, ensure you have access to the GitHub repository\nFor visualization errors, check that all required libraries are properly installed\nWhen processing custom files, ensure they follow the expected format conventions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#prepare-colabpython-environment-import-libraries-packages",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#prepare-colabpython-environment-import-libraries-packages",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "D.1 0.1 Prepare Colab/Python Environment — Import Libraries & Packages",
    "text": "D.1 0.1 Prepare Colab/Python Environment — Import Libraries & Packages\n\n\nCode\n# @title 0.1.0 --- Install & Import Libraries & Packages (One-Time Setup) --- [install_import_libraries]\n\n\"\"\"\nBLOCK PURPOSE:\nEstablishes the core technical environment for the AMTAIR prototype.\nSets up all required libraries for Bayesian network processing, visualization,\nand data manipulation.\nUses a flag-based approach to ensure setup only runs once per session,\nenhancing efficiency.\n\nThe setup follows a three-stage process:\n1. Install required packages not available in Colab by default\n2. Import all necessary libraries with error handling\n3. Set a global flag to prevent redundant execution\n\nDEPENDENCIES: Requires internet connection for package installation\nOUTPUTS: Global variable _setup_imports_done and loaded Python libraries\n\"\"\"\n\n# Check if setup has already been completed in this session using environment flag\ntry:\n    # If this variable exists, setup was already done successfully\n    _setup_imports_done\n    print(\"✅ Libraries already installed and imported in this session. Skipping setup.\")\n\nexcept NameError:\n    print(\"⏳ Performing one-time library installation and imports...\")\n\n    # --- STAGE 1: Install required packages ---\n    # Install visualization and network analysis libraries\n    !pip install -q pyvis  # Network visualization library\n    !apt-get install pandoc -y  # Document conversion utility\n\n    # Install Google API and data processing packages\n    # Data manipulation and Google integration\n    !pip install -q --upgrade gspread pandas google-auth google-colab\n\n    # Install Bayesian network and probabilistic modeling tools\n    !pip install -q pgmpy  # Probabilistic graphical models library\n\n    # Install notebook conversion tools\n    !pip install -q nbconvert  # Often pre-installed, but ensures availability\n\n    print(\"   --&gt; Installations complete.\")\n\n    # --- STAGE 2: Import libraries with error handling ---\n    try:\n        # Network and HTTP libraries\n        import requests      # For making HTTP requests to APIs and GitHub\n        import io            # For handling in-memory file-like objects\n\n        # Data processing libraries\n        import pandas as pd  # For structured data manipulation\n        import numpy as np   # For numerical operations\n        import json          # For JSON parsing and serialization\n        import re            # For regular expression pattern matching\n\n        # Visualization libraries\n        import matplotlib.pyplot as plt  # For creating plots and charts\n        from IPython.display import HTML, display, Markdown  # For rich output in notebook\n\n        # --- Specialized libraries requiring installation ---\n        # Network analysis library\n        import networkx as nx  # For graph representation and analysis\n\n        # Probabilistic modeling libraries\n        from pgmpy.models import BayesianNetwork  # For Bayesian network structure\n        from pgmpy.factors.discrete import TabularCPD  # For conditional probability tables\n        from pgmpy.inference import VariableElimination  # For probabilistic inference\n\n        # Interactive network visualization\n        from pyvis.network import Network  # For interactive network visualization\n\n        # Output version information for key libraries\n        print(f\"      pandas version: {pd.__version__}\")\n        print(f\"      networkx version: {nx.__version__}\")\n        # Add others if specific versions are critical\n\n        print(\"   --&gt; Imports complete.\")\n\n        # --- STAGE 3: Set flag to indicate successful setup ---\n        _setup_imports_done = True\n        print(\"✅ One-time setup finished successfully.\")\n\n    except ImportError as e:\n        # Handle specific import failures\n        print(f\"❌ ERROR during import: {e}\")\n        print(\"   --&gt; Setup did not complete successfully. Please check installations.\")\n    except Exception as e:\n        # Handle unexpected errors\n        print(f\"❌ UNEXPECTED ERROR during setup: {e}\")\n        print(\"   --&gt; Setup did not complete successfully.\")\n\n# Environment is now ready for AMTAIR processing\n\n\n✅ Libraries already installed and imported in this session. Skipping setup.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#connect-to-github-repository",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#connect-to-github-repository",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "D.2 0.2 Connect to GitHub Repository",
    "text": "D.2 0.2 Connect to GitHub Repository\nThe Public GitHub Repo Url in use:\nhttps://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/\nNote: When encountering errors, accessing the data, try using “RAW” Urls.\n\n\nCode\n# @title 0.2.0 --- Connect to GitHub Repository --- Load Files [connect_to_github_repository]\n\n\"\"\"\nBLOCK PURPOSE: Establishes connection to the AMTAIR GitHub repository and provides\nfunctions to load example data files for processing.\n\nThis block creates a reusable function for accessing files from the project's\nGitHub repository, enabling access to example files like the rain-sprinkler-lawn\nBayesian network that serves as our canonical test case.\n\nDEPENDENCIES: requests library, io library\nOUTPUTS: load_file_from_repo function and test file loads\n\"\"\"\n\nfrom requests.exceptions import HTTPError\n\n# Specify the base repository URL for the AMTAIR project\nrepo_url = \"https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/\"\nprint(f\"Connecting to repository: {repo_url}\")\n\ndef load_file_from_repo(relative_path):\n    \"\"\"\n    Loads a file from the specified GitHub repository using a relative path.\n\n    Args:\n        relative_path (str): Path to the file relative to the repo_url\n\n    Returns:\n        For CSV/JSON: pandas DataFrame\n        For MD: string containing file contents\n\n    Raises:\n        HTTPError: If file not found or other HTTP error occurs\n        ValueError: If unsupported file type is requested\n    \"\"\"\n    file_url = repo_url + relative_path\n    print(f\"Attempting to load: {file_url}\")\n\n    # Fetch the file content from GitHub\n    response = requests.get(file_url)\n\n    # Check for bad status codes with enhanced error messages\n    if response.status_code == 404:\n        raise HTTPError(f\"File not found at URL: {file_url}. Check the file path/name and ensure the file is publicly accessible.\", response=response)\n    else:\n        response.raise_for_status()  # Raise for other error codes\n\n    # Convert response to file-like object\n    file_object = io.StringIO(response.text)\n\n    # Process different file types appropriately\n    if relative_path.endswith(\".csv\"):\n        return pd.read_csv(file_object)  # Return DataFrame for CSV\n    elif relative_path.endswith(\".json\"):\n        return pd.read_json(file_object)  # Return DataFrame for JSON\n    elif relative_path.endswith(\".md\"):\n        return file_object.read()  # Return raw content for MD files\n    else:\n        raise ValueError(f\"Unsupported file type: {relative_path.split('.')[-1]}. Add support in the GitHub Connection section of this notebook.\")\n\n# Load example files to test connection\ntry:\n    # Load the extracted data CSV file\n#    df = load_file_from_repo(\"extracted_data.csv\")\n\n    # Load the ArgDown test text\n    md_content = load_file_from_repo(\"ArgDown.md\")\n\n    print(\"✅ Successfully connected to repository and loaded test files.\")\nexcept Exception as e:\n    print(f\"❌ Error loading files: {str(e)}\")\n    print(\"Please check your internet connection and the repository URL.\")\n\n# Display preview of loaded content (commented out to avoid cluttering output)\nprint(md_content)\n\n\nConnecting to repository: https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/\nAttempting to load: https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/ArgDown.md\n✅ Successfully connected to repository and loaded test files.\n[Existential_Catastrophe]: The destruction of humanity's long-term potential due to AI systems we've lost control over. {\"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"]}\n- [Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"]}\n    - [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {\"instantiations\": [\"scale_of_power_seeking_TRUE\", \"scale_of_power_seeking_FALSE\"]}\n        - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n            - [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {\"instantiations\": [\"aps_systems_TRUE\", \"aps_systems_FALSE\"]}\n                - [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {\"instantiations\": [\"advanced_ai_capability_TRUE\", \"advanced_ai_capability_FALSE\"]}\n                - [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {\"instantiations\": [\"agentic_planning_TRUE\", \"agentic_planning_FALSE\"]}\n                - [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {\"instantiations\": [\"strategic_awareness_TRUE\", \"strategic_awareness_FALSE\"]}\n            - [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {\"instantiations\": [\"difficulty_of_alignment_TRUE\", \"difficulty_of_alignment_FALSE\"]}\n                - [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {\"instantiations\": [\"instrumental_convergence_TRUE\", \"instrumental_convergence_FALSE\"]}\n                - [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {\"instantiations\": [\"problems_with_proxies_TRUE\", \"problems_with_proxies_FALSE\"]}\n                - [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {\"instantiations\": [\"problems_with_search_TRUE\", \"problems_with_search_FALSE\"]}\n            - [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"]}\n                - [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {\"instantiations\": [\"incentives_to_build_aps_STRONG\", \"incentives_to_build_aps_WEAK\"]}\n                    - [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {\"instantiations\": [\"usefulness_of_aps_HIGH\", \"usefulness_of_aps_LOW\"]}\n                    - [Competitive_Dynamics]: Competitive pressures between AI developers. {\"instantiations\": [\"competitive_dynamics_STRONG\", \"competitive_dynamics_WEAK\"]}\n                - [Deception_By_AI]: AI systems deceiving humans about their true objectives. {\"instantiations\": [\"deception_by_ai_TRUE\", \"deception_by_ai_FALSE\"]}\n        - [Corrective_Feedback]: Human society implementing corrections after observing problems. {\"instantiations\": [\"corrective_feedback_EFFECTIVE\", \"corrective_feedback_INEFFECTIVE\"]}\n            - [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {\"instantiations\": [\"warning_shots_OBSERVED\", \"warning_shots_UNOBSERVED\"]}\n            - [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {\"instantiations\": [\"rapid_capability_escalation_TRUE\", \"rapid_capability_escalation_FALSE\"]}\n[Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {\"instantiations\": [\"barriers_to_understanding_HIGH\", \"barriers_to_understanding_LOW\"]}\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n[Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {\"instantiations\": [\"adversarial_dynamics_TRUE\", \"adversarial_dynamics_FALSE\"]}\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\n[Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {\"instantiations\": [\"stakes_of_error_HIGH\", \"stakes_of_error_LOW\"]}\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#file-import",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#file-import",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "D.3 0.3 File Import",
    "text": "D.3 0.3 File Import\n\n\nCode\n# @title\nmd_content\n\n\n'[Existential_Catastrophe]: The destruction of humanity\\'s long-term potential due to AI systems we\\'ve lost control over. {\"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"]}\\n- [Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"]}\\n    - [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {\"instantiations\": [\"scale_of_power_seeking_TRUE\", \"scale_of_power_seeking_FALSE\"]}\\n        - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\\n            - [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {\"instantiations\": [\"aps_systems_TRUE\", \"aps_systems_FALSE\"]}\\n                - [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {\"instantiations\": [\"advanced_ai_capability_TRUE\", \"advanced_ai_capability_FALSE\"]}\\n                - [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {\"instantiations\": [\"agentic_planning_TRUE\", \"agentic_planning_FALSE\"]}\\n                - [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {\"instantiations\": [\"strategic_awareness_TRUE\", \"strategic_awareness_FALSE\"]}\\n            - [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {\"instantiations\": [\"difficulty_of_alignment_TRUE\", \"difficulty_of_alignment_FALSE\"]}\\n                - [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {\"instantiations\": [\"instrumental_convergence_TRUE\", \"instrumental_convergence_FALSE\"]}\\n                - [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {\"instantiations\": [\"problems_with_proxies_TRUE\", \"problems_with_proxies_FALSE\"]}\\n                - [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {\"instantiations\": [\"problems_with_search_TRUE\", \"problems_with_search_FALSE\"]}\\n            - [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"]}\\n                - [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {\"instantiations\": [\"incentives_to_build_aps_STRONG\", \"incentives_to_build_aps_WEAK\"]}\\n                    - [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {\"instantiations\": [\"usefulness_of_aps_HIGH\", \"usefulness_of_aps_LOW\"]}\\n                    - [Competitive_Dynamics]: Competitive pressures between AI developers. {\"instantiations\": [\"competitive_dynamics_STRONG\", \"competitive_dynamics_WEAK\"]}\\n                - [Deception_By_AI]: AI systems deceiving humans about their true objectives. {\"instantiations\": [\"deception_by_ai_TRUE\", \"deception_by_ai_FALSE\"]}\\n        - [Corrective_Feedback]: Human society implementing corrections after observing problems. {\"instantiations\": [\"corrective_feedback_EFFECTIVE\", \"corrective_feedback_INEFFECTIVE\"]}\\n            - [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {\"instantiations\": [\"warning_shots_OBSERVED\", \"warning_shots_UNOBSERVED\"]}\\n            - [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {\"instantiations\": [\"rapid_capability_escalation_TRUE\", \"rapid_capability_escalation_FALSE\"]}\\n[Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {\"instantiations\": [\"barriers_to_understanding_HIGH\", \"barriers_to_understanding_LOW\"]}\\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\\n[Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {\"instantiations\": [\"adversarial_dynamics_TRUE\", \"adversarial_dynamics_FALSE\"]}\\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\\n[Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {\"instantiations\": [\"stakes_of_error_HIGH\", \"stakes_of_error_LOW\"]}\\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"]}\\n'",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#sources-to-argdown-structured-argument-extraction",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#sources-to-argdown-structured-argument-extraction",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "E.1 1.0 Sources to ArgDown: Structured Argument Extraction",
    "text": "E.1 1.0 Sources to ArgDown: Structured Argument Extraction\n\nE.1.1 Process Overview\nThis section implements the first major stage of the AMTAIR pipeline: transforming source documents (such as research papers, blog posts, or expert analyses) into structured argument representations using the ArgDown format.\nArgDown is a markdown-like notation for representing arguments in a hierarchical structure. In the context of AMTAIR, it serves as the first step toward creating formal Bayesian networks by: 1. Identifying key variables/statements in the text 2. Capturing their hierarchical relationships 3. Preserving their descriptive content 4. Defining their possible states (instantiations)\nThe extraction process uses Large Language Models (LLMs) to identify the structure and relationships in the text, though in this notebook we focus on processing pre-formatted examples rather than performing the full extraction from raw text.\n\n\nE.1.2 What is ArgDown?\nArgDown uses a simple syntax where: - Statements are represented as [Statement]: Description - Relationships are indicated with + symbols and indentation - Metadata is added in JSON format, including possible states of each variable\nFor example:\n[MainClaim]: Description of the main claim. {\"instantiations\": [\"claim_TRUE\", \"claim_FALSE\"]}\n\n + [SupportingEvidence]: Description of evidence. {\"instantiations\": [\"evidence_TRUE\", \"evidence_FALSE\"]}\nThis structure will later be enhanced with probability information to create BayesDown, which can be transformed into a Bayesian network for analysis and visualization.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#specify-source-document-e.g.-pdf",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#specify-source-document-e.g.-pdf",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "E.2 1.1 Specify Source Document (e.g. PDF)",
    "text": "E.2 1.1 Specify Source Document (e.g. PDF)\nReview the source document, ensure it is suitable for API call and upload to / store it in the correct location.\n\n\nCode\n# @title 1.1.0 --- MTAIR Online Model (Analytica) --- [online_model]\n\nfrom IPython.display import IFrame\n\nIFrame(src=\"https://acp.analytica.com/view0?invite=4560&code=3000289064591444815\", width=\"100%\", height=\"900px\")\n\n\n\n        \n        \nMTAIR Online Model (Analytica)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#generate-argdown-extraction-prompt",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#generate-argdown-extraction-prompt",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "E.3 1.2 Generate ArgDown Extraction Prompt",
    "text": "E.3 1.2 Generate ArgDown Extraction Prompt\nGenerate Extraction Prompt\n\n\nCode\n# @title 1.2.0 --- Prompt Template Function Definitions --- [prompt_template_function]\n\n\"\"\"\nBLOCK PURPOSE: Defines a flexible template system for LLM prompts used in the extraction pipeline.\n\nThis block implements two key classes:\n1. PromptTemplate: A template class supporting variable substitution for dynamic prompts\n2. PromptLibrary: A collection of pre-defined prompt templates for different extraction tasks\n\nThese templates are used in the ArgDown and BayesDown probability extraction\nstages of the pipeline, providing consistent and well-structured prompts to the LLMs.\n\nDEPENDENCIES: string.Template for variable substitution\nOUTPUTS: PromptTemplate and PromptLibrary classes\n\"\"\"\n\nfrom string import Template\nfrom typing import Dict, Optional, Union, List\n\nclass PromptTemplate:\n    \"\"\"Template system for LLM prompts with variable substitution\"\"\"\n\n    def __init__(self, template: str):\n        \"\"\"Initialize with template string using $variable format\"\"\"\n        self.template = Template(template)\n\n    def format(self, **kwargs) -&gt; str:\n        \"\"\"Substitute variables in the template\"\"\"\n        return self.template.safe_substitute(**kwargs)\n\n    @classmethod\n    def from_file(cls, filepath: str) -&gt; 'PromptTemplate':\n        \"\"\"Load template from a file\"\"\"\n        with open(filepath, 'r') as f:\n            template = f.read()\n        return cls(template)\n\nclass PromptLibrary:\n    \"\"\"Collection of prompt templates for different extraction tasks\"\"\"\n\n    # ArgDown extraction prompt - transforms source text into structured argument map\n    ARGDOWN_EXTRACTION = PromptTemplate(\"\"\"\nYou are participating in the AMTAIR (Automating Transformative AI Risk Modeling)\nproject and you are tasked with converting natural language arguments into\nArgDown syntax by extracting and formalizing causal world models from\nunstructured text.\nYour specific task is to extract the implicit causal model from the provided\ndocument in structured ArgDown format.\n\n## Epistemic Foundation & Purpose\n\nThis extraction represents one possible interpretation of the implicit causal\nmodel in the document. Multiple extractions from the same text help reveal\npatterns of convergence (where the model is clearly articulated) and\ndivergence (where the model contains ambiguities). This approach acknowledges\nthat expert texts often contain implicit rather than explicit causal models.\n\nYour role is to reveal the causal structure already present in the author's\nthinking, maintaining epistemic humility about your interpretation while\nadhering strictly to the required format.\n\n## ArgDown Format Specification\n\n### Core Syntax\n\nArgDown represents causal relationships using a hierarchical structure:\n\n1. Variables appear in square brackets with descriptive text:\n   `[Variable_Name]: Description of the variable.`\n\n2. Causal relationships use indentation (2 spaces per level) and '+' symbols:\n\n[Effect]: Description of effect. + [Cause]: Description of cause. + [Deeper_Cause]: Description of deeper cause.\n\n3. Causality flows from bottom (more indented) to top (less indented):\n- More indented variables (causes) influence less indented variables (effects)\n- The top-level variable is the ultimate effect or outcome\n- Deeper indentation levels represent root causes or earlier factors\n\n4. Each variable must include JSON metadata with possible states (instantiations):\n`[Variable]: Description. {\"instantiations\": [\"variable_STATE1\", \"variable_STATE2\"]}`\n\n### JSON Metadata Format\n\nThe JSON metadata must follow this exact structure:\n\n```json\n{\"instantiations\": [\"variable_STATE1\", \"variable_STATE2\"]}\n\nRequirements:\n* Double quotes (not single) around field names and string values\n* Square brackets enclosing the instantiations array\n* Comma separation between array elements\n* No trailing comma after the last element\n* Must be valid JSON syntax that can be parsed by standard JSON parsers\n\nFor binary variables (most common case):\n{\"instantiations\": [\"variable_TRUE\", \"variable_FALSE\"]}\n\nFor multi-state variables (when clearly specified in the text):\n{\"instantiations\": [\"variable_HIGH\", \"variable_MEDIUM\", \"variable_LOW\"]}\n\nThe metadata must appear on the same line as the variable definition, after the description.\n## Complex Structural Patterns\n### Variables Influencing Multiple Effects\nThe same variable can appear multiple times in different places in the hierarchy if it influences multiple effects:\n[Effect1]: First effect description. {\"instantiations\": [\"effect1_TRUE\", \"effect1_FALSE\"]}\n  + [Cause_A]: Description of cause A. {\"instantiations\": [\"cause_a_TRUE\", \"cause_a_FALSE\"]}\n\n[Effect2]: Second effect description. {\"instantiations\": [\"effect2_TRUE\", \"effect2_FALSE\"]}\n  + [Cause_A]\n  + [Cause_B]: Description of cause B. {\"instantiations\": [\"cause_b_TRUE\", \"cause_b_FALSE\"]}\n\n### Multiple Causes of the Same Effect\nMultiple causes can influence the same effect by being listed at the same indentation level:\n[Effect]: Description of effect. {\"instantiations\": [\"effect_TRUE\", \"effect_FALSE\"]}\n  + [Cause1]: Description of first cause. {\"instantiations\": [\"cause1_TRUE\", \"cause1_FALSE\"]}\n  + [Cause2]: Description of second cause. {\"instantiations\": [\"cause2_TRUE\", \"cause2_FALSE\"]}\n    + [Deeper_Cause]: A cause that influences Cause2. {\"instantiations\": [\"deeper_cause_TRUE\", \"deeper_cause_FALSE\"]}\n\n### Causal Chains\nCausal chains are represented through multiple levels of indentation:\n[Ultimate_Effect]: The final outcome. {\"instantiations\": [\"ultimate_effect_TRUE\", \"ultimate_effect_FALSE\"]}\n  + [Intermediate_Effect]: A mediating variable. {\"instantiations\": [\"intermediate_effect_TRUE\", \"intermediate_effect_FALSE\"]}\n    + [Root_Cause]: The initial cause. {\"instantiations\": [\"root_cause_TRUE\", \"root_cause_FALSE\"]}\n  + [2nd_Intermediate_Effect]: A mediating variable. {\"instantiations\": [\"intermediate_effect_TRUE\", \"intermediate_effect_FALSE\"]}\n\n\n### Common Cause of Multiple Variables\nA common cause affecting multiple variables is represented by referencing the same variable in multiple places:\n[Effect1]: First effect description. {\"instantiations\": [\"effect1_TRUE\", \"effect1_FALSE\"]}\n  + [Common_Cause]: Description of common cause. {\"instantiations\": [\"common_cause_TRUE\", \"common_cause_FALSE\"]}\n\n[Effect2]: Second effect description. {\"instantiations\": [\"effect2_TRUE\", \"effect2_FALSE\"]}\n  + [Common_Cause]\n\n## Detailed Extraction Workflow\nPlease follow this step-by-step process, documenting your reasoning in XML tags:\n&lt;analysis&gt;\nFirst, conduct a holistic analysis of the document:\n1. Identify the main subject matter or domain\n2. Note key concepts, variables, and factors discussed\n3. Pay attention to language indicating causal relationships (causes, affects, influences, depends on, etc.)\n4. Look for the ultimate outcomes or effects that are the focus of the document\n5. Record your general understanding of the document's implicit causal structure\n&lt;/analysis&gt;\n&lt;variable_identification&gt;\nNext, identify and list the key variables in the causal model:\n* Focus on factors that are discussed as having an influence or being influenced\n* For each variable:\n  * Create a descriptive name in [square_brackets]\n  * Write a concise description based directly on the text\n  * Determine possible states (usually binary TRUE/FALSE unless clearly specified)\n* Distinguish between:\n  * Outcome variables (effects the author is concerned with)\n  * Intermediate variables (both causes and effects in chains)\n  * Root cause variables (exogenous factors in the model)\n* List all identified variables with their descriptions and possible states\n&lt;/variable_identification&gt;\n\n&lt;causal_structure&gt;\nThen, determine the causal relationships between variables:\n* For each variable, identify what factors influence it\n* Note the direction of causality (what causes what)\n* Look for mediating variables in causal chains\n* Identify common causes of multiple effects\n* Capture feedback loops if present (though they must be represented as DAGs)\n* Map out the hierarchical structure of the causal model\n&lt;/causal_structure&gt;\n\n&lt;format_conversion&gt;\nNow, convert your analysis into proper ArgDown format:\n* Start with the ultimate outcome variables at the top level\n* Place direct causes indented below with \\+ symbols\n* Continue with deeper causes at further indentation levels\n* Add variable descriptions and instantiations metadata\n* Ensure variables appearing in multiple places have consistent names\n* Check that the entire structure forms a valid directed acyclic graph\n&lt;/format_conversion&gt;\n\n&lt;validation&gt;\n\nFinally, review your extraction for quality and format correctness:\n1. Verify all variables have properly formatted metadata\n2. Check that indentation properly represents causal direction\n3. Confirm the extraction accurately reflects the document's implicit model\n4. Ensure no cycles exist in the causal structure\n5. Verify that variables referenced multiple times are consistent\n6. Check that the extraction would be useful for subsequent analysis\n\n&lt;/validation&gt;\n\n\n## Source Document Analysis Guidance\nWhen analyzing the source document:\n* Focus on revealing the author's own causal model, not imposing an external framework\n* Maintain the author's terminology where possible\n* Look for both explicit statements of causality and implicit assumptions\n* Pay attention to the relative importance the author assigns to different factors\n* Notice where the author expresses certainty versus uncertainty\n* Consider the level of granularity appropriate to the document's own analysis\n\nRemember that your goal is to make the implicit model explicit, not to evaluate or improve it.\nThe value lies in accurately representing the author's perspective, even if you might personally disagree or see limitations in their model.\n\n\"\"\")\n\n    # BayesDown probability extraction prompt - enhances ArgDown with probability information\n    BAYESDOWN_EXTRACTION = PromptTemplate(\"\"\"\nYou are an expert in probabilistic reasoning and Bayesian networks. Your task is\nto extend the provided ArgDown structure with probability information,\ncreating a BayesDown representation.\n\nFor each statement in the ArgDown structure, you need to:\n1. Estimate prior probabilities for each possible state\n2. Estimate conditional probabilities given parent states\n3. Maintain the original structure and relationships\n\nHere is the format to follow:\n[Node]: Description. { \"instantiations\": [\"node_TRUE\", \"node_FALSE\"], \"priors\": { \"p(node_TRUE)\": \"0.7\", \"p(node_FALSE)\": \"0.3\" }, \"posteriors\": { \"p(node_TRUE|parent_TRUE)\": \"0.9\", \"p(node_TRUE|parent_FALSE)\": \"0.4\", \"p(node_FALSE|parent_TRUE)\": \"0.1\", \"p(node_FALSE|parent_FALSE)\": \"0.6\" } }\n [Parent]: Parent description. {...}\n\n\nHere are the specific probability questions to answer:\n$questions\n\nArgDown structure to enhance:\n$argdown\n\nProvide the complete BayesDown representation with probabilities:\n\"\"\")\n\n    @classmethod\n    def get_template(cls, template_name: str) -&gt; PromptTemplate:\n        \"\"\"Get a prompt template by name\"\"\"\n        if hasattr(cls, template_name):\n            return getattr(cls, template_name)\n        else:\n            raise ValueError(f\"Template not found: {template_name}\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#prepare-llm-api-call",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#prepare-llm-api-call",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "E.4 1.3 Prepare LLM API Call",
    "text": "E.4 1.3 Prepare LLM API Call\nCombine Systemprompt + API Specifications + ArgDown Instructions + Prompt + Source PDF for API Call\n\n\nCode\n# @title 1.3.0 --- Provider-Agnostic LLM API Interface --- [provider_agnostic-interface]\n\n\"\"\"\nBLOCK PURPOSE: Provides a unified interface for interacting with different LLM providers.\n\nThis block implements a flexible, provider-agnostic system for making LLM API calls:\n1. Base abstract class (LLMProvider) defining the common interface\n2. Implementation classes for specific providers (OpenAI and Anthropic)\n3. Factory class for creating appropriate provider instances\n\nThis abstraction allows the extraction pipeline to work with different LLM providers\nwithout changing the core code, supporting both current and future LLM backends.\n\nDEPENDENCIES: requests for API calls, os for environment variables, abstract base classes\nOUTPUTS: LLMProvider abstract class and concrete implementations for OpenAI and Anthropic\n\"\"\"\n\nimport os\nimport json\nimport time\nimport requests\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Union, Any\nfrom dataclasses import dataclass\n\n@dataclass\nclass LLMResponse:\n    \"\"\"Standard response object for LLM completions\"\"\"\n    content: str            # The generated text response\n    model: str              # The model used for generation\n    usage: Dict[str, int]   # Token usage statistics\n    raw_response: Dict[str, Any]  # Complete provider-specific response\n    created_at: float = time.time()  # Timestamp of response creation\n\nclass LLMProvider(ABC):\n    \"\"\"Abstract base class for LLM providers\"\"\"\n\n    @abstractmethod\n    def complete(self,\n                prompt: str,\n                system_prompt: Optional[str] = None,\n                temperature: float = 0.7,\n                max_tokens: int = 4000) -&gt; LLMResponse:\n        \"\"\"Generate a completion from the LLM\"\"\"\n        pass\n\n    @abstractmethod\n    def get_available_models(self) -&gt; List[str]:\n        \"\"\"Return a list of available models from this provider\"\"\"\n        pass\n\nclass OpenAIProvider(LLMProvider):\n    \"\"\"OpenAI API implementation\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, organization: Optional[str] = None):\n        \"\"\"Initialize with API key from args or environment\"\"\"\n        self.api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key is required. Provide as argument \"\n              + \"or set OPENAI_API_KEY environment variable.\")\n\n        self.organization = organization or os.environ.get(\"OPENAI_ORGANIZATION\")\n        self.api_base = \"https://api.openai.com/v1\"\n\n    def complete(self,\n                prompt: str,\n                system_prompt: Optional[str] = None,\n                model: str = \"gpt-4-turbo\",\n                temperature: float = 0.7,\n                max_tokens: int = 4000) -&gt; LLMResponse:\n        \"\"\"Generate a completion using OpenAI's API\"\"\"\n\n        # Prepare request headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        if self.organization:\n            headers[\"OpenAI-Organization\"] = self.organization\n\n        # Create message structure\n        messages = []\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n        # Prepare request data\n        data = {\n            \"model\": model,\n            \"messages\": messages,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens\n        }\n\n        # Make API call\n        response = requests.post(\n            f\"{self.api_base}/chat/completions\",\n            headers=headers,\n            json=data\n        )\n\n        response.raise_for_status()\n        result = response.json()\n\n        # Transform into standardized response format\n        return LLMResponse(\n            content=result[\"choices\"][0][\"message\"][\"content\"],\n            model=result[\"model\"],\n            usage=result[\"usage\"],\n            raw_response=result\n        )\n\n    def get_available_models(self) -&gt; List[str]:\n        \"\"\"Return a list of available OpenAI models\"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        if self.organization:\n            headers[\"OpenAI-Organization\"] = self.organization\n\n        response = requests.get(\n            f\"{self.api_base}/models\",\n            headers=headers\n        )\n\n        response.raise_for_status()\n        models = response.json()[\"data\"]\n        return [model[\"id\"] for model in models]\n\nclass AnthropicProvider(LLMProvider):\n    \"\"\"Anthropic Claude API implementation\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None):\n        \"\"\"Initialize with API key from args or environment\"\"\"\n        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Anthropic API key is required. Provide as\"\n              + \" argument or set ANTHROPIC_API_KEY environment variable.\")\n\n        self.api_base = \"https://api.anthropic.com/v1\"\n\n    def complete(self,\n                prompt: str,\n                system_prompt: Optional[str] = None,\n                model: str = \"claude-3-opus-20240229\",\n                temperature: float = 0.7,\n                max_tokens: int = 4000) -&gt; LLMResponse:\n        \"\"\"Generate a completion using Anthropic's API\"\"\"\n\n        # Prepare request headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"X-API-Key\": self.api_key,\n            \"anthropic-version\": \"2023-06-01\"\n        }\n\n        # Prepare request data in Anthropic-specific format\n        data = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens\n        }\n\n        # Add system prompt if provided (Anthropic uses a different format)\n        if system_prompt:\n            data[\"system\"] = system_prompt\n\n        # Make API call\n        response = requests.post(\n            f\"{self.api_base}/messages\",\n            headers=headers,\n            json=data\n        )\n\n        response.raise_for_status()\n        result = response.json()\n\n        # Transform into standardized response format\n        return LLMResponse(\n            content=result[\"content\"][0][\"text\"],\n            model=result[\"model\"],\n            usage={\"prompt_tokens\": result.get(\"usage\", {}).get(\"input_tokens\", 0),\n                   \"completion_tokens\": result.get(\"usage\", {}).get(\"output_tokens\", 0)},\n            raw_response=result\n        )\n\n    def get_available_models(self) -&gt; List[str]:\n        \"\"\"Return a list of available Anthropic models\"\"\"\n        # Anthropic doesn't have a models endpoint, so we return a static list\n        return [\n            \"claude-3-opus-20240229\",\n            \"claude-3-sonnet-20240229\",\n            \"claude-3-haiku-20240307\"\n        ]\n\nclass LLMFactory:\n    \"\"\"Factory for creating LLM providers\"\"\"\n\n    @staticmethod\n    def create_provider(provider_name: str, **kwargs) -&gt; LLMProvider:\n        \"\"\"Create and return an LLM provider instance\"\"\"\n        if provider_name.lower() == \"openai\":\n            return OpenAIProvider(**kwargs)\n        elif provider_name.lower() == \"anthropic\":\n            return AnthropicProvider(**kwargs)\n        else:\n            raise ValueError(f\"Unsupported provider: {provider_name}\")\n\n\n\n\nCode\n# @title 1.3.1 --- API Call Function Definitions --- [api_call_function_definitions]\n\n\"\"\"\nBLOCK PURPOSE: Provides core functions for extracting ArgDown representations from text using LLMs.\n\nThis block implements the main extraction functionality:\n1. extract_argdown_from_text: Sends text to LLM to extract structured ArgDown representation\n2. validate_argdown: Verifies the extracted ArgDown for correctness and completeness\n3. process_source_document: Handles source files (PDF, TXT, MD) and manages extraction\n4. save_argdown_extraction: Saves extraction results with metadata for further processing\n\nThese functions form the first stage of the AMTAIR pipeline, transforming\nunstructured text into structured argument representations.\n\nDEPENDENCIES: LLMFactory from previous cell, re for pattern matching\nOUTPUTS: Functions for ArgDown extraction, validation, and storage\n\"\"\"\n\ndef extract_argdown_from_text(text: str, provider_name: str = \"openai\", model: str = None) -&gt; str:\n    \"\"\"\n    Extract ArgDown representation from text using LLM\n\n    Args:\n        text: The source text to extract arguments from\n        provider_name: The LLM provider to use (openai or anthropic)\n        model: Specific model to use, or None for default\n\n    Returns:\n        Extracted ArgDown representation\n    \"\"\"\n    # Create LLM provider\n    provider = LLMFactory.create_provider(provider_name)\n\n    # Get extraction prompt\n    prompt_template = PromptLibrary.get_template(\"ARGDOWN_EXTRACTION\")\n    prompt = prompt_template.format(text=text)\n\n    # Set model-specific parameters\n    if provider_name.lower() == \"openai\":\n        model = model or \"gpt-4-turbo\"\n        temperature = 0.3  # Lower temperature for more deterministic extraction\n        max_tokens = 4000\n    elif provider_name.lower() == \"anthropic\":\n        model = model or \"claude-3-opus-20240229\"\n        temperature = 0.2\n        max_tokens = 4000\n\n    # Call the LLM\n    system_prompt = \"You are an expert in argument mapping and causal reasoning.\"\n    response = provider.complete(\n        prompt=prompt,\n        system_prompt=system_prompt,\n        model=model,\n        temperature=temperature,\n        max_tokens=max_tokens\n    )\n\n    # Extract the ArgDown content (remove any markdown code blocks if present)\n    argdown_content = response.content\n    if \"```\" in argdown_content:\n        # Extract content between code blocks if present\n        import re\n        matches = re.findall(r\"```(?:argdown)?\\n([\\s\\S]*?)\\n```\", argdown_content)\n        if matches:\n            argdown_content = matches[0]\n\n    return argdown_content\n\ndef validate_argdown(argdown_text: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Validate ArgDown representation to ensure it's well-formed\n\n    Args:\n        argdown_text: ArgDown representation to validate\n\n    Returns:\n        Dictionary with validation results\n    \"\"\"\n    # Initialize validation results\n    results = {\n        \"is_valid\": True,\n        \"errors\": [],\n        \"warnings\": [],\n        \"stats\": {\n            \"node_count\": 0,\n            \"relationship_count\": 0,\n            \"max_depth\": 0\n        }\n    }\n\n    # Basic syntax checks\n    lines = argdown_text.split(\"\\n\")\n    node_pattern = r'\\[(.*?)\\]:'\n    instantiation_pattern = r'{\"instantiations\":'\n\n    # Track nodes and relationships\n    nodes = set()\n    relationships = []\n    current_depth = 0\n    max_depth = 0\n\n    for i, line in enumerate(lines):\n        # Skip empty lines\n        if not line.strip():\n            continue\n\n        # Calculate indentation depth\n        indent = 0\n        if '+' in line:\n            indent = line.find('+') // 2\n\n        current_depth = indent\n        max_depth = max(max_depth, current_depth)\n\n        # Check for node definitions\n        import re\n        node_matches = re.findall(node_pattern, line)\n        if node_matches:\n            node = node_matches[0]\n            nodes.add(node)\n            results[\"stats\"][\"node_count\"] += 1\n\n            # Check for instantiations\n            if instantiation_pattern not in line:\n                results[\"warnings\"].append(f\"Line {i+1}: Node '{node}' is missing instantiations metadata\")\n\n        # Check parent-child relationships\n        if indent &gt; 0 and '+' in line and node_matches:\n            # This is a child node; find its parent\n            parent_indent = indent - 1\n            j = i - 1\n            while j &gt;= 0:\n                if '+' in lines[j] and lines[j].find('+') // 2 == parent_indent:\n                    parent_matches = re.findall(node_pattern, lines[j])\n                    if parent_matches:\n                        parent = parent_matches[0]\n                        relationships.append((parent, node))\n                        results[\"stats\"][\"relationship_count\"] += 1\n                        break\n                j -= 1\n\n    results[\"stats\"][\"max_depth\"] = max_depth\n\n    # If we didn't find any nodes, that's a problem\n    if results[\"stats\"][\"node_count\"] == 0:\n        results[\"is_valid\"] = False\n        results[\"errors\"].append(\"No valid nodes found in ArgDown representation\")\n\n    return results\n\ndef process_source_document(file_path: str, provider_name: str = \"openai\") -&gt; Dict[str, Any]:\n    \"\"\"\n    Process a source document to extract ArgDown representation\n\n    Args:\n        file_path: Path to the source document\n        provider_name: The LLM provider to use\n\n    Returns:\n        Dictionary with extraction results\n    \"\"\"\n    # Load the source document\n    text = \"\"\n    if file_path.endswith(\".pdf\"):\n        # PDF handling requires additional libraries\n        try:\n            import PyPDF2\n            with open(file_path, 'rb') as file:\n                reader = PyPDF2.PdfReader(file)\n                text = \"\"\n                for page in reader.pages:\n                    text += page.extract_text() + \"\\n\"\n        except ImportError:\n            raise ImportError(\"PyPDF2 is required for PDF processing. \"\n              + \"Install it with: pip install PyPDF2\")\n    elif file_path.endswith(\".txt\"):\n        with open(file_path, 'r') as file:\n            text = file.read()\n    elif file_path.endswith(\".md\"):\n        with open(file_path, 'r') as file:\n            text = file.read()\n    else:\n        raise ValueError(f\"Unsupported file format: {file_path}\")\n\n    # Extract ArgDown\n    argdown_content = extract_argdown_from_text(text, provider_name)\n\n    # Validate the extraction\n    validation_results = validate_argdown(argdown_content)\n\n    # Prepare results\n    results = {\n        \"source_path\": file_path,\n        \"extraction_timestamp\": time.time(),\n        \"argdown_content\": argdown_content,\n        \"validation\": validation_results,\n        \"provider\": provider_name\n    }\n\n    return results\n\ndef save_argdown_extraction(results: Dict[str, Any], output_path: str) -&gt; None:\n    \"\"\"\n    Save ArgDown extraction results\n\n    Args:\n        results: Extraction results dictionary\n        output_path: Path to save the results\n    \"\"\"\n    # Save the ArgDown content\n    with open(output_path, 'w') as file:\n        file.write(results[\"argdown_content\"])\n\n    # Save metadata alongside\n    metadata_path = output_path.replace('.md', '_metadata.json')\n    metadata = {\n        \"source_path\": results[\"source_path\"],\n        \"extraction_timestamp\": results[\"extraction_timestamp\"],\n        \"validation\": results[\"validation\"],\n        \"provider\": results[\"provider\"]\n    }\n\n    with open(metadata_path, 'w') as file:\n        json.dump(metadata, file, indent=2)\n\n\n\n\nCode\n# @title 1.3.2 --- Prepare LLM API Call --- [prepare_api_call]\n\n\"\"\"\nBLOCK PURPOSE: Prepares parameters for LLM API calls used in ArgDown extraction.\n\nThis function handles the configuration for LLM API calls, including:\n1. Source document path validation\n2. LLM provider selection and validation\n3. Model selection with appropriate defaults\n\nThe function returns a configuration dictionary that can be passed to the\nextraction function in the next step of the pipeline.\n\nDEPENDENCIES: None (uses standard Python functionality)\nOUTPUTS: Dictionary with extraction configuration parameters\n\"\"\"\n\ndef prepare_extraction_call(source_path, provider_name=\"openai\", model=None):\n    \"\"\"\n    Prepare the LLM API call for ArgDown extraction\n\n    Args:\n        source_path (str): Path to the source document to extract from\n        provider_name (str): LLM provider to use ('openai' or 'anthropic')\n        model (str, optional): Specific model to use. Defaults to None (uses provider's default).\n\n    Returns:\n        dict: Configuration parameters for extraction\n\n    Raises:\n        ValueError: If an unsupported provider is specified\n    \"\"\"\n    # Load the source document\n    print(f\"Processing source document: {source_path}\")\n\n    # Determine provider and model\n    provider = provider_name.lower()\n    if provider not in [\"openai\", \"anthropic\"]:\n        raise ValueError(f\"Unsupported provider: {provider}. Use 'openai' or 'anthropic'.\")\n\n    # Set default model if none provided\n    if model is None:\n        if provider == \"openai\":\n            model = \"gpt-4-turbo\"\n        elif provider == \"anthropic\":\n            model = \"claude-3-opus-20240229\"\n\n    # Print configuration\n    print(f\"Using provider: {provider}\")\n    print(f\"Selected model: {model}\")\n\n    return {\n        \"source_path\": source_path,\n        \"provider\": provider,\n        \"model\": model\n    }\n\n# Usage example:\nsource_path = \"example_document.pdf\"  # Replace with actual document path\nextraction_config = prepare_extraction_call(source_path, provider_name=\"openai\")\n\n\nProcessing source document: example_document.pdf\nUsing provider: openai\nSelected model: gpt-4-turbo",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#make-argdown-extraction-llm-api-call",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#make-argdown-extraction-llm-api-call",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "E.5 1.4 Make ArgDown Extraction LLM API Call",
    "text": "E.5 1.4 Make ArgDown Extraction LLM API Call\n\n\nCode\n# @title 1.4.0 --- Make ArgDown Extraction LLM API Call --- [extraction_api_call]\n\n\"\"\"\nBLOCK PURPOSE: Executes the ArgDown extraction process using the LLM API.\n\nThis function performs the actual extraction of ArgDown representations from\nsource documents:\n1. Takes the configuration parameters prepared in the previous step\n2. Processes the document using the LLM API\n3. Validates the extraction results\n4. Provides timing and statistics about the extraction\n\nThe extraction process transforms unstructured text into a structured argument\nrepresentation following the ArgDown syntax defined in the AMTAIR project.\n\nDEPENDENCIES: process_source_document function from previous cells\nOUTPUTS: Dictionary with extraction results including ArgDown content and validation info\n\"\"\"\n\ndef execute_extraction(extraction_config):\n    \"\"\"\n    Execute the ArgDown extraction using the LLM API\n\n    Args:\n        extraction_config (dict): Configuration parameters for extraction\n\n    Returns:\n        dict: Extraction results including ArgDown content and validation info\n\n    Raises:\n        Exception: For any errors during extraction\n    \"\"\"\n    print(f\"Starting extraction from {extraction_config['source_path']}\")\n    start_time = time.time()\n\n    try:\n        # Process the document\n        results = process_source_document(\n            extraction_config[\"source_path\"],\n            provider_name=extraction_config[\"provider\"]\n        )\n\n        # Print success message\n        elapsed_time = time.time() - start_time\n        print(f\"Extraction completed in {elapsed_time:.2f} seconds\")\n        print(f\"Extracted {results['validation']['stats']['node_count']} nodes with \"\n              f\"{results['validation']['stats']['relationship_count']} relationships\")\n\n        # Print any warnings\n        if results['validation']['warnings']:\n            print(\"\\nWarnings:\")\n            for warning in results['validation']['warnings']:\n                print(f\"- {warning}\")\n\n        return results\n\n    except Exception as e:\n        print(f\"Error during extraction: {str(e)}\")\n        raise\n\n# Usage example:\nextraction_results = execute_extraction(extraction_config)\n\n\nStarting extraction from example_document.pdf\nError during extraction: PyPDF2 is required for PDF processing. Install it with: pip install PyPDF2\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-19-fd592eb962ab&gt; in process_source_document(file_path, provider_name)\n    166         try:\n--&gt; 167             import PyPDF2\n    168             with open(file_path, 'rb') as file:\n\nModuleNotFoundError: No module named 'PyPDF2'\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-21-27555067c1d2&gt; in &lt;cell line: 0&gt;()\n     59 \n     60 # Usage example:\n---&gt; 61 extraction_results = execute_extraction(extraction_config)\n\n&lt;ipython-input-21-27555067c1d2&gt; in execute_extraction(extraction_config)\n     35     try:\n     36         # Process the document\n---&gt; 37         results = process_source_document(\n     38             extraction_config[\"source_path\"],\n     39             provider_name=extraction_config[\"provider\"]\n\n&lt;ipython-input-19-fd592eb962ab&gt; in process_source_document(file_path, provider_name)\n    172                     text += page.extract_text() + \"\\n\"\n    173         except ImportError:\n--&gt; 174             raise ImportError(\"PyPDF2 is required for PDF processing. Install it with: pip install PyPDF2\")\n    175     elif file_path.endswith(\".txt\"):\n    176         with open(file_path, 'r') as file:\n\nImportError: PyPDF2 is required for PDF processing. Install it with: pip install PyPDF2\n\n---------------------------------------------------------------------------\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n---------------------------------------------------------------------------",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#save-argdown-extraction-response",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#save-argdown-extraction-response",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "E.6 1.5 Save ArgDown Extraction Response",
    "text": "E.6 1.5 Save ArgDown Extraction Response\n\nSave and log API return\nSave ArgDown.md file for further Proecessing\n\n\n\nCode\n# @title 1.5.0 --- Save ArgDown Extraction Response --- [save_extraction_response]\n\n\"\"\"\nBLOCK PURPOSE: Saves the extracted ArgDown content to files for further processing.\n\nThis function handles saving the extraction results:\n1. Creates an output directory if it doesn't exist\n2. Saves the extracted ArgDown content with a timestamp in the filename\n3. Saves accompanying metadata in a JSON file\n4. Saves a copy at a standard location for the next steps in the pipeline\n5. Provides a preview of the extracted content\n\nThe saved files serve as inputs for the next stage of the pipeline where\nprobability information will be added to create BayesDown.\n\nDEPENDENCIES: os module for directory operations\nOUTPUTS: Saved ArgDown files and preview of extracted content\n\"\"\"\n\ndef save_extraction_results(results, output_directory=\"./outputs\"):\n    \"\"\"\n    Save the extraction results to file\n\n    Args:\n        results (dict): Extraction results from execute_extraction\n        output_directory (str): Directory to save results\n\n    Returns:\n        str: Path to the saved ArgDown file\n    \"\"\"\n    # Ensure output directory exists\n    import os\n    os.makedirs(output_directory, exist_ok=True)\n\n    # Create base filename from source\n    import os.path\n    base_name = os.path.basename(results[\"source_path\"]).split('.')[0]\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    output_filename = f\"{base_name}_argdown_{timestamp}.md\"\n    output_path = os.path.join(output_directory, output_filename)\n\n    # Save the results\n    save_argdown_extraction(results, output_path)\n\n    print(f\"Saved ArgDown extraction to: {output_path}\")\n    print(f\"Metadata saved to: {output_path.replace('.md', '_metadata.json')}\")\n\n    # Also save to standard location for further processing\n    standard_path = os.path.join(output_directory, \"ArgDown.md\")\n    with open(standard_path, 'w') as f:\n        f.write(results[\"argdown_content\"])\n    print(f\"Also saved to standard location: {standard_path}\")\n\n    return output_path\n\n# Usage example:\noutput_path = save_extraction_results(extraction_results)\n\n# Preview the extracted ArgDown\nfrom IPython.display import Markdown, display\n\n# Display the first 500 characters of the extracted ArgDown\npreview = extraction_results[\"argdown_content\"][:500] + \"...\" if len(extraction_results[\"argdown_content\"]) &gt; 500 else extraction_results[\"argdown_content\"]\ndisplay(Markdown(f\"## Extracted ArgDown Preview\\n\\n```\\n{preview}\\n```\"))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-57-84ee4ea64739&gt; in &lt;cell line: 0&gt;()\n     55 \n     56 # Usage example:\n---&gt; 57 output_path = save_extraction_results(extraction_results)\n     58 \n     59 # Preview the extracted ArgDown\n\nNameError: name 'extraction_results' is not defined",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#review-and-check-argdown.md-file",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#review-and-check-argdown.md-file",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "E.7 1.6 Review and Check ArgDown.md File",
    "text": "E.7 1.6 Review and Check ArgDown.md File\n\n\nCode\ndisplay(Markdown(md_content))\n\n\n[Existential_Catastrophe]: The destruction of humanity’s long-term potential due to AI systems we’ve lost control over. {“instantiations”: [“existential_catastrophe_TRUE”, “existential_catastrophe_FALSE”]} - [Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {“instantiations”: [“human_disempowerment_TRUE”, “human_disempowerment_FALSE”]} - [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {“instantiations”: [“scale_of_power_seeking_TRUE”, “scale_of_power_seeking_FALSE”]} - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {“instantiations”: [“misaligned_power_seeking_TRUE”, “misaligned_power_seeking_FALSE”]} - [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {“instantiations”: [“aps_systems_TRUE”, “aps_systems_FALSE”]} - [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {“instantiations”: [“advanced_ai_capability_TRUE”, “advanced_ai_capability_FALSE”]} - [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {“instantiations”: [“agentic_planning_TRUE”, “agentic_planning_FALSE”]} - [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {“instantiations”: [“strategic_awareness_TRUE”, “strategic_awareness_FALSE”]} - [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {“instantiations”: [“difficulty_of_alignment_TRUE”, “difficulty_of_alignment_FALSE”]} - [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {“instantiations”: [“instrumental_convergence_TRUE”, “instrumental_convergence_FALSE”]} - [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {“instantiations”: [“problems_with_proxies_TRUE”, “problems_with_proxies_FALSE”]} - [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {“instantiations”: [“problems_with_search_TRUE”, “problems_with_search_FALSE”]} - [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {“instantiations”: [“deployment_decisions_DEPLOY”, “deployment_decisions_WITHHOLD”]} - [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {“instantiations”: [“incentives_to_build_aps_STRONG”, “incentives_to_build_aps_WEAK”]} - [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {“instantiations”: [“usefulness_of_aps_HIGH”, “usefulness_of_aps_LOW”]} - [Competitive_Dynamics]: Competitive pressures between AI developers. {“instantiations”: [“competitive_dynamics_STRONG”, “competitive_dynamics_WEAK”]} - [Deception_By_AI]: AI systems deceiving humans about their true objectives. {“instantiations”: [“deception_by_ai_TRUE”, “deception_by_ai_FALSE”]} - [Corrective_Feedback]: Human society implementing corrections after observing problems. {“instantiations”: [“corrective_feedback_EFFECTIVE”, “corrective_feedback_INEFFECTIVE”]} - [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {“instantiations”: [“warning_shots_OBSERVED”, “warning_shots_UNOBSERVED”]} - [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {“instantiations”: [“rapid_capability_escalation_TRUE”, “rapid_capability_escalation_FALSE”]} [Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {“instantiations”: [“barriers_to_understanding_HIGH”, “barriers_to_understanding_LOW”]} - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {“instantiations”: [“misaligned_power_seeking_TRUE”, “misaligned_power_seeking_FALSE”]} [Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {“instantiations”: [“adversarial_dynamics_TRUE”, “adversarial_dynamics_FALSE”]} - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {“instantiations”: [“misaligned_power_seeking_TRUE”, “misaligned_power_seeking_FALSE”]} [Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {“instantiations”: [“stakes_of_error_HIGH”, “stakes_of_error_LOW”]} - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {“instantiations”: [“misaligned_power_seeking_TRUE”, “misaligned_power_seeking_FALSE”]}\n\n\n\nE.7.1 1.6.0 Check the Graph Structure with the ArgDown Sandbox Online\nCopy and paste the BayesDown formatted … in the ArgDown Sandbox below to quickly verify that the network renders correctly.\n\n\nCode\n# @title 1.6.1 --- ArgDown Online Sandbox --- [argdown_online_sandbox]\n\nfrom IPython.display import IFrame\n\nIFrame(src=\"https://argdown.org/sandbox/map/\", width=\"100%\", height=\"600px\")\n\n\n\n        \n        \nArgDown Online Sandbox",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#extract-argdown-graph-information-as-dataframe",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#extract-argdown-graph-information-as-dataframe",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "E.8 1.7 Extract ArgDown Graph Information as DataFrame",
    "text": "E.8 1.7 Extract ArgDown Graph Information as DataFrame\nExtract:\n\nNodes (Variable_Title)\nEdges (Parents)\nInstantiations\nDescription\n\nImplementation nodes: - One function for ArgDown and BayesDown extraction, but: - IF YOU ONLY WANT ARGDOWN EXTRACTION: USE ARGUMENT IN FUNCTION CALL “parse_markdown_hierarchy(markdown_text, ArgDown = True)” - so if you set ArgDown = True, it gives you only instantiations, no probabilities.\n\n\nCode\n# @title 1.7.0 --- Parsing ArgDown & BayesDown (.md to .csv) --- [parsing_argdown_bayesdown]\n\n\"\"\"\nBLOCK PURPOSE: Provides the core parsing functionality for transforming ArgDown\nand BayesDown text representations into structured DataFrame format for further\nprocessing.\n\nThis block implements the critical extraction pipeline described in the AMTAIR\nproject (see PY_TechnicalImplementation) that converts argument structures\ninto Bayesian networks.\nThe function can handle both basic ArgDown (structure-only) and\nBayesDown (with probabilities).\n\nKey steps in the parsing process:\n1. Remove comments from the markdown text\n2. Extract titles, descriptions, and indentation levels\n3. Establish parent-child relationships based on indentation\n4. Convert the structured information into a DataFrame\n5. Add derived columns for network analysis\n\nDEPENDENCIES: pandas, re, json libraries\nINPUTS: Markdown text in ArgDown/BayesDown format\nOUTPUTS: Structured DataFrame with node information, relationships, and properties\n\"\"\"\n\ndef parse_markdown_hierarchy_fixed(markdown_text, ArgDown=False):\n    \"\"\"\n    Parse ArgDown or BayesDown format into a structured DataFrame with parent-child relationships.\n\n    Args:\n        markdown_text (str): Text in ArgDown or BayesDown format\n        ArgDown (bool): If True, extracts only structure without probabilities\n                        If False, extracts both structure and probability information\n\n    Returns:\n        pandas.DataFrame: Structured data with node information, relationships, and attributes\n    \"\"\"\n    # PHASE 1: Clean and prepare the text\n    clean_text = remove_comments(markdown_text)\n\n    # PHASE 2: Extract basic information about nodes\n    titles_info = extract_titles_info(clean_text)\n\n    # PHASE 3: Determine the hierarchical relationships\n    titles_with_relations = establish_relationships_fixed(titles_info, clean_text)\n\n    # PHASE 4: Convert to structured DataFrame format\n    df = convert_to_dataframe(titles_with_relations, ArgDown)\n\n    # PHASE 5: Add derived columns for analysis\n    df = add_no_parent_no_child_columns_to_df(df)\n    df = add_parents_instantiation_columns_to_df(df)\n\n    return df\n\ndef remove_comments(markdown_text):\n    \"\"\"\n    Remove comment blocks from markdown text using regex pattern matching.\n\n    Args:\n        markdown_text (str): Text containing potential comment blocks\n\n    Returns:\n        str: Text with comment blocks removed\n    \"\"\"\n    # Remove anything between /* and */ using regex\n    return re.sub(r'/\\*.*?\\*/', '', markdown_text, flags=re.DOTALL)\n\ndef extract_titles_info(text):\n    \"\"\"\n    Extract titles with their descriptions and indentation levels from markdown text.\n\n    Args:\n        text (str): Cleaned markdown text\n\n    Returns:\n        dict: Dictionary with titles as keys and dictionaries of attributes as values\n    \"\"\"\n    lines = text.split('\\n')\n    titles_info = {}\n\n    for line in lines:\n        # Skip empty lines\n        if not line.strip():\n            continue\n\n        # Extract title within square or angle brackets\n        title_match = re.search(r'[&lt;\\[](.+?)[&gt;\\]]', line)\n        if not title_match:\n            continue\n\n        title = title_match.group(1)\n\n        # Extract description and metadata\n        title_pattern_in_line = r'[&lt;\\[]' + re.escape(title) + r'[&gt;\\]]:'\n        description_match = re.search(title_pattern_in_line + r'\\s*(.*)', line)\n\n        if description_match:\n            full_text = description_match.group(1).strip()\n\n            # Split description and metadata at the first \"{\"\n            if \"{\" in full_text:\n                split_index = full_text.find(\"{\")\n                description = full_text[:split_index].strip()\n                metadata = full_text[split_index:].strip()\n            else:\n                # Keep the entire description and no metadata\n                description = full_text\n                metadata = ''  # Initialize as empty string\n        else:\n            description = ''\n            metadata = ''  # Ensure metadata is initialized\n\n        # Calculate indentation level based on spaces before + or - symbol\n        indentation = 0\n        if '+' in line:\n            symbol_index = line.find('+')\n            # Count spaces before the '+' symbol\n            i = symbol_index - 1\n            while i &gt;= 0 and line[i] == ' ':\n                indentation += 1\n                i -= 1\n        elif '-' in line:\n            symbol_index = line.find('-')\n            # Count spaces before the '-' symbol\n            i = symbol_index - 1\n            while i &gt;= 0 and line[i] == ' ':\n                indentation += 1\n                i -= 1\n\n        # If neither symbol exists, indentation remains 0\n\n        if title in titles_info:\n            # Only update description if it's currently empty and we found a new one\n            if not titles_info[title]['description'] and description:\n                titles_info[title]['description'] = description\n\n            # Store all indentation levels for this title\n            titles_info[title]['indentation_levels'].append(indentation)\n\n            # Keep max indentation for backward compatibility\n            if indentation &gt; titles_info[title]['indentation']:\n                titles_info[title]['indentation'] = indentation\n\n            # Do NOT update metadata here - keep the original metadata\n        else:\n            # First time seeing this title, create a new entry\n            titles_info[title] = {\n                'description': description,\n                'indentation': indentation,\n                'indentation_levels': [indentation],  # Initialize with first indentation level\n                'parents': [],\n                'children': [],\n                'line': None,\n                'line_numbers': [],  # Initialize an empty list for all occurrences\n                'metadata': metadata  # Set metadata explicitly from what we found\n            }\n\n    return titles_info\n\ndef establish_relationships_fixed(titles_info, text):\n    \"\"\"\n    Establish parent-child relationships between titles using BayesDown\n    indentation rules.\n\n    In BayesDown syntax:\n    - More indented nodes (with + symbol) are PARENTS of less indented nodes\n    - The relationship reads as \"Effect is caused by Cause\" (Effect + Cause)\n    - This aligns with how Bayesian networks represent causality\n\n    Args:\n        titles_info (dict): Dictionary with information about titles\n        text (str): Original markdown text (for identifying line numbers)\n\n    Returns:\n        dict: Updated dictionary with parent-child relationships\n    \"\"\"\n    lines = text.split('\\n')\n\n    # Dictionary to store line numbers for each title occurrence\n    title_occurrences = {}\n\n    # Record line number for each title (including multiple occurrences)\n    line_number = 0\n    for line in lines:\n        if not line.strip():\n            line_number += 1\n            continue\n\n        title_match = re.search(r'[&lt;\\[](.+?)[&gt;\\]]', line)\n        if not title_match:\n            line_number += 1\n            continue\n\n        title = title_match.group(1)\n\n        # Store all occurrences of each title with their line numbers\n        if title not in title_occurrences:\n            title_occurrences[title] = []\n        title_occurrences[title].append(line_number)\n\n        # Store all line numbers where this title appears\n        if 'line_numbers' not in titles_info[title]:\n            titles_info[title]['line_numbers'] = []\n        titles_info[title]['line_numbers'].append(line_number)\n\n        # For backward compatibility, keep the first occurrence in 'line'\n        if titles_info[title]['line'] is None:\n            titles_info[title]['line'] = line_number\n\n        line_number += 1\n\n    # Create an ordered list of all title occurrences with their line numbers\n    all_occurrences = []\n    for title, occurrences in title_occurrences.items():\n        for line_num in occurrences:\n            all_occurrences.append((title, line_num))\n\n    # Sort occurrences by line number\n    all_occurrences.sort(key=lambda x: x[1])\n\n    # Get indentation for each occurrence\n    occurrence_indents = {}\n    for title, line_num in all_occurrences:\n        for line in lines[line_num:line_num+1]:  # Only check the current line\n            indent = 0\n            if '+' in line:\n                symbol_index = line.find('+')\n                # Count spaces before the '+' symbol\n                j = symbol_index - 1\n                while j &gt;= 0 and line[j] == ' ':\n                    indent += 1\n                    j -= 1\n            elif '-' in line:\n                symbol_index = line.find('-')\n                # Count spaces before the '-' symbol\n                j = symbol_index - 1\n                while j &gt;= 0 and line[j] == ' ':\n                    indent += 1\n                    j -= 1\n            occurrence_indents[(title, line_num)] = indent\n\n    # Enhanced backward pass for correct parent-child relationships\n    for i, (title, line_num) in enumerate(all_occurrences):\n        current_indent = occurrence_indents[(title, line_num)]\n\n        # Skip root nodes (indentation 0) for processing\n        if current_indent == 0:\n            continue\n\n        # Look for the immediately preceding node with lower indentation\n        j = i - 1\n        while j &gt;= 0:\n            prev_title, prev_line = all_occurrences[j]\n            prev_indent = occurrence_indents[(prev_title, prev_line)]\n\n            # If we find a node with less indentation, it's a child of current node\n            if prev_indent &lt; current_indent:\n                # In BayesDown:\n                # More indented node is a parent (cause) of less indented node (effect)\n                if title not in titles_info[prev_title]['parents']:\n                    titles_info[prev_title]['parents'].append(title)\n                if prev_title not in titles_info[title]['children']:\n                    titles_info[title]['children'].append(prev_title)\n\n                # Only need to find the immediate child\n                # (closest preceding node with lower indentation)\n                break\n\n            j -= 1\n\n    return titles_info\n\ndef convert_to_dataframe(titles_info, ArgDown):\n    \"\"\"\n    Convert the titles information dictionary to a pandas DataFrame.\n\n    Args:\n        titles_info (dict): Dictionary with information about titles\n        ArgDown (bool): If True, extract only structural information without probabilities\n\n    Returns:\n        pandas.DataFrame: Structured data with node information and relationships\n    \"\"\"\n    if ArgDown == True:\n        # For ArgDown, exclude probability columns\n        df = pd.DataFrame(columns=['Title', 'Description', 'line', 'line_numbers', 'indentation',\n                               'indentation_levels', 'Parents', 'Children', 'instantiations'])\n    else:\n        # For BayesDown, include probability columns\n        df = pd.DataFrame(columns=['Title', 'Description', 'line', 'line_numbers', 'indentation',\n                               'indentation_levels', 'Parents', 'Children', 'instantiations',\n                               'priors', 'posteriors'])\n\n    for title, info in titles_info.items():\n        # Parse the metadata JSON string into a Python dictionary\n        if 'metadata' in info and info['metadata']:\n            try:\n                # Only try to parse if metadata is not empty\n                if info['metadata'].strip():\n                    jsonMetadata = json.loads(info['metadata'])\n                    if ArgDown == True:\n                        # Create the row dictionary with instantiations as\n                        # metadata only, no probabilities yet\n                        row = {\n                            'Title': title,\n                            'Description': info.get('description', ''),\n                            'line': info.get('line',''),\n                            'line_numbers': info.get('line_numbers', []),\n                            'indentation': info.get('indentation',''),\n                            'indentation_levels': info.get('indentation_levels', []),\n                            'Parents': info.get('parents', []),\n                            'Children': info.get('children', []),\n                            # Extract specific metadata fields,\n                            # defaulting to empty if not present\n                            'instantiations': jsonMetadata.get('instantiations', []),\n                        }\n                    else:\n                        # Create dict with probabilities for BayesDown\n                        row = {\n                            'Title': title,\n                            'Description': info.get('description', ''),\n                            'line': info.get('line',''),\n                            'line_numbers': info.get('line_numbers', []),\n                            'indentation': info.get('indentation',''),\n                            'indentation_levels': info.get('indentation_levels', []),\n                            'Parents': info.get('parents', []),\n                            'Children': info.get('children', []),\n                            # Extract specific metadata fields, defaulting to empty if not present\n                            'instantiations': jsonMetadata.get('instantiations', []),\n                            'priors': jsonMetadata.get('priors', {}),\n                            'posteriors': jsonMetadata.get('posteriors', {})\n                        }\n                else:\n                    # Empty metadata case\n                    row = {\n                        'Title': title,\n                        'Description': info.get('description', ''),\n                        'line': info.get('line',''),\n                        'line_numbers': info.get('line_numbers', []),\n                        'indentation': info.get('indentation',''),\n                        'indentation_levels': info.get('indentation_levels', []),\n                        'Parents': info.get('parents', []),\n                        'Children': info.get('children', []),\n                        'instantiations': [],\n                        'priors': {},\n                        'posteriors': {}\n                    }\n            except json.JSONDecodeError:\n                # Handle case where metadata isn't valid JSON\n                row = {\n                    'Title': title,\n                    'Description': info.get('description', ''),\n                    'line': info.get('line',''),\n                    'line_numbers': info.get('line_numbers', []),\n                    'indentation': info.get('indentation',''),\n                    'indentation_levels': info.get('indentation_levels', []),\n                    'Parents': info.get('parents', []),\n                    'Children': info.get('children', []),\n                    'instantiations': [],\n                    'priors': {},\n                    'posteriors': {}\n                }\n        else:\n            # Handle case where metadata field doesn't exist or is empty\n            row = {\n                'Title': title,\n                'Description': info.get('description', ''),\n                'line': info.get('line',''),\n                'line_numbers': info.get('line_numbers', []),\n                'indentation': info.get('indentation',''),\n                'indentation_levels': info.get('indentation_levels', []),\n                'Parents': info.get('parents', []),\n                'Children': info.get('children', []),\n                'instantiations': [],\n                'priors': {},\n                'posteriors': {}\n            }\n\n        # Add the row to the DataFrame\n        df.loc[len(df)] = row\n\n    return df\n\ndef add_no_parent_no_child_columns_to_df(dataframe):\n    \"\"\"\n    Add No_Parent and No_Children boolean columns to the DataFrame to\n    identify root and leaf nodes.\n\n    Args:\n        dataframe (pandas.DataFrame): The DataFrame to enhance\n\n    Returns:\n        pandas.DataFrame: Enhanced DataFrame with additional boolean columns\n    \"\"\"\n    no_parent = []\n    no_children = []\n\n    for _, row in dataframe.iterrows():\n        no_parent.append(not row['Parents'])  # True if Parents list is empty\n        no_children.append(not row['Children'])  # True if Children list is empty\n\n    dataframe['No_Parent'] = no_parent\n    dataframe['No_Children'] = no_children\n\n    return dataframe\n\ndef add_parents_instantiation_columns_to_df(dataframe):\n    \"\"\"\n    Add all possible instantiations of parents as a list of lists column\n    to the DataFrame.\n    This is crucial for generating conditional probability tables.\n\n    Args:\n        dataframe (pandas.DataFrame): The DataFrame to enhance\n\n    Returns:\n        pandas.DataFrame: Enhanced DataFrame with parent_instantiations column\n    \"\"\"\n    # Create a new column to store parent instantiations\n    parent_instantiations = []\n\n    # Iterate through each row in the dataframe\n    for _, row in dataframe.iterrows():\n        parents = row['Parents']\n        parent_insts = []\n\n        # For each parent, find its instantiations and add to the list\n        for parent in parents:\n            # Find the row where Title matches the parent\n            parent_row = dataframe[dataframe['Title'] == parent]\n\n            # If parent found in the dataframe\n            if not parent_row.empty:\n                # Get the instantiations of this parent\n                parent_instantiation = parent_row['instantiations'].iloc[0]\n                parent_insts.append(parent_instantiation)\n\n        # Add the list of parent instantiations to our new column\n        parent_instantiations.append(parent_insts)\n\n    # Add the new column to the dataframe\n    dataframe['parent_instantiations'] = parent_instantiations\n\n    return dataframe\n\n\n\n\nCode\n# example use case:\nex_csv = parse_markdown_hierarchy_fixed(md_content, ArgDown = True)\nex_csv\n\n\n\n    \n\n\n\n\n\n\nTitle\nDescription\nline\nline_numbers\nindentation\nindentation_levels\nParents\nChildren\ninstantiations\nNo_Parent\nNo_Children\nparent_instantiations\n\n\n\n\n0\nExistential_Catastrophe\nThe destruction of humanity's long-term potent...\n0\n[0]\n0\n[0]\n[]\n[]\n[existential_catastrophe_TRUE, existential_cat...\nTrue\nTrue\n[]\n\n\n1\nHuman_Disempowerment\nPermanent and collective disempowerment of hum...\n1\n[1]\n0\n[0]\n[Scale_Of_Power_Seeking]\n[]\n[human_disempowerment_TRUE, human_disempowerme...\nFalse\nTrue\n[[scale_of_power_seeking_TRUE, scale_of_power_...\n\n\n2\nScale_Of_Power_Seeking\nPower-seeking by AI systems scaling to the poi...\n2\n[2]\n4\n[4]\n[Misaligned_Power_Seeking, Corrective_Feedback]\n[Human_Disempowerment]\n[scale_of_power_seeking_TRUE, scale_of_power_s...\nFalse\nFalse\n[[misaligned_power_seeking_TRUE, misaligned_po...\n\n\n3\nMisaligned_Power_Seeking\nDeployed AI systems seeking power in unintende...\n3\n[3, 21, 23, 25]\n8\n[8, 0, 0, 0]\n[APS_Systems, Difficulty_Of_Alignment, Deploym...\n[Scale_Of_Power_Seeking]\n[misaligned_power_seeking_TRUE, misaligned_pow...\nFalse\nFalse\n[[aps_systems_TRUE, aps_systems_FALSE], [diffi...\n\n\n4\nAPS_Systems\nAI systems with advanced capabilities, agentic...\n4\n[4]\n12\n[12]\n[Advanced_AI_Capability, Agentic_Planning, Str...\n[Misaligned_Power_Seeking]\n[aps_systems_TRUE, aps_systems_FALSE]\nFalse\nFalse\n[[advanced_ai_capability_TRUE, advanced_ai_cap...\n\n\n5\nAdvanced_AI_Capability\nAI systems that outperform humans on tasks tha...\n5\n[5]\n16\n[16]\n[]\n[APS_Systems]\n[advanced_ai_capability_TRUE, advanced_ai_capa...\nTrue\nFalse\n[]\n\n\n6\nAgentic_Planning\nAI systems making and executing plans based on...\n6\n[6]\n16\n[16]\n[]\n[APS_Systems]\n[agentic_planning_TRUE, agentic_planning_FALSE]\nTrue\nFalse\n[]\n\n\n7\nStrategic_Awareness\nAI systems with models accurately representing...\n7\n[7]\n16\n[16]\n[]\n[APS_Systems]\n[strategic_awareness_TRUE, strategic_awareness...\nTrue\nFalse\n[]\n\n\n8\nDifficulty_Of_Alignment\nIt is harder to build aligned systems than mis...\n8\n[8]\n12\n[12]\n[Instrumental_Convergence, Problems_With_Proxi...\n[Misaligned_Power_Seeking]\n[difficulty_of_alignment_TRUE, difficulty_of_a...\nFalse\nFalse\n[[instrumental_convergence_TRUE, instrumental_...\n\n\n9\nInstrumental_Convergence\nAI systems with misaligned objectives tend to ...\n9\n[9]\n16\n[16]\n[]\n[Difficulty_Of_Alignment]\n[instrumental_convergence_TRUE, instrumental_c...\nTrue\nFalse\n[]\n\n\n10\nProblems_With_Proxies\nOptimizing for proxy objectives breaks correla...\n10\n[10]\n16\n[16]\n[]\n[Difficulty_Of_Alignment]\n[problems_with_proxies_TRUE, problems_with_pro...\nTrue\nFalse\n[]\n\n\n11\nProblems_With_Search\nSearch processes can yield systems pursuing di...\n11\n[11]\n16\n[16]\n[]\n[Difficulty_Of_Alignment]\n[problems_with_search_TRUE, problems_with_sear...\nTrue\nFalse\n[]\n\n\n12\nDeployment_Decisions\nDecisions to deploy potentially misaligned AI ...\n12\n[12]\n12\n[12]\n[Incentives_To_Build_APS, Deception_By_AI]\n[Misaligned_Power_Seeking]\n[deployment_decisions_DEPLOY, deployment_decis...\nFalse\nFalse\n[[incentives_to_build_aps_STRONG, incentives_t...\n\n\n13\nIncentives_To_Build_APS\nStrong incentives to build and deploy APS syst...\n13\n[13]\n16\n[16]\n[Usefulness_Of_APS, Competitive_Dynamics]\n[Deployment_Decisions]\n[incentives_to_build_aps_STRONG, incentives_to...\nFalse\nFalse\n[[usefulness_of_aps_HIGH, usefulness_of_aps_LO...\n\n\n14\nUsefulness_Of_APS\nAPS systems are very useful for many valuable ...\n14\n[14]\n20\n[20]\n[]\n[Incentives_To_Build_APS]\n[usefulness_of_aps_HIGH, usefulness_of_aps_LOW]\nTrue\nFalse\n[]\n\n\n15\nCompetitive_Dynamics\nCompetitive pressures between AI developers.\n15\n[15]\n20\n[20]\n[]\n[Incentives_To_Build_APS]\n[competitive_dynamics_STRONG, competitive_dyna...\nTrue\nFalse\n[]\n\n\n16\nDeception_By_AI\nAI systems deceiving humans about their true o...\n16\n[16]\n16\n[16]\n[]\n[Deployment_Decisions]\n[deception_by_ai_TRUE, deception_by_ai_FALSE]\nTrue\nFalse\n[]\n\n\n17\nCorrective_Feedback\nHuman society implementing corrections after o...\n17\n[17]\n8\n[8]\n[Warning_Shots, Rapid_Capability_Escalation]\n[Scale_Of_Power_Seeking]\n[corrective_feedback_EFFECTIVE, corrective_fee...\nFalse\nFalse\n[[warning_shots_OBSERVED, warning_shots_UNOBSE...\n\n\n18\nWarning_Shots\nObservable failures in weaker systems before c...\n18\n[18]\n12\n[12]\n[]\n[Corrective_Feedback]\n[warning_shots_OBSERVED, warning_shots_UNOBSER...\nTrue\nFalse\n[]\n\n\n19\nRapid_Capability_Escalation\nAI capabilities escalating very rapidly, allow...\n19\n[19]\n12\n[12]\n[]\n[Corrective_Feedback]\n[rapid_capability_escalation_TRUE, rapid_capab...\nTrue\nFalse\n[]\n\n\n20\nBarriers_To_Understanding\nDifficulty in understanding the internal worki...\n20\n[20]\n0\n[0]\n[]\n[]\n[barriers_to_understanding_HIGH, barriers_to_u...\nTrue\nTrue\n[]\n\n\n21\nAdversarial_Dynamics\nPotentially adversarial relationships between ...\n22\n[22]\n0\n[0]\n[]\n[]\n[adversarial_dynamics_TRUE, adversarial_dynami...\nTrue\nTrue\n[]\n\n\n22\nStakes_Of_Error\nThe escalating impact of mistakes with power-s...\n24\n[24]\n0\n[0]\n[]\n[]\n[stakes_of_error_HIGH, stakes_of_error_LOW]\nTrue\nTrue\n[]\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \nexample use case",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#store-argdown-information-as-argdown.csv-file",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#store-argdown-information-as-argdown.csv-file",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "E.9 1.8 Store ArgDown Information as ‘ArgDown.csv’ file",
    "text": "E.9 1.8 Store ArgDown Information as ‘ArgDown.csv’ file\n\n\nCode\n# Assuming 'md_content' holds the markdown text\n# Store the results of running the function parse_markdown_hierarchy(md_content, ArgDown = True) as the file 'ArgDown.csv'\nresult_df = parse_markdown_hierarchy_fixed(md_content, ArgDown = True)\n\n# Save to CSV\nresult_df.to_csv('ArgDown.csv', index=False)\n\n\n\n\nCode\n# Test if 'ArgDown.csv' has been saved correctly with the correct information\n# Load the data from the CSV file\nargdown_df = pd.read_csv('ArgDown.csv')\n\n# Display the DataFrame\nprint(argdown_df)\n\n\n                          Title  \\\n0       Existential_Catastrophe   \n1          Human_Disempowerment   \n2        Scale_Of_Power_Seeking   \n3      Misaligned_Power_Seeking   \n4                   APS_Systems   \n5        Advanced_AI_Capability   \n6              Agentic_Planning   \n7           Strategic_Awareness   \n8       Difficulty_Of_Alignment   \n9      Instrumental_Convergence   \n10        Problems_With_Proxies   \n11         Problems_With_Search   \n12         Deployment_Decisions   \n13      Incentives_To_Build_APS   \n14            Usefulness_Of_APS   \n15         Competitive_Dynamics   \n16              Deception_By_AI   \n17          Corrective_Feedback   \n18                Warning_Shots   \n19  Rapid_Capability_Escalation   \n20    Barriers_To_Understanding   \n21         Adversarial_Dynamics   \n22              Stakes_Of_Error   \n\n                                          Description  line     line_numbers  \\\n0   The destruction of humanity's long-term potent...     0              [0]   \n1   Permanent and collective disempowerment of hum...     1              [1]   \n2   Power-seeking by AI systems scaling to the poi...     2              [2]   \n3   Deployed AI systems seeking power in unintende...     3  [3, 21, 23, 25]   \n4   AI systems with advanced capabilities, agentic...     4              [4]   \n5   AI systems that outperform humans on tasks tha...     5              [5]   \n6   AI systems making and executing plans based on...     6              [6]   \n7   AI systems with models accurately representing...     7              [7]   \n8   It is harder to build aligned systems than mis...     8              [8]   \n9   AI systems with misaligned objectives tend to ...     9              [9]   \n10  Optimizing for proxy objectives breaks correla...    10             [10]   \n11  Search processes can yield systems pursuing di...    11             [11]   \n12  Decisions to deploy potentially misaligned AI ...    12             [12]   \n13  Strong incentives to build and deploy APS syst...    13             [13]   \n14  APS systems are very useful for many valuable ...    14             [14]   \n15       Competitive pressures between AI developers.    15             [15]   \n16  AI systems deceiving humans about their true o...    16             [16]   \n17  Human society implementing corrections after o...    17             [17]   \n18  Observable failures in weaker systems before c...    18             [18]   \n19  AI capabilities escalating very rapidly, allow...    19             [19]   \n20  Difficulty in understanding the internal worki...    20             [20]   \n21  Potentially adversarial relationships between ...    22             [22]   \n22  The escalating impact of mistakes with power-s...    24             [24]   \n\n    indentation indentation_levels  \\\n0             0                [0]   \n1             0                [0]   \n2             4                [4]   \n3             8       [8, 0, 0, 0]   \n4            12               [12]   \n5            16               [16]   \n6            16               [16]   \n7            16               [16]   \n8            12               [12]   \n9            16               [16]   \n10           16               [16]   \n11           16               [16]   \n12           12               [12]   \n13           16               [16]   \n14           20               [20]   \n15           20               [20]   \n16           16               [16]   \n17            8                [8]   \n18           12               [12]   \n19           12               [12]   \n20            0                [0]   \n21            0                [0]   \n22            0                [0]   \n\n                                              Parents  \\\n0                                                  []   \n1                          ['Scale_Of_Power_Seeking']   \n2   ['Misaligned_Power_Seeking', 'Corrective_Feedb...   \n3   ['APS_Systems', 'Difficulty_Of_Alignment', 'De...   \n4   ['Advanced_AI_Capability', 'Agentic_Planning',...   \n5                                                  []   \n6                                                  []   \n7                                                  []   \n8   ['Instrumental_Convergence', 'Problems_With_Pr...   \n9                                                  []   \n10                                                 []   \n11                                                 []   \n12     ['Incentives_To_Build_APS', 'Deception_By_AI']   \n13      ['Usefulness_Of_APS', 'Competitive_Dynamics']   \n14                                                 []   \n15                                                 []   \n16                                                 []   \n17   ['Warning_Shots', 'Rapid_Capability_Escalation']   \n18                                                 []   \n19                                                 []   \n20                                                 []   \n21                                                 []   \n22                                                 []   \n\n                        Children  \\\n0                             []   \n1                             []   \n2       ['Human_Disempowerment']   \n3     ['Scale_Of_Power_Seeking']   \n4   ['Misaligned_Power_Seeking']   \n5                ['APS_Systems']   \n6                ['APS_Systems']   \n7                ['APS_Systems']   \n8   ['Misaligned_Power_Seeking']   \n9    ['Difficulty_Of_Alignment']   \n10   ['Difficulty_Of_Alignment']   \n11   ['Difficulty_Of_Alignment']   \n12  ['Misaligned_Power_Seeking']   \n13      ['Deployment_Decisions']   \n14   ['Incentives_To_Build_APS']   \n15   ['Incentives_To_Build_APS']   \n16      ['Deployment_Decisions']   \n17    ['Scale_Of_Power_Seeking']   \n18       ['Corrective_Feedback']   \n19       ['Corrective_Feedback']   \n20                            []   \n21                            []   \n22                            []   \n\n                                       instantiations  No_Parent  No_Children  \\\n0   ['existential_catastrophe_TRUE', 'existential_...       True         True   \n1   ['human_disempowerment_TRUE', 'human_disempowe...      False         True   \n2   ['scale_of_power_seeking_TRUE', 'scale_of_powe...      False        False   \n3   ['misaligned_power_seeking_TRUE', 'misaligned_...      False        False   \n4           ['aps_systems_TRUE', 'aps_systems_FALSE']      False        False   \n5   ['advanced_ai_capability_TRUE', 'advanced_ai_c...       True        False   \n6   ['agentic_planning_TRUE', 'agentic_planning_FA...       True        False   \n7   ['strategic_awareness_TRUE', 'strategic_awaren...       True        False   \n8   ['difficulty_of_alignment_TRUE', 'difficulty_o...      False        False   \n9   ['instrumental_convergence_TRUE', 'instrumenta...       True        False   \n10  ['problems_with_proxies_TRUE', 'problems_with_...       True        False   \n11  ['problems_with_search_TRUE', 'problems_with_s...       True        False   \n12  ['deployment_decisions_DEPLOY', 'deployment_de...      False        False   \n13  ['incentives_to_build_aps_STRONG', 'incentives...      False        False   \n14  ['usefulness_of_aps_HIGH', 'usefulness_of_aps_...       True        False   \n15  ['competitive_dynamics_STRONG', 'competitive_d...       True        False   \n16  ['deception_by_ai_TRUE', 'deception_by_ai_FALSE']       True        False   \n17  ['corrective_feedback_EFFECTIVE', 'corrective_...      False        False   \n18  ['warning_shots_OBSERVED', 'warning_shots_UNOB...       True        False   \n19  ['rapid_capability_escalation_TRUE', 'rapid_ca...       True        False   \n20  ['barriers_to_understanding_HIGH', 'barriers_t...       True         True   \n21  ['adversarial_dynamics_TRUE', 'adversarial_dyn...       True         True   \n22    ['stakes_of_error_HIGH', 'stakes_of_error_LOW']       True         True   \n\n                                parent_instantiations  \n0                                                  []  \n1   [['scale_of_power_seeking_TRUE', 'scale_of_pow...  \n2   [['misaligned_power_seeking_TRUE', 'misaligned...  \n3   [['aps_systems_TRUE', 'aps_systems_FALSE'], ['...  \n4   [['advanced_ai_capability_TRUE', 'advanced_ai_...  \n5                                                  []  \n6                                                  []  \n7                                                  []  \n8   [['instrumental_convergence_TRUE', 'instrument...  \n9                                                  []  \n10                                                 []  \n11                                                 []  \n12  [['incentives_to_build_aps_STRONG', 'incentive...  \n13  [['usefulness_of_aps_HIGH', 'usefulness_of_aps...  \n14                                                 []  \n15                                                 []  \n16                                                 []  \n17  [['warning_shots_OBSERVED', 'warning_shots_UNO...  \n18                                                 []  \n19                                                 []  \n20                                                 []  \n21                                                 []  \n22                                                 []",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#argdown-to-bayesdown-adding-probability-information",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#argdown-to-bayesdown-adding-probability-information",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "F.1 2.0 ArgDown to BayesDown: Adding Probability Information",
    "text": "F.1 2.0 ArgDown to BayesDown: Adding Probability Information\n\nF.1.1 Process Overview\nThis section implements the second major stage of the AMTAIR pipeline: enhancing the structured argument representation (ArgDown) with probability information to create BayesDown.\nBayesDown extends ArgDown by adding: 1. Prior probabilities for each variable (unconditional beliefs) 2. Conditional probabilities representing the relationships between variables 3. The full parameter specification needed for a Bayesian network\nThe process follows these steps: 1. Generate probability questions for each node and its relationships 2. Create a BayesDown template with placeholders for these probabilities 3. Answer the probability questions (manually or via LLM) 4. Substitute the answers into the BayesDown representation\nThis enhanced representation contains all the information needed to construct a formal Bayesian network, enabling probabilistic reasoning and policy evaluation.\n\n\nF.1.2 What is BayesDown?\nBayesDown maintains the ArgDown structure but adds probability metadata:\n[Node]: Description. {\n\"instantiations\": [\"node_TRUE\", \"node_FALSE\"],\n\"priors\": { \"p(node_TRUE)\": \"0.7\", \"p(node_FALSE)\": \"0.3\" },\n\"posteriors\": { \"p(node_TRUE|parent_TRUE)\": \"0.9\", \"p(node_TRUE|parent_FALSE)\": \"0.4\" }\n}\nThe result is a hybrid representation that preserves the narrative structure of arguments while adding the mathematical precision of Bayesian networks.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#probability-extraction-questions-argdown.csv-to-argdown_withquestions.csv",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#probability-extraction-questions-argdown.csv-to-argdown_withquestions.csv",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "F.2 2.1 Probability Extraction Questions — ‘ArgDown.csv’ to ‘ArgDown_WithQuestions.csv’",
    "text": "F.2 2.1 Probability Extraction Questions — ‘ArgDown.csv’ to ‘ArgDown_WithQuestions.csv’\n\n\nCode\n# @title 2.1.0 --- Probability Extraction Questions Generation --- [probability_extraction_questions_generation]\n\n\"\"\"\nBLOCK PURPOSE: Generates probability questions for ArgDown nodes to prepare for BayesDown conversion.\n\nThis block implements a key step in the pipeline where structure (from ArgDown)\nis prepared for probability integration (to create BayesDown). It:\n\n1. Processes a CSV file containing ArgDown structure\n2. For each node, generates appropriate probability questions:\n   - Prior probability questions for all nodes\n   - Conditional probability questions for nodes with parents\n3. Creates a new CSV file with these questions ready for the next stage\n\nThe generated questions serve as placeholders that will be answered in the\nprobability extraction phase to complete the Bayesian network.\n\nDEPENDENCIES: pandas, json, itertools libraries\nINPUTS: ArgDown CSV file\nOUTPUTS: Enhanced CSV with probability questions for each node\n\"\"\"\n\nimport pandas as pd\nimport re\nimport json\nimport itertools\nfrom IPython.display import Markdown, display\n\n\ndef parse_instantiations(instantiations_str):\n    \"\"\"\n    Parse instantiations from string or list format.\n    Handles various input formats flexibly.\n\n    Args:\n        instantiations_str: Instantiations in string or list format\n\n    Returns:\n        list: Parsed instantiations as a list\n    \"\"\"\n    if pd.isna(instantiations_str) or instantiations_str == '':\n        return []\n\n    if isinstance(instantiations_str, list):\n        return instantiations_str\n\n    try:\n        # Try to parse as JSON\n        return json.loads(instantiations_str)\n    except:\n        # Try to parse as string list\n        if isinstance(instantiations_str, str):\n            # Remove brackets and split by comma\n            clean_str = instantiations_str.strip('[]\"\\'')\n            if not clean_str:\n                return []\n            return [s.strip(' \"\\'') for s in clean_str.split(',') if s.strip()]\n\n    return []\n\ndef parse_parents(parents_str):\n    \"\"\"\n    Parse parents from string or list format.\n    Handles various input formats flexibly.\n\n    Args:\n        parents_str: Parents in string or list format\n\n    Returns:\n        list: Parsed parents as a list\n    \"\"\"\n    if pd.isna(parents_str) or parents_str == '':\n        return []\n\n    if isinstance(parents_str, list):\n        return parents_str\n\n    try:\n        # Try to parse as JSON\n        return json.loads(parents_str)\n    except:\n        # Try to parse as string list\n        if isinstance(parents_str, str):\n            # Remove brackets and split by comma\n            clean_str = parents_str.strip('[]\"\\'')\n            if not clean_str:\n                return []\n            return [s.strip(' \"\\'') for s in clean_str.split(',') if s.strip()]\n\n    return []\n\ndef get_parent_instantiations(parent, df):\n    \"\"\"\n    Get the instantiations for a parent node from the DataFrame.\n    Returns default instantiations if not found.\n\n    Args:\n        parent (str): Parent node name\n        df (DataFrame): DataFrame containing node information\n\n    Returns:\n        list: Instantiations for the parent node\n    \"\"\"\n    parent_row = df[df['Title'] == parent]\n    if parent_row.empty:\n        return [f\"{parent}_TRUE\", f\"{parent}_FALSE\"]\n\n    instantiations = parse_instantiations(parent_row.iloc[0]['instantiations'])\n    if not instantiations:\n        return [f\"{parent}_TRUE\", f\"{parent}_FALSE\"]\n\n    return instantiations\n\ndef generate_instantiation_questions(title, instantiation, parents, df):\n    \"\"\"\n    Generate questions for a specific instantiation of a node.\n\n    Args:\n        title (str): The title of the node\n        instantiation (str): The specific instantiation (e.g., \"title_TRUE\")\n        parents (list): List of parent nodes\n        df (DataFrame): The full DataFrame for looking up parent instantiations\n\n    Returns:\n        dict: Dictionary mapping questions to estimate keys\n    \"\"\"\n    questions = {}\n\n    # Always generate a prior probability question, regardless of parents\n    prior_question = f\"What is the probability for {title}={instantiation}?\"\n    questions[prior_question] = 'prior' # Question is the key, 'prior' is the value\n\n    # If no parents, return only the prior question\n    if not parents:\n        return questions\n\n    # For nodes with parents, generate conditional probability questions\n    # Get all combinations of parent instantiations\n    parent_instantiations = []\n    for parent in parents:\n        parent_insts = get_parent_instantiations(parent, df)\n        parent_instantiations.append([(parent, inst) for inst in parent_insts])\n\n    # Generate all combinations\n    all_combinations = list(itertools.product(*parent_instantiations))\n\n    # Create conditional probability questions for each combination\n    # and use questions as keys, estimate_i as values\n    for i, combination in enumerate(all_combinations):\n        condition_str = \", \".join([f\"{parent}={inst}\" for parent, inst in combination])\n        question = f\"What is the probability for {title}={instantiation} if {condition_str}?\"\n        questions[question] = f'estimate_{i + 1}'  # Question is the key,\n                                                   # estimate_i is the value\n\n    return questions\n\n\ndef generate_argdown_with_questions(argdown_csv_path, output_csv_path):\n    \"\"\"\n    Generate probability questions based on the ArgDown CSV file and save\n    to a new CSV file.\n\n    Args:\n        argdown_csv_path (str): Path to the input ArgDown CSV file\n        output_csv_path (str): Path to save the output CSV file with questions\n\n    Returns:\n        DataFrame: Enhanced DataFrame with probability questions\n\n    Raises:\n        Exception: If CSV loading fails or required columns are missing\n    \"\"\"\n    print(f\"Loading ArgDown CSV from {argdown_csv_path}...\")\n\n    # Load the ArgDown CSV file\n    try:\n        df = pd.read_csv(argdown_csv_path)\n        print(f\"Successfully loaded CSV with {len(df)} rows.\")\n    except Exception as e:\n        raise Exception(f\"Error loading ArgDown CSV: {e}\")\n\n    # Validate required columns\n    required_columns = ['Title', 'Parents', 'instantiations']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise Exception(f\"Missing required columns: {', '.join(missing_columns)}\")\n\n    # Initialize columns for questions\n    df['Generate_Positive_Instantiation_Questions'] = None\n    df['Generate_Negative_Instantiation_Questions'] = None\n\n    print(\"Generating probability questions for each node...\")\n\n    # Process each row to generate questions\n    for idx, row in df.iterrows():\n        title = row['Title']\n        instantiations = parse_instantiations(row['instantiations'])\n        parents = parse_parents(row['Parents'])\n\n        if len(instantiations) &lt; 2:\n            # Default instantiations if not provided\n            instantiations = [f\"{title}_TRUE\", f\"{title}_FALSE\"]\n\n        # Generate positive instantiation questions\n        positive_questions = generate_instantiation_questions(title, instantiations[0], parents, df)\n\n        # Generate negative instantiation questions\n        negative_questions = generate_instantiation_questions(title, instantiations[1], parents, df)\n\n        # Update the DataFrame\n        df.at[idx, 'Generate_Positive_Instantiation_Questions'] = json.dumps(positive_questions)\n        df.at[idx, 'Generate_Negative_Instantiation_Questions'] = json.dumps(negative_questions)\n\n    # Save the enhanced DataFrame\n    df.to_csv(output_csv_path, index=False)\n    print(f\"Generated questions saved to {output_csv_path}\")\n\n    return df\n\n# Example usage:\ndf_with_questions = generate_argdown_with_questions(\"ArgDown.csv\", \"ArgDown_WithQuestions.csv\")\n\n\nLoading ArgDown CSV from ArgDown.csv...\nSuccessfully loaded CSV with 23 rows.\nGenerating probability questions for each node...\nGenerated questions saved to ArgDown_WithQuestions.csv\n\n\n\n\nCode\n# Load the data from the ArgDown_WithQuestions CSV file\nargdown_with_questions_df = pd.read_csv('ArgDown_WithQuestions.csv')\n\n# Display the DataFrame\nprint(argdown_with_questions_df)\nargdown_with_questions_df\n\n\n                          Title  \\\n0       Existential_Catastrophe   \n1          Human_Disempowerment   \n2        Scale_Of_Power_Seeking   \n3      Misaligned_Power_Seeking   \n4                   APS_Systems   \n5        Advanced_AI_Capability   \n6              Agentic_Planning   \n7           Strategic_Awareness   \n8       Difficulty_Of_Alignment   \n9      Instrumental_Convergence   \n10        Problems_With_Proxies   \n11         Problems_With_Search   \n12         Deployment_Decisions   \n13      Incentives_To_Build_APS   \n14            Usefulness_Of_APS   \n15         Competitive_Dynamics   \n16              Deception_By_AI   \n17          Corrective_Feedback   \n18                Warning_Shots   \n19  Rapid_Capability_Escalation   \n20    Barriers_To_Understanding   \n21         Adversarial_Dynamics   \n22              Stakes_Of_Error   \n\n                                          Description  line     line_numbers  \\\n0   The destruction of humanity's long-term potent...     0              [0]   \n1   Permanent and collective disempowerment of hum...     1              [1]   \n2   Power-seeking by AI systems scaling to the poi...     2              [2]   \n3   Deployed AI systems seeking power in unintende...     3  [3, 21, 23, 25]   \n4   AI systems with advanced capabilities, agentic...     4              [4]   \n5   AI systems that outperform humans on tasks tha...     5              [5]   \n6   AI systems making and executing plans based on...     6              [6]   \n7   AI systems with models accurately representing...     7              [7]   \n8   It is harder to build aligned systems than mis...     8              [8]   \n9   AI systems with misaligned objectives tend to ...     9              [9]   \n10  Optimizing for proxy objectives breaks correla...    10             [10]   \n11  Search processes can yield systems pursuing di...    11             [11]   \n12  Decisions to deploy potentially misaligned AI ...    12             [12]   \n13  Strong incentives to build and deploy APS syst...    13             [13]   \n14  APS systems are very useful for many valuable ...    14             [14]   \n15       Competitive pressures between AI developers.    15             [15]   \n16  AI systems deceiving humans about their true o...    16             [16]   \n17  Human society implementing corrections after o...    17             [17]   \n18  Observable failures in weaker systems before c...    18             [18]   \n19  AI capabilities escalating very rapidly, allow...    19             [19]   \n20  Difficulty in understanding the internal worki...    20             [20]   \n21  Potentially adversarial relationships between ...    22             [22]   \n22  The escalating impact of mistakes with power-s...    24             [24]   \n\n    indentation indentation_levels  \\\n0             0                [0]   \n1             0                [0]   \n2             4                [4]   \n3             8       [8, 0, 0, 0]   \n4            12               [12]   \n5            16               [16]   \n6            16               [16]   \n7            16               [16]   \n8            12               [12]   \n9            16               [16]   \n10           16               [16]   \n11           16               [16]   \n12           12               [12]   \n13           16               [16]   \n14           20               [20]   \n15           20               [20]   \n16           16               [16]   \n17            8                [8]   \n18           12               [12]   \n19           12               [12]   \n20            0                [0]   \n21            0                [0]   \n22            0                [0]   \n\n                                              Parents  \\\n0                                                  []   \n1                          ['Scale_Of_Power_Seeking']   \n2   ['Misaligned_Power_Seeking', 'Corrective_Feedb...   \n3   ['APS_Systems', 'Difficulty_Of_Alignment', 'De...   \n4   ['Advanced_AI_Capability', 'Agentic_Planning',...   \n5                                                  []   \n6                                                  []   \n7                                                  []   \n8   ['Instrumental_Convergence', 'Problems_With_Pr...   \n9                                                  []   \n10                                                 []   \n11                                                 []   \n12     ['Incentives_To_Build_APS', 'Deception_By_AI']   \n13      ['Usefulness_Of_APS', 'Competitive_Dynamics']   \n14                                                 []   \n15                                                 []   \n16                                                 []   \n17   ['Warning_Shots', 'Rapid_Capability_Escalation']   \n18                                                 []   \n19                                                 []   \n20                                                 []   \n21                                                 []   \n22                                                 []   \n\n                        Children  \\\n0                             []   \n1                             []   \n2       ['Human_Disempowerment']   \n3     ['Scale_Of_Power_Seeking']   \n4   ['Misaligned_Power_Seeking']   \n5                ['APS_Systems']   \n6                ['APS_Systems']   \n7                ['APS_Systems']   \n8   ['Misaligned_Power_Seeking']   \n9    ['Difficulty_Of_Alignment']   \n10   ['Difficulty_Of_Alignment']   \n11   ['Difficulty_Of_Alignment']   \n12  ['Misaligned_Power_Seeking']   \n13      ['Deployment_Decisions']   \n14   ['Incentives_To_Build_APS']   \n15   ['Incentives_To_Build_APS']   \n16      ['Deployment_Decisions']   \n17    ['Scale_Of_Power_Seeking']   \n18       ['Corrective_Feedback']   \n19       ['Corrective_Feedback']   \n20                            []   \n21                            []   \n22                            []   \n\n                                       instantiations  No_Parent  No_Children  \\\n0   ['existential_catastrophe_TRUE', 'existential_...       True         True   \n1   ['human_disempowerment_TRUE', 'human_disempowe...      False         True   \n2   ['scale_of_power_seeking_TRUE', 'scale_of_powe...      False        False   \n3   ['misaligned_power_seeking_TRUE', 'misaligned_...      False        False   \n4           ['aps_systems_TRUE', 'aps_systems_FALSE']      False        False   \n5   ['advanced_ai_capability_TRUE', 'advanced_ai_c...       True        False   \n6   ['agentic_planning_TRUE', 'agentic_planning_FA...       True        False   \n7   ['strategic_awareness_TRUE', 'strategic_awaren...       True        False   \n8   ['difficulty_of_alignment_TRUE', 'difficulty_o...      False        False   \n9   ['instrumental_convergence_TRUE', 'instrumenta...       True        False   \n10  ['problems_with_proxies_TRUE', 'problems_with_...       True        False   \n11  ['problems_with_search_TRUE', 'problems_with_s...       True        False   \n12  ['deployment_decisions_DEPLOY', 'deployment_de...      False        False   \n13  ['incentives_to_build_aps_STRONG', 'incentives...      False        False   \n14  ['usefulness_of_aps_HIGH', 'usefulness_of_aps_...       True        False   \n15  ['competitive_dynamics_STRONG', 'competitive_d...       True        False   \n16  ['deception_by_ai_TRUE', 'deception_by_ai_FALSE']       True        False   \n17  ['corrective_feedback_EFFECTIVE', 'corrective_...      False        False   \n18  ['warning_shots_OBSERVED', 'warning_shots_UNOB...       True        False   \n19  ['rapid_capability_escalation_TRUE', 'rapid_ca...       True        False   \n20  ['barriers_to_understanding_HIGH', 'barriers_t...       True         True   \n21  ['adversarial_dynamics_TRUE', 'adversarial_dyn...       True         True   \n22    ['stakes_of_error_HIGH', 'stakes_of_error_LOW']       True         True   \n\n                                parent_instantiations  \\\n0                                                  []   \n1   [['scale_of_power_seeking_TRUE', 'scale_of_pow...   \n2   [['misaligned_power_seeking_TRUE', 'misaligned...   \n3   [['aps_systems_TRUE', 'aps_systems_FALSE'], ['...   \n4   [['advanced_ai_capability_TRUE', 'advanced_ai_...   \n5                                                  []   \n6                                                  []   \n7                                                  []   \n8   [['instrumental_convergence_TRUE', 'instrument...   \n9                                                  []   \n10                                                 []   \n11                                                 []   \n12  [['incentives_to_build_aps_STRONG', 'incentive...   \n13  [['usefulness_of_aps_HIGH', 'usefulness_of_aps...   \n14                                                 []   \n15                                                 []   \n16                                                 []   \n17  [['warning_shots_OBSERVED', 'warning_shots_UNO...   \n18                                                 []   \n19                                                 []   \n20                                                 []   \n21                                                 []   \n22                                                 []   \n\n            Generate_Positive_Instantiation_Questions  \\\n0   {\"What is the probability for Existential_Cata...   \n1   {\"What is the probability for Human_Disempower...   \n2   {\"What is the probability for Scale_Of_Power_S...   \n3   {\"What is the probability for Misaligned_Power...   \n4   {\"What is the probability for APS_Systems=aps_...   \n5   {\"What is the probability for Advanced_AI_Capa...   \n6   {\"What is the probability for Agentic_Planning...   \n7   {\"What is the probability for Strategic_Awaren...   \n8   {\"What is the probability for Difficulty_Of_Al...   \n9   {\"What is the probability for Instrumental_Con...   \n10  {\"What is the probability for Problems_With_Pr...   \n11  {\"What is the probability for Problems_With_Se...   \n12  {\"What is the probability for Deployment_Decis...   \n13  {\"What is the probability for Incentives_To_Bu...   \n14  {\"What is the probability for Usefulness_Of_AP...   \n15  {\"What is the probability for Competitive_Dyna...   \n16  {\"What is the probability for Deception_By_AI=...   \n17  {\"What is the probability for Corrective_Feedb...   \n18  {\"What is the probability for Warning_Shots=wa...   \n19  {\"What is the probability for Rapid_Capability...   \n20  {\"What is the probability for Barriers_To_Unde...   \n21  {\"What is the probability for Adversarial_Dyna...   \n22  {\"What is the probability for Stakes_Of_Error=...   \n\n            Generate_Negative_Instantiation_Questions  \n0   {\"What is the probability for Existential_Cata...  \n1   {\"What is the probability for Human_Disempower...  \n2   {\"What is the probability for Scale_Of_Power_S...  \n3   {\"What is the probability for Misaligned_Power...  \n4   {\"What is the probability for APS_Systems=aps_...  \n5   {\"What is the probability for Advanced_AI_Capa...  \n6   {\"What is the probability for Agentic_Planning...  \n7   {\"What is the probability for Strategic_Awaren...  \n8   {\"What is the probability for Difficulty_Of_Al...  \n9   {\"What is the probability for Instrumental_Con...  \n10  {\"What is the probability for Problems_With_Pr...  \n11  {\"What is the probability for Problems_With_Se...  \n12  {\"What is the probability for Deployment_Decis...  \n13  {\"What is the probability for Incentives_To_Bu...  \n14  {\"What is the probability for Usefulness_Of_AP...  \n15  {\"What is the probability for Competitive_Dyna...  \n16  {\"What is the probability for Deception_By_AI=...  \n17  {\"What is the probability for Corrective_Feedb...  \n18  {\"What is the probability for Warning_Shots=wa...  \n19  {\"What is the probability for Rapid_Capability...  \n20  {\"What is the probability for Barriers_To_Unde...  \n21  {\"What is the probability for Adversarial_Dyna...  \n22  {\"What is the probability for Stakes_Of_Error=...  \n\n\n\n    \n\n\n\n\n\n\nTitle\nDescription\nline\nline_numbers\nindentation\nindentation_levels\nParents\nChildren\ninstantiations\nNo_Parent\nNo_Children\nparent_instantiations\nGenerate_Positive_Instantiation_Questions\nGenerate_Negative_Instantiation_Questions\n\n\n\n\n0\nExistential_Catastrophe\nThe destruction of humanity's long-term potent...\n0\n[0]\n0\n[0]\n[]\n[]\n['existential_catastrophe_TRUE', 'existential_...\nTrue\nTrue\n[]\n{\"What is the probability for Existential_Cata...\n{\"What is the probability for Existential_Cata...\n\n\n1\nHuman_Disempowerment\nPermanent and collective disempowerment of hum...\n1\n[1]\n0\n[0]\n['Scale_Of_Power_Seeking']\n[]\n['human_disempowerment_TRUE', 'human_disempowe...\nFalse\nTrue\n[['scale_of_power_seeking_TRUE', 'scale_of_pow...\n{\"What is the probability for Human_Disempower...\n{\"What is the probability for Human_Disempower...\n\n\n2\nScale_Of_Power_Seeking\nPower-seeking by AI systems scaling to the poi...\n2\n[2]\n4\n[4]\n['Misaligned_Power_Seeking', 'Corrective_Feedb...\n['Human_Disempowerment']\n['scale_of_power_seeking_TRUE', 'scale_of_powe...\nFalse\nFalse\n[['misaligned_power_seeking_TRUE', 'misaligned...\n{\"What is the probability for Scale_Of_Power_S...\n{\"What is the probability for Scale_Of_Power_S...\n\n\n3\nMisaligned_Power_Seeking\nDeployed AI systems seeking power in unintende...\n3\n[3, 21, 23, 25]\n8\n[8, 0, 0, 0]\n['APS_Systems', 'Difficulty_Of_Alignment', 'De...\n['Scale_Of_Power_Seeking']\n['misaligned_power_seeking_TRUE', 'misaligned_...\nFalse\nFalse\n[['aps_systems_TRUE', 'aps_systems_FALSE'], ['...\n{\"What is the probability for Misaligned_Power...\n{\"What is the probability for Misaligned_Power...\n\n\n4\nAPS_Systems\nAI systems with advanced capabilities, agentic...\n4\n[4]\n12\n[12]\n['Advanced_AI_Capability', 'Agentic_Planning',...\n['Misaligned_Power_Seeking']\n['aps_systems_TRUE', 'aps_systems_FALSE']\nFalse\nFalse\n[['advanced_ai_capability_TRUE', 'advanced_ai_...\n{\"What is the probability for APS_Systems=aps_...\n{\"What is the probability for APS_Systems=aps_...\n\n\n5\nAdvanced_AI_Capability\nAI systems that outperform humans on tasks tha...\n5\n[5]\n16\n[16]\n[]\n['APS_Systems']\n['advanced_ai_capability_TRUE', 'advanced_ai_c...\nTrue\nFalse\n[]\n{\"What is the probability for Advanced_AI_Capa...\n{\"What is the probability for Advanced_AI_Capa...\n\n\n6\nAgentic_Planning\nAI systems making and executing plans based on...\n6\n[6]\n16\n[16]\n[]\n['APS_Systems']\n['agentic_planning_TRUE', 'agentic_planning_FA...\nTrue\nFalse\n[]\n{\"What is the probability for Agentic_Planning...\n{\"What is the probability for Agentic_Planning...\n\n\n7\nStrategic_Awareness\nAI systems with models accurately representing...\n7\n[7]\n16\n[16]\n[]\n['APS_Systems']\n['strategic_awareness_TRUE', 'strategic_awaren...\nTrue\nFalse\n[]\n{\"What is the probability for Strategic_Awaren...\n{\"What is the probability for Strategic_Awaren...\n\n\n8\nDifficulty_Of_Alignment\nIt is harder to build aligned systems than mis...\n8\n[8]\n12\n[12]\n['Instrumental_Convergence', 'Problems_With_Pr...\n['Misaligned_Power_Seeking']\n['difficulty_of_alignment_TRUE', 'difficulty_o...\nFalse\nFalse\n[['instrumental_convergence_TRUE', 'instrument...\n{\"What is the probability for Difficulty_Of_Al...\n{\"What is the probability for Difficulty_Of_Al...\n\n\n9\nInstrumental_Convergence\nAI systems with misaligned objectives tend to ...\n9\n[9]\n16\n[16]\n[]\n['Difficulty_Of_Alignment']\n['instrumental_convergence_TRUE', 'instrumenta...\nTrue\nFalse\n[]\n{\"What is the probability for Instrumental_Con...\n{\"What is the probability for Instrumental_Con...\n\n\n10\nProblems_With_Proxies\nOptimizing for proxy objectives breaks correla...\n10\n[10]\n16\n[16]\n[]\n['Difficulty_Of_Alignment']\n['problems_with_proxies_TRUE', 'problems_with_...\nTrue\nFalse\n[]\n{\"What is the probability for Problems_With_Pr...\n{\"What is the probability for Problems_With_Pr...\n\n\n11\nProblems_With_Search\nSearch processes can yield systems pursuing di...\n11\n[11]\n16\n[16]\n[]\n['Difficulty_Of_Alignment']\n['problems_with_search_TRUE', 'problems_with_s...\nTrue\nFalse\n[]\n{\"What is the probability for Problems_With_Se...\n{\"What is the probability for Problems_With_Se...\n\n\n12\nDeployment_Decisions\nDecisions to deploy potentially misaligned AI ...\n12\n[12]\n12\n[12]\n['Incentives_To_Build_APS', 'Deception_By_AI']\n['Misaligned_Power_Seeking']\n['deployment_decisions_DEPLOY', 'deployment_de...\nFalse\nFalse\n[['incentives_to_build_aps_STRONG', 'incentive...\n{\"What is the probability for Deployment_Decis...\n{\"What is the probability for Deployment_Decis...\n\n\n13\nIncentives_To_Build_APS\nStrong incentives to build and deploy APS syst...\n13\n[13]\n16\n[16]\n['Usefulness_Of_APS', 'Competitive_Dynamics']\n['Deployment_Decisions']\n['incentives_to_build_aps_STRONG', 'incentives...\nFalse\nFalse\n[['usefulness_of_aps_HIGH', 'usefulness_of_aps...\n{\"What is the probability for Incentives_To_Bu...\n{\"What is the probability for Incentives_To_Bu...\n\n\n14\nUsefulness_Of_APS\nAPS systems are very useful for many valuable ...\n14\n[14]\n20\n[20]\n[]\n['Incentives_To_Build_APS']\n['usefulness_of_aps_HIGH', 'usefulness_of_aps_...\nTrue\nFalse\n[]\n{\"What is the probability for Usefulness_Of_AP...\n{\"What is the probability for Usefulness_Of_AP...\n\n\n15\nCompetitive_Dynamics\nCompetitive pressures between AI developers.\n15\n[15]\n20\n[20]\n[]\n['Incentives_To_Build_APS']\n['competitive_dynamics_STRONG', 'competitive_d...\nTrue\nFalse\n[]\n{\"What is the probability for Competitive_Dyna...\n{\"What is the probability for Competitive_Dyna...\n\n\n16\nDeception_By_AI\nAI systems deceiving humans about their true o...\n16\n[16]\n16\n[16]\n[]\n['Deployment_Decisions']\n['deception_by_ai_TRUE', 'deception_by_ai_FALSE']\nTrue\nFalse\n[]\n{\"What is the probability for Deception_By_AI=...\n{\"What is the probability for Deception_By_AI=...\n\n\n17\nCorrective_Feedback\nHuman society implementing corrections after o...\n17\n[17]\n8\n[8]\n['Warning_Shots', 'Rapid_Capability_Escalation']\n['Scale_Of_Power_Seeking']\n['corrective_feedback_EFFECTIVE', 'corrective_...\nFalse\nFalse\n[['warning_shots_OBSERVED', 'warning_shots_UNO...\n{\"What is the probability for Corrective_Feedb...\n{\"What is the probability for Corrective_Feedb...\n\n\n18\nWarning_Shots\nObservable failures in weaker systems before c...\n18\n[18]\n12\n[12]\n[]\n['Corrective_Feedback']\n['warning_shots_OBSERVED', 'warning_shots_UNOB...\nTrue\nFalse\n[]\n{\"What is the probability for Warning_Shots=wa...\n{\"What is the probability for Warning_Shots=wa...\n\n\n19\nRapid_Capability_Escalation\nAI capabilities escalating very rapidly, allow...\n19\n[19]\n12\n[12]\n[]\n['Corrective_Feedback']\n['rapid_capability_escalation_TRUE', 'rapid_ca...\nTrue\nFalse\n[]\n{\"What is the probability for Rapid_Capability...\n{\"What is the probability for Rapid_Capability...\n\n\n20\nBarriers_To_Understanding\nDifficulty in understanding the internal worki...\n20\n[20]\n0\n[0]\n[]\n[]\n['barriers_to_understanding_HIGH', 'barriers_t...\nTrue\nTrue\n[]\n{\"What is the probability for Barriers_To_Unde...\n{\"What is the probability for Barriers_To_Unde...\n\n\n21\nAdversarial_Dynamics\nPotentially adversarial relationships between ...\n22\n[22]\n0\n[0]\n[]\n[]\n['adversarial_dynamics_TRUE', 'adversarial_dyn...\nTrue\nTrue\n[]\n{\"What is the probability for Adversarial_Dyna...\n{\"What is the probability for Adversarial_Dyna...\n\n\n22\nStakes_Of_Error\nThe escalating impact of mistakes with power-s...\n24\n[24]\n0\n[0]\n[]\n[]\n['stakes_of_error_HIGH', 'stakes_of_error_LOW']\nTrue\nTrue\n[]\n{\"What is the probability for Stakes_Of_Error=...\n{\"What is the probability for Stakes_Of_Error=...",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#argdown_withquestions.csv-to-bayesdownquestions.md",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#argdown_withquestions.csv-to-bayesdownquestions.md",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "F.3 2.2 ‘ArgDown_WithQuestions.csv’ to ‘BayesDownQuestions.md’",
    "text": "F.3 2.2 ‘ArgDown_WithQuestions.csv’ to ‘BayesDownQuestions.md’\n2.2 Save BayesDown Extraction Questions as ‘BayesDownQuestions.md’\n\n\nCode\n# @title 2.2.0 --- BayesDown Questions Generation --- [bayesdown_questions_generation]\n\n\"\"\"\nBLOCK PURPOSE: Transforms the ArgDown with questions into a BayesDown template with placeholders.\n\nThis function creates a BayesDown representation with probability placeholders\nbased on the questions generated in the previous step. It:\n\n1. Loads the CSV file with probability questions\n2. Constructs a directed graph to represent the causal structure\n3. Processes each node to create BayesDown syntax with probability placeholders\n4. Optionally includes comments with the specific questions to be answered\n5. Saves the result as a markdown file for the next stage of the pipeline\n\nThe output is a BayesDown template that can be used in the probability\nextraction phase, where the placeholders will be replaced with actual\nprobability values.\n\nDEPENDENCIES: networkx, pandas, json libraries\nINPUTS: CSV file with ArgDown structure and probability questions\nOUTPUTS: BayesDown markdown file with probability placeholders\n\"\"\"\n\ndef extract_bayesdown_questions_fixed(argdown_with_questions_path, output_md_path, include_questions_as_comments=True):\n  \"\"\"\n  Generate BayesDown syntax from the ArgDown_WithQuestions CSV file with\n  correct parent-child relationships.\n\n  Args:\n      argdown_with_questions_path (str): Path to the CSV file with probability questions\n      output_md_path (str): Path to save the output BayesDown file\n      include_questions_as_comments (bool, optional): Whether to include the original\n                                                    questions as comments. Defaults to True.\n\n  Returns:\n      str: The generated BayesDown content\n\n  Raises:\n      Exception: If CSV loading fails or required columns are missing\n  \"\"\"\n  print(f\"Loading CSV from {argdown_with_questions_path}...\")\n\n  # Load the CSV file\n  try:\n      df = pd.read_csv(argdown_with_questions_path)\n      print(f\"Successfully loaded CSV with {len(df)} rows.\")\n  except Exception as e:\n      raise Exception(f\"Error loading CSV: {e}\")\n\n  # Validate required columns\n  required_columns = ['Title', 'Description', 'Parents', 'Children', 'instantiations']\n  missing_columns = [col for col in required_columns if col not in df.columns]\n  if missing_columns:\n      raise Exception(f\"Missing required columns: {', '.join(missing_columns)}\")\n\n  print(\"Generating BayesDown syntax with placeholder probabilities...\")\n\n  # Build a directed graph of nodes\n  G = nx.DiGraph()\n\n  # Add nodes to the graph\n  for idx, row in df.iterrows():\n      G.add_node(row['Title'], data=row)\n\n  # Add edges to the graph based on parent-child relationships - CORRECTLY\n  for idx, row in df.iterrows():\n      child = row['Title']\n\n      # Parse parents and add edges\n      parents = row['Parents']\n      if isinstance(parents, str):\n          # Handle string representation of list\n          if parents.startswith('[') and parents.endswith(']'):\n              parents = parents.strip('[]')\n              if parents:  # Check if not empty\n                  parent_list = [p.strip().strip('\\'\"') for p in parents.split(',')]\n                  for parent in parent_list:\n                      if parent in G.nodes():\n                          # In BayesDown: Parent (cause) -&gt; Child (effect)\n                          G.add_edge(parent, child)\n      elif isinstance(parents, list):\n          # Handle actual list\n          for parent in parents:\n              if parent in G.nodes():\n                  G.add_edge(parent, child)\n\n  # Function to safely parse JSON strings\n  def safe_parse_json(json_str):\n      if pd.isna(json_str):\n          return {}\n\n      if isinstance(json_str, dict):\n          return json_str\n\n      try:\n          return json.loads(json_str)\n      except:\n          return {}\n\n  # Start building the BayesDown content\n  bayesdown_content = \"\"  # Initialize as empty\n\n  if include_questions_as_comments:\n    bayesdown_content = \"# BayesDown Representation with Placeholder Probabilities\\n\\n\"\n    bayesdown_content += \"/* This file contains BayesDown syntax with placeholder probabilities.\\n\"\n    bayesdown_content += \"   Replace the placeholders with actual probability values based on the \\n\"\n    bayesdown_content += \"   questions in the comments. */\\n\\n\"\n\n  # Get leaf nodes (nodes with no outgoing edges) - these are effects without children\n  leaf_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n\n  # Helper function to process a node and its parents recursively\n  def process_node(node, indent_level=0, processed_nodes=None):\n      if processed_nodes is None:\n          processed_nodes = set()\n\n      # Create the indentation string\n      indent = ' ' * (indent_level * 2)\n      prefix = f\"{indent}+ \" if indent_level &gt; 0 else \"\"\n\n      # Get node data\n      node_data = G.nodes[node]['data']\n      title = node_data['Title']\n      description = node_data['Description'] if not pd.isna(node_data['Description']) else \"\"\n\n      # Parse instantiations from the row data\n      instantiations = parse_instantiations_safely(node_data['instantiations'])\n\n      # Build the node string\n      node_output = \"\"\n\n      # Add comments with questions if requested\n      if include_questions_as_comments:\n          # Add positive questions as comments\n          if 'Generate_Positive_Instantiation_Questions' in node_data:\n              positive_questions = safe_parse_json(node_data['Generate_Positive_Instantiation_Questions'])\n              for question in positive_questions.keys():\n                  node_output += f\"{indent}/* {question} */\\n\"\n\n          # Add negative questions as comments\n          if 'Generate_Negative_Instantiation_Questions' in node_data:\n              negative_questions = safe_parse_json(node_data['Generate_Negative_Instantiation_Questions'])\n              for question in negative_questions.keys():\n                  node_output += f\"{indent}/* {question} */\\n\"\n\n      # Check if this node was already fully defined elsewhere\n      if node in processed_nodes:\n          # Just add a reference to the node\n          node_output += f\"{prefix}[{title}]\\n\"\n          return node_output\n\n      # Mark this node as processed\n      processed_nodes.add(node)\n\n      # Prepare the metadata JSON\n      metadata = {\n          \"instantiations\": instantiations\n      }\n\n      # Add priors with full questions as keys\n      priors = {}\n      if 'Generate_Positive_Instantiation_Questions' in node_data:\n          positive_questions = safe_parse_json(node_data['Generate_Positive_Instantiation_Questions'])\n          for question, estimate_key in positive_questions.items():\n              if estimate_key == 'prior':\n                  priors[question] = \"%?\"  # Default placeholder\n\n      if 'Generate_Negative_Instantiation_Questions' in node_data:\n          negative_questions = safe_parse_json(node_data['Generate_Negative_Instantiation_Questions'])\n          for question, estimate_key in negative_questions.items():\n              if estimate_key == 'prior':\n                  priors[question] = \"%?\"  # Default placeholder\n\n      metadata[\"priors\"] = priors\n\n      # Add posteriors with full questions as keys\n      parents = list(G.predecessors(node))\n      if parents:\n          posteriors = {}\n          if 'Generate_Positive_Instantiation_Questions' in node_data:\n              positive_questions = safe_parse_json(node_data['Generate_Positive_Instantiation_Questions'])\n              for question, estimate_key in positive_questions.items():\n                  if estimate_key.startswith('estimate_'):\n                      posteriors[question] = \"?%\"  # Default placeholder\n\n          if 'Generate_Negative_Instantiation_Questions' in node_data:\n              negative_questions = safe_parse_json(node_data['Generate_Negative_Instantiation_Questions'])\n              for question, estimate_key in negative_questions.items():\n                  if estimate_key.startswith('estimate_'):\n                      posteriors[question] = \"?%\"  # Default placeholder\n\n          metadata[\"posteriors\"] = posteriors\n\n      # Format the node with metadata\n      node_output += f\"{prefix}[{title}]: {description} {json.dumps(metadata)}\\n\"\n\n      # Process parent nodes\n      for parent in parents:\n          if parent != node:  # Avoid self-references\n              parent_output = process_node(parent, indent_level + 1, processed_nodes)\n              node_output += parent_output\n\n      return node_output\n\n  # Helper function to parse instantiations safely\n  def parse_instantiations_safely(instantiations_data):\n      if isinstance(instantiations_data, list):\n          return instantiations_data if instantiations_data else [f\"TRUE\", f\"FALSE\"]\n\n      if isinstance(instantiations_data, str):\n          try:\n              parsed = json.loads(instantiations_data)\n              if isinstance(parsed, list):\n                  return parsed if parsed else [f\"TRUE\", f\"FALSE\"]\n          except:\n              if instantiations_data.startswith('[') and instantiations_data.endswith(']'):\n                  items = instantiations_data.strip('[]').split(',')\n                  result = [item.strip(' \"\\'') for item in items if item.strip()]\n                  return result if result else [f\"TRUE\", f\"FALSE\"]\n\n      return [f\"TRUE\", f\"FALSE\"]  # Default\n\n  # Process each leaf node and its ancestors\n  for leaf in leaf_nodes:\n      bayesdown_content += process_node(leaf)\n\n  # Save the BayesDown content\n  with open(output_md_path, 'w') as f:\n      f.write(bayesdown_content)\n\n  print(f\"BayesDown Questions saved to {output_md_path}\")\n  return bayesdown_content\n\n\n\n\nCode\n# Explicitly set the value of include_questions_as_comments\ninclude_questions_as_comments=False  # or False, depending on your needs\n\n# Get the markdown content\nbayesdown_questions = extract_bayesdown_questions_fixed(\n  \"ArgDown_WithQuestions.csv\",\n  \"BayesDownQuestions.md\", include_questions_as_comments=include_questions_as_comments\n)\n\n# Determine the output file path based on include_questions_as_comments\nif include_questions_as_comments: # Assuming include_questions_as_comments is defined somewhere in previous cells\n    output_file_path = \"FULL_BayesDownQuestions.md\"\nelse:\n    output_file_path = \"BayesDownQuestions.md\"\n\n# Save the markdown content to the appropriate file\nwith open(output_file_path, 'w') as f:\n    f.write(md_content)\n\nprint(f\"Markdown content saved to {output_file_path}\")\n\n\nLoading CSV from ArgDown_WithQuestions.csv...\nSuccessfully loaded CSV with 23 rows.\nGenerating BayesDown syntax with placeholder probabilities...\nBayesDown Questions saved to BayesDownQuestions.md\nMarkdown content saved to BayesDownQuestions.md\n\n\n\n\nCode\n# Generate BayesDown format\nbayesdown_questions = extract_bayesdown_questions_fixed(\n    \"ArgDown_WithQuestions.csv\",\n    \"FULL_BayesDownQuestions.md\",\n    include_questions_as_comments=True\n)\n\n# Display a preview of the format\nprint(\"\\nBayesDown Format Preview:\")\nprint(bayesdown_questions[:5000] + \"...\\n\")\n\n\nLoading CSV from ArgDown_WithQuestions.csv...\nSuccessfully loaded CSV with 23 rows.\nGenerating BayesDown syntax with placeholder probabilities...\nBayesDown Questions saved to FULL_BayesDownQuestions.md\n\nBayesDown Format Preview:\n# BayesDown Representation with Placeholder Probabilities\n\n/* This file contains BayesDown syntax with placeholder probabilities.\n   Replace the placeholders with actual probability values based on the \n   questions in the comments. */\n\n/* What is the probability for Existential_Catastrophe=existential_catastrophe_TRUE? */\n/* What is the probability for Existential_Catastrophe=existential_catastrophe_FALSE? */\n[Existential_Catastrophe]: The destruction of humanity's long-term potential due to AI systems we've lost control over. {\"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"], \"priors\": {\"What is the probability for Existential_Catastrophe=existential_catastrophe_TRUE?\": \"%?\", \"What is the probability for Existential_Catastrophe=existential_catastrophe_FALSE?\": \"%?\"}}\n/* What is the probability for Human_Disempowerment=human_disempowerment_TRUE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_FALSE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE? */\n[Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"], \"priors\": {\"What is the probability for Human_Disempowerment=human_disempowerment_TRUE?\": \"%?\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"?%\"}}\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  + [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {\"instantiations\": [\"scale_of_power_seeking_TRUE\", \"scale_of_power_seeking_FALSE\"], \"priors\": {\"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"%?\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\"}}\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    + [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"], \"priors\": {\"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE?\": \"%?\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\"}}\n      /* What is the probability for APS_Systems=aps_systems_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      + [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {\"instantiations\": [\"aps_systems_TRUE\", \"aps_systems_FALSE\"], \"priors\": {\"What is the probability for APS_Systems=aps_systems_TRUE?\": \"%?\", \"What is the probability for APS_Systems=aps_systems_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\"}}\n        /* What is the probability for Advanced_AI_Capability=advanced_ai_capability_TRUE? */\n        /* What is the probability for Advanced_AI_Capability=advanced_ai_capability_FALSE? */\n        + [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {\"instantiations\": [\"advanced_ai_capability_TRUE\", \"advanced_ai_capability_FALSE\"], \"priors\": {\"What is the probability for Advanced_AI_Capability=advanced_ai_capability_TRUE?\": \"%?\", \"What is the probability for Advanced_AI_Capability=advanced_ai_capability_FALSE?\": \"%?\"}}\n        /* What is the probability for Agentic_Planning=agentic_planning_TRUE? */\n        /* What is the probability for Agentic_Planning=agentic_planning_FALSE? */\n        + [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {\"instantiations\": [\"agentic_planning_TRUE\", \"agentic_planning_FALSE\"], \"priors\": {\"What is the probability for Agentic_Planning=agentic_planning_TRUE?\": \"%?\", \"What is the probability for Agentic_Planning=agentic_planning_FALSE?\": \"%?\"}}\n        /* What is the probability for Strategic_Awareness=strategic_awareness_TRUE? */\n        /* What is the probability for Strategic_Awareness=strategic_awareness_FALSE? */\n        + [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {\"instantiations\": [\"strategic_awareness_TRUE\", \"strategic_awareness_FALSE\"], \"priors\": {\"What is the probability for Strategic_Awareness=strategic_awareness_TRUE?\": \"%?\", \"What is the probability for Strategic_Awareness=strategic_awareness_FALSE?\": \"%?\"}}\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      + [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {\"instantiations\": [\"difficulty_of_alignment_TRUE\", \"difficulty_of_alignment_FALSE\"], \"priors\": {\"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE?\": \"%?\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\"}}\n        /* What is the probability for Instrumental_Convergence=instrumental_convergence_TRUE? */\n        /* What is the probability for Instrumental_Convergence=instrumental_convergence_FALSE? */\n        + [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {\"instantiations\": [\"instrumental_convergence_TRUE\", \"instrumental_convergence_FALSE\"], \"priors\": {\"What is the probability for Instrumental_Convergence=instrumental_convergence_TRUE?\": \"%?\", \"What is the probability for Instrumental_Convergence=instrumental_convergence_FALSE?\": \"%?\"}}\n        /* What is the probability for Problems_With_Proxies=problems_with_proxies_TRUE? */\n        /* What is the probability for Problems_With_Proxies=problems_with_proxies_FALSE? */\n        + [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {\"instantiations\": [\"problems_with_proxies_TRUE\", \"problems_with_proxies_FALSE\"], \"priors\": {\"What is the probability for Problems_With_Proxies=problems_with_proxies_TRUE?\": \"%?\", \"What is the probability for Problems_With_Proxies=problems_with_proxies_FALSE?\": \"%?\"}}\n        /* What is the probability for Problems_With_Search=problems_with_search_TRUE? */\n        /* What is the probability for Problems_With_Search=problems_with_search_FALSE? */\n        + [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {\"instantiations\": [\"problems_with_search_TRUE\", \"problems_with_search_FALSE\"], \"priors\": {\"What is the probability for Problems_With_Search=problems_with_search_TRUE?\": \"%?\", \"What is the probability for Problems_With_Search=problems_with_search_FALSE?\": \"%?\"}}\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE? */\n      + [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"], \"priors\": {\"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY?\": \"%?\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"%?\"}, \"posteriors\": {\"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\"}}\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        + [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {\"instantiations\": [\"incentives_to_build_aps_STRONG\", \"incentives_to_build_aps_WEAK\"], \"priors\": {\"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG?\": \"%?\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK?\": \"%?\"}, \"posteriors\": {\"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\"}}\n          /* What is the probability for Usefulness_Of_APS=usefulness_of_aps_HIGH? */\n          /* What is the probability for Usefulness_Of_APS=usefulness_of_aps_LOW? */\n          + [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {\"instantiations\": [\"usefulness_of_aps_HIGH\", \"usefulness_of_aps_LOW\"], \"priors\": {\"What is the probability for Usefulness_Of_APS=usefulness_of_aps_HIGH?\": \"%?\", \"What is the probability for Usefulness_Of_APS=usefulness_of_aps_LOW?\": \"%?\"}}\n          /* What is the probability for Competitive_Dynamics=competitive_dynamics_STRONG? */\n          /* What is the probability for Competitive_Dynamics=competitive_dynamics_WEAK? */\n          + [Competitive_Dynamics]: Competitive pressures between AI developers. {\"instantiations\": [\"competitive_dynamics_STRONG\", \"competitive_dynamics_WEAK\"], \"priors\": {\"What is the probability for Competitive_Dynamics=competitive_dynamics_STRONG?\": \"%?\", \"What is the probability for Competitive_Dynamics=competitive_dynamics_WEAK?\": \"%?\"}}\n        /* What is the probability for Deception_By_AI=deception_by_ai_TRUE? */\n        /* What is the probability for Deception_By_AI=deception_by_ai_FALSE? */\n        + [Deception_By_AI]: AI systems deceiving humans about their true objectives. {\"instantiations\": [\"deception_by_ai_TRUE\", \"deception_by_ai_FALSE\"], \"priors\": {\"What is the probability for Deception_By_AI=deception_by_ai_TRUE?\": \"%?\", \"What is the probability for Deception_By_AI=deception_by_ai_FALSE?\": \"%?\"}}\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    + [Corrective_Feedback]: Human society implementing corrections after observing problems. {\"instantiations\": [\"corrective_feedback_EFFECTIVE\", \"corrective_feedback_INEFFECTIVE\"], \"priors\": {\"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"%?\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\"}}\n      /* What is the probability for Warning_Shots=warning_shots_OBSERVED? */\n      /* What is the probability for Warning_Shots=warning_shots_UNOBSERVED? */\n      + [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {\"instantiations\": [\"warning_shots_OBSERVED\", \"warning_shots_UNOBSERVED\"], \"priors\": {\"What is the probability for Warning_Shots=warning_shots_OBSERVED?\": \"%?\", \"What is the probability for Warning_Shots=warning_shots_UNOBSERVED?\": \"%?\"}}\n      /* What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n      /* What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n      + [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {\"instantiations\": [\"rapid_capability_escalation_TRUE\", \"rapid_capability_escalation_FALSE\"], \"priors\": {\"What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"%?\", \"What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"%?\"}}\n/* What is the probability for Barriers_To_Understanding=barriers_to_understanding_HIGH? */\n/* What is the probability for Barriers_To_Understanding=barriers_to_understanding_LOW? */\n[Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {\"instantiations\": [\"barriers_to_understanding_HIGH\", \"barriers_to_understanding_LOW\"], \"priors\": {\"What is the probability for Barriers_To_Understanding=barriers_to_understanding_HIGH?\": \"%?\", \"What is the probability for Barriers_To_Understanding=barriers_to_understanding_LOW?\": \"%?\"}}\n/* What is the probability for Adversarial_Dynamics=adversarial_dynamics_TRUE? */\n/* What is the probability for Adversarial_Dynamics=adversarial_dynamics_FALSE? */\n[Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {\"instantiations\": [\"adversarial_dynamics_TRUE\", \"adversarial_dynamics_FALSE\"], \"priors\": {\"What is the probability for Adversarial_Dynamics=adversarial_dynamics_TRUE?\": \"%?\", \"What is the probability for Adversarial_Dynamics=adversarial_dynamics_FALSE?\": \"%?\"}}\n/* What is the probability for Stakes_Of_Error=stakes_of_error_HIGH? */\n/* What is the probability for Stakes_Of_Error=stakes_of_error_LOW? */\n[Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {\"instantiations\": [\"stakes_of_error_HIGH\", \"stakes_of_error_LOW\"], \"priors\": {\"What is the probability for Stakes_Of_Error=stakes_of_error_HIGH?\": \"%?\", \"What is the probability for Stakes_Of_Error=stakes_of_error_LOW?\": \"%?\"}}\n...\n\n\n\n\n\nCode\n# Load and print the content of the 'FULL_BayesDownQuestions.md' file\nwith open(\"FULL_BayesDownQuestions.md\", \"r\") as f:\n    file_content = f.read()\n    print(file_content)\n\n\n# BayesDown Representation with Placeholder Probabilities\n\n/* This file contains BayesDown syntax with placeholder probabilities.\n   Replace the placeholders with actual probability values based on the \n   questions in the comments. */\n\n/* What is the probability for Existential_Catastrophe=existential_catastrophe_TRUE? */\n/* What is the probability for Existential_Catastrophe=existential_catastrophe_FALSE? */\n[Existential_Catastrophe]: The destruction of humanity's long-term potential due to AI systems we've lost control over. {\"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"], \"priors\": {\"What is the probability for Existential_Catastrophe=existential_catastrophe_TRUE?\": \"%?\", \"What is the probability for Existential_Catastrophe=existential_catastrophe_FALSE?\": \"%?\"}}\n/* What is the probability for Human_Disempowerment=human_disempowerment_TRUE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_FALSE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE? */\n[Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"], \"priors\": {\"What is the probability for Human_Disempowerment=human_disempowerment_TRUE?\": \"%?\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"?%\"}}\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  + [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {\"instantiations\": [\"scale_of_power_seeking_TRUE\", \"scale_of_power_seeking_FALSE\"], \"priors\": {\"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"%?\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\"}}\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    + [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"], \"priors\": {\"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE?\": \"%?\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\"}}\n      /* What is the probability for APS_Systems=aps_systems_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      + [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {\"instantiations\": [\"aps_systems_TRUE\", \"aps_systems_FALSE\"], \"priors\": {\"What is the probability for APS_Systems=aps_systems_TRUE?\": \"%?\", \"What is the probability for APS_Systems=aps_systems_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\"}}\n        /* What is the probability for Advanced_AI_Capability=advanced_ai_capability_TRUE? */\n        /* What is the probability for Advanced_AI_Capability=advanced_ai_capability_FALSE? */\n        + [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {\"instantiations\": [\"advanced_ai_capability_TRUE\", \"advanced_ai_capability_FALSE\"], \"priors\": {\"What is the probability for Advanced_AI_Capability=advanced_ai_capability_TRUE?\": \"%?\", \"What is the probability for Advanced_AI_Capability=advanced_ai_capability_FALSE?\": \"%?\"}}\n        /* What is the probability for Agentic_Planning=agentic_planning_TRUE? */\n        /* What is the probability for Agentic_Planning=agentic_planning_FALSE? */\n        + [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {\"instantiations\": [\"agentic_planning_TRUE\", \"agentic_planning_FALSE\"], \"priors\": {\"What is the probability for Agentic_Planning=agentic_planning_TRUE?\": \"%?\", \"What is the probability for Agentic_Planning=agentic_planning_FALSE?\": \"%?\"}}\n        /* What is the probability for Strategic_Awareness=strategic_awareness_TRUE? */\n        /* What is the probability for Strategic_Awareness=strategic_awareness_FALSE? */\n        + [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {\"instantiations\": [\"strategic_awareness_TRUE\", \"strategic_awareness_FALSE\"], \"priors\": {\"What is the probability for Strategic_Awareness=strategic_awareness_TRUE?\": \"%?\", \"What is the probability for Strategic_Awareness=strategic_awareness_FALSE?\": \"%?\"}}\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      + [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {\"instantiations\": [\"difficulty_of_alignment_TRUE\", \"difficulty_of_alignment_FALSE\"], \"priors\": {\"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE?\": \"%?\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\"}}\n        /* What is the probability for Instrumental_Convergence=instrumental_convergence_TRUE? */\n        /* What is the probability for Instrumental_Convergence=instrumental_convergence_FALSE? */\n        + [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {\"instantiations\": [\"instrumental_convergence_TRUE\", \"instrumental_convergence_FALSE\"], \"priors\": {\"What is the probability for Instrumental_Convergence=instrumental_convergence_TRUE?\": \"%?\", \"What is the probability for Instrumental_Convergence=instrumental_convergence_FALSE?\": \"%?\"}}\n        /* What is the probability for Problems_With_Proxies=problems_with_proxies_TRUE? */\n        /* What is the probability for Problems_With_Proxies=problems_with_proxies_FALSE? */\n        + [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {\"instantiations\": [\"problems_with_proxies_TRUE\", \"problems_with_proxies_FALSE\"], \"priors\": {\"What is the probability for Problems_With_Proxies=problems_with_proxies_TRUE?\": \"%?\", \"What is the probability for Problems_With_Proxies=problems_with_proxies_FALSE?\": \"%?\"}}\n        /* What is the probability for Problems_With_Search=problems_with_search_TRUE? */\n        /* What is the probability for Problems_With_Search=problems_with_search_FALSE? */\n        + [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {\"instantiations\": [\"problems_with_search_TRUE\", \"problems_with_search_FALSE\"], \"priors\": {\"What is the probability for Problems_With_Search=problems_with_search_TRUE?\": \"%?\", \"What is the probability for Problems_With_Search=problems_with_search_FALSE?\": \"%?\"}}\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE? */\n      + [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"], \"priors\": {\"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY?\": \"%?\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"%?\"}, \"posteriors\": {\"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\"}}\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        + [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {\"instantiations\": [\"incentives_to_build_aps_STRONG\", \"incentives_to_build_aps_WEAK\"], \"priors\": {\"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG?\": \"%?\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK?\": \"%?\"}, \"posteriors\": {\"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\"}}\n          /* What is the probability for Usefulness_Of_APS=usefulness_of_aps_HIGH? */\n          /* What is the probability for Usefulness_Of_APS=usefulness_of_aps_LOW? */\n          + [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {\"instantiations\": [\"usefulness_of_aps_HIGH\", \"usefulness_of_aps_LOW\"], \"priors\": {\"What is the probability for Usefulness_Of_APS=usefulness_of_aps_HIGH?\": \"%?\", \"What is the probability for Usefulness_Of_APS=usefulness_of_aps_LOW?\": \"%?\"}}\n          /* What is the probability for Competitive_Dynamics=competitive_dynamics_STRONG? */\n          /* What is the probability for Competitive_Dynamics=competitive_dynamics_WEAK? */\n          + [Competitive_Dynamics]: Competitive pressures between AI developers. {\"instantiations\": [\"competitive_dynamics_STRONG\", \"competitive_dynamics_WEAK\"], \"priors\": {\"What is the probability for Competitive_Dynamics=competitive_dynamics_STRONG?\": \"%?\", \"What is the probability for Competitive_Dynamics=competitive_dynamics_WEAK?\": \"%?\"}}\n        /* What is the probability for Deception_By_AI=deception_by_ai_TRUE? */\n        /* What is the probability for Deception_By_AI=deception_by_ai_FALSE? */\n        + [Deception_By_AI]: AI systems deceiving humans about their true objectives. {\"instantiations\": [\"deception_by_ai_TRUE\", \"deception_by_ai_FALSE\"], \"priors\": {\"What is the probability for Deception_By_AI=deception_by_ai_TRUE?\": \"%?\", \"What is the probability for Deception_By_AI=deception_by_ai_FALSE?\": \"%?\"}}\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    + [Corrective_Feedback]: Human society implementing corrections after observing problems. {\"instantiations\": [\"corrective_feedback_EFFECTIVE\", \"corrective_feedback_INEFFECTIVE\"], \"priors\": {\"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"%?\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\"}}\n      /* What is the probability for Warning_Shots=warning_shots_OBSERVED? */\n      /* What is the probability for Warning_Shots=warning_shots_UNOBSERVED? */\n      + [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {\"instantiations\": [\"warning_shots_OBSERVED\", \"warning_shots_UNOBSERVED\"], \"priors\": {\"What is the probability for Warning_Shots=warning_shots_OBSERVED?\": \"%?\", \"What is the probability for Warning_Shots=warning_shots_UNOBSERVED?\": \"%?\"}}\n      /* What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n      /* What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n      + [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {\"instantiations\": [\"rapid_capability_escalation_TRUE\", \"rapid_capability_escalation_FALSE\"], \"priors\": {\"What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"%?\", \"What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"%?\"}}\n/* What is the probability for Barriers_To_Understanding=barriers_to_understanding_HIGH? */\n/* What is the probability for Barriers_To_Understanding=barriers_to_understanding_LOW? */\n[Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {\"instantiations\": [\"barriers_to_understanding_HIGH\", \"barriers_to_understanding_LOW\"], \"priors\": {\"What is the probability for Barriers_To_Understanding=barriers_to_understanding_HIGH?\": \"%?\", \"What is the probability for Barriers_To_Understanding=barriers_to_understanding_LOW?\": \"%?\"}}\n/* What is the probability for Adversarial_Dynamics=adversarial_dynamics_TRUE? */\n/* What is the probability for Adversarial_Dynamics=adversarial_dynamics_FALSE? */\n[Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {\"instantiations\": [\"adversarial_dynamics_TRUE\", \"adversarial_dynamics_FALSE\"], \"priors\": {\"What is the probability for Adversarial_Dynamics=adversarial_dynamics_TRUE?\": \"%?\", \"What is the probability for Adversarial_Dynamics=adversarial_dynamics_FALSE?\": \"%?\"}}\n/* What is the probability for Stakes_Of_Error=stakes_of_error_HIGH? */\n/* What is the probability for Stakes_Of_Error=stakes_of_error_LOW? */\n[Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {\"instantiations\": [\"stakes_of_error_HIGH\", \"stakes_of_error_LOW\"], \"priors\": {\"What is the probability for Stakes_Of_Error=stakes_of_error_HIGH?\": \"%?\", \"What is the probability for Stakes_Of_Error=stakes_of_error_LOW?\": \"%?\"}}\n\n\n\n\n\nCode\n# Generate BayesDown format\nbayesdown_questions = extract_bayesdown_questions_fixed(\n    \"ArgDown_WithQuestions.csv\",\n    \"BayesDownQuestions.md\",\n    include_questions_as_comments=False\n)\n\n# Display a preview of the format\nprint(\n\n)\nprint(bayesdown_questions[:50000] + \"...\\n\")\n\n\nLoading CSV from ArgDown_WithQuestions.csv...\nSuccessfully loaded CSV with 23 rows.\nGenerating BayesDown syntax with placeholder probabilities...\nBayesDown Questions saved to BayesDownQuestions.md\n\n[Existential_Catastrophe]: The destruction of humanity's long-term potential due to AI systems we've lost control over. {\"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"], \"priors\": {\"What is the probability for Existential_Catastrophe=existential_catastrophe_TRUE?\": \"%?\", \"What is the probability for Existential_Catastrophe=existential_catastrophe_FALSE?\": \"%?\"}}\n[Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"], \"priors\": {\"What is the probability for Human_Disempowerment=human_disempowerment_TRUE?\": \"%?\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"?%\"}}\n  + [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {\"instantiations\": [\"scale_of_power_seeking_TRUE\", \"scale_of_power_seeking_FALSE\"], \"priors\": {\"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"%?\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\"}}\n    + [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"], \"priors\": {\"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE?\": \"%?\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\"}}\n      + [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {\"instantiations\": [\"aps_systems_TRUE\", \"aps_systems_FALSE\"], \"priors\": {\"What is the probability for APS_Systems=aps_systems_TRUE?\": \"%?\", \"What is the probability for APS_Systems=aps_systems_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\"}}\n        + [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {\"instantiations\": [\"advanced_ai_capability_TRUE\", \"advanced_ai_capability_FALSE\"], \"priors\": {\"What is the probability for Advanced_AI_Capability=advanced_ai_capability_TRUE?\": \"%?\", \"What is the probability for Advanced_AI_Capability=advanced_ai_capability_FALSE?\": \"%?\"}}\n        + [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {\"instantiations\": [\"agentic_planning_TRUE\", \"agentic_planning_FALSE\"], \"priors\": {\"What is the probability for Agentic_Planning=agentic_planning_TRUE?\": \"%?\", \"What is the probability for Agentic_Planning=agentic_planning_FALSE?\": \"%?\"}}\n        + [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {\"instantiations\": [\"strategic_awareness_TRUE\", \"strategic_awareness_FALSE\"], \"priors\": {\"What is the probability for Strategic_Awareness=strategic_awareness_TRUE?\": \"%?\", \"What is the probability for Strategic_Awareness=strategic_awareness_FALSE?\": \"%?\"}}\n      + [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {\"instantiations\": [\"difficulty_of_alignment_TRUE\", \"difficulty_of_alignment_FALSE\"], \"priors\": {\"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE?\": \"%?\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\"}}\n        + [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {\"instantiations\": [\"instrumental_convergence_TRUE\", \"instrumental_convergence_FALSE\"], \"priors\": {\"What is the probability for Instrumental_Convergence=instrumental_convergence_TRUE?\": \"%?\", \"What is the probability for Instrumental_Convergence=instrumental_convergence_FALSE?\": \"%?\"}}\n        + [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {\"instantiations\": [\"problems_with_proxies_TRUE\", \"problems_with_proxies_FALSE\"], \"priors\": {\"What is the probability for Problems_With_Proxies=problems_with_proxies_TRUE?\": \"%?\", \"What is the probability for Problems_With_Proxies=problems_with_proxies_FALSE?\": \"%?\"}}\n        + [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {\"instantiations\": [\"problems_with_search_TRUE\", \"problems_with_search_FALSE\"], \"priors\": {\"What is the probability for Problems_With_Search=problems_with_search_TRUE?\": \"%?\", \"What is the probability for Problems_With_Search=problems_with_search_FALSE?\": \"%?\"}}\n      + [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"], \"priors\": {\"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY?\": \"%?\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"%?\"}, \"posteriors\": {\"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\"}}\n        + [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {\"instantiations\": [\"incentives_to_build_aps_STRONG\", \"incentives_to_build_aps_WEAK\"], \"priors\": {\"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG?\": \"%?\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK?\": \"%?\"}, \"posteriors\": {\"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\"}}\n          + [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {\"instantiations\": [\"usefulness_of_aps_HIGH\", \"usefulness_of_aps_LOW\"], \"priors\": {\"What is the probability for Usefulness_Of_APS=usefulness_of_aps_HIGH?\": \"%?\", \"What is the probability for Usefulness_Of_APS=usefulness_of_aps_LOW?\": \"%?\"}}\n          + [Competitive_Dynamics]: Competitive pressures between AI developers. {\"instantiations\": [\"competitive_dynamics_STRONG\", \"competitive_dynamics_WEAK\"], \"priors\": {\"What is the probability for Competitive_Dynamics=competitive_dynamics_STRONG?\": \"%?\", \"What is the probability for Competitive_Dynamics=competitive_dynamics_WEAK?\": \"%?\"}}\n        + [Deception_By_AI]: AI systems deceiving humans about their true objectives. {\"instantiations\": [\"deception_by_ai_TRUE\", \"deception_by_ai_FALSE\"], \"priors\": {\"What is the probability for Deception_By_AI=deception_by_ai_TRUE?\": \"%?\", \"What is the probability for Deception_By_AI=deception_by_ai_FALSE?\": \"%?\"}}\n    + [Corrective_Feedback]: Human society implementing corrections after observing problems. {\"instantiations\": [\"corrective_feedback_EFFECTIVE\", \"corrective_feedback_INEFFECTIVE\"], \"priors\": {\"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"%?\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\"}}\n      + [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {\"instantiations\": [\"warning_shots_OBSERVED\", \"warning_shots_UNOBSERVED\"], \"priors\": {\"What is the probability for Warning_Shots=warning_shots_OBSERVED?\": \"%?\", \"What is the probability for Warning_Shots=warning_shots_UNOBSERVED?\": \"%?\"}}\n      + [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {\"instantiations\": [\"rapid_capability_escalation_TRUE\", \"rapid_capability_escalation_FALSE\"], \"priors\": {\"What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"%?\", \"What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"%?\"}}\n[Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {\"instantiations\": [\"barriers_to_understanding_HIGH\", \"barriers_to_understanding_LOW\"], \"priors\": {\"What is the probability for Barriers_To_Understanding=barriers_to_understanding_HIGH?\": \"%?\", \"What is the probability for Barriers_To_Understanding=barriers_to_understanding_LOW?\": \"%?\"}}\n[Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {\"instantiations\": [\"adversarial_dynamics_TRUE\", \"adversarial_dynamics_FALSE\"], \"priors\": {\"What is the probability for Adversarial_Dynamics=adversarial_dynamics_TRUE?\": \"%?\", \"What is the probability for Adversarial_Dynamics=adversarial_dynamics_FALSE?\": \"%?\"}}\n[Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {\"instantiations\": [\"stakes_of_error_HIGH\", \"stakes_of_error_LOW\"], \"priors\": {\"What is the probability for Stakes_Of_Error=stakes_of_error_HIGH?\": \"%?\", \"What is the probability for Stakes_Of_Error=stakes_of_error_LOW?\": \"%?\"}}\n...",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#generate-bayesdown-probability-extraction-prompt",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#generate-bayesdown-probability-extraction-prompt",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "F.4 2.3 Generate BayesDown Probability Extraction Prompt",
    "text": "F.4 2.3 Generate BayesDown Probability Extraction Prompt\nGenerate 2nd Extraction Prompt for Probabilities based on the questions generated from the ‘ArgDown.csv’ extraction\n\nF.4.1 2.3.0 BayesDown Format Specification\nBayesDown extends ArgDown with probability data in a structured JSON format to represent Bayesian networks. This intermediate representation bridges the gap between natural language arguments and formal probabilistic models, preserving both narrative structure and quantitative relationships.\n\nF.4.1.1 Core Structure\nA BayesDown representation consists of:\n\nNodes: Variables or statements in brackets [Node_Name] with descriptive text\nRelationships: Hierarchical structure with indentation and + symbols\nMetadata: JSON objects containing probability information:\n\n{\n  \"instantiations\": [\"state_TRUE\", \"state_FALSE\"],  // Possible states of variable\n  \"priors\": {\n    \"p(state_TRUE)\": \"0.7\",   // Unconditional probability of state_TRUE\n    \"p(state_FALSE)\": \"0.3\"   // Unconditional probability of state_FALSE\n  },\n  \"posteriors\": {\n    \"p(state_TRUE|condition1_TRUE,condition2_FALSE)\": \"0.9\",  // Conditional on parent states\n    \"p(state_TRUE|condition1_FALSE,condition2_TRUE)\": \"0.4\"   // Different parent configuration\n  }\n}\n\n### 2.3.1 Rain-Sprinkler-Lawn Example\n[Grass_Wet]: Concentrated moisture on grass. {\"instantiations\": [\"grass_wet_TRUE\", \"grass_wet_FALSE\"],\n\"priors\": {\"p(grass_wet_TRUE)\": \"0.322\", \"p(grass_wet_FALSE)\": \"0.678\"},\n\"posteriors\": {\"p(grass_wet_TRUE|sprinkler_TRUE,rain_TRUE)\": \"0.99\",\n\"p(grass_wet_TRUE|sprinkler_TRUE,rain_FALSE)\": \"0.9\",\n\"p(grass_wet_TRUE|sprinkler_FALSE,rain_TRUE)\": \"0.8\",\n\"p(grass_wet_TRUE|sprinkler_FALSE,rain_FALSE)\": \"0.0\"}}\n + [Rain]: Water falling from the sky. {\"instantiations\": [\"rain_TRUE\", \"rain_FALSE\"],\n \"priors\": {\"p(rain_TRUE)\": \"0.2\", \"p(rain_FALSE)\": \"0.8\"}}\n + [Sprinkler]: Artificial watering system. {\"instantiations\": [\"sprinkler_TRUE\", \"sprinkler_FALSE\"],\n \"priors\": {\"p(sprinkler_TRUE)\": \"0.44838\", \"p(sprinkler_FALSE)\": \"0.55162\"},\n \"posteriors\": {\"p(sprinkler_TRUE|rain_TRUE)\": \"0.01\", \"p(sprinkler_TRUE|rain_FALSE)\": \"0.4\"}}\n   + [Rain]\n\n\nIn this example:\n\n+ Grass_Wet is the effect/outcome node\n+ Rain and Sprinkler are parent nodes (causes)\n+ Rain also influences Sprinkler (people tend not to use sprinklers when it's raining)\n\nRole in AMTAIR\nBayesDown serves as the critical intermediate representation in the AMTAIR extraction pipeline, bridging between qualitative arguments in AI safety literature and formal Bayesian networks that can be used for probabilistic reasoning and policy evaluation. By preserving both narrative explanation and probabilistic information, it enables the automated extraction of world models while maintaining traceability to the original arguments.\nFor full syntax details, see the BayesDownSyntax.md file in the repository.\n\n### 2.3.2 Probability Extraction Process\nThe probability extraction pipeline follows these steps:\n\n\nIdentify variables and their possible states\nExtract prior probability statements\nIdentify conditional relationships\nExtract conditional probability statements\nFormat the data in BayesDown syntax\n\n### 2.3.3 Implementation Steps\nTo extract probabilities and create BayesDown format:\n\nRun the extract_probabilities function on ArgDown text\nProcess the results into a structured format\nValidate the probability distributions (ensure they sum to 1)\nGenerate the enhanced BayesDown representation\n\n### 2.3.4 Validation and Quality Control\nThe probability extraction process includes validation steps:\n\nEnsuring coherent probability distributions\nChecking for logical consistency in conditional relationships\nVerifying that all required probability statements are present\nHandling missing data with appropriate default values\n\n## 2.4 Prepare 2nd API call\n\n## 2.5 Make BayesDown Probability Extraction API Call\n\n## 2.6 Save BayesDown with Probability Estimates (.csv)\n\n## 2.7 Review & Verify BayesDown Probability Estimates\n\n## 2.7.2 Check the Graph Structure with the ArgDown Sandbox Online\nCopy and paste the BayesDown formatted ... in the ArgDown Sandbox below to quickly verify that the network renders correctly.\n\n## 2.8 Extract BayesDown with Probability Estimates as Dataframe\n\n# 3 Data Extraction: BayesDown (.md) to Database (.csv)\n\n## 3.0 BayesDown to Structured Data: Network Construction\n\n## Extraction Pipeline Overview\n\nThis section implements the core extraction pipeline described in the AMTAIR project documentation (see `PY_TechnicalImplementation.md`), which transforms structured argument representations into formal Bayesian networks through a series of processing steps:\n\n1. **Input**: Text in BayesDown format (see Section 2.3.1)\n2. **Parsing**: Extract nodes, relationships, and probability information\n3. **Structuring**: Organize into a DataFrame with formal relationships\n4. **Enhancement**: Add derived properties and network metrics\n5. **Output**: Structured data ready for Bayesian network construction\n\n### Theoretical Foundation\n\nThis implementation follows the extraction algorithm outlined in the AMTAIR project description:\n\n1. Get nodes: All premises and conclusions from the argument structure\n2. Get edges: Parent-child relationships between nodes\n3. Extract probability distributions: Prior and conditional probabilities\n4. Calculate derived metrics: Network statistics and node classifications\n\nThe resulting structured data maintains the complete information needed to reconstruct the Bayesian network while enabling additional analysis and visualization.\n\n### Role in Thesis Research\n\nThis extraction pipeline represents a key contribution of the Master's thesis, demonstrating how argument structures from AI safety literature can be automatically transformed into formal probabilistic models. While the current implementation focuses on pre-formatted BayesDown, the architecture is designed to be extended with LLM-powered extraction directly from natural language in future work.\n\nThe rain-sprinkler-lawn example serves as a simple but complete test case, demonstrating every step in the pipeline from structured text to interactive Bayesian network visualization.\n\n### 3.0.0 ExtractBayesDown-Data_v1\nBuild data frame with extractable information from BayesDown\n\n::: {#cell-64 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":157}}' outputId='e27e2c8c-bcb6-4e6b-e507-2b67d48ba814'}\n``` {.python .cell-code}\n# read sprinkler example -- Occam Colab Online\nfile_path_ex_rain = \"https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/BayesDown.md\"\n\n# Use requests.get to fetch content from URL\nresponse = requests.get(file_path_ex_rain)\nresponse.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n\n# Read content from the response\nmd_content_ex_rain = response.text\n\nmd_content_ex_rain\n\n'[Existential_Catastrophe]: The destruction of humanity\\'s long-term potential due to AI systems we\\'ve lost control over. {\"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"], \"priors\": {\"p(existential_catastrophe_TRUE)\": \"0.05\", \"p(existential_catastrophe_FALSE)\": \"0.95\"}, \"posteriors\": {\"p(existential_catastrophe_TRUE|human_disempowerment_TRUE)\": \"0.95\", \"p(existential_catastrophe_TRUE|human_disempowerment_FALSE)\": \"0.0\", \"p(existential_catastrophe_FALSE|human_disempowerment_TRUE)\": \"0.05\", \"p(existential_catastrophe_FALSE|human_disempowerment_FALSE)\": \"1.0\"}}\\n- [Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"], \"priors\": {\"p(human_disempowerment_TRUE)\": \"0.208\", \"p(human_disempowerment_FALSE)\": \"0.792\"}, \"posteriors\": {\"p(human_disempowerment_TRUE|scale_of_power_seeking_TRUE)\": \"1.0\", \"p(human_disempowerment_TRUE|scale_of_power_seeking_FALSE)\": \"0.0\", \"p(human_disempowerment_FALSE|scale_of_power_seeking_TRUE)\": \"0.0\", \"p(human_disempowerment_FALSE|scale_of_power_seeking_FALSE)\": \"1.0\"}}\\n    - [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {\"instantiations\": [\"scale_of_power_seeking_TRUE\", \"scale_of_power_seeking_FALSE\"], \"priors\": {\"p(scale_of_power_seeking_TRUE)\": \"0.208\", \"p(scale_of_power_seeking_FALSE)\": \"0.792\"}, \"posteriors\": {\"p(scale_of_power_seeking_TRUE|misaligned_power_seeking_TRUE, corrective_feedback_EFFECTIVE)\": \"0.25\", \"p(scale_of_power_seeking_TRUE|misaligned_power_seeking_TRUE, corrective_feedback_INEFFECTIVE)\": \"0.60\", \"p(scale_of_power_seeking_TRUE|misaligned_power_seeking_FALSE, corrective_feedback_EFFECTIVE)\": \"0.0\", \"p(scale_of_power_seeking_TRUE|misaligned_power_seeking_FALSE, corrective_feedback_INEFFECTIVE)\": \"0.0\", \"p(scale_of_power_seeking_FALSE|misaligned_power_seeking_TRUE, corrective_feedback_EFFECTIVE)\": \"0.75\", \"p(scale_of_power_seeking_FALSE|misaligned_power_seeking_TRUE, corrective_feedback_INEFFECTIVE)\": \"0.40\", \"p(scale_of_power_seeking_FALSE|misaligned_power_seeking_FALSE, corrective_feedback_EFFECTIVE)\": \"1.0\", \"p(scale_of_power_seeking_FALSE|misaligned_power_seeking_FALSE, corrective_feedback_INEFFECTIVE)\": \"1.0\"}}\\n        - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"], \"priors\": {\"p(misaligned_power_seeking_TRUE)\": \"0.338\", \"p(misaligned_power_seeking_FALSE)\": \"0.662\"}, \"posteriors\": {\"p(misaligned_power_seeking_TRUE|aps_systems_TRUE, difficulty_of_alignment_TRUE, deployment_decisions_DEPLOY)\": \"0.90\", \"p(misaligned_power_seeking_TRUE|aps_systems_TRUE, difficulty_of_alignment_TRUE, deployment_decisions_WITHHOLD)\": \"0.10\", \"p(misaligned_power_seeking_TRUE|aps_systems_TRUE, difficulty_of_alignment_FALSE, deployment_decisions_DEPLOY)\": \"0.25\", \"p(misaligned_power_seeking_TRUE|aps_systems_TRUE, difficulty_of_alignment_FALSE, deployment_decisions_WITHHOLD)\": \"0.05\", \"p(misaligned_power_seeking_TRUE|aps_systems_FALSE, difficulty_of_alignment_TRUE, deployment_decisions_DEPLOY)\": \"0.0\", \"p(misaligned_power_seeking_TRUE|aps_systems_FALSE, difficulty_of_alignment_TRUE, deployment_decisions_WITHHOLD)\": \"0.0\", \"p(misaligned_power_seeking_TRUE|aps_systems_FALSE, difficulty_of_alignment_FALSE, deployment_decisions_DEPLOY)\": \"0.0\", \"p(misaligned_power_seeking_TRUE|aps_systems_FALSE, difficulty_of_alignment_FALSE, deployment_decisions_WITHHOLD)\": \"0.0\", \"p(misaligned_power_seeking_FALSE|aps_systems_TRUE, difficulty_of_alignment_TRUE, deployment_decisions_DEPLOY)\": \"0.10\", \"p(misaligned_power_seeking_FALSE|aps_systems_TRUE, difficulty_of_alignment_TRUE, deployment_decisions_WITHHOLD)\": \"0.90\", \"p(misaligned_power_seeking_FALSE|aps_systems_TRUE, difficulty_of_alignment_FALSE, deployment_decisions_DEPLOY)\": \"0.75\", \"p(misaligned_power_seeking_FALSE|aps_systems_TRUE, difficulty_of_alignment_FALSE, deployment_decisions_WITHHOLD)\": \"0.95\", \"p(misaligned_power_seeking_FALSE|aps_systems_FALSE, difficulty_of_alignment_TRUE, deployment_decisions_DEPLOY)\": \"1.0\", \"p(misaligned_power_seeking_FALSE|aps_systems_FALSE, difficulty_of_alignment_TRUE, deployment_decisions_WITHHOLD)\": \"1.0\", \"p(misaligned_power_seeking_FALSE|aps_systems_FALSE, difficulty_of_alignment_FALSE, deployment_decisions_DEPLOY)\": \"1.0\", \"p(misaligned_power_seeking_FALSE|aps_systems_FALSE, difficulty_of_alignment_FALSE, deployment_decisions_WITHHOLD)\": \"1.0\"}}\\n            - [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {\"instantiations\": [\"aps_systems_TRUE\", \"aps_systems_FALSE\"], \"priors\": {\"p(aps_systems_TRUE)\": \"0.65\", \"p(aps_systems_FALSE)\": \"0.35\"}, \"posteriors\": {\"p(aps_systems_TRUE|advanced_ai_capability_TRUE, agentic_planning_TRUE, strategic_awareness_TRUE)\": \"1.0\", \"p(aps_systems_TRUE|advanced_ai_capability_TRUE, agentic_planning_TRUE, strategic_awareness_FALSE)\": \"0.0\", \"p(aps_systems_TRUE|advanced_ai_capability_TRUE, agentic_planning_FALSE, strategic_awareness_TRUE)\": \"0.0\", \"p(aps_systems_TRUE|advanced_ai_capability_TRUE, agentic_planning_FALSE, strategic_awareness_FALSE)\": \"0.0\", \"p(aps_systems_TRUE|advanced_ai_capability_FALSE, agentic_planning_TRUE, strategic_awareness_TRUE)\": \"0.0\", \"p(aps_systems_TRUE|advanced_ai_capability_FALSE, agentic_planning_TRUE, strategic_awareness_FALSE)\": \"0.0\", \"p(aps_systems_TRUE|advanced_ai_capability_FALSE, agentic_planning_FALSE, strategic_awareness_TRUE)\": \"0.0\", \"p(aps_systems_TRUE|advanced_ai_capability_FALSE, agentic_planning_FALSE, strategic_awareness_FALSE)\": \"0.0\", \"p(aps_systems_FALSE|advanced_ai_capability_TRUE, agentic_planning_TRUE, strategic_awareness_TRUE)\": \"0.0\", \"p(aps_systems_FALSE|advanced_ai_capability_TRUE, agentic_planning_TRUE, strategic_awareness_FALSE)\": \"1.0\", \"p(aps_systems_FALSE|advanced_ai_capability_TRUE, agentic_planning_FALSE, strategic_awareness_TRUE)\": \"1.0\", \"p(aps_systems_FALSE|advanced_ai_capability_TRUE, agentic_planning_FALSE, strategic_awareness_FALSE)\": \"1.0\", \"p(aps_systems_FALSE|advanced_ai_capability_FALSE, agentic_planning_TRUE, strategic_awareness_TRUE)\": \"1.0\", \"p(aps_systems_FALSE|advanced_ai_capability_FALSE, agentic_planning_TRUE, strategic_awareness_FALSE)\": \"1.0\", \"p(aps_systems_FALSE|advanced_ai_capability_FALSE, agentic_planning_FALSE, strategic_awareness_TRUE)\": \"1.0\", \"p(aps_systems_FALSE|advanced_ai_capability_FALSE, agentic_planning_FALSE, strategic_awareness_FALSE)\": \"1.0\"}}\\n                - [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {\"instantiations\": [\"advanced_ai_capability_TRUE\", \"advanced_ai_capability_FALSE\"], \"priors\": {\"p(advanced_ai_capability_TRUE)\": \"0.80\", \"p(advanced_ai_capability_FALSE)\": \"0.20\"}}\\n                - [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {\"instantiations\": [\"agentic_planning_TRUE\", \"agentic_planning_FALSE\"], \"priors\": {\"p(agentic_planning_TRUE)\": \"0.85\", \"p(agentic_planning_FALSE)\": \"0.15\"}}\\n                - [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {\"instantiations\": [\"strategic_awareness_TRUE\", \"strategic_awareness_FALSE\"], \"priors\": {\"p(strategic_awareness_TRUE)\": \"0.75\", \"p(strategic_awareness_FALSE)\": \"0.25\"}}\\n            - [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {\"instantiations\": [\"difficulty_of_alignment_TRUE\", \"difficulty_of_alignment_FALSE\"], \"priors\": {\"p(difficulty_of_alignment_TRUE)\": \"0.40\", \"p(difficulty_of_alignment_FALSE)\": \"0.60\"}, \"posteriors\": {\"p(difficulty_of_alignment_TRUE|instrumental_convergence_TRUE, problems_with_proxies_TRUE, problems_with_search_TRUE)\": \"0.85\", \"p(difficulty_of_alignment_TRUE|instrumental_convergence_TRUE, problems_with_proxies_TRUE, problems_with_search_FALSE)\": \"0.70\", \"p(difficulty_of_alignment_TRUE|instrumental_convergence_TRUE, problems_with_proxies_FALSE, problems_with_search_TRUE)\": \"0.60\", \"p(difficulty_of_alignment_TRUE|instrumental_convergence_TRUE, problems_with_proxies_FALSE, problems_with_search_FALSE)\": \"0.40\", \"p(difficulty_of_alignment_TRUE|instrumental_convergence_FALSE, problems_with_proxies_TRUE, problems_with_search_TRUE)\": \"0.55\", \"p(difficulty_of_alignment_TRUE|instrumental_convergence_FALSE, problems_with_proxies_TRUE, problems_with_search_FALSE)\": \"0.40\", \"p(difficulty_of_alignment_TRUE|instrumental_convergence_FALSE, problems_with_proxies_FALSE, problems_with_search_TRUE)\": \"0.30\", \"p(difficulty_of_alignment_TRUE|instrumental_convergence_FALSE, problems_with_proxies_FALSE, problems_with_search_FALSE)\": \"0.10\", \"p(difficulty_of_alignment_FALSE|instrumental_convergence_TRUE, problems_with_proxies_TRUE, problems_with_search_TRUE)\": \"0.15\", \"p(difficulty_of_alignment_FALSE|instrumental_convergence_TRUE, problems_with_proxies_TRUE, problems_with_search_FALSE)\": \"0.30\", \"p(difficulty_of_alignment_FALSE|instrumental_convergence_TRUE, problems_with_proxies_FALSE, problems_with_search_TRUE)\": \"0.40\", \"p(difficulty_of_alignment_FALSE|instrumental_convergence_TRUE, problems_with_proxies_FALSE, problems_with_search_FALSE)\": \"0.60\", \"p(difficulty_of_alignment_FALSE|instrumental_convergence_FALSE, problems_with_proxies_TRUE, problems_with_search_TRUE)\": \"0.45\", \"p(difficulty_of_alignment_FALSE|instrumental_convergence_FALSE, problems_with_proxies_TRUE, problems_with_search_FALSE)\": \"0.60\", \"p(difficulty_of_alignment_FALSE|instrumental_convergence_FALSE, problems_with_proxies_FALSE, problems_with_search_TRUE)\": \"0.70\", \"p(difficulty_of_alignment_FALSE|instrumental_convergence_FALSE, problems_with_proxies_FALSE, problems_with_search_FALSE)\": \"0.90\"}}\\n                - [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {\"instantiations\": [\"instrumental_convergence_TRUE\", \"instrumental_convergence_FALSE\"], \"priors\": {\"p(instrumental_convergence_TRUE)\": \"0.75\", \"p(instrumental_convergence_FALSE)\": \"0.25\"}}\\n                - [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {\"instantiations\": [\"problems_with_proxies_TRUE\", \"problems_with_proxies_FALSE\"], \"priors\": {\"p(problems_with_proxies_TRUE)\": \"0.80\", \"p(problems_with_proxies_FALSE)\": \"0.20\"}}\\n                - [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {\"instantiations\": [\"problems_with_search_TRUE\", \"problems_with_search_FALSE\"], \"priors\": {\"p(problems_with_search_TRUE)\": \"0.70\", \"p(problems_with_search_FALSE)\": \"0.30\"}}\\n            - [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"], \"priors\": {\"p(deployment_decisions_DEPLOY)\": \"0.70\", \"p(deployment_decisions_WITHHOLD)\": \"0.30\"}, \"posteriors\": {\"p(deployment_decisions_DEPLOY|incentives_to_build_aps_STRONG, deception_by_ai_TRUE)\": \"0.90\", \"p(deployment_decisions_DEPLOY|incentives_to_build_aps_STRONG, deception_by_ai_FALSE)\": \"0.75\", \"p(deployment_decisions_DEPLOY|incentives_to_build_aps_WEAK, deception_by_ai_TRUE)\": \"0.60\", \"p(deployment_decisions_DEPLOY|incentives_to_build_aps_WEAK, deception_by_ai_FALSE)\": \"0.30\", \"p(deployment_decisions_WITHHOLD|incentives_to_build_aps_STRONG, deception_by_ai_TRUE)\": \"0.10\", \"p(deployment_decisions_WITHHOLD|incentives_to_build_aps_STRONG, deception_by_ai_FALSE)\": \"0.25\", \"p(deployment_decisions_WITHHOLD|incentives_to_build_aps_WEAK, deception_by_ai_TRUE)\": \"0.40\", \"p(deployment_decisions_WITHHOLD|incentives_to_build_aps_WEAK, deception_by_ai_FALSE)\": \"0.70\"}}\\n                - [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {\"instantiations\": [\"incentives_to_build_aps_STRONG\", \"incentives_to_build_aps_WEAK\"], \"priors\": {\"p(incentives_to_build_aps_STRONG)\": \"0.80\", \"p(incentives_to_build_aps_WEAK)\": \"0.20\"}, \"posteriors\": {\"p(incentives_to_build_aps_STRONG|usefulness_of_aps_HIGH, competitive_dynamics_STRONG)\": \"0.95\", \"p(incentives_to_build_aps_STRONG|usefulness_of_aps_HIGH, competitive_dynamics_WEAK)\": \"0.80\", \"p(incentives_to_build_aps_STRONG|usefulness_of_aps_LOW, competitive_dynamics_STRONG)\": \"0.70\", \"p(incentives_to_build_aps_STRONG|usefulness_of_aps_LOW, competitive_dynamics_WEAK)\": \"0.30\", \"p(incentives_to_build_aps_WEAK|usefulness_of_aps_HIGH, competitive_dynamics_STRONG)\": \"0.05\", \"p(incentives_to_build_aps_WEAK|usefulness_of_aps_HIGH, competitive_dynamics_WEAK)\": \"0.20\", \"p(incentives_to_build_aps_WEAK|usefulness_of_aps_LOW, competitive_dynamics_STRONG)\": \"0.30\", \"p(incentives_to_build_aps_WEAK|usefulness_of_aps_LOW, competitive_dynamics_WEAK)\": \"0.70\"}}\\n                    - [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {\"instantiations\": [\"usefulness_of_aps_HIGH\", \"usefulness_of_aps_LOW\"], \"priors\": {\"p(usefulness_of_aps_HIGH)\": \"0.85\", \"p(usefulness_of_aps_LOW)\": \"0.15\"}}\\n                    - [Competitive_Dynamics]: Competitive pressures between AI developers. {\"instantiations\": [\"competitive_dynamics_STRONG\", \"competitive_dynamics_WEAK\"], \"priors\": {\"p(competitive_dynamics_STRONG)\": \"0.75\", \"p(competitive_dynamics_WEAK)\": \"0.25\"}}\\n                - [Deception_By_AI]: AI systems deceiving humans about their true objectives. {\"instantiations\": [\"deception_by_ai_TRUE\", \"deception_by_ai_FALSE\"], \"priors\": {\"p(deception_by_ai_TRUE)\": \"0.50\", \"p(deception_by_ai_FALSE)\": \"0.50\"}}\\n        - [Corrective_Feedback]: Human society implementing corrections after observing problems. {\"instantiations\": [\"corrective_feedback_EFFECTIVE\", \"corrective_feedback_INEFFECTIVE\"], \"priors\": {\"p(corrective_feedback_EFFECTIVE)\": \"0.60\", \"p(corrective_feedback_INEFFECTIVE)\": \"0.40\"}, \"posteriors\": {\"p(corrective_feedback_EFFECTIVE|warning_shots_OBSERVED, rapid_capability_escalation_TRUE)\": \"0.40\", \"p(corrective_feedback_EFFECTIVE|warning_shots_OBSERVED, rapid_capability_escalation_FALSE)\": \"0.80\", \"p(corrective_feedback_EFFECTIVE|warning_shots_UNOBSERVED, rapid_capability_escalation_TRUE)\": \"0.15\", \"p(corrective_feedback_EFFECTIVE|warning_shots_UNOBSERVED, rapid_capability_escalation_FALSE)\": \"0.50\", \"p(corrective_feedback_INEFFECTIVE|warning_shots_OBSERVED, rapid_capability_escalation_TRUE)\": \"0.60\", \"p(corrective_feedback_INEFFECTIVE|warning_shots_OBSERVED, rapid_capability_escalation_FALSE)\": \"0.20\", \"p(corrective_feedback_INEFFECTIVE|warning_shots_UNOBSERVED, rapid_capability_escalation_TRUE)\": \"0.85\", \"p(corrective_feedback_INEFFECTIVE|warning_shots_UNOBSERVED, rapid_capability_escalation_FALSE)\": \"0.50\"}}\\n            - [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {\"instantiations\": [\"warning_shots_OBSERVED\", \"warning_shots_UNOBSERVED\"], \"priors\": {\"p(warning_shots_OBSERVED)\": \"0.70\", \"p(warning_shots_UNOBSERVED)\": \"0.30\"}}\\n            - [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {\"instantiations\": [\"rapid_capability_escalation_TRUE\", \"rapid_capability_escalation_FALSE\"], \"priors\": {\"p(rapid_capability_escalation_TRUE)\": \"0.45\", \"p(rapid_capability_escalation_FALSE)\": \"0.55\"}}\\n[Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {\"instantiations\": [\"barriers_to_understanding_HIGH\", \"barriers_to_understanding_LOW\"], \"priors\": {\"p(barriers_to_understanding_HIGH)\": \"0.70\", \"p(barriers_to_understanding_LOW)\": \"0.30\"}, \"posteriors\": {\"p(barriers_to_understanding_HIGH|misaligned_power_seeking_TRUE)\": \"0.85\", \"p(barriers_to_understanding_HIGH|misaligned_power_seeking_FALSE)\": \"0.60\", \"p(barriers_to_understanding_LOW|misaligned_power_seeking_TRUE)\": \"0.15\", \"p(barriers_to_understanding_LOW|misaligned_power_seeking_FALSE)\": \"0.40\"}}\\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"], \"priors\": {\"p(misaligned_power_seeking_TRUE)\": \"0.338\", \"p(misaligned_power_seeking_FALSE)\": \"0.662\"}}\\n[Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {\"instantiations\": [\"adversarial_dynamics_TRUE\", \"adversarial_dynamics_FALSE\"], \"priors\": {\"p(adversarial_dynamics_TRUE)\": \"0.60\", \"p(adversarial_dynamics_FALSE)\": \"0.40\"}, \"posteriors\": {\"p(adversarial_dynamics_TRUE|misaligned_power_seeking_TRUE)\": \"0.95\", \"p(adversarial_dynamics_TRUE|misaligned_power_seeking_FALSE)\": \"0.10\", \"p(adversarial_dynamics_FALSE|misaligned_power_seeking_TRUE)\": \"0.05\", \"p(adversarial_dynamics_FALSE|misaligned_power_seeking_FALSE)\": \"0.90\"}}\\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"], \"priors\": {\"p(misaligned_power_seeking_TRUE)\": \"0.338\", \"p(misaligned_power_seeking_FALSE)\": \"0.662\"}}\\n[Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {\"instantiations\": [\"stakes_of_error_HIGH\", \"stakes_of_error_LOW\"], \"priors\": {\"p(stakes_of_error_HIGH)\": \"0.85\", \"p(stakes_of_error_LOW)\": \"0.15\"}, \"posteriors\": {\"p(stakes_of_error_HIGH|misaligned_power_seeking_TRUE)\": \"0.95\", \"p(stakes_of_error_HIGH|misaligned_power_seeking_FALSE)\": \"0.50\", \"p(stakes_of_error_LOW|misaligned_power_seeking_TRUE)\": \"0.05\", \"p(stakes_of_error_LOW|misaligned_power_seeking_FALSE)\": \"0.50\"}}\\n- [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"], \"priors\": {\"p(misaligned_power_seeking_TRUE)\": \"0.338\", \"p(misaligned_power_seeking_FALSE)\": \"0.662\"}}\\n'\n\n:::\n\n\n\nF.4.2 3.0.1 Test BayesDown Extraction\n\n\nCode\ndisplay(Markdown(md_content_ex_rain)) # view BayesDown file formatted as MarkDown\n\n\n[Existential_Catastrophe]: The destruction of humanity’s long-term potential due to AI systems we’ve lost control over. {“instantiations”: [“existential_catastrophe_TRUE”, “existential_catastrophe_FALSE”], “priors”: {“p(existential_catastrophe_TRUE)”: “0.05”, “p(existential_catastrophe_FALSE)”: “0.95”}, “posteriors”: {“p(existential_catastrophe_TRUE|human_disempowerment_TRUE)”: “0.95”, “p(existential_catastrophe_TRUE|human_disempowerment_FALSE)”: “0.0”, “p(existential_catastrophe_FALSE|human_disempowerment_TRUE)”: “0.05”, “p(existential_catastrophe_FALSE|human_disempowerment_FALSE)”: “1.0”}} - [Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {“instantiations”: [“human_disempowerment_TRUE”, “human_disempowerment_FALSE”], “priors”: {“p(human_disempowerment_TRUE)”: “0.208”, “p(human_disempowerment_FALSE)”: “0.792”}, “posteriors”: {“p(human_disempowerment_TRUE|scale_of_power_seeking_TRUE)”: “1.0”, “p(human_disempowerment_TRUE|scale_of_power_seeking_FALSE)”: “0.0”, “p(human_disempowerment_FALSE|scale_of_power_seeking_TRUE)”: “0.0”, “p(human_disempowerment_FALSE|scale_of_power_seeking_FALSE)”: “1.0”}} - [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {“instantiations”: [“scale_of_power_seeking_TRUE”, “scale_of_power_seeking_FALSE”], “priors”: {“p(scale_of_power_seeking_TRUE)”: “0.208”, “p(scale_of_power_seeking_FALSE)”: “0.792”}, “posteriors”: {“p(scale_of_power_seeking_TRUE|misaligned_power_seeking_TRUE, corrective_feedback_EFFECTIVE)”: “0.25”, “p(scale_of_power_seeking_TRUE|misaligned_power_seeking_TRUE, corrective_feedback_INEFFECTIVE)”: “0.60”, “p(scale_of_power_seeking_TRUE|misaligned_power_seeking_FALSE, corrective_feedback_EFFECTIVE)”: “0.0”, “p(scale_of_power_seeking_TRUE|misaligned_power_seeking_FALSE, corrective_feedback_INEFFECTIVE)”: “0.0”, “p(scale_of_power_seeking_FALSE|misaligned_power_seeking_TRUE, corrective_feedback_EFFECTIVE)”: “0.75”, “p(scale_of_power_seeking_FALSE|misaligned_power_seeking_TRUE, corrective_feedback_INEFFECTIVE)”: “0.40”, “p(scale_of_power_seeking_FALSE|misaligned_power_seeking_FALSE, corrective_feedback_EFFECTIVE)”: “1.0”, “p(scale_of_power_seeking_FALSE|misaligned_power_seeking_FALSE, corrective_feedback_INEFFECTIVE)”: “1.0”}} - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {“instantiations”: [“misaligned_power_seeking_TRUE”, “misaligned_power_seeking_FALSE”], “priors”: {“p(misaligned_power_seeking_TRUE)”: “0.338”, “p(misaligned_power_seeking_FALSE)”: “0.662”}, “posteriors”: {“p(misaligned_power_seeking_TRUE|aps_systems_TRUE, difficulty_of_alignment_TRUE, deployment_decisions_DEPLOY)”: “0.90”, “p(misaligned_power_seeking_TRUE|aps_systems_TRUE, difficulty_of_alignment_TRUE, deployment_decisions_WITHHOLD)”: “0.10”, “p(misaligned_power_seeking_TRUE|aps_systems_TRUE, difficulty_of_alignment_FALSE, deployment_decisions_DEPLOY)”: “0.25”, “p(misaligned_power_seeking_TRUE|aps_systems_TRUE, difficulty_of_alignment_FALSE, deployment_decisions_WITHHOLD)”: “0.05”, “p(misaligned_power_seeking_TRUE|aps_systems_FALSE, difficulty_of_alignment_TRUE, deployment_decisions_DEPLOY)”: “0.0”, “p(misaligned_power_seeking_TRUE|aps_systems_FALSE, difficulty_of_alignment_TRUE, deployment_decisions_WITHHOLD)”: “0.0”, “p(misaligned_power_seeking_TRUE|aps_systems_FALSE, difficulty_of_alignment_FALSE, deployment_decisions_DEPLOY)”: “0.0”, “p(misaligned_power_seeking_TRUE|aps_systems_FALSE, difficulty_of_alignment_FALSE, deployment_decisions_WITHHOLD)”: “0.0”, “p(misaligned_power_seeking_FALSE|aps_systems_TRUE, difficulty_of_alignment_TRUE, deployment_decisions_DEPLOY)”: “0.10”, “p(misaligned_power_seeking_FALSE|aps_systems_TRUE, difficulty_of_alignment_TRUE, deployment_decisions_WITHHOLD)”: “0.90”, “p(misaligned_power_seeking_FALSE|aps_systems_TRUE, difficulty_of_alignment_FALSE, deployment_decisions_DEPLOY)”: “0.75”, “p(misaligned_power_seeking_FALSE|aps_systems_TRUE, difficulty_of_alignment_FALSE, deployment_decisions_WITHHOLD)”: “0.95”, “p(misaligned_power_seeking_FALSE|aps_systems_FALSE, difficulty_of_alignment_TRUE, deployment_decisions_DEPLOY)”: “1.0”, “p(misaligned_power_seeking_FALSE|aps_systems_FALSE, difficulty_of_alignment_TRUE, deployment_decisions_WITHHOLD)”: “1.0”, “p(misaligned_power_seeking_FALSE|aps_systems_FALSE, difficulty_of_alignment_FALSE, deployment_decisions_DEPLOY)”: “1.0”, “p(misaligned_power_seeking_FALSE|aps_systems_FALSE, difficulty_of_alignment_FALSE, deployment_decisions_WITHHOLD)”: “1.0”}} - [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {“instantiations”: [“aps_systems_TRUE”, “aps_systems_FALSE”], “priors”: {“p(aps_systems_TRUE)”: “0.65”, “p(aps_systems_FALSE)”: “0.35”}, “posteriors”: {“p(aps_systems_TRUE|advanced_ai_capability_TRUE, agentic_planning_TRUE, strategic_awareness_TRUE)”: “1.0”, “p(aps_systems_TRUE|advanced_ai_capability_TRUE, agentic_planning_TRUE, strategic_awareness_FALSE)”: “0.0”, “p(aps_systems_TRUE|advanced_ai_capability_TRUE, agentic_planning_FALSE, strategic_awareness_TRUE)”: “0.0”, “p(aps_systems_TRUE|advanced_ai_capability_TRUE, agentic_planning_FALSE, strategic_awareness_FALSE)”: “0.0”, “p(aps_systems_TRUE|advanced_ai_capability_FALSE, agentic_planning_TRUE, strategic_awareness_TRUE)”: “0.0”, “p(aps_systems_TRUE|advanced_ai_capability_FALSE, agentic_planning_TRUE, strategic_awareness_FALSE)”: “0.0”, “p(aps_systems_TRUE|advanced_ai_capability_FALSE, agentic_planning_FALSE, strategic_awareness_TRUE)”: “0.0”, “p(aps_systems_TRUE|advanced_ai_capability_FALSE, agentic_planning_FALSE, strategic_awareness_FALSE)”: “0.0”, “p(aps_systems_FALSE|advanced_ai_capability_TRUE, agentic_planning_TRUE, strategic_awareness_TRUE)”: “0.0”, “p(aps_systems_FALSE|advanced_ai_capability_TRUE, agentic_planning_TRUE, strategic_awareness_FALSE)”: “1.0”, “p(aps_systems_FALSE|advanced_ai_capability_TRUE, agentic_planning_FALSE, strategic_awareness_TRUE)”: “1.0”, “p(aps_systems_FALSE|advanced_ai_capability_TRUE, agentic_planning_FALSE, strategic_awareness_FALSE)”: “1.0”, “p(aps_systems_FALSE|advanced_ai_capability_FALSE, agentic_planning_TRUE, strategic_awareness_TRUE)”: “1.0”, “p(aps_systems_FALSE|advanced_ai_capability_FALSE, agentic_planning_TRUE, strategic_awareness_FALSE)”: “1.0”, “p(aps_systems_FALSE|advanced_ai_capability_FALSE, agentic_planning_FALSE, strategic_awareness_TRUE)”: “1.0”, “p(aps_systems_FALSE|advanced_ai_capability_FALSE, agentic_planning_FALSE, strategic_awareness_FALSE)”: “1.0”}} - [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {“instantiations”: [“advanced_ai_capability_TRUE”, “advanced_ai_capability_FALSE”], “priors”: {“p(advanced_ai_capability_TRUE)”: “0.80”, “p(advanced_ai_capability_FALSE)”: “0.20”}} - [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {“instantiations”: [“agentic_planning_TRUE”, “agentic_planning_FALSE”], “priors”: {“p(agentic_planning_TRUE)”: “0.85”, “p(agentic_planning_FALSE)”: “0.15”}} - [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {“instantiations”: [“strategic_awareness_TRUE”, “strategic_awareness_FALSE”], “priors”: {“p(strategic_awareness_TRUE)”: “0.75”, “p(strategic_awareness_FALSE)”: “0.25”}} - [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {“instantiations”: [“difficulty_of_alignment_TRUE”, “difficulty_of_alignment_FALSE”], “priors”: {“p(difficulty_of_alignment_TRUE)”: “0.40”, “p(difficulty_of_alignment_FALSE)”: “0.60”}, “posteriors”: {“p(difficulty_of_alignment_TRUE|instrumental_convergence_TRUE, problems_with_proxies_TRUE, problems_with_search_TRUE)”: “0.85”, “p(difficulty_of_alignment_TRUE|instrumental_convergence_TRUE, problems_with_proxies_TRUE, problems_with_search_FALSE)”: “0.70”, “p(difficulty_of_alignment_TRUE|instrumental_convergence_TRUE, problems_with_proxies_FALSE, problems_with_search_TRUE)”: “0.60”, “p(difficulty_of_alignment_TRUE|instrumental_convergence_TRUE, problems_with_proxies_FALSE, problems_with_search_FALSE)”: “0.40”, “p(difficulty_of_alignment_TRUE|instrumental_convergence_FALSE, problems_with_proxies_TRUE, problems_with_search_TRUE)”: “0.55”, “p(difficulty_of_alignment_TRUE|instrumental_convergence_FALSE, problems_with_proxies_TRUE, problems_with_search_FALSE)”: “0.40”, “p(difficulty_of_alignment_TRUE|instrumental_convergence_FALSE, problems_with_proxies_FALSE, problems_with_search_TRUE)”: “0.30”, “p(difficulty_of_alignment_TRUE|instrumental_convergence_FALSE, problems_with_proxies_FALSE, problems_with_search_FALSE)”: “0.10”, “p(difficulty_of_alignment_FALSE|instrumental_convergence_TRUE, problems_with_proxies_TRUE, problems_with_search_TRUE)”: “0.15”, “p(difficulty_of_alignment_FALSE|instrumental_convergence_TRUE, problems_with_proxies_TRUE, problems_with_search_FALSE)”: “0.30”, “p(difficulty_of_alignment_FALSE|instrumental_convergence_TRUE, problems_with_proxies_FALSE, problems_with_search_TRUE)”: “0.40”, “p(difficulty_of_alignment_FALSE|instrumental_convergence_TRUE, problems_with_proxies_FALSE, problems_with_search_FALSE)”: “0.60”, “p(difficulty_of_alignment_FALSE|instrumental_convergence_FALSE, problems_with_proxies_TRUE, problems_with_search_TRUE)”: “0.45”, “p(difficulty_of_alignment_FALSE|instrumental_convergence_FALSE, problems_with_proxies_TRUE, problems_with_search_FALSE)”: “0.60”, “p(difficulty_of_alignment_FALSE|instrumental_convergence_FALSE, problems_with_proxies_FALSE, problems_with_search_TRUE)”: “0.70”, “p(difficulty_of_alignment_FALSE|instrumental_convergence_FALSE, problems_with_proxies_FALSE, problems_with_search_FALSE)”: “0.90”}} - [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {“instantiations”: [“instrumental_convergence_TRUE”, “instrumental_convergence_FALSE”], “priors”: {“p(instrumental_convergence_TRUE)”: “0.75”, “p(instrumental_convergence_FALSE)”: “0.25”}} - [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {“instantiations”: [“problems_with_proxies_TRUE”, “problems_with_proxies_FALSE”], “priors”: {“p(problems_with_proxies_TRUE)”: “0.80”, “p(problems_with_proxies_FALSE)”: “0.20”}} - [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {“instantiations”: [“problems_with_search_TRUE”, “problems_with_search_FALSE”], “priors”: {“p(problems_with_search_TRUE)”: “0.70”, “p(problems_with_search_FALSE)”: “0.30”}} - [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {“instantiations”: [“deployment_decisions_DEPLOY”, “deployment_decisions_WITHHOLD”], “priors”: {“p(deployment_decisions_DEPLOY)”: “0.70”, “p(deployment_decisions_WITHHOLD)”: “0.30”}, “posteriors”: {“p(deployment_decisions_DEPLOY|incentives_to_build_aps_STRONG, deception_by_ai_TRUE)”: “0.90”, “p(deployment_decisions_DEPLOY|incentives_to_build_aps_STRONG, deception_by_ai_FALSE)”: “0.75”, “p(deployment_decisions_DEPLOY|incentives_to_build_aps_WEAK, deception_by_ai_TRUE)”: “0.60”, “p(deployment_decisions_DEPLOY|incentives_to_build_aps_WEAK, deception_by_ai_FALSE)”: “0.30”, “p(deployment_decisions_WITHHOLD|incentives_to_build_aps_STRONG, deception_by_ai_TRUE)”: “0.10”, “p(deployment_decisions_WITHHOLD|incentives_to_build_aps_STRONG, deception_by_ai_FALSE)”: “0.25”, “p(deployment_decisions_WITHHOLD|incentives_to_build_aps_WEAK, deception_by_ai_TRUE)”: “0.40”, “p(deployment_decisions_WITHHOLD|incentives_to_build_aps_WEAK, deception_by_ai_FALSE)”: “0.70”}} - [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {“instantiations”: [“incentives_to_build_aps_STRONG”, “incentives_to_build_aps_WEAK”], “priors”: {“p(incentives_to_build_aps_STRONG)”: “0.80”, “p(incentives_to_build_aps_WEAK)”: “0.20”}, “posteriors”: {“p(incentives_to_build_aps_STRONG|usefulness_of_aps_HIGH, competitive_dynamics_STRONG)”: “0.95”, “p(incentives_to_build_aps_STRONG|usefulness_of_aps_HIGH, competitive_dynamics_WEAK)”: “0.80”, “p(incentives_to_build_aps_STRONG|usefulness_of_aps_LOW, competitive_dynamics_STRONG)”: “0.70”, “p(incentives_to_build_aps_STRONG|usefulness_of_aps_LOW, competitive_dynamics_WEAK)”: “0.30”, “p(incentives_to_build_aps_WEAK|usefulness_of_aps_HIGH, competitive_dynamics_STRONG)”: “0.05”, “p(incentives_to_build_aps_WEAK|usefulness_of_aps_HIGH, competitive_dynamics_WEAK)”: “0.20”, “p(incentives_to_build_aps_WEAK|usefulness_of_aps_LOW, competitive_dynamics_STRONG)”: “0.30”, “p(incentives_to_build_aps_WEAK|usefulness_of_aps_LOW, competitive_dynamics_WEAK)”: “0.70”}} - [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {“instantiations”: [“usefulness_of_aps_HIGH”, “usefulness_of_aps_LOW”], “priors”: {“p(usefulness_of_aps_HIGH)”: “0.85”, “p(usefulness_of_aps_LOW)”: “0.15”}} - [Competitive_Dynamics]: Competitive pressures between AI developers. {“instantiations”: [“competitive_dynamics_STRONG”, “competitive_dynamics_WEAK”], “priors”: {“p(competitive_dynamics_STRONG)”: “0.75”, “p(competitive_dynamics_WEAK)”: “0.25”}} - [Deception_By_AI]: AI systems deceiving humans about their true objectives. {“instantiations”: [“deception_by_ai_TRUE”, “deception_by_ai_FALSE”], “priors”: {“p(deception_by_ai_TRUE)”: “0.50”, “p(deception_by_ai_FALSE)”: “0.50”}} - [Corrective_Feedback]: Human society implementing corrections after observing problems. {“instantiations”: [“corrective_feedback_EFFECTIVE”, “corrective_feedback_INEFFECTIVE”], “priors”: {“p(corrective_feedback_EFFECTIVE)”: “0.60”, “p(corrective_feedback_INEFFECTIVE)”: “0.40”}, “posteriors”: {“p(corrective_feedback_EFFECTIVE|warning_shots_OBSERVED, rapid_capability_escalation_TRUE)”: “0.40”, “p(corrective_feedback_EFFECTIVE|warning_shots_OBSERVED, rapid_capability_escalation_FALSE)”: “0.80”, “p(corrective_feedback_EFFECTIVE|warning_shots_UNOBSERVED, rapid_capability_escalation_TRUE)”: “0.15”, “p(corrective_feedback_EFFECTIVE|warning_shots_UNOBSERVED, rapid_capability_escalation_FALSE)”: “0.50”, “p(corrective_feedback_INEFFECTIVE|warning_shots_OBSERVED, rapid_capability_escalation_TRUE)”: “0.60”, “p(corrective_feedback_INEFFECTIVE|warning_shots_OBSERVED, rapid_capability_escalation_FALSE)”: “0.20”, “p(corrective_feedback_INEFFECTIVE|warning_shots_UNOBSERVED, rapid_capability_escalation_TRUE)”: “0.85”, “p(corrective_feedback_INEFFECTIVE|warning_shots_UNOBSERVED, rapid_capability_escalation_FALSE)”: “0.50”}} - [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {“instantiations”: [“warning_shots_OBSERVED”, “warning_shots_UNOBSERVED”], “priors”: {“p(warning_shots_OBSERVED)”: “0.70”, “p(warning_shots_UNOBSERVED)”: “0.30”}} - [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {“instantiations”: [“rapid_capability_escalation_TRUE”, “rapid_capability_escalation_FALSE”], “priors”: {“p(rapid_capability_escalation_TRUE)”: “0.45”, “p(rapid_capability_escalation_FALSE)”: “0.55”}} [Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {“instantiations”: [“barriers_to_understanding_HIGH”, “barriers_to_understanding_LOW”], “priors”: {“p(barriers_to_understanding_HIGH)”: “0.70”, “p(barriers_to_understanding_LOW)”: “0.30”}, “posteriors”: {“p(barriers_to_understanding_HIGH|misaligned_power_seeking_TRUE)”: “0.85”, “p(barriers_to_understanding_HIGH|misaligned_power_seeking_FALSE)”: “0.60”, “p(barriers_to_understanding_LOW|misaligned_power_seeking_TRUE)”: “0.15”, “p(barriers_to_understanding_LOW|misaligned_power_seeking_FALSE)”: “0.40”}} - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {“instantiations”: [“misaligned_power_seeking_TRUE”, “misaligned_power_seeking_FALSE”], “priors”: {“p(misaligned_power_seeking_TRUE)”: “0.338”, “p(misaligned_power_seeking_FALSE)”: “0.662”}} [Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {“instantiations”: [“adversarial_dynamics_TRUE”, “adversarial_dynamics_FALSE”], “priors”: {“p(adversarial_dynamics_TRUE)”: “0.60”, “p(adversarial_dynamics_FALSE)”: “0.40”}, “posteriors”: {“p(adversarial_dynamics_TRUE|misaligned_power_seeking_TRUE)”: “0.95”, “p(adversarial_dynamics_TRUE|misaligned_power_seeking_FALSE)”: “0.10”, “p(adversarial_dynamics_FALSE|misaligned_power_seeking_TRUE)”: “0.05”, “p(adversarial_dynamics_FALSE|misaligned_power_seeking_FALSE)”: “0.90”}} - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {“instantiations”: [“misaligned_power_seeking_TRUE”, “misaligned_power_seeking_FALSE”], “priors”: {“p(misaligned_power_seeking_TRUE)”: “0.338”, “p(misaligned_power_seeking_FALSE)”: “0.662”}} [Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {“instantiations”: [“stakes_of_error_HIGH”, “stakes_of_error_LOW”], “priors”: {“p(stakes_of_error_HIGH)”: “0.85”, “p(stakes_of_error_LOW)”: “0.15”}, “posteriors”: {“p(stakes_of_error_HIGH|misaligned_power_seeking_TRUE)”: “0.95”, “p(stakes_of_error_HIGH|misaligned_power_seeking_FALSE)”: “0.50”, “p(stakes_of_error_LOW|misaligned_power_seeking_TRUE)”: “0.05”, “p(stakes_of_error_LOW|misaligned_power_seeking_FALSE)”: “0.50”}} - [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {“instantiations”: [“misaligned_power_seeking_TRUE”, “misaligned_power_seeking_FALSE”], “priors”: {“p(misaligned_power_seeking_TRUE)”: “0.338”, “p(misaligned_power_seeking_FALSE)”: “0.662”}}\n\n\n\n\nF.4.3 3.0.2 Check the Graph Structure with the ArgDown Sandbox Online\nCopy and paste the BayesDown formatted … in the ArgDown Sandbox below to quickly verify that the network renders correctly.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#extraction",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#extraction",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "F.5 3.1 Extraction",
    "text": "F.5 3.1 Extraction\nBayesDown Extraction Code already part of ArgDown extraction code, therefore just use same function “parse_markdown_hierarchy(markdown_data)” and ignore the extra argument (“ArgDown”) because it is automatically set to false amd will by default extract BayesDown.\n\n\nCode\nresult_df = parse_markdown_hierarchy_fixed(md_content_ex_rain)\nresult_df\n\n\n\n    \n\n\n\n\n\n\nTitle\nDescription\nline\nline_numbers\nindentation\nindentation_levels\nParents\nChildren\ninstantiations\npriors\nposteriors\nNo_Parent\nNo_Children\nparent_instantiations\n\n\n\n\n0\nExistential_Catastrophe\nThe destruction of humanity's long-term potent...\n0\n[0]\n0\n[0]\n[]\n[]\n[existential_catastrophe_TRUE, existential_cat...\n{'p(existential_catastrophe_TRUE)': '0.05', 'p...\n{'p(existential_catastrophe_TRUE|human_disempo...\nTrue\nTrue\n[]\n\n\n1\nHuman_Disempowerment\nPermanent and collective disempowerment of hum...\n1\n[1]\n0\n[0]\n[Scale_Of_Power_Seeking]\n[]\n[human_disempowerment_TRUE, human_disempowerme...\n{'p(human_disempowerment_TRUE)': '0.208', 'p(h...\n{'p(human_disempowerment_TRUE|scale_of_power_s...\nFalse\nTrue\n[[scale_of_power_seeking_TRUE, scale_of_power_...\n\n\n2\nScale_Of_Power_Seeking\nPower-seeking by AI systems scaling to the poi...\n2\n[2]\n4\n[4]\n[Misaligned_Power_Seeking, Corrective_Feedback]\n[Human_Disempowerment]\n[scale_of_power_seeking_TRUE, scale_of_power_s...\n{'p(scale_of_power_seeking_TRUE)': '0.208', 'p...\n{'p(scale_of_power_seeking_TRUE|misaligned_pow...\nFalse\nFalse\n[[misaligned_power_seeking_TRUE, misaligned_po...\n\n\n3\nMisaligned_Power_Seeking\nDeployed AI systems seeking power in unintende...\n3\n[3, 21, 23, 25]\n8\n[8, 0, 0, 0]\n[APS_Systems, Difficulty_Of_Alignment, Deploym...\n[Scale_Of_Power_Seeking]\n[misaligned_power_seeking_TRUE, misaligned_pow...\n{'p(misaligned_power_seeking_TRUE)': '0.338', ...\n{'p(misaligned_power_seeking_TRUE|aps_systems_...\nFalse\nFalse\n[[aps_systems_TRUE, aps_systems_FALSE], [diffi...\n\n\n4\nAPS_Systems\nAI systems with advanced capabilities, agentic...\n4\n[4]\n12\n[12]\n[Advanced_AI_Capability, Agentic_Planning, Str...\n[Misaligned_Power_Seeking]\n[aps_systems_TRUE, aps_systems_FALSE]\n{'p(aps_systems_TRUE)': '0.65', 'p(aps_systems...\n{'p(aps_systems_TRUE|advanced_ai_capability_TR...\nFalse\nFalse\n[[advanced_ai_capability_TRUE, advanced_ai_cap...\n\n\n5\nAdvanced_AI_Capability\nAI systems that outperform humans on tasks tha...\n5\n[5]\n16\n[16]\n[]\n[APS_Systems]\n[advanced_ai_capability_TRUE, advanced_ai_capa...\n{'p(advanced_ai_capability_TRUE)': '0.80', 'p(...\n{}\nTrue\nFalse\n[]\n\n\n6\nAgentic_Planning\nAI systems making and executing plans based on...\n6\n[6]\n16\n[16]\n[]\n[APS_Systems]\n[agentic_planning_TRUE, agentic_planning_FALSE]\n{'p(agentic_planning_TRUE)': '0.85', 'p(agenti...\n{}\nTrue\nFalse\n[]\n\n\n7\nStrategic_Awareness\nAI systems with models accurately representing...\n7\n[7]\n16\n[16]\n[]\n[APS_Systems]\n[strategic_awareness_TRUE, strategic_awareness...\n{'p(strategic_awareness_TRUE)': '0.75', 'p(str...\n{}\nTrue\nFalse\n[]\n\n\n8\nDifficulty_Of_Alignment\nIt is harder to build aligned systems than mis...\n8\n[8]\n12\n[12]\n[Instrumental_Convergence, Problems_With_Proxi...\n[Misaligned_Power_Seeking]\n[difficulty_of_alignment_TRUE, difficulty_of_a...\n{'p(difficulty_of_alignment_TRUE)': '0.40', 'p...\n{'p(difficulty_of_alignment_TRUE|instrumental_...\nFalse\nFalse\n[[instrumental_convergence_TRUE, instrumental_...\n\n\n9\nInstrumental_Convergence\nAI systems with misaligned objectives tend to ...\n9\n[9]\n16\n[16]\n[]\n[Difficulty_Of_Alignment]\n[instrumental_convergence_TRUE, instrumental_c...\n{'p(instrumental_convergence_TRUE)': '0.75', '...\n{}\nTrue\nFalse\n[]\n\n\n10\nProblems_With_Proxies\nOptimizing for proxy objectives breaks correla...\n10\n[10]\n16\n[16]\n[]\n[Difficulty_Of_Alignment]\n[problems_with_proxies_TRUE, problems_with_pro...\n{'p(problems_with_proxies_TRUE)': '0.80', 'p(p...\n{}\nTrue\nFalse\n[]\n\n\n11\nProblems_With_Search\nSearch processes can yield systems pursuing di...\n11\n[11]\n16\n[16]\n[]\n[Difficulty_Of_Alignment]\n[problems_with_search_TRUE, problems_with_sear...\n{'p(problems_with_search_TRUE)': '0.70', 'p(pr...\n{}\nTrue\nFalse\n[]\n\n\n12\nDeployment_Decisions\nDecisions to deploy potentially misaligned AI ...\n12\n[12]\n12\n[12]\n[Incentives_To_Build_APS, Deception_By_AI]\n[Misaligned_Power_Seeking]\n[deployment_decisions_DEPLOY, deployment_decis...\n{'p(deployment_decisions_DEPLOY)': '0.70', 'p(...\n{'p(deployment_decisions_DEPLOY|incentives_to_...\nFalse\nFalse\n[[incentives_to_build_aps_STRONG, incentives_t...\n\n\n13\nIncentives_To_Build_APS\nStrong incentives to build and deploy APS syst...\n13\n[13]\n16\n[16]\n[Usefulness_Of_APS, Competitive_Dynamics]\n[Deployment_Decisions]\n[incentives_to_build_aps_STRONG, incentives_to...\n{'p(incentives_to_build_aps_STRONG)': '0.80', ...\n{'p(incentives_to_build_aps_STRONG|usefulness_...\nFalse\nFalse\n[[usefulness_of_aps_HIGH, usefulness_of_aps_LO...\n\n\n14\nUsefulness_Of_APS\nAPS systems are very useful for many valuable ...\n14\n[14]\n20\n[20]\n[]\n[Incentives_To_Build_APS]\n[usefulness_of_aps_HIGH, usefulness_of_aps_LOW]\n{'p(usefulness_of_aps_HIGH)': '0.85', 'p(usefu...\n{}\nTrue\nFalse\n[]\n\n\n15\nCompetitive_Dynamics\nCompetitive pressures between AI developers.\n15\n[15]\n20\n[20]\n[]\n[Incentives_To_Build_APS]\n[competitive_dynamics_STRONG, competitive_dyna...\n{'p(competitive_dynamics_STRONG)': '0.75', 'p(...\n{}\nTrue\nFalse\n[]\n\n\n16\nDeception_By_AI\nAI systems deceiving humans about their true o...\n16\n[16]\n16\n[16]\n[]\n[Deployment_Decisions]\n[deception_by_ai_TRUE, deception_by_ai_FALSE]\n{'p(deception_by_ai_TRUE)': '0.50', 'p(decepti...\n{}\nTrue\nFalse\n[]\n\n\n17\nCorrective_Feedback\nHuman society implementing corrections after o...\n17\n[17]\n8\n[8]\n[Warning_Shots, Rapid_Capability_Escalation]\n[Scale_Of_Power_Seeking]\n[corrective_feedback_EFFECTIVE, corrective_fee...\n{'p(corrective_feedback_EFFECTIVE)': '0.60', '...\n{'p(corrective_feedback_EFFECTIVE|warning_shot...\nFalse\nFalse\n[[warning_shots_OBSERVED, warning_shots_UNOBSE...\n\n\n18\nWarning_Shots\nObservable failures in weaker systems before c...\n18\n[18]\n12\n[12]\n[]\n[Corrective_Feedback]\n[warning_shots_OBSERVED, warning_shots_UNOBSER...\n{'p(warning_shots_OBSERVED)': '0.70', 'p(warni...\n{}\nTrue\nFalse\n[]\n\n\n19\nRapid_Capability_Escalation\nAI capabilities escalating very rapidly, allow...\n19\n[19]\n12\n[12]\n[]\n[Corrective_Feedback]\n[rapid_capability_escalation_TRUE, rapid_capab...\n{'p(rapid_capability_escalation_TRUE)': '0.45'...\n{}\nTrue\nFalse\n[]\n\n\n20\nBarriers_To_Understanding\nDifficulty in understanding the internal worki...\n20\n[20]\n0\n[0]\n[]\n[]\n[barriers_to_understanding_HIGH, barriers_to_u...\n{'p(barriers_to_understanding_HIGH)': '0.70', ...\n{'p(barriers_to_understanding_HIGH|misaligned_...\nTrue\nTrue\n[]\n\n\n21\nAdversarial_Dynamics\nPotentially adversarial relationships between ...\n22\n[22]\n0\n[0]\n[]\n[]\n[adversarial_dynamics_TRUE, adversarial_dynami...\n{'p(adversarial_dynamics_TRUE)': '0.60', 'p(ad...\n{'p(adversarial_dynamics_TRUE|misaligned_power...\nTrue\nTrue\n[]\n\n\n22\nStakes_Of_Error\nThe escalating impact of mistakes with power-s...\n24\n[24]\n0\n[0]\n[]\n[]\n[stakes_of_error_HIGH, stakes_of_error_LOW]\n{'p(stakes_of_error_HIGH)': '0.85', 'p(stakes_...\n{'p(stakes_of_error_HIGH|misaligned_power_seek...\nTrue\nTrue\n[]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#data-post-processing",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#data-post-processing",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "F.6 3.2 Data-Post-Processing",
    "text": "F.6 3.2 Data-Post-Processing\nAdd rows to data frame that can be calculated from the extracted rows\n\n\nCode\n# @title 3.2.0 Data Post-Processing Functions --- [data_post_processing_functions]\n\n\"\"\"\nBLOCK PURPOSE: Enhances the extracted BayesDown data with calculated metrics and network properties.\n\nThis block provides functions to enrich the basic extracted data with additional\ncalculated columns that are useful for analysis and visualization:\n\n1. Joint probabilities - Calculating P(A,B) from conditional and prior probabilities\n2. Network metrics - Centrality measures that indicate importance of nodes in the network\n3. Markov blanket - Identifying the minimal set of nodes that shield a node from the rest\n\nThese enhancements provide valuable context for understanding the network structure\nand the relationships between variables, enabling more advanced analysis and\nimproving visualization.\n\nDEPENDENCIES: networkx for graph calculations\nINPUTS: DataFrame with basic extracted BayesDown data\nOUTPUTS: Enhanced DataFrame with additional calculated columns\n\"\"\"\n\ndef enhance_extracted_data(df):\n    \"\"\"\n    Enhance the extracted data with calculated columns\n\n    Args:\n        df: DataFrame with extracted BayesDown data\n\n    Returns:\n        Enhanced DataFrame with additional columns\n    \"\"\"\n    # Create a copy to avoid modifying the original\n    enhanced_df = df.copy()\n\n    # 1. Calculate joint probabilities - P(A,B) = P(A|B) * P(B)\n    enhanced_df['joint_probabilities'] = None\n\n    for idx, row in enhanced_df.iterrows():\n        title = row['Title']\n        priors = row['priors'] if isinstance(row['priors'], dict) else {}\n        posteriors = row['posteriors'] if isinstance(row['posteriors'], dict) else {}\n        parents = row['Parents'] if isinstance(row['Parents'], list) else []\n\n        # Skip if no parents or no priors\n        if not parents or not priors:\n            continue\n\n        # Initialize joint probabilities dictionary\n        joint_probs = {}\n\n        # Get instantiations\n        instantiations = row['instantiations']\n        if not isinstance(instantiations, list) or not instantiations:\n            continue\n\n        # For each parent and child instantiation combination, calculate joint probability\n        for inst in instantiations:\n            # Get this instantiation's prior probability\n            inst_prior_key = f\"p({inst})\"\n            if inst_prior_key not in priors:\n                continue\n\n            try:\n                inst_prior = float(priors[inst_prior_key])\n            except (ValueError, TypeError):\n                continue\n\n            # For each parent\n            for parent in parents:\n                parent_row = enhanced_df[enhanced_df['Title'] == parent]\n                if parent_row.empty:\n                    continue\n\n                parent_insts = parent_row.iloc[0]['instantiations']\n                if not isinstance(parent_insts, list) or not parent_insts:\n                    continue\n\n                for parent_inst in parent_insts:\n                    # Get conditional probability\n                    cond_key = f\"p({inst}|{parent}={parent_inst})\"\n                    if cond_key in posteriors:\n                        try:\n                            cond_prob = float(posteriors[cond_key])\n\n                            # Get parent's prior\n                            parent_priors = parent_row.iloc[0]['priors']\n                            if not isinstance(parent_priors, dict):\n                                continue\n\n                            parent_prior_key = f\"p({parent_inst})\"\n                            if parent_prior_key not in parent_priors:\n                                continue\n\n                            try:\n                                parent_prior = float(parent_priors[parent_prior_key])\n\n                                # Calculate joint probability:\n                                # P(A,B) = P(A|B) * P(B)\n                                joint_prob = cond_prob * parent_prior\n                                joint_key = f\"p({inst},{parent}={parent_inst})\"\n                                joint_probs[joint_key] = str(round(joint_prob, 4))\n                            except (ValueError, TypeError):\n                                joint_prob = cond_prob * parent_prior\n                                joint_key = f\"p({inst},{parent}={parent_inst})\"\n                                joint_probs[joint_key] = str(round(joint_prob, 4))\n                            except (ValueError, TypeError):\n                                continue\n                        except (ValueError, TypeError):\n                            continue\n\n        # Store joint probabilities in dataframe\n        enhanced_df.at[idx, 'joint_probabilities'] = joint_probs\n\n    # 2. Calculate network metrics\n    # Create a directed graph\n    import networkx as nx\n    G = nx.DiGraph()\n\n    # Add nodes\n    for idx, row in enhanced_df.iterrows():\n        G.add_node(row['Title'])\n\n    # Add edges\n    for idx, row in enhanced_df.iterrows():\n        child = row['Title']\n        parents = row['Parents'] if isinstance(row['Parents'], list) else []\n\n        for parent in parents:\n            if parent in G.nodes():\n                G.add_edge(parent, child)\n\n    # Calculate centrality measures\n    degree_centrality = nx.degree_centrality(G)  # Overall connectedness\n    in_degree_centrality = nx.in_degree_centrality(G)  # How many nodes affect this one\n    out_degree_centrality = nx.out_degree_centrality(G)  # How many nodes this one affects\n\n    try:\n        betweenness_centrality = nx.betweenness_centrality(G)  # Node's role as a connector\n    except:\n        betweenness_centrality = {node: 0 for node in G.nodes()}\n\n    # Add metrics to dataframe\n    enhanced_df['degree_centrality'] = None\n    enhanced_df['in_degree_centrality'] = None\n    enhanced_df['out_degree_centrality'] = None\n    enhanced_df['betweenness_centrality'] = None\n\n    for idx, row in enhanced_df.iterrows():\n        title = row['Title']\n        enhanced_df.at[idx, 'degree_centrality'] = degree_centrality.get(title, 0)\n        enhanced_df.at[idx, 'in_degree_centrality'] = in_degree_centrality.get(title, 0)\n        enhanced_df.at[idx, 'out_degree_centrality'] = out_degree_centrality.get(title, 0)\n        enhanced_df.at[idx, 'betweenness_centrality'] = betweenness_centrality.get(title, 0)\n\n    # 3. Add Markov blanket information (parents, children, and children's parents)\n    enhanced_df['markov_blanket'] = None\n\n    for idx, row in enhanced_df.iterrows():\n        title = row['Title']\n        parents = row['Parents'] if isinstance(row['Parents'], list) else []\n        children = row['Children'] if isinstance(row['Children'], list) else []\n\n        # Get children's parents (excluding this node)\n        childrens_parents = []\n        for child in children:\n            child_row = enhanced_df[enhanced_df['Title'] == child]\n            if not child_row.empty:\n                child_parents = child_row.iloc[0]['Parents']\n                if isinstance(child_parents, list):\n                    childrens_parents.extend([p for p in child_parents if p != title])\n\n        # Remove duplicates\n        childrens_parents = list(set(childrens_parents))\n\n        # Combine to get Markov blanket\n        markov_blanket = list(set(parents + children + childrens_parents))\n        enhanced_df.at[idx, 'markov_blanket'] = markov_blanket\n\n    return enhanced_df\n\n\n\n\nCode\n# @title 3.2.1 --- Enhance Extracted Data with Network Metrics --- [enhance_extracted_data_with_network_metrics]\n\n\"\"\"\nBLOCK PURPOSE: Applies the post-processing functions to enhance the extracted data.\n\nThis block takes the basic extracted DataFrame from the BayesDown parsing step\nand enriches it with calculated metrics that provide deeper insight into the\nnetwork structure and relationships. It:\n\n1. Applies the enhancement functions defined previously\n2. Displays summary information about key calculated metrics\n3. Saves the enhanced data for further analysis and visualization\n\nThe enhanced DataFrame provides a richer representation of the Bayesian network,\nincluding measures of node importance and conditional relationships that are\nessential for effective analysis and visualization.\n\nDEPENDENCIES: enhance_extracted_data function\nINPUTS: DataFrame with basic extracted BayesDown data\nOUTPUTS: Enhanced DataFrame with additional calculated columns, saved to CSV\n\"\"\"\n\n# Enhance the extracted dataframe with calculated columns\nenhanced_df = enhance_extracted_data(result_df)\n\n# Display the enhanced dataframe\nprint(\"Enhanced DataFrame with additional calculated columns:\")\nenhanced_df.head()\n\n# Check some calculated metrics\nprint(\"\\nJoint Probabilities Example:\")\nexample_node = enhanced_df.loc[0, 'Title']\njoint_probs = enhanced_df.loc[0, 'joint_probabilities']\nprint(f\"Joint probabilities for {example_node}:\")\nprint(joint_probs)\n\nprint(\"\\nNetwork Metrics:\")\nfor idx, row in enhanced_df.iterrows():\n    print(f\"{row['Title']}:\")\n    print(f\"  Degree Centrality: {row['degree_centrality']:.3f}\")\n    print(f\"  Betweenness Centrality: {row['betweenness_centrality']:.3f}\")\n\n# Save the enhanced dataframe\nenhanced_df.to_csv('enhanced_extracted_data.csv', index=False)\nprint(\"\\nEnhanced data saved to 'enhanced_extracted_data.csv'\")\n\n\nEnhanced DataFrame with additional calculated columns:\n\nJoint Probabilities Example:\nJoint probabilities for Existential_Catastrophe:\nNone\n\nNetwork Metrics:\nExistential_Catastrophe:\n  Degree Centrality: 0.000\n  Betweenness Centrality: 0.000\nHuman_Disempowerment:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nScale_Of_Power_Seeking:\n  Degree Centrality: 0.136\n  Betweenness Centrality: 0.037\nMisaligned_Power_Seeking:\n  Degree Centrality: 0.182\n  Betweenness Centrality: 0.056\nAPS_Systems:\n  Degree Centrality: 0.182\n  Betweenness Centrality: 0.019\nAdvanced_AI_Capability:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nAgentic_Planning:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nStrategic_Awareness:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nDifficulty_Of_Alignment:\n  Degree Centrality: 0.182\n  Betweenness Centrality: 0.019\nInstrumental_Convergence:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nProblems_With_Proxies:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nProblems_With_Search:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nDeployment_Decisions:\n  Degree Centrality: 0.136\n  Betweenness Centrality: 0.026\nIncentives_To_Build_APS:\n  Degree Centrality: 0.136\n  Betweenness Centrality: 0.017\nUsefulness_Of_APS:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nCompetitive_Dynamics:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nDeception_By_AI:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nCorrective_Feedback:\n  Degree Centrality: 0.136\n  Betweenness Centrality: 0.009\nWarning_Shots:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nRapid_Capability_Escalation:\n  Degree Centrality: 0.045\n  Betweenness Centrality: 0.000\nBarriers_To_Understanding:\n  Degree Centrality: 0.000\n  Betweenness Centrality: 0.000\nAdversarial_Dynamics:\n  Degree Centrality: 0.000\n  Betweenness Centrality: 0.000\nStakes_Of_Error:\n  Degree Centrality: 0.000\n  Betweenness Centrality: 0.000\n\nEnhanced data saved to 'enhanced_extracted_data.csv'",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#download-and-save-finished-data-frame-as-.csv-file",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#download-and-save-finished-data-frame-as-.csv-file",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "F.7 3.4 Download and save finished data frame as .csv file",
    "text": "F.7 3.4 Download and save finished data frame as .csv file\n\n\nCode\n# @title 3.4.0 --- Save Extracted Data for Further Processing --- [save_extracted_data_for_further_processing]\n\n\"\"\"\nBLOCK PURPOSE: Saves the extracted data to a CSV file for further processing.\n\nThis step is essential for:\n1. Persisting the structured representation of the Bayesian network\n2. Enabling further analysis in other tools or notebook sections\n3. Creating a permanent record of the extraction results\n4. Making the data available for the visualization pipeline\n\nThe CSV format provides a standardized, tabular representation of the network\nthat can be easily loaded and processed in subsequent analysis steps.\n\nDEPENDENCIES: pandas DataFrame operations\nINPUTS: Extracted DataFrame from the parsing step\nOUTPUTS: CSV file containing the structured network data\n\"\"\"\n\n# Save the extracted data as a CSV file\nresult_df.to_csv('extracted_data.csv', index=False)\n\nprint(\"✅ Extracted data saved successfully to 'extracted_data.csv'\")\nprint(\"Note: If using updated data in future steps, \"\n        + \"the file must be pushed to the GitHub repository\")\n\n\n✅ Extracted data saved successfully to 'extracted_data.csv'\nNote: If using updated data in future steps, the file must be pushed to the GitHub repository",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#bayesian-network-visualization-approach",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#bayesian-network-visualization-approach",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "G.1 4.0 Bayesian Network Visualization Approach",
    "text": "G.1 4.0 Bayesian Network Visualization Approach\nThis section implements the visualization component of the AMTAIR project, transforming the structured data extracted from BayesDown into an interactive network visualization that makes complex probabilistic relationships accessible to human understanding.\n\nG.1.1 Visualization Philosophy\nA key challenge in AI governance is making complex probabilistic relationships understandable to diverse stakeholders. This visualization system addresses this challenge through:\n\nVisual Encoding of Probability: Node colors reflect probability values (green for high probability, red for low)\nStructural Classification: Border colors indicate node types (blue for root causes, purple for intermediate nodes, magenta for leaf nodes)\nProgressive Disclosure: Basic information in tooltips, detailed probability tables in modal popups\nInteractive Exploration: Draggable nodes, configurable physics, click interactions\n\n\n\nG.1.2 Connection to AMTAIR Goals\nThis visualization approach directly supports the AMTAIR project’s goal of improving coordination in AI governance by:\n\nMaking implicit models explicit through visual representation\nProviding a common language for discussing probabilistic relationships\nEnabling non-technical stakeholders to engage with formal models\nCreating shareable artifacts that facilitate collaboration\n\n\n\nG.1.3 Implementation Structure\nThe visualization system is implemented in four phases:\n\nNetwork Construction: Creating a directed graph representation using NetworkX\nNode Classification: Identifying node types based on network position\nVisual Enhancement: Adding color coding, tooltips, and interactive elements\nInteractive Features: Implementing click handling for detailed exploration\n\nThe resulting visualization serves as both an analytical tool for experts and a communication tool for broader audiences, bridging the gap between technical and policy domains in AI governance discussions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#phase-1-dependenciesfunctions",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#phase-1-dependenciesfunctions",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "G.2 4.1 Phase 1: Dependencies/Functions",
    "text": "G.2 4.1 Phase 1: Dependencies/Functions\n\n\nCode\n# @title 4.1.0 --- Bayesian Network Visualization Functions --- [bayesian_network_visualization_functions]\n\n\"\"\"\nBLOCK PURPOSE: Provides functions to create interactive Bayesian network\nvisualizations from DataFrame representations of ArgDown/BayesDown data.\n\nThis block implements the visualization pipeline described in the AMTAIR project,\ntransforming the structured DataFrame extracted from ArgDown/BayesDown into an\ninteractive network graph that displays nodes, relationships, and probability\ninformation. The visualization leverages NetworkX for graph representation and\nPyVis for interactive display.\n\nKey visualization features:\n1. Color-coding of nodes based on probability values\n2. Border styling to indicate node types (root, intermediate, leaf)\n3. Interactive tooltips with probability information\n4. Modal popups with detailed conditional probability tables\n5. Physics-based layout for intuitive exploration\n\nDEPENDENCIES: networkx, pyvis, HTML display from IPython\nINPUTS: DataFrame with node information, relationships, and probabilities\nOUTPUTS: Interactive HTML visualization of the Bayesian network\n\"\"\"\n\nfrom pyvis.network import Network\nimport networkx as nx\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport io\nimport base64\nimport colorsys\nimport json\n\ndef create_bayesian_network_with_probabilities(df):\n    \"\"\"\n    Create an interactive Bayesian network visualization with enhanced\n    probability visualization and node classification based on network structure.\n\n    Args:\n        df (pandas.DataFrame): DataFrame containing node information, relationships, and probabilities\n\n    Returns:\n        IPython.display.HTML: Interactive HTML visualization of the Bayesian network\n    \"\"\"\n    # PHASE 1: Create a directed graph representation\n    G = nx.DiGraph()\n\n    # Add nodes with proper attributes\n    for idx, row in df.iterrows():\n        title = row['Title']\n        description = row['Description']\n\n        # Process probability information\n        priors = get_priors(row)\n        instantiations = get_instantiations(row)\n\n        # Add node with base information\n        G.add_node(\n            title,\n            description=description,\n            priors=priors,\n            instantiations=instantiations,\n            posteriors=get_posteriors(row)\n        )\n\n    # Add edges based on parent-child relationships\n    for idx, row in df.iterrows():\n        child = row['Title']\n        parents = get_parents(row)\n\n        # Add edges from each parent to this child\n        for parent in parents:\n            if parent in G.nodes():\n                G.add_edge(parent, child)\n\n    # PHASE 2: Classify nodes based on network structure\n    classify_nodes(G)\n\n    # PHASE 3: Create interactive network visualization\n    net = Network(notebook=True, directed=True, cdn_resources=\"in_line\", height=\"600px\", width=\"100%\")\n\n    # Configure physics for better layout\n    net.force_atlas_2based(gravity=-50, spring_length=100, spring_strength=0.02)\n    net.show_buttons(filter_=['physics'])  # Allow user to adjust physics settings\n\n    # Add the graph to the network\n    net.from_nx(G)\n\n    # PHASE 4: Enhance node appearance with probability information\n    for node in net.nodes:\n        node_id = node['id']\n        node_data = G.nodes[node_id]\n\n        # Get node type and set border color\n        node_type = node_data.get('node_type', 'unknown')\n        border_color = get_border_color(node_type)\n\n        # Get probability information\n        priors = node_data.get('priors', {})\n        true_prob = priors.get('true_prob', 0.5) if priors else 0.5\n\n        # Get proper state names\n        instantiations = node_data.get('instantiations', [\"TRUE\", \"FALSE\"])\n        true_state = instantiations[0] if len(instantiations) &gt; 0 else \"TRUE\"\n        false_state = instantiations[1] if len(instantiations) &gt; 1 else \"FALSE\"\n\n        # Create background color based on probability\n        background_color = get_probability_color(priors)\n\n        # Create tooltip with probability information\n        tooltip = create_tooltip(node_id, node_data)\n\n        # Create a simpler node label with probability\n        simple_label = f\"{node_id}\\np={true_prob:.2f}\"\n\n        # Store expanded content as a node attribute for use in click handler\n        node_data['expanded_content'] = create_expanded_content(node_id, node_data)\n\n        # Set node attributes\n        node['title'] = tooltip  # Tooltip HTML\n        node['label'] = simple_label  # Simple text label\n        node['shape'] = 'box'\n        node['color'] = {\n            'background': background_color,\n            'border': border_color,\n            'highlight': {\n                'background': background_color,\n                'border': border_color\n            }\n        }\n\n    # PHASE 5: Setup interactive click handling\n    # Prepare data for click handler\n    setup_data = {\n        'nodes_data': {node_id: {\n            'expanded_content': json.dumps(G.nodes[node_id].get('expanded_content', '')),\n            'description': G.nodes[node_id].get('description', ''),\n            'priors': G.nodes[node_id].get('priors', {}),\n            'posteriors': G.nodes[node_id].get('posteriors', {})\n        } for node_id in G.nodes()}\n    }\n\n    # JavaScript code for handling node clicks\n    click_js = \"\"\"\n    // Store node data for click handling\n    var nodesData = %s;\n\n    // Add event listener for node clicks\n    network.on(\"click\", function(params) {\n        if (params.nodes.length &gt; 0) {\n            var nodeId = params.nodes[0];\n            var nodeInfo = nodesData[nodeId];\n\n            if (nodeInfo) {\n                // Create a modal popup for expanded content\n                var modal = document.createElement('div');\n                modal.style.position = 'fixed';\n                modal.style.left = '50%%';\n                modal.style.top = '50%%';\n                modal.style.transform = 'translate(-50%%, -50%%)';\n                modal.style.backgroundColor = 'white';\n                modal.style.padding = '20px';\n                modal.style.borderRadius = '5px';\n                modal.style.boxShadow = '0 0 10px rgba(0,0,0,0.5)';\n                modal.style.zIndex = '1000';\n                modal.style.maxWidth = '80%%';\n                modal.style.maxHeight = '80%%';\n                modal.style.overflow = 'auto';\n\n                // Add expanded content\n                modal.innerHTML = nodeInfo.expanded_content || 'No detailed information available';\n\n                // Add close button\n                var closeBtn = document.createElement('button');\n                closeBtn.innerHTML = 'Close';\n                closeBtn.style.marginTop = '10px';\n                closeBtn.style.padding = '5px 10px';\n                closeBtn.style.cursor = 'pointer';\n                closeBtn.onclick = function() {\n                    document.body.removeChild(modal);\n                };\n                modal.appendChild(closeBtn);\n\n                // Add modal to body\n                document.body.appendChild(modal);\n            }\n        }\n    });\n    \"\"\" % json.dumps(setup_data['nodes_data'])\n\n    # PHASE 6: Save the graph to HTML and inject custom click handling\n    html_file = \"bayesian_network.html\"\n    net.save_graph(html_file)\n\n    # Inject custom click handling into HTML\n    try:\n        with open(html_file, \"r\") as f:\n            html_content = f.read()\n\n        # Insert click handling script before the closing body tag\n        html_content = html_content.replace('&lt;/body&gt;', f'&lt;script&gt;{click_js}&lt;/script&gt;&lt;/body&gt;')\n\n        # Write back the modified HTML\n        with open(html_file, \"w\") as f:\n            f.write(html_content)\n\n        return HTML(html_content)\n    except Exception as e:\n        return HTML(f\"&lt;p&gt;Error rendering HTML: {str(e)}&lt;/p&gt;\"\n          + \"&lt;p&gt;The network visualization has been saved to '{html_file}'&lt;/p&gt;\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#phase-2-node-classification-and-styling-module",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#phase-2-node-classification-and-styling-module",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "G.3 4.2 Phase 2: Node Classification and Styling Module",
    "text": "G.3 4.2 Phase 2: Node Classification and Styling Module\n\n\nCode\n# @title 4.2.0 --- Node Classification and Styling Functions --- [node_classification_and_styling_functions]\n\n\"\"\"\nBLOCK PURPOSE: Implements the visual classification and styling of nodes in the Bayesian network.\n\nThis module handles the identification of node types based on their position in\nthe network and provides appropriate visual styling for each type.\nThe functions:\n\n1. Classify nodes as parents (causes), children (intermediate effects), or leaves (final effects)\n2. Assign appropriate border colors to visually distinguish node types\n3. Calculate background colors based on probability values\n4. Extract relevant information from DataFrame rows in a robust manner\n\nThe visual encoding helps users understand both the structure of the network\nand the probability distributions at a glance.\n\nDEPENDENCIES: colorsys for color manipulation\nINPUTS: Graph structure and node data\nOUTPUTS: Classification and styling information for visualization\n\"\"\"\n\ndef classify_nodes(G):\n    \"\"\"\n    Classify nodes as parent, child, or leaf based on network structure\n\n    Args:\n        G (networkx.DiGraph): Directed graph representation of the Bayesian network\n\n    Effects:\n        Adds 'node_type' attribute to each node in the graph:\n        - 'parent': Root node with no parents but has children (causal source)\n        - 'child': Node with both parents and children (intermediate)\n        - 'leaf': Node with parents but no children (final effect)\n        - 'isolated': Node with no connections (rare in Bayesian networks)\n    \"\"\"\n    for node in G.nodes():\n        predecessors = list(G.predecessors(node))  # Nodes pointing to this one (causes)\n        successors = list(G.successors(node))      # Nodes this one points to (effects)\n\n        if not predecessors:  # No parents\n            if successors:  # Has children\n                G.nodes[node]['node_type'] = 'parent'  # Root cause\n            else:  # No children either\n                G.nodes[node]['node_type'] = 'isolated'  # Disconnected node\n        else:  # Has parents\n            if not successors:  # No children\n                G.nodes[node]['node_type'] = 'leaf'  # Final effect\n            else:  # Has both parents and children\n                G.nodes[node]['node_type'] = 'child'  # Intermediate node\n\ndef get_border_color(node_type):\n    \"\"\"\n    Return border color based on node type\n\n    Args:\n        node_type (str): Type of node ('parent', 'child', 'leaf', or 'isolated')\n\n    Returns:\n        str: Hex color code for node border\n    \"\"\"\n    if node_type == 'parent':\n        return '#0000FF'  # Blue for root causes\n    elif node_type == 'child':\n        return '#800080'  # Purple for intermediate nodes\n    elif node_type == 'leaf':\n        return '#FF00FF'  # Magenta for final effects\n    else:\n        return '#000000'  # Default black for any other type\n\ndef get_probability_color(priors):\n    \"\"\"\n    Create background color based on probability (red to green gradient)\n\n    Args:\n        priors (dict): Dictionary containing probability information\n\n    Returns:\n        str: Hex color code for node background, ranging from red (low probability)\n             to green (high probability)\n    \"\"\"\n    # Default to neutral color if no probability\n    if not priors or 'true_prob' not in priors:\n        return '#F8F8F8'  # Light grey\n\n    # Get probability value\n    prob = priors['true_prob']\n\n    # Create color gradient from red (0.0) to green (1.0)\n    hue = 120 * prob  # 0 = red, 120 = green (in HSL color space)\n    saturation = 0.75\n    lightness = 0.8  # Lighter color for better text visibility\n\n    # Convert HSL to RGB\n    r, g, b = colorsys.hls_to_rgb(hue/360, lightness, saturation)\n\n    # Convert to hex format\n    hex_color = \"#{:02x}{:02x}{:02x}\".format(int(r*255), int(g*255), int(b*255))\n\n    return hex_color\n\ndef get_parents(row):\n    \"\"\"\n    Extract parent nodes from row data, with safe handling for different data types\n\n    Args:\n        row (pandas.Series): Row from DataFrame containing node information\n\n    Returns:\n        list: List of parent node names\n    \"\"\"\n    if 'Parents' not in row:\n        return []\n\n    parents_data = row['Parents']\n\n    # Handle NaN, None, or empty list\n    if isinstance(parents_data, float) and pd.isna(parents_data):\n        return []\n\n    if parents_data is None:\n        return []\n\n    # Handle different data types\n    if isinstance(parents_data, list):\n        # Return a list with NaN and empty strings removed\n        return [p for p in parents_data if not (isinstance(p, float) and pd.isna(p)) and p != '']\n\n    if isinstance(parents_data, str):\n        if not parents_data.strip():\n            return []\n\n        # Remove brackets and split by comma, removing empty strings and NaN\n        cleaned = parents_data.strip('[]\"\\'')\n        if not cleaned:\n            return []\n\n        return [p.strip(' \"\\'') for p in cleaned.split(',') if p.strip()]\n\n    # Default: empty list\n    return []\n\ndef get_instantiations(row):\n    \"\"\"\n    Extract instantiations with safe handling for different data types\n\n    Args:\n        row (pandas.Series): Row from DataFrame containing node information\n\n    Returns:\n        list: List of possible instantiations (states) for the node\n    \"\"\"\n    if 'instantiations' not in row:\n        return [\"TRUE\", \"FALSE\"]\n\n    inst_data = row['instantiations']\n\n    # Handle NaN or None\n    if isinstance(inst_data, float) and pd.isna(inst_data):\n        return [\"TRUE\", \"FALSE\"]\n\n    if inst_data is None:\n        return [\"TRUE\", \"FALSE\"]\n\n    # Handle different data types\n    if isinstance(inst_data, list):\n        return inst_data if inst_data else [\"TRUE\", \"FALSE\"]\n\n    if isinstance(inst_data, str):\n        if not inst_data.strip():\n            return [\"TRUE\", \"FALSE\"]\n\n        # Remove brackets and split by comma\n        cleaned = inst_data.strip('[]\"\\'')\n        if not cleaned:\n            return [\"TRUE\", \"FALSE\"]\n\n        return [i.strip(' \"\\'') for i in cleaned.split(',') if i.strip()]\n\n    # Default\n    return [\"TRUE\", \"FALSE\"]\n\ndef get_priors(row):\n    \"\"\"\n    Extract prior probabilities with safe handling for different data types\n\n    Args:\n        row (pandas.Series): Row from DataFrame containing node information\n\n    Returns:\n        dict: Dictionary of prior probabilities with 'true_prob' added for convenience\n    \"\"\"\n    if 'priors' not in row:\n        return {}\n\n    priors_data = row['priors']\n\n    # Handle NaN or None\n    if isinstance(priors_data, float) and pd.isna(priors_data):\n        return {}\n\n    if priors_data is None:\n        return {}\n\n    result = {}\n\n    # Handle dictionary\n    if isinstance(priors_data, dict):\n        result = priors_data\n    # Handle string representation of dictionary\n    elif isinstance(priors_data, str):\n        if not priors_data.strip() or priors_data == '{}':\n            return {}\n\n        try:\n            # Try to evaluate as Python literal\n            import ast\n            result = ast.literal_eval(priors_data)\n        except:\n            # Simple parsing for items like {'p(TRUE)': '0.2', 'p(FALSE)': '0.8'}\n            if '{' in priors_data and '}' in priors_data:\n                content = priors_data[priors_data.find('{')+1:priors_data.rfind('}')]\n                items = [item.strip() for item in content.split(',')]\n\n                for item in items:\n                    if ':' in item:\n                        key, value = item.split(':', 1)\n                        key = key.strip(' \\'\\\"')\n                        value = value.strip(' \\'\\\"')\n                        result[key] = value\n\n    # Extract main probability for TRUE state\n    instantiations = get_instantiations(row)\n    true_state = instantiations[0] if instantiations else \"TRUE\"\n    true_key = f\"p({true_state})\"\n\n    if true_key in result:\n        try:\n            result['true_prob'] = float(result[true_key])\n        except:\n            pass\n\n    return result\n\ndef get_posteriors(row):\n    \"\"\"\n    Extract posterior probabilities with safe handling for different data types\n\n    Args:\n        row (pandas.Series): Row from DataFrame containing node information\n\n    Returns:\n        dict: Dictionary of conditional probabilities\n    \"\"\"\n    if 'posteriors' not in row:\n        return {}\n\n    posteriors_data = row['posteriors']\n\n    # Handle NaN or None\n    if isinstance(posteriors_data, float) and pd.isna(posteriors_data):\n        return {}\n\n    if posteriors_data is None:\n        return {}\n\n    result = {}\n\n    # Handle dictionary\n    if isinstance(posteriors_data, dict):\n        result = posteriors_data\n    # Handle string representation of dictionary\n    elif isinstance(posteriors_data, str):\n        if not posteriors_data.strip() or posteriors_data == '{}':\n            return {}\n\n        try:\n            # Try to evaluate as Python literal\n            import ast\n            result = ast.literal_eval(posteriors_data)\n        except:\n            # Simple parsing\n            if '{' in posteriors_data and '}' in posteriors_data:\n                content = posteriors_data[posteriors_data.find('{')+1:posteriors_data.rfind('}')]\n                items = [item.strip() for item in content.split(',')]\n\n                for item in items:\n                    if ':' in item:\n                        key, value = item.split(':', 1)\n                        key = key.strip(' \\'\\\"')\n                        value = value.strip(' \\'\\\"')\n                        result[key] = value\n\n    return result",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#phase-3-html-content-generation-module",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#phase-3-html-content-generation-module",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "G.4 4.3 Phase 3: HTML Content Generation Module",
    "text": "G.4 4.3 Phase 3: HTML Content Generation Module\n\n\nCode\n# @title 4.3.0 --- HTML Content Generation Functions --- [html_content_generation_functions]\n\n\"\"\"\nBLOCK PURPOSE: Creates rich HTML content for the interactive Bayesian network visualization.\n\nThis module generates the HTML components that enhance the Bayesian network\nvisualization:\n1. Probability bars - Visual representation of probability distributions\n2. Node tooltips - Rich information displayed on hover\n3. Expanded content - Detailed probability information shown when clicking nodes\n\nThese HTML components make the mathematical concepts of Bayesian networks more\nintuitive and accessible to users without requiring deep statistical knowledge.\nThe visual encoding of probabilities (colors, bars) and the progressive\ndisclosure of information (hover, click) help users build understanding at\ntheir own pace.\n\nDEPENDENCIES: HTML generation capabilities\nINPUTS: Node data from the Bayesian network\nOUTPUTS: HTML content for visualization components\n\"\"\"\n\ndef create_probability_bar(true_prob, false_prob, height=\"15px\", show_values=True, value_prefix=\"\"):\n    \"\"\"\n    Creates a reusable HTML component to visualize probability distribution\n\n    Args:\n        true_prob (float): Probability of the true state (0.0-1.0)\n        false_prob (float): Probability of the false state (0.0-1.0)\n        height (str): CSS height of the bar\n        show_values (bool): Whether to display numerical values\n        value_prefix (str): Prefix to add before values (e.g., \"p=\")\n\n    Returns:\n        str: HTML for a horizontal bar showing probabilities\n    \"\"\"\n    # Prepare display labels if showing values\n    true_label = f\"{value_prefix}{true_prob:.3f}\" if show_values else \"\"\n    false_label = f\"{value_prefix}{false_prob:.3f}\" if show_values else \"\"\n\n    # Create the HTML for a horizontal stacked bar\n    html = f\"\"\"\n    &lt;div style=\"width:100%; height:{height}; display:flex; border:1px solid #ccc; overflow:hidden; border-radius:3px; margin-top:3px; margin-bottom:3px;\"&gt;\n        &lt;div style=\"flex-basis:{true_prob*100}%; background:linear-gradient(to bottom, rgba(0,180,0,0.9), rgba(0,140,0,0.7)); border-right:2px solid #008800; display:flex; align-items:center; justify-content:center; overflow:hidden; min-width:{2 if true_prob &gt; 0 else 0}px;\"&gt;\n            &lt;span style=\"font-size:10px; color:white; text-shadow:0px 0px 2px #000;\"&gt;{true_label}&lt;/span&gt;\n        &lt;/div&gt;\n        &lt;div style=\"flex-basis:{false_prob*100}%; background:linear-gradient(to bottom, rgba(220,0,0,0.9), rgba(180,0,0,0.7)); border-left:2px solid #880000; display:flex; align-items:center; justify-content:center; overflow:hidden; min-width:{2 if false_prob &gt; 0 else 0}px;\"&gt;\n            &lt;span style=\"font-size:10px; color:white; text-shadow:0px 0px 2px #000;\"&gt;{false_label}&lt;/span&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    \"\"\"\n    return html\n\ndef create_tooltip(node_id, node_data):\n    \"\"\"\n    Create rich HTML tooltip with probability information\n\n    Args:\n        node_id (str): Identifier of the node\n        node_data (dict): Node attributes including probabilities\n\n    Returns:\n        str: HTML content for tooltip displayed on hover\n    \"\"\"\n    # Extract node information\n    description = node_data.get('description', '')\n    priors = node_data.get('priors', {})\n    instantiations = node_data.get('instantiations', [\"TRUE\", \"FALSE\"])\n\n    # Start building the HTML tooltip\n    html = f\"\"\"\n    &lt;div style=\"max-width:350px; padding:10px; background-color:#f8f9fa; border-radius:5px; font-family:Arial, sans-serif;\"&gt;\n        &lt;h3 style=\"margin-top:0; color:#202124;\"&gt;{node_id}&lt;/h3&gt;\n        &lt;p style=\"font-style:italic;\"&gt;{description}&lt;/p&gt;\n    \"\"\"\n\n    # Add prior probabilities section\n    if priors and 'true_prob' in priors:\n        true_prob = priors['true_prob']\n        false_prob = 1.0 - true_prob\n\n        # Get proper state names\n        true_state = instantiations[0] if len(instantiations) &gt; 0 else \"TRUE\"\n        false_state = instantiations[1] if len(instantiations) &gt; 1 else \"FALSE\"\n\n        html += f\"\"\"\n        &lt;div style=\"margin-top:10px; background-color:#fff; padding:8px; border-radius:4px; border:1px solid #ddd;\"&gt;\n            &lt;h4 style=\"margin-top:0; font-size:14px;\"&gt;Prior Probabilities:&lt;/h4&gt;\n            &lt;div style=\"display:flex; justify-content:space-between; margin-bottom:4px;\"&gt;\n                &lt;div style=\"font-size:12px;\"&gt;{true_state}: {true_prob:.3f}&lt;/div&gt;\n                &lt;div style=\"font-size:12px;\"&gt;{false_state}: {false_prob:.3f}&lt;/div&gt;\n            &lt;/div&gt;\n            {create_probability_bar(true_prob, false_prob, \"20px\", True)}\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add click instruction\n    html += \"\"\"\n    &lt;div style=\"margin-top:8px; font-size:12px; color:#666; text-align:center;\"&gt;\n        Click node to see full probability details\n    &lt;/div&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return html\n\ndef create_expanded_content(node_id, node_data):\n    \"\"\"\n    Create expanded content shown when a node is clicked\n\n    Args:\n        node_id (str): Identifier of the node\n        node_data (dict): Node attributes including probabilities\n\n    Returns:\n        str: HTML content for detailed view displayed on click\n    \"\"\"\n    # Extract node information\n    description = node_data.get('description', '')\n    priors = node_data.get('priors', {})\n    posteriors = node_data.get('posteriors', {})\n    instantiations = node_data.get('instantiations', [\"TRUE\", \"FALSE\"])\n\n    # Get proper state names\n    true_state = instantiations[0] if len(instantiations) &gt; 0 else \"TRUE\"\n    false_state = instantiations[1] if len(instantiations) &gt; 1 else \"FALSE\"\n\n    # Extract probabilities\n    true_prob = priors.get('true_prob', 0.5)\n    false_prob = 1.0 - true_prob\n\n    # Start building the expanded content\n    html = f\"\"\"\n    &lt;div style=\"max-width:500px; padding:15px; font-family:Arial, sans-serif;\"&gt;\n        &lt;h2 style=\"margin-top:0; color:#333;\"&gt;{node_id}&lt;/h2&gt;\n        &lt;p style=\"font-style:italic; margin-bottom:15px;\"&gt;{description}&lt;/p&gt;\n\n        &lt;div style=\"margin-bottom:20px; padding:12px; border:1px solid #ddd; background-color:#f9f9f9; border-radius:5px;\"&gt;\n            &lt;h3 style=\"margin-top:0; color:#333;\"&gt;Prior Probabilities&lt;/h3&gt;\n            &lt;div style=\"display:flex; justify-content:space-between; margin-bottom:5px;\"&gt;\n                &lt;div&gt;&lt;strong&gt;{true_state}:&lt;/strong&gt; {true_prob:.3f}&lt;/div&gt;\n                &lt;div&gt;&lt;strong&gt;{false_state}:&lt;/strong&gt; {false_prob:.3f}&lt;/div&gt;\n            &lt;/div&gt;\n            {create_probability_bar(true_prob, false_prob, \"25px\", True)}\n        &lt;/div&gt;\n    \"\"\"\n\n    # Add conditional probability table if available\n    if posteriors:\n        html += \"\"\"\n        &lt;div style=\"padding:12px; border:1px solid #ddd; background-color:#f9f9f9; border-radius:5px;\"&gt;\n            &lt;h3 style=\"margin-top:0; color:#333;\"&gt;Conditional Probabilities&lt;/h3&gt;\n            &lt;table style=\"width:100%; border-collapse:collapse; font-size:13px;\"&gt;\n                &lt;tr style=\"background-color:#eee;\"&gt;\n                    &lt;th style=\"padding:8px; text-align:left; border:1px solid #ddd;\"&gt;Condition&lt;/th&gt;\n                    &lt;th style=\"padding:8px; text-align:center; border:1px solid #ddd; width:80px;\"&gt;Value&lt;/th&gt;\n                    &lt;th style=\"padding:8px; text-align:center; border:1px solid #ddd;\"&gt;Visualization&lt;/th&gt;\n                &lt;/tr&gt;\n        \"\"\"\n\n        # Sort posteriors to group by similar conditions\n        posterior_items = list(posteriors.items())\n        posterior_items.sort(key=lambda x: x[0])\n\n        # Add rows for conditional probabilities\n        for key, value in posterior_items:\n            try:\n                # Try to parse probability value\n                prob_value = float(value)\n                inv_prob = 1.0 - prob_value\n\n                # Add row with probability visualization\n                html += f\"\"\"\n                &lt;tr&gt;\n                    &lt;td style=\"padding:8px; border:1px solid #ddd;\"&gt;{key}&lt;/td&gt;\n                    &lt;td style=\"padding:8px; text-align:center; border:1px solid #ddd;\"&gt;{prob_value:.3f}&lt;/td&gt;\n                    &lt;td style=\"padding:8px; border:1px solid #ddd;\"&gt;\n                        {create_probability_bar(prob_value, inv_prob, \"20px\", False)}\n                    &lt;/td&gt;\n                &lt;/tr&gt;\n                \"\"\"\n            except:\n                # Fallback for non-numeric values\n                html += f\"\"\"\n                &lt;tr&gt;\n                    &lt;td style=\"padding:8px; border:1px solid #ddd;\"&gt;{key}&lt;/td&gt;\n                    &lt;td style=\"padding:8px; text-align:center; border:1px solid #ddd;\" colspan=\"2\"&gt;{value}&lt;/td&gt;\n                &lt;/tr&gt;\n                \"\"\"\n\n        html += \"\"\"\n            &lt;/table&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    html += \"&lt;/div&gt;\"\n\n    return html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#phase-4-main-visualization-function",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#phase-4-main-visualization-function",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "G.5 4.4 Phase 4: Main Visualization Function",
    "text": "G.5 4.4 Phase 4: Main Visualization Function\n\n\nCode\n# @title 4.4.0 --- Main Visualization Function --- [main_visualization_function]\n\ndef create_bayesian_network_with_probabilities(df):\n    \"\"\"\n    Create an interactive Bayesian network visualization with enhanced\n    probability visualization and node classification based on network structure.\n    \"\"\"\n    # Create a directed graph\n    G = nx.DiGraph()\n\n    # Add nodes with proper attributes\n    for idx, row in df.iterrows():\n        title = row['Title']\n        description = row['Description']\n\n        # Process probability information\n        priors = get_priors(row)\n        instantiations = get_instantiations(row)\n\n        # Add node with base information\n        G.add_node(\n            title,\n            description=description,\n            priors=priors,\n            instantiations=instantiations,\n            posteriors=get_posteriors(row)\n        )\n\n    # Add edges\n    for idx, row in df.iterrows():\n        child = row['Title']\n        parents = get_parents(row)\n\n        # Add edges from each parent to this child\n        for parent in parents:\n            if parent in G.nodes():\n                G.add_edge(parent, child)\n\n    # Classify nodes based on network structure\n    classify_nodes(G)\n\n    # Create network visualization\n    net = Network(notebook=True, directed=True, cdn_resources=\"in_line\", height=\"600px\", width=\"100%\")\n\n    # Configure physics for better layout\n    net.force_atlas_2based(gravity=-50, spring_length=100, spring_strength=0.02)\n    net.show_buttons(filter_=['physics'])\n\n    # Add the graph to the network\n    net.from_nx(G)\n\n    # Enhance node appearance with probability information and classification\n    for node in net.nodes:\n        node_id = node['id']\n        node_data = G.nodes[node_id]\n\n        # Get node type and set border color\n        node_type = node_data.get('node_type', 'unknown')\n        border_color = get_border_color(node_type)\n\n        # Get probability information\n        priors = node_data.get('priors', {})\n        true_prob = priors.get('true_prob', 0.5) if priors else 0.5\n\n        # Get proper state names\n        instantiations = node_data.get('instantiations', [\"TRUE\", \"FALSE\"])\n        true_state = instantiations[0] if len(instantiations) &gt; 0 else \"TRUE\"\n        false_state = instantiations[1] if len(instantiations) &gt; 1 else \"FALSE\"\n\n        # Create background color based on probability\n        background_color = get_probability_color(priors)\n\n        # Create tooltip with probability information\n        tooltip = create_tooltip(node_id, node_data)\n\n        # Create a simpler node label with probability\n        simple_label = f\"{node_id}\\np={true_prob:.2f}\"\n\n        # Store expanded content as a node attribute for use in click handler\n        node_data['expanded_content'] = create_expanded_content(node_id, node_data)\n\n        # Set node attributes\n        node['title'] = tooltip  # Tooltip HTML\n        node['label'] = simple_label  # Simple text label\n        node['shape'] = 'box'\n        node['color'] = {\n            'background': background_color,\n            'border': border_color,\n            'highlight': {\n                'background': background_color,\n                'border': border_color\n            }\n        }\n\n    # Set up the click handler with proper data\n    setup_data = {\n        'nodes_data': {node_id: {\n            'expanded_content': json.dumps(G.nodes[node_id].get('expanded_content', '')),\n            'description': G.nodes[node_id].get('description', ''),\n            'priors': G.nodes[node_id].get('priors', {}),\n            'posteriors': G.nodes[node_id].get('posteriors', {})\n        } for node_id in G.nodes()}\n    }\n\n    # Add custom click handling JavaScript\n    click_js = \"\"\"\n    // Store node data for click handling\n    var nodesData = %s;\n\n    // Add event listener for node clicks\n    network.on(\"click\", function(params) {\n        if (params.nodes.length &gt; 0) {\n            var nodeId = params.nodes[0];\n            var nodeInfo = nodesData[nodeId];\n\n            if (nodeInfo) {\n                // Create a modal popup for expanded content\n                var modal = document.createElement('div');\n                modal.style.position = 'fixed';\n                modal.style.left = '50%%';\n                modal.style.top = '50%%';\n                modal.style.transform = 'translate(-50%%, -50%%)';\n                modal.style.backgroundColor = 'white';\n                modal.style.padding = '20px';\n                modal.style.borderRadius = '5px';\n                modal.style.boxShadow = '0 0 10px rgba(0,0,0,0.5)';\n                modal.style.zIndex = '1000';\n                modal.style.maxWidth = '80%%';\n                modal.style.maxHeight = '80%%';\n                modal.style.overflow = 'auto';\n\n                // Parse the JSON string back to HTML content\n                try {\n                    var expandedContent = JSON.parse(nodeInfo.expanded_content);\n                    modal.innerHTML = expandedContent;\n                } catch (e) {\n                    modal.innerHTML = 'Error displaying content: ' + e.message;\n                }\n\n                // Add close button\n                var closeBtn = document.createElement('button');\n                closeBtn.innerHTML = 'Close';\n                closeBtn.style.marginTop = '10px';\n                closeBtn.style.padding = '5px 10px';\n                closeBtn.style.cursor = 'pointer';\n                closeBtn.onclick = function() {\n                    document.body.removeChild(modal);\n                };\n                modal.appendChild(closeBtn);\n\n                // Add modal to body\n                document.body.appendChild(modal);\n            }\n        }\n    });\n    \"\"\" % json.dumps(setup_data['nodes_data'])\n\n    # Save the graph to HTML\n    html_file = \"bayesian_network.html\"\n    net.save_graph(html_file)\n\n    # Inject custom click handling into HTML\n    try:\n        with open(html_file, \"r\") as f:\n            html_content = f.read()\n\n        # Insert click handling script before the closing body tag\n        html_content = html_content.replace('&lt;/body&gt;', f'&lt;script&gt;{click_js}&lt;/script&gt;&lt;/body&gt;')\n\n        # Write back the modified HTML\n        with open(html_file, \"w\") as f:\n            f.write(html_content)\n\n        return HTML(html_content)\n    except Exception as e:\n        return HTML(f\"&lt;p&gt;Error rendering HTML: {str(e)}&lt;/p&gt;\"\n        + \"&lt;p&gt;The network visualization has been saved to '{html_file}'&lt;/p&gt;\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#summary-of-achievements",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#summary-of-achievements",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "I.1 Summary of Achievements",
    "text": "I.1 Summary of Achievements\nThis notebook has successfully demonstrated the core AMTAIR extraction pipeline, transforming structured argument representations into interactive Bayesian network visualizations through the following steps:\n\nEnvironment Setup: Established a reproducible environment with necessary libraries and data access\nArgument Extraction: Processed structured ArgDown representations preserving the hierarchical relationships\nProbability Integration: Enhanced arguments with probability information to create BayesDown\nData Transformation: Converted BayesDown into structured DataFrame representation\nVisualization & Analysis: Created interactive Bayesian network visualizations with probability encoding\n\nThe rain-sprinkler-lawn example, though simple, demonstrates all the key components of the extraction pipeline that can be applied to more complex AI safety arguments.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#limitations-and-future-work",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#limitations-and-future-work",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "I.2 Limitations and Future Work",
    "text": "I.2 Limitations and Future Work\nWhile this prototype successfully demonstrates the core pipeline, several limitations and opportunities for future work remain:\n\nLLM Extraction: The current implementation focuses on processing pre-formatted ArgDown rather than performing extraction directly from unstructured text. Future work will integrate LLM-powered extraction.\nScalability: The system has been tested on small examples; scaling to larger, more complex arguments will require additional optimization and handling of computational complexity.\nPolicy Evaluation: The current implementation focuses on representation and visualization; future work will add policy evaluation capabilities by implementing intervention modeling.\nPrediction Market Integration: Future versions will integrate with forecasting platforms to incorporate live data into the models.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#connection-to-amtair-project",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#connection-to-amtair-project",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "I.3 Connection to AMTAIR Project",
    "text": "I.3 Connection to AMTAIR Project\nThis prototype represents just one component of the broader AMTAIR project described in the project documentation (see PY_AMTAIRDescription and PY_AMTAIR_SoftwareToolsNMilestones). The full project includes:\n\nAI Risk Pathway Analyzer (ARPA): The core extraction and visualization system demonstrated in this notebook\nWorldview Comparator: Tools for comparing different perspectives on AI risk\nPolicy Impact Evaluator: Systems for evaluating intervention effects across scenarios\nStrategic Intervention Generator: Tools for identifying robust governance strategies\n\nTogether, these components aim to address the coordination crisis in AI governance by providing computational tools that make implicit models explicit, identify cruxes of disagreement, and evaluate policy impacts across diverse worldviews.\nBy transforming unstructured text into formal, analyzable representations, the AMTAIR project helps bridge the gaps between technical researchers, policy specialists, and other stakeholders, enabling more effective coordination in addressing existential risks from advanced AI.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#saving-and-exporting-results",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#saving-and-exporting-results",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "J.1 6.0 Saving and Exporting Results",
    "text": "J.1 6.0 Saving and Exporting Results\nThis section provides tools for saving the notebook results and visualizations in various formats:\n\nHTML Export: Creates a self-contained HTML version of the notebook with all visualizations\nMarkdown Export: Generates documentation-friendly Markdown version of the notebook\nPDF Export: Creates a PDF document for formal sharing (requires LaTeX installation)\n\nThese exports are essential for: - Sharing analysis results with colleagues and stakeholders - Including visualizations in presentations and reports - Creating documentation for the AMTAIR project - Preserving results for future reference\nThe different formats serve different purposes, from interactive exploration (HTML) to documentation (Markdown) to formal presentation (PDF).\nInstruction:\nDownload the ipynb, which you want to convert, on your local computer. Run the code below to upload the ipynb.\nThe html version will be downloaded automatically on your local machine. Enjoy it!\n\n\nCode\n# @title 6.0.0 --- Save Visualization and Notebook Outputs as .HTML--- [save_visualization_and_notebook_outputs_as_html]\n\n\"\"\"\nBLOCK PURPOSE: Provides tools for saving the notebook results in various formats.\n\nThis block offers functions to:\n1. Convert the notebook to HTML for easy sharing and viewing\n2. Convert the notebook to Markdown for documentation purposes\n3. Save the visualization outputs for external use\n\nThese tools are essential for preserving the analysis results and making them\naccessible outside the notebook environment, supporting knowledge transfer\nand integration with other AMTAIR project components.\n\nDEPENDENCIES: nbformat, nbconvert modules\nINPUTS: Current notebook state\nOUTPUTS: HTML, Markdown, or other format versions of the notebook\n\"\"\"\n\nimport nbformat\nfrom nbconvert import HTMLExporter\nimport os\n\n# Repository URL variable for file access\nrepo_url = \"https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/\"\nnotebook_name = \"AMTAIR_Prototype_example_carlsmith\"  # Change when working with different examples\n\n# Download the notebook file\n!wget {repo_url}{notebook_name}.ipynb -O {notebook_name}.ipynb\n\n# Load the notebook\ntry:\n  with open(f\"{notebook_name}.ipynb\") as f:\n    nb = nbformat.read(f, as_version=4)\n  print(f\"✅ Successfully loaded notebook: {notebook_name}.ipynb\")\nexcept FileNotFoundError:\n  print(f\"❌ Error: File '{notebook_name}.ipynb' not found. Please check if it was downloaded correctly.\")\n\n# Initialize the HTML exporter\nexporter = HTMLExporter()\n\n# Convert the notebook to HTML\ntry:\n    (body, resources) = exporter.from_notebook_node(nb)\n\n    # Save the HTML to a file\n    with open(f\"{notebook_name}IPYNB.html\", \"w\") as f:\n        f.write(body)\n    print(f\"✅ Successfully saved HTML version to: {notebook_name}IPYNB.html\")\nexcept Exception as e:\n    print(f\"❌ Error converting notebook to HTML: {str(e)}\")\n\n\n--2025-05-24 20:09:38--  https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1689816 (1.6M) [text/plain]\nSaving to: ‘AMTAIR_Prototype_example_carlsmith.ipynb’\n\nAMTAIR_Prototype_ex 100%[===================&gt;]   1.61M  6.36MB/s    in 0.3s    \n\n2025-05-24 20:09:38 (6.36 MB/s) - ‘AMTAIR_Prototype_example_carlsmith.ipynb’ saved [1689816/1689816]\n\n✅ Successfully loaded notebook: AMTAIR_Prototype_example_carlsmith.ipynb\n✅ Successfully saved HTML version to: AMTAIR_Prototype_example_carlsmithIPYNB.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#convert-.ipynb-notebook-to-markdown",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#convert-.ipynb-notebook-to-markdown",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "J.2 6.1 Convert .ipynb Notebook to MarkDown",
    "text": "J.2 6.1 Convert .ipynb Notebook to MarkDown\n\n\nCode\n# @title 6.1.0 --- Convert .ipynb Notebook to MarkDown --- [convert_notebook_to_markdown]\n\nimport nbformat\nfrom nbconvert import MarkdownExporter\nimport os\n\n# repo_url = \"https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_1/\"\nnotebook_name = \"AMTAIR_Prototype_example_carlsmith\"  #Change Notebook name and path when working on different examples\n\n# Download the notebook file\n!wget {repo_url}{notebook_name}.ipynb -O {notebook_name}.ipynb  # Corrected line\n\n# Load the notebook\n# add error handling for file not found\ntry:\n  with open(f\"{notebook_name}.ipynb\") as f:\n    nb = nbformat.read(f, as_version=4)\nexcept FileNotFoundError:\n  print(f\"Error: File '{notebook_name}.ipynb' not found. Please check if it was downloaded correctly.\")\n\n\n# Initialize the Markdown exporter\nexporter = MarkdownExporter(exclude_output=True)  # Correct initialization\n\n# Convert the notebook to Markdown\n(body, resources) = exporter.from_notebook_node(nb)\n\n# Save the Markdown to a file\nwith open(f\"{notebook_name}IPYNB.md\", \"w\") as f:\n    f.write(body)\n\n\n--2025-05-24 20:09:47--  https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1689816 (1.6M) [text/plain]\nSaving to: ‘AMTAIR_Prototype_example_carlsmith.ipynb’\n\nAMTAIR_Prototype_ex 100%[===================&gt;]   1.61M  5.38MB/s    in 0.3s    \n\n2025-05-24 20:09:48 (5.38 MB/s) - ‘AMTAIR_Prototype_example_carlsmith.ipynb’ saved [1689816/1689816]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#convert-notebook-to-markdown-documentation",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#convert-notebook-to-markdown-documentation",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "J.3 6.2 Convert Notebook to Markdown Documentation",
    "text": "J.3 6.2 Convert Notebook to Markdown Documentation\n\n\nCode\n# @title 6.2.0 --- Convert Notebook to Markdown Documentation --- [convert_notebook_to_markdown_documentation]\n\n\"\"\"\nBLOCK PURPOSE: Converts the notebook to Markdown format for documentation purposes.\n\nMarkdown is a lightweight markup language that is widely used for documentation\nand is easily readable in both plain text and rendered formats. This conversion:\n\n1. Preserves the structure and content of the notebook\n2. Creates a format suitable for inclusion in documentation systems\n3. Excludes code outputs to focus on the process and methodology\n4. Supports version control and collaboration on GitHub\n\nThe resulting Markdown file can be used in project documentation, GitHub wikis,\nor as a standalone reference guide to the AMTAIR extraction pipeline.\n\nDEPENDENCIES: nbformat, nbconvert.MarkdownExporter modules\nINPUTS: Current notebook state\nOUTPUTS: Markdown version of the notebook\n\"\"\"\n\nimport nbformat\nfrom nbconvert import MarkdownExporter\nimport os\n\n# Repository URL variable for file access\n# repo_url = \"https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/\"\nnotebook_name = \"AMTAIR_Prototype_example_carlsmith\"  # Change when working with different examples\n\n# Download the notebook file\n!wget {repo_url}{notebook_name}.ipynb -O {notebook_name}.ipynb\n\n# Load the notebook\ntry:\n  with open(f\"{notebook_name}.ipynb\") as f:\n    nb = nbformat.read(f, as_version=4)\n  print(f\"✅ Successfully loaded notebook: {notebook_name}.ipynb\")\nexcept FileNotFoundError:\n  print(f\"❌ Error: File '{notebook_name}.ipynb' not found. Please check \"\n    + \"if it was downloaded correctly.\")\n\n\n# Initialize the Markdown exporter\nexporter = MarkdownExporter(exclude_output=True)  # Exclude outputs for cleaner documentation\n\n# Convert the notebook to Markdown\ntry:\n    (body, resources) = exporter.from_notebook_node(nb)\n\n    # Save the Markdown to a file\n    with open(f\"{notebook_name}IPYNB.md\", \"w\") as f:\n        f.write(body)\n    print(f\"✅ Successfully saved Markdown version to: {notebook_name}IPYNB.md\")\nexcept Exception as e:\n    print(f\"❌ Error converting notebook to Markdown: {str(e)}\")\n\n\n--2025-05-24 20:09:53--  https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1689816 (1.6M) [text/plain]\nSaving to: ‘AMTAIR_Prototype_example_carlsmith.ipynb’\n\nAMTAIR_Prototype_ex 100%[===================&gt;]   1.61M  5.78MB/s    in 0.3s    \n\n2025-05-24 20:09:53 (5.78 MB/s) - ‘AMTAIR_Prototype_example_carlsmith.ipynb’ saved [1689816/1689816]\n\n✅ Successfully loaded notebook: AMTAIR_Prototype_example_carlsmith.ipynb\n✅ Successfully saved Markdown version to: AMTAIR_Prototype_example_carlsmithIPYNB.md",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  },
  {
    "objectID": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#create-pdf-and-latex",
    "href": "AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html#create-pdf-and-latex",
    "title": "Appendix B — AMTAIR Prototype Demonstration (Public Colab Notebook)",
    "section": "J.4 6.3 Create PDF and Latex",
    "text": "J.4 6.3 Create PDF and Latex\n\n\nCode\n# @title 6.3.0 --- PDF and Latex--- [pdf_and_latex]\n\nimport nbformat\nfrom nbconvert import PDFExporter\nimport os\nimport subprocess\nimport re\n\ndef escape_latex_special_chars(text):\n  \"\"\"Escapes special LaTeX characters in a string.\"\"\"\n  latex_special_chars = ['&', '%', '#', '_', '{', '}', '~', '^', '\\\\']\n  replacement_patterns = [\n      (char, '\\\\' + char) for char in latex_special_chars\n  ]\n\n  # Escape reserved characters\n  for original, replacement in replacement_patterns:\n    text = text.replace(original, replacement) # This is the fix\n  return text\n\n# Function to check if a command is available\ndef is_command_available(command):\n    try:\n        subprocess.run([command], capture_output=True, check=True)\n        return True\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        return False\n\n# Check if xelatex is installed, and install if necessary\nif not is_command_available(\"xelatex\"):\n    print(\"Installing necessary TeX packages...\")\n    !apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic\n    print(\"TeX packages installed successfully.\")\nelse:\n    print(\"xelatex is already installed. Skipping installation.\")\n\n# repo_url = \"https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_1/\"\nnotebook_name = \"AMTAIR_Prototype_example_carlsmith\"  # Change Notebook name\n                                  # and path when working on different examples\n\n# Download the notebook file\n!wget {repo_url}{notebook_name}.ipynb -O {notebook_name}.ipynb  # Corrected line\n\n# Load the notebook\n# add error handling for file not found\ntry:\n  with open(f\"{notebook_name}.ipynb\") as f:\n    nb = nbformat.read(f, as_version=4)\nexcept FileNotFoundError:\n  print(f\"Error: File '{notebook_name}.ipynb' not found. Please check if it was downloaded correctly.\")\n\n\n# Initialize the PDF exporter\nexporter = PDFExporter(exclude_output=True)  # Changed to PDFExporter\n\n# Sanitize notebook cell titles to escape special LaTeX characters like '&'\nfor cell in nb.cells:\n    if 'cell_type' in cell and cell['cell_type'] == 'markdown':\n        if 'source' in cell and isinstance(cell['source'], str):\n            # Replace '&' with '\\protect&' in markdown cell titles AND CONTENT\n            # Updated to use escape_latex_special_chars function\n            cell['source'] = escape_latex_special_chars(cell['source'])\n            # Additionally, escape special characters in headings\n            cell['source'] = re.sub(r'(#+)\\s*(.*)', lambda m: m.group(1) + ' ' + escape_latex_special_chars(m.group(2)), cell['source'])\n\n\n\n# Convert the notebook to PDF\n(body, resources) = exporter.from_notebook_node(nb)\n\n\n# Save the PDF to a file\nwith open(f\"{notebook_name}IPYNB.pdf\", \"wb\") as f:  # Changed to 'wb' for binary writing\n    f.write(body)\n\n\nInstalling necessary TeX packages...\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono\n  fonts-texgyre fonts-urw-base35 libapache-pom-java libcommons-logging-java\n  libcommons-parent-java libfontbox-java libgs9 libgs9-common libidn12\n  libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0\n  libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13\n  lmodern poppler-data preview-latex-style rake ruby ruby-net-telnet\n  ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils\n  teckit tex-common tex-gyre texlive-base texlive-binaries texlive-latex-base\n  texlive-latex-extra texlive-latex-recommended texlive-pictures tipa\n  xfonts-encodings xfonts-utils\nSuggested packages:\n  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java\n  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java\n  poppler-utils ghostscript fonts-japanese-mincho | fonts-ipafont-mincho\n  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n  fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv\n  | postscript-viewer perl-tk xpdf | pdf-viewer xzdec\n  texlive-fonts-recommended-doc texlive-latex-base-doc python3-pygments\n  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl\n  texlive-latex-extra-doc texlive-latex-recommended-doc texlive-luatex\n  texlive-pstricks dot2tex prerex texlive-pictures-doc vprerex\n  default-jre-headless tipa-doc\nThe following NEW packages will be installed:\n  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono\n  fonts-texgyre fonts-urw-base35 libapache-pom-java libcommons-logging-java\n  libcommons-parent-java libfontbox-java libgs9 libgs9-common libidn12\n  libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0\n  libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13\n  lmodern poppler-data preview-latex-style rake ruby ruby-net-telnet\n  ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils\n  teckit tex-common tex-gyre texlive-base texlive-binaries\n  texlive-fonts-recommended texlive-latex-base texlive-latex-extra\n  texlive-latex-recommended texlive-pictures texlive-plain-generic\n  texlive-xetex tipa xfonts-encodings xfonts-utils\n0 upgraded, 53 newly installed, 0 to remove and 34 not upgraded.\nNeed to get 182 MB of archives.\nAfter this operation, 571 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.11 [753 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.11 [5,031 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\nGet:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\nGet:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\nGet:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.2 [39.1 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\nGet:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.10 [50.1 kB]\nGet:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\nGet:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\nGet:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\nGet:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\nGet:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-webrick all 1.7.0-3ubuntu0.1 [52.1 kB]\nGet:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\nGet:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.10 [5,114 kB]\nGet:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]\nGet:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\nGet:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.2 [120 kB]\nGet:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.2 [267 kB]\nGet:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\nGet:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\nGet:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\nGet:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\nGet:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\nGet:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\nGet:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]\nGet:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\nGet:42 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.2 [9,860 kB]\nGet:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\nGet:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\nGet:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\nGet:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\nGet:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\nGet:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\nGet:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\nGet:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\nGet:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\nGet:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\nGet:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]\nFetched 182 MB in 12s (15.2 MB/s)\nExtracting templates from packages: 100%\nPreconfiguring packages ...\nSelecting previously unselected package fonts-droid-fallback.\n(Reading database ... 126327 files and directories currently installed.)\nPreparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\nUnpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\nSelecting previously unselected package fonts-lato.\nPreparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\nUnpacking fonts-lato (2.0-2.1) ...\nSelecting previously unselected package poppler-data.\nPreparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\nUnpacking poppler-data (0.4.11-1) ...\nSelecting previously unselected package tex-common.\nPreparing to unpack .../03-tex-common_6.17_all.deb ...\nUnpacking tex-common (6.17) ...\nSelecting previously unselected package fonts-urw-base35.\nPreparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\nUnpacking fonts-urw-base35 (20200910-1) ...\nSelecting previously unselected package libgs9-common.\nPreparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.11_all.deb ...\nUnpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.11) ...\nSelecting previously unselected package libidn12:amd64.\nPreparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\nUnpacking libidn12:amd64 (1.38-4ubuntu1) ...\nSelecting previously unselected package libijs-0.35:amd64.\nPreparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\nUnpacking libijs-0.35:amd64 (0.35-15build2) ...\nSelecting previously unselected package libjbig2dec0:amd64.\nPreparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\nUnpacking libjbig2dec0:amd64 (0.19-3build2) ...\nSelecting previously unselected package libgs9:amd64.\nPreparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.11_amd64.deb ...\nUnpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.11) ...\nSelecting previously unselected package libkpathsea6:amd64.\nPreparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\nUnpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSelecting previously unselected package libwoff1:amd64.\nPreparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\nUnpacking libwoff1:amd64 (1.0.2-1build4) ...\nSelecting previously unselected package dvisvgm.\nPreparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\nUnpacking dvisvgm (2.13.1-1) ...\nSelecting previously unselected package fonts-lmodern.\nPreparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\nUnpacking fonts-lmodern (2.004.5-6.1) ...\nSelecting previously unselected package fonts-noto-mono.\nPreparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...\nUnpacking fonts-noto-mono (20201225-1build1) ...\nSelecting previously unselected package fonts-texgyre.\nPreparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...\nUnpacking fonts-texgyre (20180621-3.1) ...\nSelecting previously unselected package libapache-pom-java.\nPreparing to unpack .../16-libapache-pom-java_18-1_all.deb ...\nUnpacking libapache-pom-java (18-1) ...\nSelecting previously unselected package libcommons-parent-java.\nPreparing to unpack .../17-libcommons-parent-java_43-1_all.deb ...\nUnpacking libcommons-parent-java (43-1) ...\nSelecting previously unselected package libcommons-logging-java.\nPreparing to unpack .../18-libcommons-logging-java_1.2-2_all.deb ...\nUnpacking libcommons-logging-java (1.2-2) ...\nSelecting previously unselected package libptexenc1:amd64.\nPreparing to unpack .../19-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\nUnpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSelecting previously unselected package rubygems-integration.\nPreparing to unpack .../20-rubygems-integration_1.18_all.deb ...\nUnpacking rubygems-integration (1.18) ...\nSelecting previously unselected package ruby3.0.\nPreparing to unpack .../21-ruby3.0_3.0.2-7ubuntu2.10_amd64.deb ...\nUnpacking ruby3.0 (3.0.2-7ubuntu2.10) ...\nSelecting previously unselected package ruby-rubygems.\nPreparing to unpack .../22-ruby-rubygems_3.3.5-2_all.deb ...\nUnpacking ruby-rubygems (3.3.5-2) ...\nSelecting previously unselected package ruby.\nPreparing to unpack .../23-ruby_1%3a3.0~exp1_amd64.deb ...\nUnpacking ruby (1:3.0~exp1) ...\nSelecting previously unselected package rake.\nPreparing to unpack .../24-rake_13.0.6-2_all.deb ...\nUnpacking rake (13.0.6-2) ...\nSelecting previously unselected package ruby-net-telnet.\nPreparing to unpack .../25-ruby-net-telnet_0.1.1-2_all.deb ...\nUnpacking ruby-net-telnet (0.1.1-2) ...\nSelecting previously unselected package ruby-webrick.\nPreparing to unpack .../26-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...\nUnpacking ruby-webrick (1.7.0-3ubuntu0.1) ...\nSelecting previously unselected package ruby-xmlrpc.\nPreparing to unpack .../27-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\nUnpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\nSelecting previously unselected package libruby3.0:amd64.\nPreparing to unpack .../28-libruby3.0_3.0.2-7ubuntu2.10_amd64.deb ...\nUnpacking libruby3.0:amd64 (3.0.2-7ubuntu2.10) ...\nSelecting previously unselected package libsynctex2:amd64.\nPreparing to unpack .../29-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\nUnpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSelecting previously unselected package libteckit0:amd64.\nPreparing to unpack .../30-libteckit0_2.5.11+ds1-1_amd64.deb ...\nUnpacking libteckit0:amd64 (2.5.11+ds1-1) ...\nSelecting previously unselected package libtexlua53:amd64.\nPreparing to unpack .../31-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\nUnpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSelecting previously unselected package libtexluajit2:amd64.\nPreparing to unpack .../32-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\nUnpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSelecting previously unselected package libzzip-0-13:amd64.\nPreparing to unpack .../33-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\nUnpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\nSelecting previously unselected package xfonts-encodings.\nPreparing to unpack .../34-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\nUnpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\nSelecting previously unselected package xfonts-utils.\nPreparing to unpack .../35-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\nUnpacking xfonts-utils (1:7.7+6build2) ...\nSelecting previously unselected package lmodern.\nPreparing to unpack .../36-lmodern_2.004.5-6.1_all.deb ...\nUnpacking lmodern (2.004.5-6.1) ...\nSelecting previously unselected package preview-latex-style.\nPreparing to unpack .../37-preview-latex-style_12.2-1ubuntu1_all.deb ...\nUnpacking preview-latex-style (12.2-1ubuntu1) ...\nSelecting previously unselected package t1utils.\nPreparing to unpack .../38-t1utils_1.41-4build2_amd64.deb ...\nUnpacking t1utils (1.41-4build2) ...\nSelecting previously unselected package teckit.\nPreparing to unpack .../39-teckit_2.5.11+ds1-1_amd64.deb ...\nUnpacking teckit (2.5.11+ds1-1) ...\nSelecting previously unselected package tex-gyre.\nPreparing to unpack .../40-tex-gyre_20180621-3.1_all.deb ...\nUnpacking tex-gyre (20180621-3.1) ...\nSelecting previously unselected package texlive-binaries.\nPreparing to unpack .../41-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\nUnpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\nSelecting previously unselected package texlive-base.\nPreparing to unpack .../42-texlive-base_2021.20220204-1_all.deb ...\nUnpacking texlive-base (2021.20220204-1) ...\nSelecting previously unselected package texlive-fonts-recommended.\nPreparing to unpack .../43-texlive-fonts-recommended_2021.20220204-1_all.deb ...\nUnpacking texlive-fonts-recommended (2021.20220204-1) ...\nSelecting previously unselected package texlive-latex-base.\nPreparing to unpack .../44-texlive-latex-base_2021.20220204-1_all.deb ...\nUnpacking texlive-latex-base (2021.20220204-1) ...\nSelecting previously unselected package libfontbox-java.\nPreparing to unpack .../45-libfontbox-java_1%3a1.8.16-2_all.deb ...\nUnpacking libfontbox-java (1:1.8.16-2) ...\nSelecting previously unselected package libpdfbox-java.\nPreparing to unpack .../46-libpdfbox-java_1%3a1.8.16-2_all.deb ...\nUnpacking libpdfbox-java (1:1.8.16-2) ...\nSelecting previously unselected package texlive-latex-recommended.\nPreparing to unpack .../47-texlive-latex-recommended_2021.20220204-1_all.deb ...\nUnpacking texlive-latex-recommended (2021.20220204-1) ...\nSelecting previously unselected package texlive-pictures.\nPreparing to unpack .../48-texlive-pictures_2021.20220204-1_all.deb ...\nUnpacking texlive-pictures (2021.20220204-1) ...\nSelecting previously unselected package texlive-latex-extra.\nPreparing to unpack .../49-texlive-latex-extra_2021.20220204-1_all.deb ...\nUnpacking texlive-latex-extra (2021.20220204-1) ...\nSelecting previously unselected package texlive-plain-generic.\nPreparing to unpack .../50-texlive-plain-generic_2021.20220204-1_all.deb ...\nUnpacking texlive-plain-generic (2021.20220204-1) ...\nSelecting previously unselected package tipa.\nPreparing to unpack .../51-tipa_2%3a1.3-21_all.deb ...\nUnpacking tipa (2:1.3-21) ...\nSelecting previously unselected package texlive-xetex.\nPreparing to unpack .../52-texlive-xetex_2021.20220204-1_all.deb ...\nUnpacking texlive-xetex (2021.20220204-1) ...\nSetting up fonts-lato (2.0-2.1) ...\nSetting up fonts-noto-mono (20201225-1build1) ...\nSetting up libwoff1:amd64 (1.0.2-1build4) ...\nSetting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSetting up libijs-0.35:amd64 (0.35-15build2) ...\nSetting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSetting up libfontbox-java (1:1.8.16-2) ...\nSetting up rubygems-integration (1.18) ...\nSetting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\nSetting up fonts-urw-base35 (20200910-1) ...\nSetting up poppler-data (0.4.11-1) ...\nSetting up tex-common (6.17) ...\nupdate-language: texlive-base not installed and configured, doing nothing!\nSetting up libjbig2dec0:amd64 (0.19-3build2) ...\nSetting up libteckit0:amd64 (2.5.11+ds1-1) ...\nSetting up libapache-pom-java (18-1) ...\nSetting up ruby-net-telnet (0.1.1-2) ...\nSetting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\nSetting up t1utils (1.41-4build2) ...\nSetting up libidn12:amd64 (1.38-4ubuntu1) ...\nSetting up fonts-texgyre (20180621-3.1) ...\nSetting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSetting up ruby-webrick (1.7.0-3ubuntu0.1) ...\nSetting up fonts-lmodern (2.004.5-6.1) ...\nSetting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\nSetting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\nSetting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSetting up libgs9-common (9.55.0~dfsg1-0ubuntu5.11) ...\nSetting up teckit (2.5.11+ds1-1) ...\nSetting up libpdfbox-java (1:1.8.16-2) ...\nSetting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.11) ...\nSetting up preview-latex-style (12.2-1ubuntu1) ...\nSetting up libcommons-parent-java (43-1) ...\nSetting up dvisvgm (2.13.1-1) ...\nSetting up libcommons-logging-java (1.2-2) ...\nSetting up xfonts-utils (1:7.7+6build2) ...\nSetting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\nSetting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\nupdate-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\nupdate-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\nSetting up lmodern (2.004.5-6.1) ...\nSetting up texlive-base (2021.20220204-1) ...\n/usr/bin/ucfr\n/usr/bin/ucfr\n/usr/bin/ucfr\n/usr/bin/ucfr\nmktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \nmktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \nmktexlsr: Updating /var/lib/texmf/ls-R... \nmktexlsr: Done.\ntl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\ntl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\ntl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\ntl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\nSetting up tex-gyre (20180621-3.1) ...\nSetting up texlive-plain-generic (2021.20220204-1) ...\nSetting up texlive-latex-base (2021.20220204-1) ...\nSetting up texlive-latex-recommended (2021.20220204-1) ...\nSetting up texlive-pictures (2021.20220204-1) ...\nSetting up texlive-fonts-recommended (2021.20220204-1) ...\nSetting up tipa (2:1.3-21) ...\nSetting up texlive-latex-extra (2021.20220204-1) ...\nSetting up texlive-xetex (2021.20220204-1) ...\nSetting up rake (13.0.6-2) ...\nSetting up libruby3.0:amd64 (3.0.2-7ubuntu2.10) ...\nSetting up ruby3.0 (3.0.2-7ubuntu2.10) ...\nSetting up ruby (1:3.0~exp1) ...\nSetting up ruby-rubygems (3.3.5-2) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for mailcap (3.70+nmu1ubuntu1) ...\nProcessing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\nProcessing triggers for tex-common (6.17) ...\nRunning updmap-sys. This may take some time... done.\nRunning mktexlsr /var/lib/texmf ... done.\nBuilding format(s) --all.\n    This may take some time... done.\nTeX packages installed successfully.\n--2025-05-24 20:12:59--  https://raw.githubusercontent.com/SingularitySmith/AMTAIR_Prototype/main/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1689816 (1.6M) [text/plain]\nSaving to: ‘AMTAIR_Prototype_example_carlsmith.ipynb’\n\nAMTAIR_Prototype_ex 100%[===================&gt;]   1.61M  5.31MB/s    in 0.3s    \n\n2025-05-24 20:12:59 (5.31 MB/s) - ‘AMTAIR_Prototype_example_carlsmith.ipynb’ saved [1689816/1689816]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>[AMTAIR Prototype Demonstration (Public Colab Notebook)](https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr)</span>"
    ]
  }
]