[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#illustrations-and-terminology-quick-references",
    "href": "index.html#illustrations-and-terminology-quick-references",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Illustrations and Terminology — Quick References",
    "text": "Illustrations and Terminology — Quick References\n\nAcknowledgments\n\nAcademic supervisor (Prof. Timo Speith) and institution (University of Bayreuth)\n\nResearch collaborators, especially those connected to the original MTAIR project\n\nTechnical advisors who provided feedback on implementation aspects\n\nPersonal supporters who enabled the research through encouragement and feedback",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#list-of-graphics-figures",
    "href": "index.html#list-of-graphics-figures",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "List of Graphics & Figures",
    "text": "List of Graphics & Figures",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#list-of-abbreviations",
    "href": "index.html#list-of-abbreviations",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "List of Abbreviations",
    "text": "List of Abbreviations\n\n\n\nAGI - Artificial General Intelligence\nAMTAIR - Automating Modeling of Transformative AI Risks\nAPI - Application Programming Interface\nAPS - Advanced, Planning, Strategic (AI systems per Carlsmith (2021))\nBN - Bayesian Network\nCPT - Conditional Probability Table\nDAG - Directed Acyclic Graph\nLLM - Large Language Model\nMTAIR - Modeling Transformative AI Risks\nTAI - Transformative Artificial Intelligence",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#glossary",
    "href": "index.html#glossary",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Glossary",
    "text": "Glossary\n\n\n\nArgument mapping: A method for visually representing the structure of arguments\n\nBayesDown: An extension of ArgDown that incorporates probabilistic information\n\nBayesian network: A probabilistic graphical model representing variables and their dependencies\n\nConditional probability: The probability of an event given that another event has occurred\n\nDirected Acyclic Graph (DAG): A graph with directed edges and no cycles\n\nExistential risk: Risk of permanent curtailment of humanity’s potential\n\nPower-seeking AI: AI systems with instrumental incentives to acquire resources and power\n\nPrediction market: A market where participants trade contracts that resolve based on future events\n\nd-separation: A criterion for identifying conditional independence relationships in Bayesian networks\n\nMonte Carlo sampling: A computational technique using random sampling to obtain numerical results\n\n\n\n\n\nCarlsmith, Joseph. 2021. “Is Power-Seeking AI an Existential Risk?” 2021. https://doi.org/10.48550/arXiv.2206.13353.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html",
    "href": "chapters/Outlines/final_draft.html",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "1.1 Frontmatter: Preface\nThis thesis represents the culmination of interdisciplinary research at the intersection of AI safety, formal epistemology, and computational social science. The work emerged from recognizing a fundamental challenge in AI governance: while investment in AI safety research has grown exponentially, coordination between different stakeholder communities remains fragmented, potentially increasing existential risk through misaligned efforts.\nThe journey from initial concept to working implementation involved iterative refinement based on feedback from advisors, domain experts, and potential users. What began as a technical exercise in automated extraction evolved into a broader framework for enhancing epistemic security in one of humanity’s most critical coordination challenges. The AMTAIR project—Automating Transformative AI Risk Modeling—represents an attempt to build computational bridges between communities that, despite shared concerns about AI risk, often struggle to communicate effectively due to incompatible frameworks, terminologies, and implicit assumptions.\nI hope this work contributes to building the intellectual and technical infrastructure necessary for humanity to navigate the transition to transformative AI safely. The tools and frameworks presented here are offered in the spirit of collaborative problem-solving, recognizing that the challenges we face require unprecedented cooperation across disciplines, institutions, and worldviews.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#acknowledgments",
    "href": "chapters/Outlines/final_draft.html#acknowledgments",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "1.2 Acknowledgments",
    "text": "1.2 Acknowledgments\n\nI thank my supervisor Dr. Timo Speith for his guidance throughout this project, providing both technical insights and philosophical grounding. The MTAIR team’s pioneering manual approach inspired this automation effort, and I am grateful for their foundational work.\n\nI acknowledge Johannes Meyer and Jelena Meyer for their invaluable assistance in verifying the automated extraction procedure through manual extraction of ArgDown and BayesDown data from the Carlsmith paper, providing crucial ground truth for validation.\nSpecial recognition goes to Coleman Snell for his partnership and research collaboration with the AMTAIR project, offering both technical expertise and strategic vision. The AI safety community’s creation of rich literature made this work possible, and I thank all researchers whose arguments provided the raw material for formalization.\nAny errors or limitations remain my own responsibility.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#list-of-figures",
    "href": "chapters/Outlines/final_draft.html#list-of-figures",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "1.3 List of Figures",
    "text": "1.3 List of Figures",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#list-of-tables",
    "href": "chapters/Outlines/final_draft.html#list-of-tables",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "1.4 List of Tables",
    "text": "1.4 List of Tables",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#list-of-abbreviations",
    "href": "chapters/Outlines/final_draft.html#list-of-abbreviations",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "1.5 List of Abbreviations",
    "text": "1.5 List of Abbreviations\n\nAI - Artificial Intelligence\nAGI - Artificial General Intelligence\nAMTAIR - Automating Transformative AI Risk Modeling\nAPI - Application Programming Interface\nAPS - Advanced, Planning, Strategic (AI systems)\nBN - Bayesian Network\nCPT - Conditional Probability Table\nDAG - Directed Acyclic Graph\nLLM - Large Language Model\nML - Machine Learning\nMTAIR - Modeling Transformative AI Risks\nNLP - Natural Language Processing\nP&E - Philosophy & Economics\nPDF - Portable Document Format\nTAI - Transformative Artificial Intelligence",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#opening-scenario-the-policymakers-dilemma",
    "href": "chapters/Outlines/final_draft.html#opening-scenario-the-policymakers-dilemma",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.1 1.1 Opening Scenario: The Policymaker’s Dilemma",
    "text": "2.1 1.1 Opening Scenario: The Policymaker’s Dilemma\n\n\nImagine a senior policy advisor preparing recommendations for AI governance legislation. On her desk lie a dozen reports from leading AI safety researchers, each painting a different picture of the risks ahead. One argues that misaligned AI could pose existential risks within the decade, citing complex technical arguments about instrumental convergence and orthogonality. Another suggests these concerns are overblown, emphasizing uncertainty and the strength of existing institutions. A third proposes specific technical standards but acknowledges deep uncertainty about their effectiveness.\nEach report seems compelling in isolation, written by credentialed experts with sophisticated arguments. Yet they reach dramatically different conclusions about both the magnitude of risk and appropriate interventions. The technical arguments involve unfamiliar concepts—mesa-optimization, corrigibility, capability amplification—expressed through different frameworks and implicit assumptions. Time is limited, stakes are high, and the legislation could shape humanity’s trajectory for decades.\n\nThis scenario1 plays out daily across government offices, corporate boardrooms, and research institutions worldwide. It exemplifies what I term the “coordination crisis” in AI governance: despite unprecedented attention and resources directed toward AI safety, we lack the epistemic infrastructure to synthesize diverse expert knowledge into actionable governance strategies Todd (2024).\n\nShow Image",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#the-coordination-crisis-in-ai-governance",
    "href": "chapters/Outlines/final_draft.html#the-coordination-crisis-in-ai-governance",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.2 1.2 The Coordination Crisis in AI Governance",
    "text": "2.2 1.2 The Coordination Crisis in AI Governance\n\n\n\nAs AI capabilities advance at an accelerating pace—demonstrated by the rapid progression from GPT-3 to GPT-4, Claude, and emerging multimodal systems Maslej (2025) Samborska (2025)—humanity faces a governance challenge unlike any in history. The task of ensuring increasingly powerful AI systems remain aligned with human values and beneficial to our long-term flourishing grows more urgent with each capability breakthrough. This challenge becomes particularly acute when considering transformative AI systems that could drastically alter civilization’s trajectory, potentially including existential risks from misaligned systems pursuing objectives counter to human welfare.\nDespite unprecedented investment in AI safety research, rapidly growing awareness among key stakeholders, and proliferating frameworks for responsible AI development, we face what I’ll term the “coordination crisis” in AI governance—a systemic failure to align diverse efforts across technical, policy, and strategic domains into a coherent response proportionate to the risks we face.\nThe current state of AI governance presents a striking paradox. On one hand, we witness extraordinary mobilization: billions in research funding, proliferating safety initiatives, major tech companies establishing alignment teams, and governments worldwide developing AI strategies. The Asilomar AI Principles garnered thousands of signatures Tegmark (2024), the EU advances comprehensive AI regulation European (2024), and technical researchers produce increasingly sophisticated work on alignment, interpretability, and robustness.\nYet alongside this activity, we observe systematic coordination failures that may prove catastrophic. Technical safety researchers develop sophisticated alignment techniques without clear implementation pathways. Policy specialists craft regulatory frameworks lacking technical grounding to ensure practical efficacy. Ethicists articulate normative principles that lack operational specificity. Strategy researchers identify critical uncertainties but struggle to translate these into actionable guidance. International bodies convene without shared frameworks for assessing interventions.\n\nShow Image\n\n2.2.1 1.2.1 Safety Gaps from Misaligned Efforts\n\nThe fragmentation problem manifests in incompatible frameworks between technical researchers, policy specialists, and strategic analysts. Each community develops sophisticated approaches within their domain, yet translation between domains remains primitive. This creates systematic blind spots where risks emerge at the interfaces between technical capabilities, institutional responses, and strategic dynamics.\nWhen different communities operate with incompatible frameworks, critical risks fall through the cracks. Technical researchers may solve alignment problems under assumptions that policymakers’ decisions invalidate. Regulations optimized for current systems may inadvertently incentivize dangerous development patterns. Without shared models of the risk landscape, our collective efforts resemble the parable of blind men describing an elephant—each accurate within their domain but missing the complete picture Paul (2023).\n\nHistorical precedents demonstrate how coordination failures in technology governance can lead to dangerous dynamics. The nuclear arms race exemplifies how lack of coordination can create negative-sum outcomes where all parties become less secure despite massive investments in safety measures. Similar dynamics may emerge in AI development without proper coordination infrastructure.\n\n\n2.2.2 1.2.2 Resource Misallocation\n\nThe AI safety community faces a complex tradeoff in resource allocation. While some duplication of efforts can improve reliability through independent verification—akin to reproducing scientific results—the current level of fragmentation often leads to wasteful redundancy. Multiple teams independently develop similar frameworks without building on each other’s work, creating opportunity costs where critical but unglamorous research areas remain understaffed. Funders struggle to identify high-impact opportunities across technical and governance domains, lacking the epistemic infrastructure to assess where marginal resources would have the greatest impact. This misallocation becomes more costly as the window for establishing effective governance narrows with accelerating AI development.\n\n\n\n\nTable 2.1: Examples of duplicated AI safety efforts across organizations\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Area\nOrganization A\nOrganization B\nDuplication Level\nOpportunity Cost\n\n\n\n\nInterpretability Methods\nAnthropic’s mechanistic interpretability\nDeepMind’s concept activation vectors\nMedium\nReduced focus on multi-agent safety\n\n\nAlignment Frameworks\nMIRI’s embedded agency\nFHI’s comprehensive AI services\nHigh\nLimited work on institutional design\n\n\nRisk Assessment Models\nGovAI’s policy models\nCSER’s existential risk frameworks\nHigh\nInsufficient capability benchmarking\n\n\n\n\n\n\n\n\n2.2.3 1.2.3 Negative-Sum Dynamics\n\nPerhaps most concerning, uncoordinated interventions can actively increase risk. Safety standards that advantage established players may accelerate risky development elsewhere. Partial transparency requirements might enable capability advances without commensurate safety improvements. International agreements lacking shared technical understanding may lock in dangerous practices. Without coordination, our cure risks becoming worse than the disease.\nThe game-theoretic structure of AI development creates particularly pernicious dynamics. Armstrong et al. Armstrong, Bostrom, and Shulman (2016) demonstrate how uncoordinated policies can incentivize a “race to the precipice” where competitive pressures override safety considerations. The situation resembles a multi-player prisoner’s dilemma or stag hunt where individually rational decisions lead to collectively catastrophic outcomes Samuel (2023) Hunt (2025).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#historical-parallels-and-temporal-urgency",
    "href": "chapters/Outlines/final_draft.html#historical-parallels-and-temporal-urgency",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.3 1.3 Historical Parallels and Temporal Urgency",
    "text": "2.3 1.3 Historical Parallels and Temporal Urgency\n\nHistory offers instructive parallels. The nuclear age began with scientists racing to understand and control forces that could destroy civilization. Early coordination failures—competing national programs, scientist-military tensions, public-expert divides—nearly led to catastrophe multiple times. Only through developing shared frameworks (deterrence theory) Schelling (1960), institutions (IAEA), and communication channels (hotlines, treaties) did humanity navigate the nuclear precipice Rehman (2025).\nYet AI presents unique coordination challenges that compress our response timeline:\nAccelerating Development: Unlike nuclear weapons requiring massive infrastructure, AI development proceeds in corporate labs and academic departments worldwide. Capability improvements come through algorithmic insights and computational scale, both advancing exponentially.\nDual-Use Ubiquity: Every AI advance potentially contributes to both beneficial applications and catastrophic risks. The same language model architectures enabling scientific breakthroughs could facilitate dangerous manipulation or deception at scale.\nComprehension Barriers: Nuclear risks were viscerally understandable—cities vaporized, radiation sickness, nuclear winter. AI risks involve abstract concepts like optimization processes, goal misspecification, and emergent capabilities that resist intuitive understanding.\nGovernance Lag: Traditional governance mechanisms—legislation, international treaties, professional standards—operate on timescales of years to decades. AI capabilities advance on timescales of months to years, creating an ever-widening capability-governance gap.\n\nShow Image",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#research-question-and-scope",
    "href": "chapters/Outlines/final_draft.html#research-question-and-scope",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.4 1.4 Research Question and Scope",
    "text": "2.4 1.4 Research Question and Scope\n\nThis thesis addresses a specific dimension of the coordination challenge by investigating the question:\nCan frontier AI technologies be utilized to automate the modeling of transformative AI risks, enabling robust prediction of policy impacts across diverse worldviews?\nMore specifically, I explore whether frontier language models can automate the extraction and formalization of probabilistic world models from AI safety literature, creating a scalable computational framework that enhances coordination in AI governance through systematic policy evaluation under uncertainty.\nTo break this down into its components:\n\nFrontier AI Technologies: Today’s most capable language models (GPT-4, Claude-3 level systems)\nAutomated Modeling: Using these systems to extract and formalize argument structures from natural language\nTransformative AI Risks: Potentially catastrophic outcomes from advanced AI systems, particularly existential risks\nPolicy Impact Prediction: Evaluating how governance interventions might alter probability distributions over outcomes\nDiverse Worldviews: Accounting for fundamental disagreements about AI development trajectories and risk factors\n\nThe investigation encompasses both theoretical development and practical implementation, focusing specifically on existential risks from misaligned AI systems rather than broader AI ethics concerns. This narrowed scope enables deep technical development while addressing the highest-stakes coordination challenges.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#the-multiplicative-benefits-framework",
    "href": "chapters/Outlines/final_draft.html#the-multiplicative-benefits-framework",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.5 1.5 The Multiplicative Benefits Framework",
    "text": "2.5 1.5 The Multiplicative Benefits Framework\n\nThe central thesis of this work is that combining three elements—automated worldview extraction, prediction market integration, and formal policy evaluation—creates multiplicative rather than merely additive benefits for AI governance. Each component enhances the others, creating a system more valuable than the sum of its parts.\n\nShow Image\n\n2.5.1 1.5.1 Automated Worldview Extraction\nCurrent approaches to AI risk modeling, exemplified by the Modeling Transformative AI Risks (MTAIR) project, demonstrate the value of formal representation but require extensive manual effort. Creating a single model demands dozens of expert-hours to translate qualitative arguments into quantitative frameworks. This bottleneck severely limits the number of perspectives that can be formalized and the speed of model updates as new arguments emerge.\nAutomation using frontier language models addresses this scaling challenge. By developing systematic methods to extract causal structures and probability judgments from natural language, we can:\n\nProcess orders of magnitude more content\nIncorporate diverse perspectives rapidly\nMaintain models that evolve with the discourse\nReduce barriers to entry for contributing worldviews\n\n\n\n2.5.2 1.5.2 Live Data Integration\nStatic models, however well-constructed, quickly become outdated in fast-moving domains. Prediction markets and forecasting platforms aggregate distributed knowledge about uncertain futures, providing continuously updated probability estimates. By connecting formal models to these live data sources, we create dynamic assessments that incorporate the latest collective intelligence P. E. Tetlock and Gardner (2015).\nThis integration serves multiple purposes:\n\nGrounding abstract models in empirical forecasts\nIdentifying which uncertainties most affect outcomes\nRevealing when model assumptions diverge from collective expectations\nGenerating new questions for forecasting communities\n\n\n\n2.5.3 1.5.3 Formal Policy Evaluation\nFormal policy evaluation transforms static risk assessments into actionable guidance by modeling how specific interventions alter critical parameters. Using causal inference techniques Pearl (2000) Pearl (2009), we can assess not just the probability of adverse outcomes but how those probabilities change under different policy regimes.\nThis enables genuinely evidence-based policy development:\n\nComparing interventions across multiple worldviews\nIdentifying robust strategies that work across scenarios\nUnderstanding which uncertainties most affect policy effectiveness\nPrioritizing research to reduce decision-relevant uncertainty\n\n\n\n2.5.4 1.5.4 The Synergy\nThe multiplicative benefits emerge from the interactions between components:\n\nAutomation enables comprehensive coverage, making prediction market integration more valuable by connecting to more perspectives\nMarket data validates and calibrates automated extractions, improving quality\nPolicy evaluation gains precision from both comprehensive models and live probability updates\nThe complete system creates feedback loops where policy analysis identifies critical uncertainties for market attention\n\nThis synergistic combination addresses the coordination crisis by providing common ground for disparate communities, translating between technical and policy languages, quantifying previously implicit disagreements, and enabling evidence-based compromise.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#thesis-structure-and-roadmap",
    "href": "chapters/Outlines/final_draft.html#thesis-structure-and-roadmap",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "2.6 1.6 Thesis Structure and Roadmap",
    "text": "2.6 1.6 Thesis Structure and Roadmap\n\nThe remainder of this thesis develops the multiplicative benefits framework from theoretical foundations to practical implementation:\nChapter 2: Context and Theoretical Foundations establishes the intellectual groundwork, examining the epistemic challenges unique to AI governance, Bayesian networks as formal tools for uncertainty representation, argument mapping as a bridge from natural language to formal models, the MTAIR project’s achievements and limitations, and requirements for effective coordination infrastructure.\nChapter 3: AMTAIR Design and Implementation presents the technical system including overall architecture and design principles, the two-stage extraction pipeline (ArgDown → BayesDown), validation methodology and results, case studies from simple examples to complex AI risk models, and integration with prediction markets and policy evaluation.\nChapter 4: Discussion - Implications and Limitations critically examines technical limitations and failure modes, conceptual concerns about formalization, integration with existing governance frameworks, scaling challenges and opportunities, and broader implications for epistemic security.\nChapter 5: Conclusion synthesizes key contributions and charts paths forward with a summary of theoretical and practical achievements, concrete recommendations for stakeholders, research agenda for community development, and vision for AI governance with proper coordination infrastructure.\nThroughout this progression, I maintain dual focus on theoretical sophistication and practical utility. The framework aims not merely to advance academic understanding but to provide actionable tools for improving coordination in AI governance during this critical period.\n\nShow Image\nHaving established the coordination crisis and outlined how automated modeling can address it, we now turn to the theoretical foundations that make this approach possible. The next chapter examines the unique epistemic challenges of AI governance and introduces the formal tools—particularly Bayesian networks—that enable rigorous reasoning under deep uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#ai-existential-risk-the-carlsmith-model",
    "href": "chapters/Outlines/final_draft.html#ai-existential-risk-the-carlsmith-model",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "3.1 2.1 AI Existential Risk: The Carlsmith Model",
    "text": "3.1 2.1 AI Existential Risk: The Carlsmith Model\n\nTo ground our discussion in concrete terms, I examine Joseph Carlsmith’s “Is Power-Seeking AI an Existential Risk?” as an exemplar of structured reasoning about AI catastrophic risk Carlsmith (2022). Carlsmith’s analysis stands out for its explicit probabilistic decomposition of the path from current AI development to potential existential catastrophe.\n\n\n\n3.1.1 2.1.1 Six-Premise Decomposition\n\nAccording to the MTAIR model Clarke et al. (2022), Carlsmith decomposes existential risk into a probabilistic chain with explicit estimates2:\n\nPremise 1: Transformative AI development this century (P≈0.80)(P ≈ 0.80) (P≈0.80)\nPremise 2: AI systems pursuing objectives in the world (P≈0.95)(P ≈ 0.95) (P≈0.95)\nPremise 3: Systems with power-seeking instrumental incentives (P≈0.40)(P ≈ 0.40) (P≈0.40)\nPremise 4: Sufficient capability for existential threat (P≈0.65)(P ≈ 0.65) (P≈0.65)\nPremise 5: Misaligned systems despite safety efforts (P≈0.50)(P ≈ 0.50) (P≈0.50)\nPremise 6: Catastrophic outcomes from misaligned power-seeking (P≈0.65)(P ≈ 0.65) (P≈0.65)\n\nComposite Risk Calculation: P(doom)≈0.05P(doom) ≈ 0.05 P(doom)≈0.05 (5%)\n\nmermaid\nflowchart TD\n    P1[Premise 1: Transformative AI&lt;br/&gt;P ≈ 0.80] --&gt; P2[Premise 2: AI pursuing objectives&lt;br/&gt;P ≈ 0.95]\n    P2 --&gt; P3[Premise 3: Power-seeking incentives&lt;br/&gt;P ≈ 0.40]\n    P3 --&gt; P4[Premise 4: Existential capability&lt;br/&gt;P ≈ 0.65]\n    P4 --&gt; P5[Premise 5: Misalignment despite safety&lt;br/&gt;P ≈ 0.50]\n    P5 --&gt; P6[Premise 6: Catastrophic outcome&lt;br/&gt;P ≈ 0.65]\n    P6 --&gt; D[Existential Catastrophe&lt;br/&gt;P ≈ 0.05]\nCarlsmith structures his argument through six conditional premises, each assigned explicit probability estimates:\nPremise 1: APS Systems by 2070 (P≈0.65)(P ≈ 0.65) (P≈0.65) “By 2070, there will be AI systems with Advanced capability, Agentic planning, and Strategic awareness”—the conjunction of capabilities that could enable systematic pursuit of objectives in the world.\nPremise 2: Alignment Difficulty (P≈0.40)(P ≈ 0.40) (P≈0.40) “It will be harder to build aligned APS systems than misaligned systems that are still attractive to deploy”—capturing the challenge that safety may conflict with capability or efficiency.\nPremise 3: Deployment Despite Misalignment (P≈0.70)(P ≈ 0.70) (P≈0.70) “Conditional on 1 and 2, we will deploy misaligned APS systems”—reflecting competitive pressures and limited coordination.\nPremise 4: Power-Seeking Behavior (P≈0.65)(P ≈ 0.65) (P≈0.65) “Conditional on 1-3, misaligned APS systems will seek power in high-impact ways”—based on instrumental convergence arguments.\nPremise 5: Disempowerment Success (P≈0.40)(P ≈ 0.40) (P≈0.40) “Conditional on 1-4, power-seeking will scale to permanent human disempowerment”—despite potential resistance and safeguards.\nPremise 6: Existential Catastrophe (P≈0.95)(P ≈ 0.95) (P≈0.95) “Conditional on 1-5, this disempowerment constitutes existential catastrophe”—connecting power loss to permanent curtailment of human potential.\nOverall Risk: Multiplying through the conditional chain yields P(doom)≈0.05P(doom) ≈ 0.05 P(doom)≈0.05 or 5% by 2070.\nThis structured approach exemplifies the type of reasoning AMTAIR aims to formalize and automate. While Carlsmith spent months developing this model manually, similar rigor exists implicitly in many AI safety arguments awaiting extraction.\n\n\n\n3.1.2 2.1.2 Why Carlsmith Exemplifies Formalizable Arguments\nCarlsmith’s model demonstrates several features that make it ideal for formal representation:\nExplicit Probabilistic Structure: Each premise receives numerical probability estimates with documented reasoning, enabling direct translation to Bayesian network parameters.\nClear Conditional Dependencies: The logical flow from capabilities through deployment decisions to catastrophic outcomes maps naturally onto directed acyclic graphs.\nTransparent Decomposition: Breaking the argument into modular premises allows independent evaluation and sensitivity analysis of each component.\nDocumented Reasoning: Extensive justification for each probability enables extraction of both structure and parameters from the source text.\n\nWe will return to Carlsmith’s model in Chapter 3 as our primary complex case study, demonstrating how AMTAIR successfully extracts and formalizes this sophisticated multi-level argument.\n\n\nBeyond Carlsmith’s model, other structured approaches to AI risk—such as Christiano’s “What failure looks like” Christiano (2019)—provide additional targets for automated extraction, enabling comparative analysis across different expert worldviews.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#the-epistemic-challenge-of-policy-evaluation",
    "href": "chapters/Outlines/final_draft.html#the-epistemic-challenge-of-policy-evaluation",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "3.2 2.2 The Epistemic Challenge of Policy Evaluation",
    "text": "3.2 2.2 The Epistemic Challenge of Policy Evaluation\n\nAI governance policy evaluation faces unique epistemic challenges that render traditional policy analysis methods insufficient. Understanding these challenges motivates the need for new computational approaches.\n\n3.2.1 2.2.1 Unique Characteristics of AI Governance\nDeep Uncertainty Rather Than Risk: Traditional policy analysis distinguishes between risk (known probability distributions) and uncertainty (known possibilities, unknown probabilities). AI governance faces deep uncertainty—we cannot confidently enumerate possible futures, much less assign probabilities Hallegatte et al. (2012). Will recursive self-improvement enable rapid capability gains? Can value alignment be solved technically? These foundational questions resist empirical resolution before their answers become catastrophically relevant.\nComplex Multi-Level Causation: Policy effects propagate through technical, institutional, and social levels with intricate feedback loops. A technical standard might alter research incentives, shifting capability development trajectories, changing competitive dynamics, and ultimately affecting existential risk through pathways invisible at the policy’s inception. Traditional linear causal models cannot capture these dynamics.\nIrreversibility and Lock-In: Many AI governance decisions create path dependencies that prove difficult or impossible to reverse. Early technical standards shape development trajectories. Institutional structures ossify. International agreements create sticky equilibria. Unlike many policy domains where course correction remains possible, AI governance mistakes may prove permanent.\nValue-Laden Technical Choices: The entanglement of technical and normative questions confounds traditional separation of facts and values. What constitutes “alignment”? How much capability development should we risk for economic benefits? Technical specifications embed ethical judgments that resist neutral expertise.\n\n\n\n\nTable 3.1: Comparison of AI governance vs traditional policy domains\n\n\n\n\n\n\n\n\n\n\nDimension\nTraditional Policy\nAI Governance\n\n\n\n\nUncertainty Type\nRisk (known distributions)\nDeep uncertainty (unknown unknowns)\n\n\nCausal Structure\nLinear, traceable\nMulti-level, feedback loops\n\n\nReversibility\nCourse correction possible\nPath dependencies, lock-in\n\n\nFact-Value Separation\nClear boundaries\nEntangled technical-normative\n\n\nEmpirical Grounding\nHistorical precedents\nUnprecedented phenomena\n\n\nTime Horizons\nYears to decades\nMonths to centuries\n\n\n\n\n\n\n\n\n3.2.2 2.2.2 Limitations of Traditional Approaches\nStandard policy evaluation tools prove inadequate for these challenges:\nCost-Benefit Analysis assumes commensurable outcomes and stable probability distributions. When potential outcomes include existential catastrophe with deeply uncertain probabilities, the mathematical machinery breaks down. Infinite negative utility resists standard decision frameworks.\nScenario Planning helps explore possible futures but typically lacks the probabilistic reasoning needed for decision-making under uncertainty. Without quantification, scenarios provide narrative richness but limited action guidance.\nExpert Elicitation aggregates specialist judgment but struggles with interdisciplinary questions where no single expert grasps all relevant factors. Moreover, experts often operate with different implicit models, making aggregation problematic.\nRed Team Exercises test specific plans but miss systemic risks emerging from component interactions. Gaming individual failures cannot reveal emergent catastrophic possibilities.\nThese limitations create a methodological gap: we need approaches that handle deep uncertainty, represent complex causation, quantify expert disagreement, and enable systematic exploration of intervention effects.\n\n\n\n3.2.3 2.2.3 The Underlying Epistemic Framework\n\nThe AMTAIR approach rests on a specific epistemic framework that combines probabilistic reasoning, conditional logic, and possible worlds semantics. This framework provides the philosophical foundation for representing deep uncertainty about AI futures.\nProbabilistic Epistemology: Following the Bayesian tradition, we treat probability as a measure of rational credence rather than objective frequency. This subjective interpretation allows meaningful probability assignments even for unique, unprecedented events like AI catastrophe. As E.T. Jaynes demonstrated, probability theory extends deductive logic to handle uncertainty, providing a calculus for rational belief Jaynes (2003).\nConditional Structure: The framework emphasizes conditional rather than absolute probabilities. Instead of asking “What is P(catastrophe)?” we ask “What is P(catastrophe | specific assumptions)?” This conditionalization makes explicit the dependency of conclusions on worldview assumptions, enabling productive disagreement about premises rather than conclusions.\nPossible Worlds Semantics: We conceptualize uncertainty as distributions over possible worlds—complete descriptions of how reality might unfold. Each world represents a coherent scenario with specific values for all relevant variables. Probability distributions over these worlds capture both what we know and what we don’t know about the future.\nThis framework enables several key capabilities:\n\nRepresenting ignorance: We can express uncertainty about uncertainty itself through hierarchical probability models\nCombining evidence: Bayesian updating provides principled methods for integrating new information\nComparing worldviews: Different probability distributions over the same space of possibilities enable systematic comparison\nEvaluating interventions: Counterfactual reasoning about how actions change probability distributions\n\n\n\n\n3.2.4 2.2.4 Toward New Epistemic Tools\n\nThe inadequacy of traditional methods for AI governance creates an urgent need for new epistemic tools. These tools must:\n\nHandle Deep Uncertainty: Move beyond point estimates to represent ranges of possibilities\nCapture Complex Causation: Model multi-level interactions and feedback loops\nQuantify Disagreement: Make explicit where experts diverge and why\nEnable Systematic Analysis: Support rigorous comparison of policy options\n\nKey Insight: The computational approaches developed in this thesis—particularly Bayesian networks enhanced with automated extraction—directly address each of these requirements by providing formal frameworks for reasoning under uncertainty.\n\nShow Image\nShow Image\nShow Image\nShow Image\n\nRecent work on conditional trees demonstrates the value of structured approaches to uncertainty. McCaslin et al. McCaslin et al. (2024) show how hierarchical conditional forecasting can identify high-value questions for reducing uncertainty about complex topics like AI risk. Their methodology, which asks experts to produce simplified Bayesian networks of informative forecasting questions, achieved nine times higher information value than standard forecasting platform questions.\n\nTetlock’s work with the Forecasting Research Institute P. Tetlock (2022) exemplifies how prediction markets can provide empirical grounding for formal models. By structuring questions as conditional trees, they enable forecasters to express complex dependencies between events, providing exactly the type of data needed for Bayesian network parameterization.\n\nGruetzemacher Gruetzemacher (2022) evaluates the tradeoffs between full Bayesian networks and conditional trees for forecasting tournaments. While conditional trees offer simplicity, Bayesian networks provide richer representation of dependencies—motivating AMTAIR’s approach of using full networks while leveraging conditional tree insights for question generation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#bayesian-networks-as-knowledge-representation",
    "href": "chapters/Outlines/final_draft.html#bayesian-networks-as-knowledge-representation",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "3.3 2.3 Bayesian Networks as Knowledge Representation",
    "text": "3.3 2.3 Bayesian Networks as Knowledge Representation\n\nBayesian networks offer a mathematical framework uniquely suited to addressing these epistemic challenges. By combining graphical structure with probability theory, they provide tools for reasoning about complex uncertain domains.\n\n3.3.1 2.3.1 Mathematical Foundations\nA Bayesian network consists of:\n\nDirected Acyclic Graph (DAG): Nodes represent variables, edges represent direct dependencies\nConditional Probability Tables (CPTs): For each node, P(node|parents) quantifies relationships\n\nThe joint probability distribution factors according to the graph structure:\n\nP(X1,X2,…,Xn)=∏i=1nP(Xi∣Parents(Xi))P(X_1, X_2, …, X_n) = _{i=1}^{n} P(X_i | Parents(X_i))P(X1​,X2​,…,Xn​)=i=1∏n​P(Xi​∣Parents(Xi​))\nThis factorization enables efficient inference and embodies causal assumptions explicitly.\n\nPearl’s foundational work Pearl (2014) established Bayesian networks as a principled approach to automated reasoning under uncertainty, providing both theoretical foundations and practical algorithms.\n\n\n\n\n3.3.2 2.3.2 The Rain-Sprinkler-Grass Example\n\nThe canonical example illustrates key concepts3:\n\n[Grass_Wet]: Concentrated moisture on grass. \n + [Rain]: Water falling from sky.\n + [Sprinkler]: Artificial watering system.\n   + [Rain]\nNetwork Structure:\n\nRain (root cause): P(rain) = 0.2\nSprinkler (intermediate): P(sprinkler|rain) varies by rain state\nGrass_Wet (effect): P(wet|rain, sprinkler) depends on both causes\n\n\nmermaid\nflowchart TD\n    R[Rain&lt;br/&gt;P(rain) = 0.2] --&gt; S[Sprinkler]\n    R --&gt; G[Grass_Wet]\n    S --&gt; G\n    \n    subgraph CPT1[Sprinkler CPT]\n        S1[P(sprinkler|rain) = 0.01]\n        S2[P(sprinkler|¬rain) = 0.4]\n    end\n    \n    subgraph CPT2[Grass_Wet CPT]\n        G1[P(wet|rain,sprinkler) = 0.99]\n        G2[P(wet|rain,¬sprinkler) = 0.8]\n        G3[P(wet|¬rain,sprinkler) = 0.9]\n        G4[P(wet|¬rain,¬sprinkler) = 0.01]\n    end\npython\n# Basic network representation\nnodes = ['Rain', 'Sprinkler', 'Grass_Wet']\nedges = [('Rain', 'Sprinkler'), ('Rain', 'Grass_Wet'), ('Sprinkler', 'Grass_Wet')]\n\n# Conditional probability specification\nP_wet_given_causes = {\n    (True, True): 0.99,    # Rain=T, Sprinkler=T\n    (True, False): 0.80,   # Rain=T, Sprinkler=F  \n    (False, True): 0.90,   # Rain=F, Sprinkler=T\n    (False, False): 0.01   # Rain=F, Sprinkler=F\n}\nThis simple network demonstrates:\n\nMarginal Inference: P(grass_wet) computed from joint distribution\nDiagnostic Reasoning: P(rain|grass_wet) reasoning from effects to causes\nIntervention Modeling: P(grass_wet|do(sprinkler=on)) for policy analysis\n\n\nShow Image\n\n3.3.2.1 Rain-Sprinkler-Grass Network Rendering\n#| label: rain_sprinkler_grass_example_network_rendering\n#| echo: true\n#| eval: true\n#| fig-cap: \"Dynamic Html Rendering of the Rain-Sprinkler-Grass DAG with Conditional Probabilities\"\n#| fig-link: \"https://singularitysmith.github.io/AMTAIR_Prototype/bayesian_network.html\"\n#| fig-alt: \"Dynamic Html Rendering of the Rain-Sprinkler-Grass DAG\"\n\nfrom IPython.display import IFrame\n\nIFrame(src=\"https://singularitysmith.github.io/AMTAIR_Prototype/bayesian_network.html\", width=\"100%\", height=\"600px\")\n\n\n\n3.3.3 2.3.3 Advantages for AI Risk Modeling\nThese features address key requirements for AI governance:\n\nHandling Uncertainty: Every parameter is a distribution, not a point estimate\nRepresenting Causation: Directed edges embody causal relationships\nEnabling Analysis: Formal inference algorithms support systematic evaluation\nFacilitating Communication: Visual structure aids cross-domain understanding\n\n\n\n\n\nArmstrong, Stuart, Nick Bostrom, and Carl Shulman. 2016. “Racing to the Precipice: A Model of Artificial Intelligence Development.” AI & SOCIETY 31 (2): 201–6. https://doi.org/10.1007/s00146-015-0590-y.\n\n\nCarlsmith, Joseph. 2021. “Is Power-Seeking AI an Existential Risk?” 2021. https://doi.org/10.48550/arXiv.2206.13353.\n\n\n———. 2022. “Is Power-Seeking AI an Existential Risk?” https://arxiv.org/abs/2206.13353.\n\n\n———. 2024. “Is Power-Seeking AI an Existential Risk?” August 13, 2024. https://doi.org/10.48550/arXiv.2206.13353.\n\n\nChristiano, Paul F. 2019. “What Failure Looks Like,” March. https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like.\n\n\nClarke, Sam, Ben Cottier, Aryeh Englander, Daniel Eth, David Manheim, Samuel Dylan Martin, and Issa Rice. 2022. “Modeling Transformative AI Risks (MTAIR) Project – Summary Report.” 2022. https://doi.org/10.48550/ARXIV.2206.09360.\n\n\nEuropean, Union. 2024. “The Act Texts | EU Artificial Intelligence Act.” 2024. https://artificialintelligenceact.eu/the-act/.\n\n\nGruetzemacher, Ross. 2022. “Bayesian Networks Vs. Conditional Trees for Creating Questions for Forecasting Tournaments.”\n\n\nHallegatte, Stéphane, Ankur Shah, Robert Lempert, Casey Brown, and Stuart Gill. 2012. “Investment Decision-Making Under Deep Uncertainty-Application to Climate Change.” Policy Research Working Paper 6193. https://enpc.hal.science/hal-00802049/document.\n\n\nHunt, Tam. 2025. “The Insane ‘Logic’ of the AI Arms Race.” Medium. March 3, 2025. https://tamhunt.medium.com/the-insane-logic-of-the-ai-arms-race-45a5f79f4c0e.\n\n\nJaynes, Edwin T. 2003. Probability Theory: The Logic of Science. Cambridge university press.\n\n\nMaslej, Nestor. 2025. “Artificial Intelligence Index Report 2025.” Artificial Intelligence.\n\n\nMcCaslin, Tegan, Josh Rosenberg, Ezra Karger, Avital Morris, Molly Hickman, Sam Glover, Zach Jacobs, and Phil Tetlock. 2024. “Conditional Trees: A Method for Generating Informative Questions about Complex Topics.” Forecasting Research Institute. https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/66ba37a144f1d6095de467df/1723479995772/AIConditionalTrees.pdf.\n\n\nPaul. 2023. “The elephAInt – Are We All Like the Six Blind Men When It Comes to AI? | PRISMAGuard LLC.” 2023. https://www.prismaguard.com/the-elephaint-are-we-all-like-the-six-blind-men-when-it-comes-to-ai/.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\n———. 2009. Causality: Models, Reasoning and Inference. 2nd ed. Cambridge University Press.\n\n\n———. 2014. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Elsevier. https://books.google.ca/books?hl=en&lr=&id=mn2jBQAAQBAJ&oi=fnd&pg=PP1&dq=Pearl,+J.+(1988).+Probabilistic+Reasoning+in+Intelligent+Systems&ots=4tEX2A4Ha8&sig=lgUs_RCoeXEEuGwM5xMEoyJy4HI.\n\n\nRehman, Iskander. 2025. “The Battle for Brilliant Minds: From the Nuclear Age to AI.” War on the Rocks. January 13, 2025. https://warontherocks.com/2025/01/the-battle-for-brilliant-minds-from-the-nuclear-age-to-ai/.\n\n\nSamborska, Veronika. 2025. “Scaling up: How Increasing Inputs Has Made Artificial Intelligence More Capable.” Our World in Data, January. https://ourworldindata.org/scaling-up-ai.\n\n\nSamuel, Sigal. 2023. “AI Is a ‘Tragedy of the Commons.’ We’ve Got Solutions for That.” Vox. July 7, 2023. https://www.vox.com/future-perfect/2023/7/7/23787011/ai-arms-race-tragedy-commons-risk-safety.\n\n\nSchelling, Thomas C. 1960. “I960. The Strategy of Conflict.” Cambridge, Mass.\n\n\nTegmark, Max. 2024. “Asilomar AI Principles.” Future of Life Institute. 2024. https://futureoflife.org/open-letter/ai-principles/.\n\n\nTetlock, Phil. 2022. “Conditional Trees: AI Risk.” 2022. https://www.metaculus.com/tournament/3508/.\n\n\nTetlock, Philip E., and Dan Gardner. 2015. Superforecasting: The Art and Science of Prediction. First paperback edition. New York: Broadway Books.\n\n\nTodd, Benjamin. 2024. “It Looks Like There Are Some Good Funding Opportunities in AI Safety Right Now.” Substack newsletter. Benjamin Todd. December 21, 2024. https://benjamintodd.substack.com/p/looks-like-there-are-some-good-funding.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "chapters/Outlines/final_draft.html#footnotes",
    "href": "chapters/Outlines/final_draft.html#footnotes",
    "title": "1  Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "The orthogonality thesis posits that intelligence and goals are independent—an AI can have any set of objectives regardless of its intelligence level. The instrumental convergence thesis suggests that different AI systems may adopt similar instrumental goals (e.g., self-preservation, resource acquisition) to achieve their objectives.↩︎\nMultiple versions of Carlsmith’s paper exist with slight updates to probability estimates: Carlsmith (2021), Carlsmith (2022), Carlsmith (2024). We primarily reference the version used by the MTAIR team for their extraction. Extended discussion and expert probability estimates can be found on LessWrong.↩︎\nThis example, while simple, demonstrates all essential features of Bayesian networks and serves as the foundation for understanding more complex applications↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Final Thesis: Automating the Modeling of Transformative Artificial Intelligence Risks</span>"
    ]
  },
  {
    "objectID": "ref/references.html",
    "href": "ref/references.html",
    "title": "Bibliography",
    "section": "",
    "text": "Armstrong, Stuart, Nick Bostrom, and Carl Shulman. 2016. “Racing\nto the Precipice: A Model of Artificial Intelligence\nDevelopment.” AI & SOCIETY 31 (2): 201–6. https://doi.org/10.1007/s00146-015-0590-y.\n\n\nCarlsmith, Joseph. 2021. “Is Power-Seeking AI an\nExistential Risk?” 2021. https://doi.org/10.48550/arXiv.2206.13353.\n\n\n———. 2022. “Is Power-Seeking AI an Existential\nRisk?” https://arxiv.org/abs/2206.13353.\n\n\n———. 2024. “Is Power-Seeking AI an Existential\nRisk?” August 13, 2024. https://doi.org/10.48550/arXiv.2206.13353.\n\n\nChristiano, Paul F. 2019. “What Failure Looks Like,” March.\nhttps://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like.\n\n\nClarke, Sam, Ben Cottier, Aryeh Englander, Daniel Eth, David Manheim,\nSamuel Dylan Martin, and Issa Rice. 2022. “Modeling\nTransformative AI Risks (MTAIR)\nProject – Summary Report.” 2022. https://doi.org/10.48550/ARXIV.2206.09360.\n\n\nEuropean, Union. 2024. “The Act Texts | EU\nArtificial Intelligence Act.” 2024. https://artificialintelligenceact.eu/the-act/.\n\n\nGruetzemacher, Ross. 2022. “Bayesian Networks Vs.\nConditional Trees for Creating Questions for\nForecasting Tournaments.”\n\n\nHallegatte, Stéphane, Ankur Shah, Robert Lempert, Casey Brown, and\nStuart Gill. 2012. “Investment Decision-Making Under Deep\nUncertainty-Application to Climate Change.” Policy Research\nWorking Paper 6193. https://enpc.hal.science/hal-00802049/document.\n\n\nHunt, Tam. 2025. “The Insane ‘Logic’ of the\nAI Arms Race.” Medium. March 3, 2025. https://tamhunt.medium.com/the-insane-logic-of-the-ai-arms-race-45a5f79f4c0e.\n\n\nJaynes, Edwin T. 2003. Probability Theory: The Logic of\nScience. Cambridge university press.\n\n\nMaslej, Nestor. 2025. “Artificial Intelligence Index\nReport 2025.” Artificial Intelligence.\n\n\nMcCaslin, Tegan, Josh Rosenberg, Ezra Karger, Avital Morris, Molly\nHickman, Sam Glover, Zach Jacobs, and Phil Tetlock. 2024.\n“Conditional Trees: A Method for\nGenerating Informative Questions about Complex\nTopics.” Forecasting Research Institute. https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/66ba37a144f1d6095de467df/1723479995772/AIConditionalTrees.pdf.\n\n\nPaul. 2023. “The elephAInt –\nAre We All Like the Six Blind Men When It Comes to\nAI? | PRISMAGuard LLC.” 2023. https://www.prismaguard.com/the-elephaint-are-we-all-like-the-six-blind-men-when-it-comes-to-ai/.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and\nInference. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\n———. 2009. Causality: Models, Reasoning and\nInference. 2nd ed. Cambridge University Press.\n\n\n———. 2014. Probabilistic Reasoning in Intelligent Systems: Networks\nof Plausible Inference. Elsevier. https://books.google.ca/books?hl=en&lr=&id=mn2jBQAAQBAJ&oi=fnd&pg=PP1&dq=Pearl,+J.+(1988).+Probabilistic+Reasoning+in+Intelligent+Systems&ots=4tEX2A4Ha8&sig=lgUs_RCoeXEEuGwM5xMEoyJy4HI.\n\n\nRehman, Iskander. 2025. “The Battle for\nBrilliant Minds: From the Nuclear\nAge to AI.” War on the Rocks. January 13,\n2025. https://warontherocks.com/2025/01/the-battle-for-brilliant-minds-from-the-nuclear-age-to-ai/.\n\n\nSamborska, Veronika. 2025. “Scaling up: How Increasing Inputs Has\nMade Artificial Intelligence More Capable.” Our World in\nData, January. https://ourworldindata.org/scaling-up-ai.\n\n\nSamuel, Sigal. 2023. “AI Is a ‘Tragedy of the\nCommons.’ We’ve Got Solutions for That.” Vox.\nJuly 7, 2023. https://www.vox.com/future-perfect/2023/7/7/23787011/ai-arms-race-tragedy-commons-risk-safety.\n\n\nSchelling, Thomas C. 1960. “I960. The Strategy of\nConflict.” Cambridge, Mass.\n\n\nTegmark, Max. 2024. “Asilomar AI Principles.”\nFuture of Life Institute. 2024. https://futureoflife.org/open-letter/ai-principles/.\n\n\nTetlock, Phil. 2022. “Conditional Trees: AI\nRisk.” 2022. https://www.metaculus.com/tournament/3508/.\n\n\nTetlock, Philip E., and Dan Gardner. 2015. Superforecasting: The Art\nand Science of Prediction. First paperback edition. New York:\nBroadway Books.\n\n\nTodd, Benjamin. 2024. “It Looks Like There Are Some Good Funding\nOpportunities in AI Safety Right Now.” Substack\nnewsletter. Benjamin Todd. December 21, 2024. https://benjamintodd.substack.com/p/looks-like-there-are-some-good-funding.",
    "crumbs": [
      "Bibliography"
    ]
  }
]