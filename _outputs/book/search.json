[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#illustrations-and-terminology-quick-references",
    "href": "index.html#illustrations-and-terminology-quick-references",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Illustrations and Terminology — Quick References",
    "text": "Illustrations and Terminology — Quick References\n\nAcknowledgments\n\nAcademic supervisor (Prof. Timo Speith) and institution (University of Bayreuth)\n\nResearch collaborators, especially those connected to the original MTAIR project\n\nTechnical advisors who provided feedback on implementation aspects\n\nPersonal supporters who enabled the research through encouragement and feedback",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#list-of-graphics-figures",
    "href": "index.html#list-of-graphics-figures",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "List of Graphics & Figures",
    "text": "List of Graphics & Figures\n\n\n\nFigure 1.1: The coordination crisis in AI governance - visualization of fragmentation\n\nFigure 2.1: The Carlsmith model - DAG representation\n\nFigure 3.1: Research design overview - workflow diagram\n\nFigure 3.2: From natural language to BayesDown - transformation process\n\nFigure 4.1: ARPA system architecture - component diagram\n\nFigure 4.2: Visualization of Rain-Sprinkler-Grass_Wet Bayesian network - screenshot\n\nFigure 5.1: Extraction quality metrics - comparative chart\n\nFigure 5.2: Comparative analysis of AI governance worldviews - network visualization",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#list-of-abbreviations",
    "href": "index.html#list-of-abbreviations",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "List of Abbreviations",
    "text": "List of Abbreviations\n\n\nesp. especially\nf., ff. following\nincl. including\np., pp. page(s)\nMAD Mutually Assured Destruction\n\nAI - Artificial Intelligence\n\nAGI - Artificial General Intelligence\n\nARPA - AI Risk Pathway Analyzer\n\nDAG - Directed Acyclic Graph\n\nLLM - Large Language Model\n\nMTAIR - Modeling Transformative AI Risks\n\nP(Doom) - Probability of existential catastrophe from misaligned AI\n\nCPT - Conditional Probability Table",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#glossary",
    "href": "index.html#glossary",
    "title": "Automating the Modelling of Transformative Artificial Intelligence Risks",
    "section": "Glossary",
    "text": "Glossary\n\n\n\nArgument mapping: A method for visually representing the structure of arguments\n\nBayesDown: An extension of ArgDown that incorporates probabilistic information\n\nBayesian network: A probabilistic graphical model representing variables and their dependencies\n\nConditional probability: The probability of an event given that another event has occurred\n\nDirected Acyclic Graph (DAG): A graph with directed edges and no cycles\n\nExistential risk: Risk of permanent curtailment of humanity’s potential\n\nPower-seeking AI: AI systems with instrumental incentives to acquire resources and power\n\nPrediction market: A market where participants trade contracts that resolve based on future events\n\nd-separation: A criterion for identifying conditional independence relationships in Bayesian networks\n\nMonte Carlo sampling: A computational technique using random sampling to obtain numerical results",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html",
    "href": "chapters/Outlines/Outline_13.html",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "",
    "text": "Frontmatter: Preface\nThis thesis represents the culmination of interdisciplinary research at the intersection of AI safety, formal epistemology, and computational social science. The work emerged from recognizing a fundamental challenge in AI governance: while investment in AI safety research has grown exponentially, coordination between different stakeholder communities remains fragmented, potentially increasing existential risk through misaligned efforts.\nI hope this work contributes to building the intellectual and technical infrastructure necessary for humanity to navigate the transition to transformative AI safely. The tools and frameworks presented here are offered in the spirit of collaborative problem-solving, recognizing that the challenges we face require unprecedented cooperation across disciplines, institutions, and worldviews.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#acknowledgments",
    "href": "chapters/Outlines/Outline_13.html#acknowledgments",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\n\nI thank my supervisor Dr. Timo Speith for guidance throughout this project, the MTAIR team for pioneering the manual approach that inspired automation, and the AI safety community for creating the rich literature that made this work possible. Special recognition goes to technical advisors who provided invaluable feedback and Coleman Snell for his partnership and research collaboration with the AMTAIR project. Any errors or limitations remain my own responsibility.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-opening-scenario",
    "href": "chapters/Outlines/Outline_13.html#sec-opening-scenario",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "1.1 Opening Scenario: The Policymaker’s Dilemma",
    "text": "1.1 Opening Scenario: The Policymaker’s Dilemma\n\n\nTodd (2024)\nImagine a senior policy advisor preparing recommendations for AI governance legislation. On her desk lie a dozen reports from leading AI safety researchers, each painting a different picture of the risks ahead. One argues that misaligned AI could pose existential risks within the decade, citing complex technical arguments about instrumental convergence and orthogonality. Another suggests these concerns are overblown, emphasizing uncertainty and the strength of existing institutions. A third proposes specific technical standards but acknowledges deep uncertainty about their effectiveness.  ::: {.redundant-content data-better-version=“Outline_12.2#sec-opening”} Each report seems compelling in isolation, written by credentialed experts with sophisticated arguments. Yet they reach dramatically different conclusions about both the magnitude of risk and appropriate interventions. The technical arguments involve unfamiliar concepts—mesa-optimization, corrigibility, capability amplification—expressed through different frameworks and implicit assumptions. Time is limited, stakes are high, and the legislation could shape humanity’s trajectory for decades. :::\nThis scenario plays out daily across government offices, corporate boardrooms, and research institutions worldwide. It exemplifies what I term the “coordination crisis” in AI governance: despite unprecedented attention and resources directed toward AI safety, we lack the epistemic infrastructure to synthesize diverse expert knowledge into actionable governance strategies.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-coordination-crisis",
    "href": "chapters/Outlines/Outline_13.html#sec-coordination-crisis",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "1.2 The Coordination Crisis in AI Governance",
    "text": "1.2 The Coordination Crisis in AI Governance\n\n\nMaslej (2025)\n\nSamborska (2025)\nAs AI capabilities advance at an accelerating pace—demonstrated by the rapid progression from GPT-3 to GPT-4, Claude, and emerging multimodal systems—humanity faces a governance challenge unlike any in history. The task of ensuring increasingly powerful AI systems remain aligned with human values and beneficial to our long-term flourishing grows more urgent with each capability breakthrough. This challenge becomes particularly acute when considering transformative AI systems that could drastically alter civilization’s trajectory, potentially including existential risks from misaligned systems pursuing objectives counter to human welfare.\n\nDespite unprecedented investment in AI safety research, rapidly growing awareness among key stakeholders, and proliferating frameworks for responsible AI development, we face what I’ll term the “coordination crisis” in AI governance—a systemic failure to align diverse efforts across technical, policy, and strategic domains into a coherent response proportionate to the risks we face.\n\nThe current state of AI governance presents a striking paradox. On one hand, we witness extraordinary mobilization: billions in research funding, proliferating safety initiatives, major tech companies establishing alignment teams, and governments worldwide developing AI strategies. The Asilomar AI Principles garnered thousands of signatures, the EU advances comprehensive AI regulation , and technical researchers produce increasingly sophisticated work on alignment, interpretability, and robustness.\n\nTegmark (2024)\n\nEuropean (2024)\n\nYet alongside this activity, we observe systematic coordination failures that may prove catastrophic. Technical safety researchers develop sophisticated alignment techniques without clear implementation pathways. Policy specialists craft regulatory frameworks lacking technical grounding to ensure practical efficacy. Ethicists articulate normative principles that lack operational specificity. Strategy researchers identify critical uncertainties but struggle to translate these into actionable guidance. International bodies convene without shared frameworks for assessing interventions.\n\n\n\n1.2.1 Safety Gaps from Misaligned Efforts\n\nThe fragmentation problem manifests in incompatible frameworks between technical researchers, policy specialists, and strategic analysts. Each community develops sophisticated approaches within their domain, yet translation between domains remains primitive. This creates systematic blind spots where risks emerge at the interfaces between technical capabilities, institutional responses, and strategic dynamics.\nWhen different communities operate with incompatible frameworks, critical risks fall through the cracks. Technical researchers may solve alignment problems under assumptions that policymakers’ decisions invalidate. Regulations optimized for current systems may inadvertently incentivize dangerous development patterns. Without shared models of the risk landscape, our collective efforts resemble the parable of blind men describing an elephant—each accurate within their domain but missing the complete picture.\n\n\nPaul (2023)\n\n\n1.2.2 Resource Misallocation\n\nThe AI safety community duplicates efforts while leaving critical areas underexplored. Multiple teams independently develop similar frameworks without building on each other’s work. Funders struggle to identify high-impact opportunities across technical and governance domains. Talent flows toward well-publicized approaches while neglected strategies remain understaffed. This misallocation becomes more costly as the window for establishing effective governance narrows.\n\n\n\n1.2.3 Negative-Sum Dynamics\n\nPerhaps most concerning, uncoordinated interventions can actively increase risk. Safety standards that advantage established players may accelerate risky development elsewhere. Partial transparency requirements might enable capability advances without commensurate safety improvements. International agreements lacking shared technical understanding may lock in dangerous practices. Without coordination, our cure risks becoming worse than the disease.\n\nCoordination failures systematically amplify existential risk through multiple pathways. Safety gaps emerge when technical solutions lack policy implementation pathways. Resource misallocation occurs when multiple teams unknowingly duplicate efforts while critical areas remain unaddressed. Most perniciously, locally optimized decisions by individual actors can create negative-sum dynamics that increase overall risk—an AI governance tragedy of the commons.\n\n\nArmstrong, Bostrom, and Shulman (2016)\n\nSamuel (2023), Hunt (2025)",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-historical-urgency",
    "href": "chapters/Outlines/Outline_13.html#sec-historical-urgency",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "1.3 Historical Parallels and Temporal Urgency",
    "text": "1.3 Historical Parallels and Temporal Urgency\n\nHistory offers instructive parallels. The nuclear age began with scientists racing to understand and control forces that could destroy civilization. Early coordination failures—competing national programs, scientist-military tensions, public-expert divides—nearly led to catastrophe multiple times. Only through developing shared frameworks (deterrence theory), institutions (IAEA), and communication channels (hotlines, treaties) did humanity navigate the nuclear precipice.\n\nSchelling (1960)\n\nRehman (2025)\n\nYet AI presents unique coordination challenges that compress our response timeline:\nAccelerating Development: Unlike nuclear weapons requiring massive infrastructure, AI development proceeds in corporate labs and academic departments worldwide. Capability improvements come through algorithmic insights and computational scale, both advancing exponentially.\nDual-Use Ubiquity: Every AI advance potentially contributes to both beneficial applications and catastrophic risks. The same language model architectures enabling scientific breakthroughs could facilitate dangerous manipulation or deception at scale.\nComprehension Barriers: Nuclear risks were viscerally understandable—cities vaporized, radiation sickness, nuclear winter. AI risks involve abstract concepts like optimization processes, goal misspecification, and emergent capabilities that resist intuitive understanding.\nGovernance Lag: Traditional governance mechanisms—legislation, international treaties, professional standards—operate on timescales of years to decades. AI capabilities advance on timescales of months to years, creating an ever-widening capability-governance gap.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-research-question",
    "href": "chapters/Outlines/Outline_13.html#sec-research-question",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "1.4 Research Question and Scope",
    "text": "1.4 Research Question and Scope\n\nThis thesis addresses a specific dimension of the coordination challenge by investigating the question:\nCan frontier AI technologies be utilized to automate the modeling of transformative AI risks, enabling robust prediction of policy impacts across diverse worldviews?\n\nMore specifically, I explore whether frontier language models can automate the extraction and formalization of probabilistic world models from AI safety literature, creating a scalable computational framework that enhances coordination in AI governance through systematic policy evaluation under uncertainty.\n\nTo break this down into its components:\n\nFrontier AI Technologies: Today’s most capable language models (GPT-4, Claude-3 level systems)\nAutomated Modeling: Using these systems to extract and formalize argument structures from natural language\nTransformative AI Risks: Potentially catastrophic outcomes from advanced AI systems, particularly existential risks\nPolicy Impact Prediction: Evaluating how governance interventions might alter probability distributions over outcomes\nDiverse Worldviews: Accounting for fundamental disagreements about AI development trajectories and risk factors\n\nThe investigation encompasses both theoretical development and practical implementation, focusing specifically on existential risks from misaligned AI systems rather than broader AI ethics concerns. This narrowed scope enables deep technical development while addressing the highest-stakes coordination challenges.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-multiplicative-benefits",
    "href": "chapters/Outlines/Outline_13.html#sec-multiplicative-benefits",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "1.5 The Multiplicative Benefits Framework",
    "text": "1.5 The Multiplicative Benefits Framework\n\nThe central thesis of this work is that combining three elements—automated worldview extraction, prediction market integration, and formal policy evaluation—creates multiplicative rather than merely additive benefits for AI governance. Each component enhances the others, creating a system more valuable than the sum of its parts.\n\n\n1.5.1 Automated Worldview Extraction\n\nAutomated worldview extraction using frontier language models addresses the scaling bottleneck in current approaches to AI risk modeling. The Modeling Transformative AI Risks (MTAIR) project demonstrated the value of formal representation but required extensive manual effort to translate qualitative arguments into quantitative models. Automation enables processing orders of magnitude more content, incorporating diverse perspectives, and maintaining models in near real-time as new arguments emerge.\n\nCurrent approaches to AI risk modeling, exemplified by the Modeling Transformative AI Risks (MTAIR) project, demonstrate the value of formal representation but require extensive manual effort. Creating a single model demands dozens of expert-hours to translate qualitative arguments into quantitative frameworks. This bottleneck severely limits the number of perspectives that can be formalized and the speed of model updates as new arguments emerge.\n\nAutomation using frontier language models addresses this scaling challenge. By developing systematic methods to extract causal structures and probability judgments from natural language, we can:\n\nProcess orders of magnitude more content\nIncorporate diverse perspectives rapidly\nMaintain models that evolve with the discourse\nReduce barriers to entry for contributing worldviews\n\n\n\n1.5.2 Live Data Integration\n\nPrediction market integration grounds these models in collective forecasting intelligence. By connecting formal representations to live forecasting platforms, the system can incorporate timely judgments about critical uncertainties from calibrated forecasters. This creates a dynamic feedback loop where models inform forecasters and forecasts update models.\n\nStatic models, however well-constructed, quickly become outdated in fast-moving domains. Prediction markets and forecasting platforms aggregate distributed knowledge about uncertain futures, providing continuously updated probability estimates. By connecting formal models to these live data sources, we create dynamic assessments that incorporate the latest collective intelligence.\nThis integration serves multiple purposes:\n\nGrounding abstract models in empirical forecasts\nIdentifying which uncertainties most affect outcomes\nRevealing when model assumptions diverge from collective expectations\nGenerating new questions for forecasting communities\n\n\nP. E. Tetlock and Gardner (2015)\n\n\n1.5.3 Formal Policy Evaluation\nFormal policy evaluation transforms static risk assessments into actionable guidance by modeling how specific interventions alter critical parameters. Using causal inference techniques, we can assess not just the probability of adverse outcomes but how those probabilities change under different policy regimes.\nThis enables genuinely evidence-based policy development:\n\nComparing interventions across multiple worldviews\nIdentifying robust strategies that work across scenarios\nUnderstanding which uncertainties most affect policy effectiveness\nPrioritizing research to reduce decision-relevant uncertainty\n\n\nPearl (2000) and Pearl (2009)\n\n\n1.5.4 The Synergy\n\nThe synergy emerges because automation enables comprehensive data integration, markets inform and validate models, and evaluation gains precision from both automated extraction and market-based calibration. The complete system creates feedback loops where policy analysis identifies critical uncertainties for market attention.\n\nThe multiplicative benefits emerge from the interactions between components:\n\nAutomation enables comprehensive coverage, making prediction market integration more valuable by connecting to more perspectives\nMarket data validates and calibrates automated extractions, improving quality\nPolicy evaluation gains precision from both comprehensive models and live probability updates\nThe complete system creates feedback loops where policy analysis identifies critical uncertainties for market attention\n\nThis synergistic combination addresses the coordination crisis by providing common ground for disparate communities, translating between technical and policy languages, quantifying previously implicit disagreements, and enabling evidence-based compromise.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-roadmap",
    "href": "chapters/Outlines/Outline_13.html#sec-roadmap",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "1.6 Thesis Structure and Roadmap",
    "text": "1.6 Thesis Structure and Roadmap\n\nThe remainder of this thesis develops the multiplicative benefits framework from theoretical foundations to practical implementation:\n\nChapter 2: Context and Theoretical Foundations establishes the intellectual groundwork, examining the epistemic challenges unique to AI governance, Bayesian networks as formal tools for uncertainty representation, argument mapping as a bridge from natural language to formal models, the MTAIR project’s achievements and limitations, and requirements for effective coordination infrastructure.\nChapter 3: AMTAIR Design and Implementation presents the technical system including overall architecture and design principles, the two-stage extraction pipeline (ArgDown → BayesDown), validation methodology and results, case studies from simple examples to complex AI risk models, and integration with prediction markets and policy evaluation.\nChapter 4: Discussion - Implications and Limitations critically examines technical limitations and failure modes, conceptual concerns about formalization, integration with existing governance frameworks, scaling challenges and opportunities, and broader implications for epistemic security.\nChapter 5: Conclusion synthesizes key contributions and charts paths forward with a summary of theoretical and practical achievements, concrete recommendations for stakeholders, research agenda for community development, and vision for AI governance with proper coordination infrastructure.\n\nThroughout this progression, I maintain dual focus on theoretical sophistication and practical utility. The framework aims not merely to advance academic understanding but to provide actionable tools for improving coordination in AI governance during this critical period.\n\nHaving established the coordination crisis and outlined how automated modeling can address it, we now turn to the theoretical foundations that make this approach possible. The next chapter examines the unique epistemic challenges of AI governance and introduces the formal tools—particularly Bayesian networks—that enable rigorous reasoning under deep uncertainty.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-carlsmith-model",
    "href": "chapters/Outlines/Outline_13.html#sec-carlsmith-model",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "2.1 AI Existential Risk: The Carlsmith Model",
    "text": "2.1 AI Existential Risk: The Carlsmith Model\n\n\nCarlsmith’s “Is Power-Seeking AI an Existential Risk?” (2021) represents one of the most structured approaches to assessing the probability of existential catastrophe from advanced AI. The analysis decomposes the overall risk into six key premises, each with an explicit probability estimate.\n\nTo ground our discussion in concrete terms, I examine Joseph Carlsmith’s “Is Power-Seeking AI an Existential Risk?” as an exemplar of structured reasoning about AI catastrophic risk. Carlsmith’s analysis stands out for its explicit probabilistic decomposition of the path from current AI development to potential existential catastrophe.\n\n\nCarlsmith (2024), Carlsmith (2021) and Carlsmith (2022)\n\n2.1.1 Six-Premise Decomposition\n\n\nAccording to the MTAIR model Carlsmith decomposes existential risk into a probabilistic chain with explicit estimates:\n\nPremise 1: Transformative AI development this century \\((P ≈ 0.80)\\)\nPremise 2: AI systems pursuing objectives in the world \\((P ≈ 0.95)\\)\nPremise 3: Systems with power-seeking instrumental incentives \\((P ≈ 0.40)\\)\nPremise 4: Sufficient capability for existential threat \\((P ≈ 0.65)\\)\nPremise 5: Misaligned systems despite safety efforts \\((P ≈ 0.50)\\)\nPremise 6: Catastrophic outcomes from misaligned power-seeking \\((P ≈ 0.65)\\)\n\nComposite Risk Calculation: \\(P(doom) ≈ 0.05\\) (5%)\n\n\nCarlsmith structures his argument through six conditional premises, each assigned explicit probability estimates:\nPremise 1: APS Systems by 2070 \\((P ≈ 0.65)\\)1 “By 2070, there will be AI systems with Advanced capability, Agentic planning, and Strategic awareness”—the conjunction of capabilities that could enable systematic pursuit of objectives in the world.\nPremise 2: Alignment Difficulty $(P ≈ 0.40) $ “It will be harder to build aligned APS systems than misaligned systems that are still attractive to deploy”—capturing the challenge that safety may conflict with capability or efficiency.\nPremise 3: Deployment Despite Misalignment $(P ≈ 0.70) $ “Conditional on 1 and 2, we will deploy misaligned APS systems”—reflecting competitive pressures and limited coordination.\nPremise 4: Power-Seeking Behavior $(P ≈ 0.65) $ “Conditional on 1-3, misaligned APS systems will seek power in high-impact ways”—based on instrumental convergence arguments.\nPremise 5: Disempowerment Success $(P ≈ 0.40) $ “Conditional on 1-4, power-seeking will scale to permanent human disempowerment”—despite potential resistance and safeguards.\nPremise 6: Existential Catastrophe $(P ≈ 0.95) $ “Conditional on 1-5, this disempowerment constitutes existential catastrophe”—connecting power loss to permanent curtailment of human potential.\nOverall Risk: Multiplying through the conditional chain yields \\(P(doom) ≈ 0.05\\) or 5% by 2070.\nThis structured approach exemplifies the type of reasoning AMTAIR aims to formalize and automate. While Carlsmith spent months developing this model manually, similar rigor exists implicitly in many AI safety arguments awaiting extraction.\n\n\n\n2.1.2 Why Carlsmith Exemplifies Formalizable Arguments\n\nCarlsmith’s model represents “low-hanging fruit” for automated formalization because it already exhibits explicit probabilistic reasoning with clear conditional dependencies. Success with this structured argument validates the approach for less explicit arguments throughout AI safety literature.\n\nCarlsmith’s model demonstrates several features that make it ideal for formal representation:\nExplicit Probabilistic Structure: Each premise receives numerical probability estimates with documented reasoning, enabling direct translation to Bayesian network parameters.\nClear Conditional Dependencies: The logical flow from capabilities through deployment decisions to catastrophic outcomes maps naturally onto directed acyclic graphs.\nTransparent Decomposition: Breaking the argument into modular premises allows independent evaluation and sensitivity analysis of each component.\nDocumented Reasoning: Extensive justification for each probability enables extraction of both structure and parameters from the source text.\n\n\n\nChristiano (2019)",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-epistemic-challenge",
    "href": "chapters/Outlines/Outline_13.html#sec-epistemic-challenge",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "2.2 The Epistemic Challenge of Policy Evaluation",
    "text": "2.2 The Epistemic Challenge of Policy Evaluation\n\nAI governance policy evaluation faces unique epistemic challenges that render traditional policy analysis methods insufficient. Understanding these challenges motivates the need for new computational approaches.\n\n2.2.1 Unique Characteristics of AI Governance\n\nAI governance policy evaluation faces unique epistemic challenges that render traditional policy analysis methods insufficient. The domain combines complex causal chains with limited empirical grounding, deep uncertainty about future capabilities, divergent stakeholder worldviews, and few opportunities for experimental testing before deployment.\n\nDeep Uncertainty Rather Than Risk: Traditional policy analysis distinguishes between risk (known probability distributions) and uncertainty (known possibilities, unknown probabilities). AI governance faces deep uncertainty—we cannot confidently enumerate possible futures, much less assign probabilities. Will recursive self-improvement enable rapid capability gains? Can value alignment be solved technically? These foundational questions resist empirical resolution before their answers become catastrophically relevant.\nComplex Multi-Level Causation: Policy effects propagate through technical, institutional, and social levels with intricate feedback loops. A technical standard might alter research incentives, shifting capability development trajectories, changing competitive dynamics, and ultimately affecting existential risk through pathways invisible at the policy’s inception. Traditional linear causal models cannot capture these dynamics.\nIrreversibility and Lock-In: Many AI governance decisions create path dependencies that prove difficult or impossible to reverse. Early technical standards shape development trajectories. Institutional structures ossify. International agreements create sticky equilibria. Unlike many policy domains where course correction remains possible, AI governance mistakes may prove permanent.\nValue-Laden Technical Choices: The entanglement of technical and normative questions confounds traditional separation of facts and values. What constitutes “alignment”? How much capability development should we risk for economic benefits? Technical specifications embed ethical judgments that resist neutral expertise.\n\n\n\n2.2.2 Limitations of Traditional Approaches\n\nTraditional methods fall short in several ways. Cost-benefit analysis struggles with existential outcomes and deep uncertainty about unprecedented events. Scenario planning often lacks the probabilistic reasoning necessary for rigorous evaluation under uncertainty. Expert elicitation alone fails to formalize interdependencies between variables and make assumptions explicit. Qualitative approaches obscure crucial assumptions that drive conclusions, making it difficult to identify cruxes of disagreement.\n\nStandard policy evaluation tools prove inadequate for these challenges:\nCost-Benefit Analysis assumes commensurable outcomes and stable probability distributions. When potential outcomes include existential catastrophe with deeply uncertain probabilities, the mathematical machinery breaks down. Infinite negative utility resists standard decision frameworks.\nScenario Planning helps explore possible futures but typically lacks the probabilistic reasoning needed for decision-making under uncertainty. Without quantification, scenarios provide narrative richness but limited action guidance.\nExpert Elicitation aggregates specialist judgment but struggles with interdisciplinary questions where no single expert grasps all relevant factors. Moreover, experts often operate with different implicit models, making aggregation problematic.\nRed Team Exercises test specific plans but miss systemic risks emerging from component interactions. Gaming individual failures cannot reveal emergent catastrophic possibilities.\nThese limitations create a methodological gap: we need approaches that handle deep uncertainty, represent complex causation, quantify expert disagreement, and enable systematic exploration of intervention effects.\n\nHallegatte et al. (2012)\n\n\n2.2.3 The Underlying Epistemic Framework\n\n–&gt;\n\n\n2.2.4 Toward New Epistemic Tools\n\nThe inadequacy of traditional methods for AI governance creates an urgent need for new epistemic tools. These tools must:\n\nHandle Deep Uncertainty: Move beyond point estimates to represent ranges of possibilities\nCapture Complex Causation: Model multi-level interactions and feedback loops\nQuantify Disagreement: Make explicit where experts diverge and why\nEnable Systematic Analysis: Support rigorous comparison of policy options\n\nKey Insight\nThe computational approaches developed in this thesis—particularly Bayesian networks enhanced with automated extraction—directly address each of these requirements by providing formal frameworks for reasoning under uncertainty. ::\nfrom P. Tetlock (2022)\n\n\n\n\n\n\nFigure 2.1: Conditional-tree AI-risk forecasts\n\n\n\nfrom Gruetzemacher (2022)\n\n\n\n\n\n\nFigure 2.2: Bayes-net pruning → crux extraction → re-expansion\n\n\n\nfrom McCaslin et al. (2024)\n\n\n\n\n\n\nFigure 2.3: Conditional-tree Guide\n\n\n\nfrom McCaslin et al. (2024)\n\n\n\n\n\n\nFigure 2.4: Experts’ conditional-tree updates (2030-2070)\n\n\n\n\nMcCaslin et al. (2024)\n\nP. Tetlock (2022)\n\nGruetzemacher (2022)",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-bayesian-networks",
    "href": "chapters/Outlines/Outline_13.html#sec-bayesian-networks",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "2.3 Bayesian Networks as Knowledge Representation",
    "text": "2.3 Bayesian Networks as Knowledge Representation\n\nBayesian networks offer a mathematical framework uniquely suited to addressing these epistemic challenges. By combining graphical structure with probability theory, they provide tools for reasoning about complex uncertain domains.\n\n2.3.1 Mathematical Foundations\nA Bayesian network consists of:\n\nDirected Acyclic Graph (DAG): Nodes represent variables, edges represent direct dependencies\nConditional Probability Tables (CPTs): For each node, P(node|parents) quantifies relationships\n\nThe joint probability distribution factors according to the graph structure:  \\[\nP(X1,X2,...,Xn)=∏i=1nP(Xi∣Parents(Xi))P(X_1, X_2, ..., X_n)\n= \\prod_{i=1}^{n} P(X_i | Parents(X_i))P(X1​,X2​,...,Xn​)=i=1∏n​P(Xi​∣Parents(Xi​))\n\\]\nThis factorization enables efficient inference and embodies causal assumptions explicitly.\n\nPearl (2014)\n\n\n\n2.3.2 The Rain-Sprinkler-Grass Example\n\nThe canonical example illustrates key concepts:2\n\n[Grass_Wet]: Concentrated moisture on grass. \n + [Rain]: Water falling from sky.\n + [Sprinkler]: Artificial watering system.\n   + [Rain]\nNetwork Structure:\n\nRain (root cause): P(rain) = 0.2\nSprinkler (intermediate): P(sprinkler|rain) varies by rain state\nGrass_Wet (effect): P(wet|rain, sprinkler) depends on both causes\n\n\npython\n# Basic network representation\nnodes = ['Rain', 'Sprinkler', 'Grass_Wet']\nedges = [('Rain', 'Sprinkler'), ('Rain', 'Grass_Wet'), ('Sprinkler', 'Grass_Wet')]\n\n# Conditional probability specification\nP_wet_given_causes = {\n    (True, True): 0.99,    # Rain=T, Sprinkler=T\n    (True, False): 0.80,   # Rain=T, Sprinkler=F  \n    (False, True): 0.90,   # Rain=F, Sprinkler=T\n    (False, False): 0.01   # Rain=F, Sprinkler=F\n}\nThis simple network demonstrates:\n\nMarginal Inference: P(grass_wet) computed from joint distribution\nDiagnostic Reasoning: P(rain|grass_wet) reasoning from effects to causes\nIntervention Modeling: P(grass_wet|do(sprinkler=on)) for policy analysis\n\n\n\nRain-Sprinkler-Grass Network Rendering\n\n\nCode\nfrom IPython.display import IFrame\n\nIFrame(src=\"https://singularitysmith.github.io/AMTAIR_Prototype/bayesian_network.html\", width=\"100%\", height=\"600px\")\n\n\n\n        \n        \nDynamic Html Rendering of the Rain-Sprinkler-Grass DAG with Conditional Probabilities\n\n\n\n\n\n2.3.3 Advantages for AI Risk Modeling\n\nBayesian networks offer several key advantages for AI risk modeling. They provide explicit uncertainty representation where all beliefs are represented with probability distributions rather than point estimates. The framework naturally supports causal reasoning through native support for intervention analysis and counterfactual reasoning via do-calculus. Evidence integration becomes principled through Bayesian updating mechanisms. The modular structure allows complex arguments to be decomposed into manageable, verifiable components. Finally, the visual communication provided by graphical representation facilitates understanding across different expertise levels.\n\nThese features address key requirements for AI governance:\n\nHandling Uncertainty: Every parameter is a distribution, not a point estimate\nRepresenting Causation: Directed edges embody causal relationships\nEnabling Analysis: Formal inference algorithms support systematic evaluation\nFacilitating Communication: Visual structure aids cross-domain understanding",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-argument-mapping",
    "href": "chapters/Outlines/Outline_13.html#sec-argument-mapping",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "2.4 Argument Mapping and Formal Representations",
    "text": "2.4 Argument Mapping and Formal Representations\n\nThe gap between natural language arguments and formal models requires systematic bridging. Argument mapping provides methods for making implicit reasoning structures explicit and analyzable.\n\n2.4.1 From Natural Language to Structure\nNatural language arguments contain rich information expressed through:\n\nCausal claims (“X leads to Y”)\nConditional relationships (“If A then likely B”)\nUncertainty expressions (“probably,” “might,” “certainly”)\nSupport/attack patterns between claims\n\n\nArgument mapping extracts this structure, identifying:\n\nCore claims and propositions\nInferential relationships\nImplicit assumptions\nUncertainty qualifications\n\n\nfrom Metropolitansky and Larson (2025)\n\n\n\n\n\n\nFigure 2.5: Claimify claim-extraction stages\n\n\n\n\nAnderson (2007)\nBenn and Macintosh (2011)\nD. Khartabil et al. (2021)\nDana Khartabil (2020)\nNgajie et al. (2020)\nProkudin, Lisanyuk, and Baymuratov (2024)\nScheuer et al. (2010)\nKuhn (1962)\nWalton (2009)\n\n\n2.4.2 ArgDown: Structured Argument Notation\n\nVoigt ([2014] 2025)\nArgDown provides a markdown-like syntax for hierarchical argument representation:\n[MainClaim]: Description of primary conclusion.\n + [SupportingEvidence]: Evidence supporting the claim.\n   + [SubEvidence]: More specific support.\n - [CounterArgument]: Evidence against the claim.\nThis notation captures argument structure while remaining human-readable and writable. Crucially, it serves as an intermediate representation between natural language and formal models.\n\nArgument mapping provides a bridge between natural language reasoning and formal probabilistic models, enabling the transformation of complex qualitative arguments into structured representations suitable for computational analysis. This section explores two key intermediate representations—ArgDown and BayesDown—that facilitate this transformation process.\nArgument maps are structured visualizations that represent the logical relationships between claims, evidence, and objections. Unlike free-form text, they make explicit how different statements support or challenge one another, forcing clarity about the logical structure of arguments. Traditional argument maps typically include:\n\nStatements (claims, premises, conclusions) presented as nodes\nSupport and attack relationships shown as arrows between nodes\nHierarchical organization reflecting logical dependencies\n\nThese visualizations help identify unstated assumptions, circular reasoning, and gaps in argumentation. However, traditional argument mapping has limited expressivity for representing uncertainty—a crucial element in complex domains like AI risk assessment.\nArgDown extends the concept of argument mapping into a structured text format with a consistent syntax. Developed by Christian Voigt with support from the Karlsruhe Institute of Technology, ArgDown provides a markdown-like notation for representing arguments in a hierarchical structure that can be automatically visualized and analyzed. The basic syntax is:\nargdown\n[Statement]: Description of the statement.\n + [Supporting_Statement]: Description of supporting statement.\n   + [Further_Support]: Description of additional support.\n - [Opposing_Statement]: Description of opposing statement.\nFor the AMTAIR project, we adapt ArgDown to focus on causal relationships rather than general argumentation, using a modified syntax where the hierarchical structure represents causal influence:\n[Effect]: Description of effect. {\"instantiations\": [\"effect_TRUE\", \"effect_FALSE\"]}\n + [Cause1]: Description of first cause. {\"instantiations\": [\"cause1_TRUE\", \"cause1_FALSE\"]}\n + [Cause2]: Description of second cause. {\"instantiations\": [\"cause2_TRUE\", \"cause2_FALSE\"]}\n   + [Root_Cause]: A cause that influences Cause2. {\"instantiations\": [\"root_TRUE\", \"root_FALSE\"]}\nThis adaptation adds metadata in JSON format to specify possible states (instantiations) of each variable, preparing the structure for probabilistic enhancement. The hierarchical relationships (indented with plus signs) represent causal influence, creating a directed graph structure.\n\n\n2.4.3 BayesDown: The Bridge to Bayesian Networks\n\n\n\nBayesDown extends ArgDown with probabilistic metadata:\n[Node]: Description. {\n  \"instantiations\": [\"node_TRUE\", \"node_FALSE\"],\n  \"priors\": {\"p(node_TRUE)\": \"0.7\", \"p(node_FALSE)\": \"0.3\"},\n  \"posteriors\": {\n    \"p(node_TRUE|parent_TRUE)\": \"0.9\",\n    \"p(node_TRUE|parent_FALSE)\": \"0.4\"\n  }\n}\n[Node]: Description. {\n  \"instantiations\": [\"node_TRUE\", \"node_FALSE\"],\n  \"priors\": {\n    \"p(node_TRUE)\": \"0.7\",\n    \"p(node_FALSE)\": \"0.3\"\n  },\n  \"posteriors\": {\n    \"p(node_TRUE|parent_TRUE)\": \"0.9\",\n    \"p(node_TRUE|parent_FALSE)\": \"0.4\",\n    \"p(node_FALSE|parent_TRUE)\": \"0.1\",\n    \"p(node_FALSE|parent_FALSE)\": \"0.6\"\n  }\n}\n\nThis representation:\n\nPreserves narrative structure from the original argument\nAdds mathematical precision through probability specifications\nEnables transformation to standard Bayesian network formats\nSupports validation by maintaining traceability to sources\n\nThe two-stage extraction process (ArgDown → BayesDown) separates concerns: first capturing structure, then quantifying relationships. This modularity enables human oversight at critical decision points.\n\nThe intermediate representations (ArgDown and BayesDown) remain human-readable, maintaining the connection to the original arguments while enabling computational analysis.\nThe key innovation in this approach is the separation of structure extraction from probability quantification, which aligns with how experts typically approach complex arguments. First, they identify what factors matter and how they relate causally, then they consider how probable different scenarios are based on those relationships. This two-stage process makes the extraction more robust and the resulting representations more interpretable.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-mtair-framework",
    "href": "chapters/Outlines/Outline_13.html#sec-mtair-framework",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "2.5 The MTAIR Framework: Achievements and Limitations",
    "text": "2.5 The MTAIR Framework: Achievements and Limitations\n\nThe Modeling Transformative AI Risks (MTAIR) project, led by RAND researchers, pioneered formal modeling of AI existential risk arguments. Understanding its approach and limitations motivates the automation efforts of AMTAIR.\n\n2.5.1 MTAIR’s Approach\nThe Modeling Transformative AI Risks (MTAIR) project, led by David Manheim and colleagues, represents a significant precursor to the current research. Launched in 2021, MTAIR aimed to create structured representations of existential risks from advanced AI using Bayesian networks, directed acyclic graphs, and probabilistic modeling. Understanding its achievements and limitations provides important context for the current AMTAIR approach.\nMTAIR emerged from the recognition that AI risk discussions often involved complex causal arguments with implicit probability judgments that were difficult to compare or integrate. By formalizing these arguments in structured models, the project sought to make assumptions explicit, enable quantitative analysis, and facilitate more productive discourse across different perspectives on AI risk.\nThe framework’s key innovations included:\n\nExplicit representation of uncertainty through probability distributions: Rather than presenting point estimates, MTAIR captured uncertainty about parameters using distributions, acknowledging the significant uncertainty in AI risk assessment.\nHierarchical structure for complex scenarios: The approach used nested models that allowed exploration of different levels of detail, from high-level risk factors to specific technical mechanisms.\nIntegration of diverse expert judgments: The framework incorporated perspectives from various specialists, creating a more comprehensive view than any single expert could provide.\nSensitivity analysis methodology: MTAIR developed techniques for identifying which parameters most significantly affected risk estimates, helping prioritize research efforts.\n\nThe project’s practical impact extended beyond its technical achievements. It influenced research prioritization by identifying critical uncertainties that warranted further investigation. It enhanced discourse quality by providing a shared vocabulary and structure for discussing causal pathways to risk. It also created visual representations that made complex arguments more accessible to stakeholders without technical backgrounds.\nDespite these achievements, MTAIR faced several important limitations:\n\nManual labor intensity limiting scalability: Creating and updating models required substantial expert time, limiting the number and complexity of models that could be developed and maintained. As one team member noted, “It often took several days of work to formalize even relatively straightforward arguments.”\nStatic nature of models once constructed: The models were essentially snapshots that did not automatically update as new information emerged, requiring manual revision to remain current.\nLimited accessibility for non-technical stakeholders: While visual representations improved accessibility, understanding and interacting with the models still required specialized knowledge.\nChallenges in representing multiple worldviews simultaneously: Comparing different perspectives required creating separate models, making it difficult to identify specific points of agreement and disagreement.\n\nThese limitations motivate the current research in automating the extraction and transformation process. As AI capabilities advance and the volume of relevant research grows, manual approaches cannot keep pace with the need for comprehensive, up-to-date models. Automation addresses the scalability limitation by dramatically reducing the time required to create formal representations of expert arguments.\nMoreover, incorporating frontier LLMs into the pipeline enables new capabilities that were not feasible in the original MTAIR framework. These include:\n\nProcessing larger volumes of literature to capture more diverse perspectives\nGenerating intermediate representations that preserve narrative structure\nAutomating the creation of probability questions based on model structure\nFacilitating integration with live data sources for continuous updates\n\nBy building on MTAIR’s foundation while addressing its key limitations, the current research maintains continuity with established approaches to AI risk modeling while pushing the boundaries of what’s possible through automation and enhanced representation formats.\nThe evolution from MTAIR to AMTAIR represents a natural progression: as the field matures and the challenges become more pressing, more sophisticated tools are needed to facilitate coordination and decision-making. Automation doesn’t replace expert judgment but amplifies it, allowing insights to be captured, formalized, and shared more efficiently across the AI governance community.\n\nThe Modeling Transformative AI Risks (MTAIR) project demonstrated the value of formal probabilistic modeling for AI safety, but also revealed significant limitations in the manual approach. While MTAIR successfully translated complex arguments into Bayesian networks and enabled sensitivity analysis, the intensive human labor required for model creation limited both scalability and timeliness.\n\nMTAIR manually translated influential AI risk arguments into Bayesian networks using Analytica software:\nSystematic Decomposition: Breaking complex arguments into variables and relationships through expert analysis.\nProbability Elicitation: Gathering quantitative estimates through structured expert interviews and literature review.\nSensitivity Analysis: Identifying which parameters most influence conclusions about AI risk levels.\nVisual Communication: Creating interactive models that stakeholders could explore and modify.\n\nClarke et al. (2022)\n\n\n2.5.2 Key Achievements\nMTAIR demonstrated several important possibilities:\nFeasibility of Formalization: Complex philosophical arguments about AI risk can be represented as Bayesian networks while preserving essential insights.\nValue of Quantification: Moving from qualitative concerns to quantitative models enables systematic analysis, comparison, and prioritization.\nCross-Perspective Communication: Formal models provide common ground for technical and policy communities to engage productively.\nResearch Prioritization: Sensitivity analysis reveals which empirical questions would most reduce uncertainty about AI risks.\n\n\n\n2.5.3 Fundamental Limitations\n\nDespite its innovations, MTAIR faces fundamental limitations that motivate the automated approach. The scalability bottleneck is severe—manual model construction requires weeks of expert effort per argument, making comprehensive coverage impossible. The static nature of manually constructed models provides no mechanisms for updating as new research and evidence emerge. Limited accessibility restricts usage to specialists with formal modeling expertise, excluding many stakeholders. Finally, the single worldview focus creates difficulty in representing multiple conflicting perspectives simultaneously, limiting the framework’s utility for coordination across diverse viewpoints.\n\nHowever, MTAIR’s manual approach faces severe constraints:\nLabor Intensity: Each model requires dozens of expert-hours to construct, limiting coverage to a few perspectives.\n\n\nStatic Nature: Models become outdated as arguments evolve but updating requires near-complete reconstruction.\nLimited Accessibility: Using the models requires Analytica software and significant technical sophistication.\nSingle Perspective: Each model represents one worldview, making comparison across perspectives difficult.\nThese limitations prevent MTAIR’s approach from scaling to meet AI governance needs. As the pace of AI development accelerates and arguments proliferate, manual modeling cannot keep pace.\nfrom Clarke et al. (2022)\n\n\n\n\n\n\nFigure 2.6: MTAIR Qualitative map structure\n\n\n\nfrom Clarke et al. (2022)\n\n\n\n\n\n\nFigure 2.7: MTAIR Quantitative map structure\n\n\n\nfrom Manheim (2021)\n\n\n\n\n\n\nFigure 2.8: Base APS causal map\n\n\n\nfrom Manheim (2021)\n\n\n\n\n\n\nFigure 2.9: Overlay of inside/outside/assimilation views\n\n\n\n\n\n2.5.4 The Automation Opportunity\n\nMTAIR’s experience reveals both the value of formal modeling and the necessity of automation. Key lessons:\n\nFormal models genuinely enhance understanding and coordination\nThe modeling process itself surfaces implicit assumptions\nQuantification enables analyses impossible with qualitative arguments alone\nBut manual approaches cannot scale to match the challenge\n\n\nThis motivates AMTAIR’s central innovation: using frontier language models to automate the extraction and formalization process while preserving the benefits MTAIR demonstrated.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-literature-review",
    "href": "chapters/Outlines/Outline_13.html#sec-literature-review",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "2.6 Literature Review: Content and Technical Levels",
    "text": "2.6 Literature Review: Content and Technical Levels\n\nfrom Cottier and Shah (2019)\n\n\n\n\n\n\nFigure 2.10: Key hypotheses in AI alignment\n\n\n\n\n2.6.1 AI Risk Models Evolution\nThe evolution of AI risk models reflects increasing sophistication in both structure and quantification. Early models focused on simple binary outcomes, while recent work incorporates complex causal chains and continuous variables.\n\n\n\n\n\n\n## Key Developments\n\nEarly Phase (2000-2010): Qualitative arguments about intelligence explosion\nFormalization Phase (2010-2018): Introduction of structured scenarios\nQuantification Phase (2018-present): Explicit probability estimates and formal models\n\n\n\n\n\nYudkowsky (2008)\n\nBostrom (2014)\n\nAmodei et al. (2016)\nThe progression from qualitative arguments to structured probabilistic models demonstrates the field’s maturation and the increasing recognition that rigorous quantitative analysis is essential for policy evaluation.\n\n\n2.6.2 Governance Proposals Taxonomy\nAI governance proposals can be categorized along several dimensions:\n\nTechnical Standards: Safety requirements, testing protocols, capability thresholds\nRegulatory Frameworks: Licensing regimes, liability structures, oversight mechanisms\nInternational Coordination: Treaties, soft law arrangements, technical cooperation\nResearch Priorities: Funding allocation, talent development, knowledge sharing\n\n\nDafoe (2021) and Dafoe (2018)\n\n\nMiotti et al. (2024)\n\n\n\n\n2.6.3 Bayesian Network Theory and Applications\nThe theoretical foundations of Bayesian networks rest on probability theory and graph theory. Key concepts include:\n\nConditional Independence: Encoded through d-separation\nMarkov Condition: Relating graph structure to probabilistic relationships\nInference Algorithms: From exact methods to approximation approaches\n\n\nKoller and Friedman (2009)\n\n\n2.6.4 Software Tools Landscape\n\nThe implementation of AMTAIR builds on established software libraries:\n\npgmpy: Python library for probabilistic graphical models\nNetworkX: Graph analysis and manipulation capabilities\nPyVis: Interactive network visualization\nPandas/NumPy: Data manipulation and numerical computation\n\n\n\n\n\n2.6.5 Formalization Approaches\nFormalizing natural language arguments into mathematical models involves several theoretical challenges:\n\nSemantic Preservation: Maintaining meaning while adding precision\nStructural Extraction: Identifying implicit relationships\nUncertainty Quantification: Mapping qualitative to quantitative expressions\n\n\nPollock (1995)\n\n\n2.6.6 Correlation Accounting Methods\n\nStandard Bayesian networks assume conditional independence given parents, but real-world AI risk factors often exhibit complex correlations. Methods for handling correlations include:\n\nCopula Methods: Modeling dependence structures separately from marginal distributions\nHierarchical Models: Capturing correlations through shared latent variables\nExplicit Correlation Nodes: Adding nodes to represent correlation mechanisms\nSensitivity Bounds: Analyzing impact of independence assumptions\n\n\n\nNelson (2006)",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-methodology",
    "href": "chapters/Outlines/Outline_13.html#sec-methodology",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "2.7 Methodology",
    "text": "2.7 Methodology\n\n\n2.7.1 Research Design Overview\nThis research combines theoretical development with practical implementation, following an iterative approach that moves between conceptual refinement and technical validation.\n\nThe methodology encompasses formal framework development, computational implementation, extraction quality assessment, and application to real-world AI governance questions.\nThe research process follows four integrated phases:\n\nFramework Development: Creating theoretical foundations for automated worldview extraction\nTechnical Implementation: Building computational tools as working prototype\nEmpirical Validation: Assessing quality against expert benchmarks\nPolicy Application: Demonstrating practical utility for governance questions\n\n\n\n\n2.7.2 Formalizing World Models from AI Safety Literature\nThe core methodological challenge involves transforming natural language arguments in AI safety literature into formal causal models with explicit probability judgments.\n\nThis extraction process identifies key variables, causal relationships, and both explicit and implicit probability estimates through a systematic pipeline.\nThe extraction approach combines several elements:\n\nIdentification of key variables and entities in text\nRecognition of causal claims and relationships\nDetection of explicit and implicit probability judgments\nTransformation into structured intermediate representations\nConversion to formal Bayesian networks\n\n\nLarge language models facilitate this process through specialized techniques:\n\nTwo-stage prompting: Separating structure from probability extraction\nTemplate specialization: Different approaches for different document types\nImplicit assumption detection: Identifying unstated relationships\nAmbiguity handling: Managing uncertainty in extraction\n\n\n\n2.7.3 From Natural Language to Computational Models\n\n\n\n\n\n\nThe Two-Stage Extraction Process\n\n\n\nAMTAIR employs a novel two-stage process that separates structural argument extraction from probability quantification, enabling modular improvement and human oversight at critical decision points.\n\n\nThe heart of the AMTAIR approach lies in its two-stage extraction process, which transforms unstructured text into structured probabilistic models through distinct steps that mirror human cognitive processes. This separation—extracting structure before probability—creates important advantages for automation quality, intermediate verification, and interpretability.\nWhen humans analyze complex arguments, they typically first determine what factors matter and how they relate causally, then assess how likely different scenarios are based on those relationships. A climate scientist reading a paper first identifies key variables (emissions, warming, effects) and their causal connections before estimating probabilities of outcomes. This natural cognitive sequence inspired AMTAIR’s two-stage approach.\nStage 1: Structure Extraction focuses on identifying key variables and their causal relationships from text, transforming unstructured arguments into ArgDown format. This process involves:\n\nVariable identification: Determining the key factors discussed in the text, including their possible states (e.g., whether a factor is present/absent or has multiple levels)\nRelationship mapping: Establishing how variables influence each other, creating a directed graph of causal connections\nHierarchical organization: Arranging variables according to their causal relationships, from root causes to final effects\nMetadata attachment: Annotating each variable with its description and possible states in structured JSON format\n\nThe LLM prompt for this stage emphasizes clear identification of causal structure without requiring probability judgments, allowing the model to focus entirely on understanding “what affects what” in the text. This specialized prompt includes detailed instructions about ArgDown syntax, examples of well-formed representations, and guidance for preserving the author’s intended meaning.\n\n\n\nCode\n# @title 1.7.0 --- Parsing ArgDown & BayesDown (.md to .csv) --- [parsing_argdown_bayesdown]\n\n\"\"\"\nBLOCK PURPOSE: Provides the core parsing functionality for transforming ArgDown\nand BayesDown text representations into structured DataFrame format for further\nprocessing.\n\nThis block implements the critical extraction pipeline described in the AMTAIR\nproject (see PY_TechnicalImplementation) that converts argument structures\ninto Bayesian networks.\nThe function can handle both basic ArgDown (structure-only) and\nBayesDown (with probabilities).\n\nKey steps in the parsing process:\n1. Remove comments from the markdown text\n2. Extract titles, descriptions, and indentation levels\n3. Establish parent-child relationships based on indentation\n4. Convert the structured information into a DataFrame\n5. Add derived columns for network analysis\n\nDEPENDENCIES: pandas, re, json libraries\nINPUTS: Markdown text in ArgDown/BayesDown format\nOUTPUTS: Structured DataFrame with node information, relationships, and properties\n\"\"\"\n\ndef parse_markdown_hierarchy_fixed(markdown_text, ArgDown=False):\n    \"\"\"\n    Parse ArgDown or BayesDown format into a structured DataFrame with parent-child relationships.\n\n    Args:\n        markdown_text (str): Text in ArgDown or BayesDown format\n        ArgDown (bool): If True, extracts only structure without probabilities\n                        If False, extracts both structure and probability information\n\n    Returns:\n        pandas.DataFrame: Structured data with node information, relationships, and attributes\n    \"\"\"\n    # PHASE 1: Clean and prepare the text\n    clean_text = remove_comments(markdown_text)\n\n    # PHASE 2: Extract basic information about nodes\n    titles_info = extract_titles_info(clean_text)\n\n    # PHASE 3: Determine the hierarchical relationships\n    titles_with_relations = establish_relationships_fixed(titles_info, clean_text)\n\n    # PHASE 4: Convert to structured DataFrame format\n    df = convert_to_dataframe(titles_with_relations, ArgDown)\n\n    # PHASE 5: Add derived columns for analysis\n    df = add_no_parent_no_child_columns_to_df(df)\n    df = add_parents_instantiation_columns_to_df(df)\n\n    return df\n\ndef remove_comments(markdown_text):\n    \"\"\"\n    Remove comment blocks from markdown text using regex pattern matching.\n\n    Args:\n        markdown_text (str): Text containing potential comment blocks\n\n    Returns:\n        str: Text with comment blocks removed\n    \"\"\"\n    # Remove anything between /* and */ using regex\n    return re.sub(r'/\\*.*?\\*/', '', markdown_text, flags=re.DOTALL)\n\ndef extract_titles_info(text):\n    \"\"\"\n    Extract titles with their descriptions and indentation levels from markdown text.\n\n    Args:\n        text (str): Cleaned markdown text\n\n    Returns:\n        dict: Dictionary with titles as keys and dictionaries of attributes as values\n    \"\"\"\n    lines = text.split('\\n')\n    titles_info = {}\n\n    for line in lines:\n        # Skip empty lines\n        if not line.strip():\n            continue\n\n        # Extract title within square or angle brackets\n        title_match = re.search(r'[&lt;\\[](.+?)[&gt;\\]]', line)\n        if not title_match:\n            continue\n\n        title = title_match.group(1)\n\n        # Extract description and metadata\n        title_pattern_in_line = r'[&lt;\\[]' + re.escape(title) + r'[&gt;\\]]:'\n        description_match = re.search(title_pattern_in_line + r'\\s*(.*)', line)\n\n        if description_match:\n            full_text = description_match.group(1).strip()\n\n            # Split description and metadata at the first \"{\"\n            if \"{\" in full_text:\n                split_index = full_text.find(\"{\")\n                description = full_text[:split_index].strip()\n                metadata = full_text[split_index:].strip()\n            else:\n                # Keep the entire description and no metadata\n                description = full_text\n                metadata = ''  # Initialize as empty string\n        else:\n            description = ''\n            metadata = ''  # Ensure metadata is initialized\n\n        # Calculate indentation level based on spaces before + or - symbol\n        indentation = 0\n        if '+' in line:\n            symbol_index = line.find('+')\n            # Count spaces before the '+' symbol\n            i = symbol_index - 1\n            while i &gt;= 0 and line[i] == ' ':\n                indentation += 1\n                i -= 1\n        elif '-' in line:\n            symbol_index = line.find('-')\n            # Count spaces before the '-' symbol\n            i = symbol_index - 1\n            while i &gt;= 0 and line[i] == ' ':\n                indentation += 1\n                i -= 1\n\n        # If neither symbol exists, indentation remains 0\n\n        if title in titles_info:\n            # Only update description if it's currently empty and we found a new one\n            if not titles_info[title]['description'] and description:\n                titles_info[title]['description'] = description\n\n            # Store all indentation levels for this title\n            titles_info[title]['indentation_levels'].append(indentation)\n\n            # Keep max indentation for backward compatibility\n            if indentation &gt; titles_info[title]['indentation']:\n                titles_info[title]['indentation'] = indentation\n\n            # Do NOT update metadata here - keep the original metadata\n        else:\n            # First time seeing this title, create a new entry\n            titles_info[title] = {\n                'description': description,\n                'indentation': indentation,\n                'indentation_levels': [indentation],  # Initialize with first indentation level\n                'parents': [],\n                'children': [],\n                'line': None,\n                'line_numbers': [],  # Initialize an empty list for all occurrences\n                'metadata': metadata  # Set metadata explicitly from what we found\n            }\n\n    return titles_info\n\ndef establish_relationships_fixed(titles_info, text):\n    \"\"\"\n    Establish parent-child relationships between titles using BayesDown\n    indentation rules.\n\n    In BayesDown syntax:\n    - More indented nodes (with + symbol) are PARENTS of less indented nodes\n    - The relationship reads as \"Effect is caused by Cause\" (Effect + Cause)\n    - This aligns with how Bayesian networks represent causality\n\n    Args:\n        titles_info (dict): Dictionary with information about titles\n        text (str): Original markdown text (for identifying line numbers)\n\n    Returns:\n        dict: Updated dictionary with parent-child relationships\n    \"\"\"\n    lines = text.split('\\n')\n\n    # Dictionary to store line numbers for each title occurrence\n    title_occurrences = {}\n\n    # Record line number for each title (including multiple occurrences)\n    line_number = 0\n    for line in lines:\n        if not line.strip():\n            line_number += 1\n            continue\n\n        title_match = re.search(r'[&lt;\\[](.+?)[&gt;\\]]', line)\n        if not title_match:\n            line_number += 1\n            continue\n\n        title = title_match.group(1)\n\n        # Store all occurrences of each title with their line numbers\n        if title not in title_occurrences:\n            title_occurrences[title] = []\n        title_occurrences[title].append(line_number)\n\n        # Store all line numbers where this title appears\n        if 'line_numbers' not in titles_info[title]:\n            titles_info[title]['line_numbers'] = []\n        titles_info[title]['line_numbers'].append(line_number)\n\n        # For backward compatibility, keep the first occurrence in 'line'\n        if titles_info[title]['line'] is None:\n            titles_info[title]['line'] = line_number\n\n        line_number += 1\n\n    # Create an ordered list of all title occurrences with their line numbers\n    all_occurrences = []\n    for title, occurrences in title_occurrences.items():\n        for line_num in occurrences:\n            all_occurrences.append((title, line_num))\n\n    # Sort occurrences by line number\n    all_occurrences.sort(key=lambda x: x[1])\n\n    # Get indentation for each occurrence\n    occurrence_indents = {}\n    for title, line_num in all_occurrences:\n        for line in lines[line_num:line_num+1]:  # Only check the current line\n            indent = 0\n            if '+' in line:\n                symbol_index = line.find('+')\n                # Count spaces before the '+' symbol\n                j = symbol_index - 1\n                while j &gt;= 0 and line[j] == ' ':\n                    indent += 1\n                    j -= 1\n            elif '-' in line:\n                symbol_index = line.find('-')\n                # Count spaces before the '-' symbol\n                j = symbol_index - 1\n                while j &gt;= 0 and line[j] == ' ':\n                    indent += 1\n                    j -= 1\n            occurrence_indents[(title, line_num)] = indent\n\n    # Enhanced backward pass for correct parent-child relationships\n    for i, (title, line_num) in enumerate(all_occurrences):\n        current_indent = occurrence_indents[(title, line_num)]\n\n        # Skip root nodes (indentation 0) for processing\n        if current_indent == 0:\n            continue\n\n        # Look for the immediately preceding node with lower indentation\n        j = i - 1\n        while j &gt;= 0:\n            prev_title, prev_line = all_occurrences[j]\n            prev_indent = occurrence_indents[(prev_title, prev_line)]\n\n            # If we find a node with less indentation, it's a child of current node\n            if prev_indent &lt; current_indent:\n                # In BayesDown:\n                # More indented node is a parent (cause) of less indented node (effect)\n                if title not in titles_info[prev_title]['parents']:\n                    titles_info[prev_title]['parents'].append(title)\n                if prev_title not in titles_info[title]['children']:\n                    titles_info[title]['children'].append(prev_title)\n\n                # Only need to find the immediate child\n                # (closest preceding node with lower indentation)\n                break\n\n            j -= 1\n\n    return titles_info\n\ndef convert_to_dataframe(titles_info, ArgDown):\n    \"\"\"\n    Convert the titles information dictionary to a pandas DataFrame.\n\n    Args:\n        titles_info (dict): Dictionary with information about titles\n        ArgDown (bool): If True, extract only structural information without probabilities\n\n    Returns:\n        pandas.DataFrame: Structured data with node information and relationships\n    \"\"\"\n    if ArgDown == True:\n        # For ArgDown, exclude probability columns\n        df = pd.DataFrame(columns=['Title', 'Description', 'line', 'line_numbers', 'indentation',\n                               'indentation_levels', 'Parents', 'Children', 'instantiations'])\n    else:\n        # For BayesDown, include probability columns\n        df = pd.DataFrame(columns=['Title', 'Description', 'line', 'line_numbers', 'indentation',\n                               'indentation_levels', 'Parents', 'Children', 'instantiations',\n                               'priors', 'posteriors'])\n\n    for title, info in titles_info.items():\n        # Parse the metadata JSON string into a Python dictionary\n        if 'metadata' in info and info['metadata']:\n            try:\n                # Only try to parse if metadata is not empty\n                if info['metadata'].strip():\n                    jsonMetadata = json.loads(info['metadata'])\n                    if ArgDown == True:\n                        # Create the row dictionary with instantiations as\n                        # metadata only, no probabilities yet\n                        row = {\n                            'Title': title,\n                            'Description': info.get('description', ''),\n                            'line': info.get('line',''),\n                            'line_numbers': info.get('line_numbers', []),\n                            'indentation': info.get('indentation',''),\n                            'indentation_levels': info.get('indentation_levels', []),\n                            'Parents': info.get('parents', []),\n                            'Children': info.get('children', []),\n                            # Extract specific metadata fields,\n                            # defaulting to empty if not present\n                            'instantiations': jsonMetadata.get('instantiations', []),\n                        }\n                    else:\n                        # Create dict with probabilities for BayesDown\n                        row = {\n                            'Title': title,\n                            'Description': info.get('description', ''),\n                            'line': info.get('line',''),\n                            'line_numbers': info.get('line_numbers', []),\n                            'indentation': info.get('indentation',''),\n                            'indentation_levels': info.get('indentation_levels', []),\n                            'Parents': info.get('parents', []),\n                            'Children': info.get('children', []),\n                            # Extract specific metadata fields, defaulting to empty if not present\n                            'instantiations': jsonMetadata.get('instantiations', []),\n                            'priors': jsonMetadata.get('priors', {}),\n                            'posteriors': jsonMetadata.get('posteriors', {})\n                        }\n                else:\n                    # Empty metadata case\n                    row = {\n                        'Title': title,\n                        'Description': info.get('description', ''),\n                        'line': info.get('line',''),\n                        'line_numbers': info.get('line_numbers', []),\n                        'indentation': info.get('indentation',''),\n                        'indentation_levels': info.get('indentation_levels', []),\n                        'Parents': info.get('parents', []),\n                        'Children': info.get('children', []),\n                        'instantiations': [],\n                        'priors': {},\n                        'posteriors': {}\n                    }\n            except json.JSONDecodeError:\n                # Handle case where metadata isn't valid JSON\n                row = {\n                    'Title': title,\n                    'Description': info.get('description', ''),\n                    'line': info.get('line',''),\n                    'line_numbers': info.get('line_numbers', []),\n                    'indentation': info.get('indentation',''),\n                    'indentation_levels': info.get('indentation_levels', []),\n                    'Parents': info.get('parents', []),\n                    'Children': info.get('children', []),\n                    'instantiations': [],\n                    'priors': {},\n                    'posteriors': {}\n                }\n        else:\n            # Handle case where metadata field doesn't exist or is empty\n            row = {\n                'Title': title,\n                'Description': info.get('description', ''),\n                'line': info.get('line',''),\n                'line_numbers': info.get('line_numbers', []),\n                'indentation': info.get('indentation',''),\n                'indentation_levels': info.get('indentation_levels', []),\n                'Parents': info.get('parents', []),\n                'Children': info.get('children', []),\n                'instantiations': [],\n                'priors': {},\n                'posteriors': {}\n            }\n\n        # Add the row to the DataFrame\n        df.loc[len(df)] = row\n\n    return df\n\ndef add_no_parent_no_child_columns_to_df(dataframe):\n    \"\"\"\n    Add No_Parent and No_Children boolean columns to the DataFrame to\n    identify root and leaf nodes.\n\n    Args:\n        dataframe (pandas.DataFrame): The DataFrame to enhance\n\n    Returns:\n        pandas.DataFrame: Enhanced DataFrame with additional boolean columns\n    \"\"\"\n    no_parent = []\n    no_children = []\n\n    for _, row in dataframe.iterrows():\n        no_parent.append(not row['Parents'])  # True if Parents list is empty\n        no_children.append(not row['Children'])  # True if Children list is empty\n\n    dataframe['No_Parent'] = no_parent\n    dataframe['No_Children'] = no_children\n\n    return dataframe\n\ndef add_parents_instantiation_columns_to_df(dataframe):\n    \"\"\"\n    Add all possible instantiations of parents as a list of lists column\n    to the DataFrame.\n    This is crucial for generating conditional probability tables.\n\n    Args:\n        dataframe (pandas.DataFrame): The DataFrame to enhance\n\n    Returns:\n        pandas.DataFrame: Enhanced DataFrame with parent_instantiations column\n    \"\"\"\n    # Create a new column to store parent instantiations\n    parent_instantiations = []\n\n    # Iterate through each row in the dataframe\n    for _, row in dataframe.iterrows():\n        parents = row['Parents']\n        parent_insts = []\n\n        # For each parent, find its instantiations and add to the list\n        for parent in parents:\n            # Find the row where Title matches the parent\n            parent_row = dataframe[dataframe['Title'] == parent]\n\n            # If parent found in the dataframe\n            if not parent_row.empty:\n                # Get the instantiations of this parent\n                parent_instantiation = parent_row['instantiations'].iloc[0]\n                parent_insts.append(parent_instantiation)\n\n        # Add the list of parent instantiations to our new column\n        parent_instantiations.append(parent_insts)\n\n    # Add the new column to the dataframe\n    dataframe['parent_instantiations'] = parent_instantiations\n\n    return dataframe\n\n\n\n\nThis key function transforms the ArgDown text into a structured DataFrame, capturing the hierarchical relationships between variables and preparing them for further processing. The function works by identifying node titles, descriptions, and indentation levels, then establishing parent-child relationships based on the hierarchy indicated by indentation.\nStage 2: Probability Integration enhances the structural representation with probability information, creating a complete BayesDown specification. This stage involves:\n\nQuestion generation: Automatically creating appropriate probability questions based on the network structure\nProbability extraction: Obtaining probability estimates for each question, either from the text or through LLM inference\nConsistency checking: Ensuring probability distributions sum to 1 and match structural constraints\nBayesDown integration: Incorporating probability information into the ArgDown structure\n\nThe key innovation in this stage is the automated generation of appropriate probability questions based on network structure. For each node, the system generates questions about prior probabilities (how likely is this variable in isolation?) and conditional probabilities (how likely is this variable given different states of its parents?).\nFigure 5 illustrates how probability questions are derived for a simple node with one parent:\n[FIGURE 5: Diagram showing how probability questions are generated based on network structure]\nFor the “Sprinkler” node with parent “Rain,” the system automatically generates questions like:\n\nWhat is the probability for Sprinkler=sprinkler_TRUE?\nWhat is the probability for Sprinkler=sprinkler_TRUE if Rain=rain_TRUE?\nWhat is the probability for Sprinkler=sprinkler_TRUE if Rain=rain_FALSE?\n\nThese questions are then answered either by extracting explicit probabilities from the text or by having the LLM infer reasonable values based on the author’s arguments. The answers are structured into a complete BayesDown representation that includes both the causal structure and all necessary probability information.\nThe visualization below demonstrates the completed extraction for a portion of Carlsmith’s model, showing how variables like “Misaligned Power Seeking” are influenced by multiple factors, each with associated probabilities:\n[VISUALIZATION: Extracted causal structure from Carlsmith’s model with probability information]\nThis two-stage approach offers several important advantages:\n\nImproved extraction quality: By focusing on one cognitive task at a time, the LLM performs better at each stage than it would attempting to extract everything simultaneously.\nIntermediate verification: Having ArgDown as an intermediate representation allows human verification before probability extraction, catching structural errors early.\nSeparation of concerns: Structure and probability can be updated independently, enabling more flexible maintenance as new information emerges.\nAlignment with human cognition: The process mirrors how experts approach complex arguments, making the system’s operation more intuitive and interpretable.\n\nPerhaps most importantly, the intermediate ArgDown representation creates a bridge between qualitative and quantitative aspects of arguments. It preserves the narrative structure and conceptual relationships from the original text while preparing for mathematical precision through probability integration. This hybrid approach maintains the strengths of both worlds: the richness of natural language and the rigor of formal models.\n\n\n2.7.4 Directed Acyclic Graphs: Structure and Semantics\n\nDirected Acyclic Graphs (DAGs) form the mathematical foundation of Bayesian networks, encoding both the qualitative structure of causal relationships and the quantitative parameters that define conditional dependencies. In AI risk modeling, these structures represent causal pathways to potential outcomes of interest.\n\nKey mathematical properties essential for AI risk modeling:\n\nAcyclicity: Ensures coherent probabilistic interpretation\nD-separation: Defines conditional independence relationships\nMarkov Condition: Each variable conditionally independent of non-descendants given parents\nPath Analysis: Reveals causal pathways and information flow\n\nThe causal interpretation follows Pearl’s framework:3\n\nEdges represent direct causal influence\nIntervention analysis through do-calculus\nCounterfactual reasoning for “what if” scenarios\nEvidence integration through Bayesian updating\n\n\n\n\n2.7.5 Quantification of Probabilistic Judgments\n\nTransforming qualitative uncertainty expressions into quantitative probabilities requires systematic interpretation frameworks that account for individual and cultural variation.\nStandard linguistic mappings (with significant individual variation) include:\n\n“Very likely” → 0.8-0.9\n“Probable” → 0.6-0.8\n“Uncertain” → 0.4-0.6\n“Unlikely” → 0.2-0.4\n“Highly improbable” → 0.05-0.15\n\n\nExpert elicitation methodologies:\n\nDirect Assessment: “What is P(outcome)?” with calibration training\nComparative Assessment: “Is A more likely than B?” for validation\nFrequency Format: “In 100 similar cases, how many…” for clarity\nBetting Odds: “What odds would you accept?” for revealed preferences\n\nCalibration challenges:\n\nIndividual variation in linguistic interpretation\nDomain-specific anchoring effects\nCultural influences on uncertainty expression\nLimited empirical basis for unprecedented scenarios\n\n\n\n2.7.6 Inference Techniques for Complex Networks\n\nOnce Bayesian networks are constructed, probabilistic inference enables reasoning about uncertainties, counterfactuals, and policy interventions. For the complex networks representing AI risks, computational approaches must balance accuracy with tractability.\nInference methods implemented include exact methods for smaller networks (variable elimination, junction trees), approximate methods for larger networks (Monte Carlo sampling, variational inference), specialized approaches for rare event analysis, and intervention modeling for policy evaluation using do-calculus.\n\nImplementation considerations:\n\nComputational Complexity: Managing exponential growth through decomposition\nSampling Efficiency: Importance sampling for rare events\nApproximation Quality: Convergence diagnostics and error bounds\nUncertainty Propagation: Representing confidence in outputs\n\n\n\n\n\n2.7.7 Integration with Prediction Markets and Forecasting Platforms\n\n\nTo maintain relevance in a rapidly evolving field, formal models must integrate with live data sources such as prediction markets and forecasting platforms.\n\nLive data sources for dynamic model updating include:\n\nMetaculus: Long-term AI predictions and technological forecasting\nGood Judgment Open: Geopolitical events and policy outcomes\nManifold Markets: Diverse question types with rapid market response\nInternal Expert Forecasting: Organization-specific predictions and assessments\n\n\n\nTechnical challenges:\n\nQuestion Mapping: Semantic matching between model variables and market questions\nTemporal Alignment: Different forecast horizons and update frequencies\nConflict Resolution: Principled aggregation of contradictory sources\nTrack Record Weighting: Incorporating forecaster calibration\n\n\nWith these theoretical foundations and methodological approaches established, we can now present the AMTAIR system implementation. The next chapter demonstrates how these concepts translate into a working prototype that automates the extraction and formalization of world models from AI safety literature.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-system-architecture",
    "href": "chapters/Outlines/Outline_13.html#sec-system-architecture",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.1 System Architecture Overview",
    "text": "3.1 System Architecture Overview\n\nThe AMTAIR system implements an end-to-end pipeline transforming unstructured text into interactive Bayesian network visualizations. Its modular architecture comprises five main components that progressively transform information from natural language into formal models suitable for policy analysis.\n\nThe AMTAIR system implements an end-to-end pipeline from unstructured text to interactive Bayesian network visualization. Its modular architecture comprises five main components that progressively transform information from natural language into formal models suitable for policy analysis.\n\n\n\n3.1.1 Five-Stage Pipeline Architecture\nThe five-stage pipeline architecture demonstrates how each component builds on the previous, with validation checkpoints preventing error propagation:  1. Text Ingestion and Preprocessing - Format normalization (PDF, HTML, Markdown) - Metadata extraction and citation tracking - Relevance filtering and section identification - Character encoding standardization 2. BayesDown Extraction - Two-stage argument structure identification - Probabilistic information integration - Quality validation and confidence scoring - Human-in-the-loop verification points 3. Structured Data Transformation - Parsing into standardized relational formats - Network topology validation - Consistency checking across relationships - Missing data imputation strategies 4. Bayesian Network Construction - Mathematical model instantiation - Conditional probability table generation - Inference engine initialization - Model validation and testing 5. Interactive Visualization - Dynamic rendering with PyVis - Probability-based visual encoding - Interactive exploration features - Export capabilities for reports\n\n\n3.1.2 Design Principles\n\n\n\n\n\n\nCore Design Philosophy\n\n\n\nThe system emphasizes scalability through modular architecture, standard interfaces for interoperability, validation checkpoints for quality assurance, and an extensible framework for future capabilities.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-two-stage-extraction",
    "href": "chapters/Outlines/Outline_13.html#sec-two-stage-extraction",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.2 The Two-Stage Extraction Process",
    "text": "3.2 The Two-Stage Extraction Process\n\nThe core innovation of AMTAIR lies in separating structural extraction from probability quantification. This two-stage approach addresses key challenges in automated formalization.\n\n3.2.1 Stage 1: Structural Extraction (ArgDown)\n\nThe first stage identifies argument structure without concerning itself with quantification:\nVariable Identification: Extract key propositions and entities from text using patterns like “X causes Y,” “If A then B,” and domain-specific indicators.\nRelationship Mapping: Identify support, attack, and conditional relationships between variables through linguistic analysis.\nHierarchy Construction: Build nested ArgDown representation preserving logical flow.\nValidation: Ensure extracted structure forms valid directed acyclic graph and preserves key argumentative relationships from source.\n\nExample ArgDown extraction:\n[Existential_Catastrophe]: Destruction of humanity's potential.\n + [Human_Disempowerment]: Loss of control to AI systems.\n   + [Misaligned_Power_Seeking]: AI pursuing problematic objectives.\n     + [APS_Systems]: Advanced, agentic, strategic AI.\n     + [Deployment_Decisions]: Choice to deploy despite risks.\n\n\n\n3.2.2 Stage 2: Probability Integration (BayesDown)\nThe second stage adds quantitative information to the structural skeleton:\nQuestion Generation: For each node, generate probability elicitation questions tailored to the specific context and relationships.\n\nExamples needed:\n- \"What is the probability of existential catastrophe?\"\n- \"What is P(catastrophe|human_disempowerment)?\"\n- Show how questions map to BayesDown structure\nProbability Extraction:\n\nIdentify explicit numerical statements\nMap qualitative expressions using calibrated scales\nApply domain-specific heuristics for common phrasings\n\nCoherence Enforcement:\n\nEnsure probabilities sum to 1.0\nComplete conditional probability tables\nCheck for logical contradictions\nFlag low-confidence extractions\n\n\n\n3.2.3 Why Two Stages?\n\n\nThis separation provides several benefits:\nTransparency: Being able to scrutinize each step of the automated workflow provides reliable insight into the work being done\nAccountability: False information (think of hallucinations) can be traced back to its origins\nVisibility:\nModular Validation: Structure can be verified independently from probability estimates, simplifying quality assurance.\nHuman Oversight: Experts can review and correct structural extraction before probability quantification.\nFlexible Quantification: Different methods (LLM extraction, expert elicitation, market data) can provide probabilities for the same structure.\nError Isolation: Structural errors don’t contaminate probability extraction and vice versa.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-implementation-tech",
    "href": "chapters/Outlines/Outline_13.html#sec-implementation-tech",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.3 Implementation Technologies",
    "text": "3.3 Implementation Technologies\n\n\n3.3.1 Technology Stack\nThe system leverages established libraries while adding novel extraction capabilities:\n\n\n\nTable 3.1: Technology stack components\n\n\n\n\n\nComponent\nTechnology\nPurpose\n\n\n\n\nLanguage Models\nGPT-4, Claude\nArgument extraction\n\n\nNetwork Analysis\nNetworkX\nGraph algorithms\n\n\nProbabilistic Modeling\npgmpy\nBayesian operations\n\n\nVisualization\nPyVis\nInteractive rendering\n\n\nData Processing\nPandas\nStructured manipulation\n\n\n\n\n\n\n\n\n3.3.2 Key Algorithms\nHierarchical Parsing: The system parses ArgDown/BayesDown syntax recognizing indentation-based hierarchy, a critical innovation for preserving argument structure.\nProbability Completion: When sources don’t specify all required probabilities, the system uses:\n\nMaximum entropy principles for missing values\nCoherence constraint propagation\nExpert-specified defaults with confidence scoring\n\nVisual Encoding Strategy:\n\nGreen-to-red gradient for probability magnitude\nBorder colors indicating node types\nInteractive elements for exploration\n\n\n\n\n3.3.3 (Expected) Performance Characteristics\n\n–&gt;\n\n\n3.3.4 Deterministic vs. Probabilistic Components of the Workflow",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-case-rain-sprinkler",
    "href": "chapters/Outlines/Outline_13.html#sec-case-rain-sprinkler",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.4 Case Study: Rain-Sprinkler-Grass",
    "text": "3.4 Case Study: Rain-Sprinkler-Grass\n\nI begin with the canonical example to demonstrate the complete pipeline on a simple, well-understood case.\n\n3.4.1 Processing Steps\nThe system processes this input through five steps:\n\nArgDown Parsing: Extract three nodes with relationships in ArgDown syntax\nQuestion Generation: Generate questions based on the possible instantiations and combinations of each parent note to identify the probabilities to be extracted in the next step\nBayesDown Extraction: LLM call extracting the full conditional probability tables for each node\nConstruction: Building of the formal Bayesian network\nVisualization: Render interactive display\n\n\n\n\n3.4.2 Example Conversion Steps\n\n\nArgDown Example\n[Grass_Wet]: Concentrated moisture on, between and around the blades of grass.{\"instantiations\": [\"grass_wet_TRUE\", \"grass_wet_FALSE\"]}    \n    +[Rain]: Tears of angles crying high up in the skies hitting the ground.{\"instantiations\": [\"rain_TRUE\", \"rain_FALSE\"]}\n    +[Sprinkler]: Activation of a centrifugal force based CO2 droplet distribution system.{\"instantiations\": [\"sprinkler_TRUE\", \"sprinkler_FALSE\"]}\n        +[Rain]\n\n\nExample of Questions for BayesDown extraction\nBayesDown Format Preview:\n# BayesDown Representation with Placeholder Probabilities\n\n/* This file contains BayesDown syntax with placeholder probabilities.\n   Replace the placeholders with actual probability values based on the \n   questions in the comments. */\n\n    /* What is the probability for Grass_Wet=grass_wet_TRUE? */\n    /* What is the probability for Grass_Wet=grass_wet_TRUE if Rain=rain_TRUE, Sprinkler=sprinkler_TRUE? */\n    /* What is the probability for Grass_Wet=grass_wet_TRUE if Rain=rain_TRUE, Sprinkler=sprinkler_FALSE? */\n    /* What is the probability for Grass_Wet=grass_wet_TRUE if Rain=rain_FALSE, Sprinkler=sprinkler_TRUE? */\n    /* What is the probability for Grass_Wet=grass_wet_TRUE if Rain=rain_FALSE, Sprinkler=sprinkler_FALSE? */\n    /* What is the probability for Grass_Wet=grass_wet_FALSE? */\n    /* What is the probability for Grass_Wet=grass_wet_FALSE if Rain=rain_TRUE, Sprinkler=sprinkler_TRUE? */\n    /* What is the probability for Grass_Wet=grass_wet_FALSE if Rain=rain_TRUE, Sprinkler=sprinkler_FALSE? */\n    /* What is the probability for Grass_Wet=grass_wet_FALSE if Rain=rain_FALSE, Sprinkler=sprinkler_TRUE? */\n    /* What is the probability for Grass_Wet=grass_wet_FALSE if Rain=rain_FALSE, Sprinkler=sprinkler_FALSE? */\n    [Grass_Wet]: Concentrated moisture on, between and around the blades of grass. {\"instantiations\": [\"grass_wet_TRUE\", \"grass_wet_FALSE\"], \"priors\": {\"What is the probability for Grass_Wet=grass_wet_TRUE?\": \"%?\", \"What is the probability for Grass_Wet=grass_wet_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Grass_Wet=grass_wet_TRUE if Rain=rain_TRUE, Sprinkler=sprinkler_TRUE?\": \"?%\", \"What is the probability for Grass_Wet=grass_wet_TRUE if Rain=rain_TRUE, Sprinkler=sprinkler_FALSE?\": \"?%\", \"What is the probability for Grass_Wet=grass_wet_TRUE if Rain=rain_FALSE, Sprinkler=sprinkler_TRUE?\": \"?%\", \"What is the probability for Grass_Wet=grass_wet_TRUE if Rain=rain_FALSE, Sprinkler=sprinkler_FALSE?\": \"?%\", \"What is the probability for Grass_Wet=grass_wet_FALSE if Rain=rain_TRUE, Sprinkler=sprinkler_TRUE?\": \"?%\", \"What is the probability for Grass_Wet=grass_wet_FALSE if Rain=rain_TRUE, Sprinkler=sprinkler_FALSE?\": \"?%\", \"What is the probability for Grass_Wet=grass_wet_FALSE if Rain=rain_FALSE, Sprinkler=sprinkler_TRUE?\": \"?%\", \"What is the probability for Grass_Wet=grass_wet_FALSE if Rain=rain_FALSE, Sprinkler=sprinkler_FALSE?\": \"?%\"}}\n        /* What is the probability for Rain=rain_TRUE? */\n        /* What is the probability for Rain=rain_FALSE? */\n        + [Rain]: Tears of angles crying high up in the skies hitting the ground. {\"instantiations\": [\"rain_TRUE\", \"rain_FALSE\"], \"priors\": {\"What is the probability for Rain=rain_TRUE?\": \"%?\", \"What is the probability for Rain=rain_FALSE?\": \"%?\"}}\n        /* What is the probability for Sprinkler=sprinkler_TRUE? */\n        /* What is the probability for Sprinkler=sprinkler_TRUE if Rain=rain_TRUE? */\n        /* What is the probability for Sprinkler=sprinkler_TRUE if Rain=rain_FALSE? */\n        /* What is the probability for Sprinkler=sprinkler_FALSE? */\n        /* What is the probability for Sprinkler=sprinkler_FALSE if Rain=rain_TRUE? */\n        /* What is the probability for Sprinkler=sprinkler_FALSE if Rain=rain_FALSE? */\n        + [Sprinkler]: Activation of a centrifugal force based CO2 droplet distribution system. {\"instantiations\": [\"sprinkler_TRUE\", \"sprinkler_FALSE\"], \"priors\": {\"What is the probability for Sprinkler=sprinkler_TRUE?\": \"%?\", \"What is the probability for Sprinkler=sprinkler_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Sprinkler=sprinkler_TRUE if Rain=rain_TRUE?\": \"?%\", \"What is the probability for Sprinkler=sprinkler_TRUE if Rain=rain_FALSE?\": \"?%\", \"What is the probability for Sprinkler=sprinkler_FALSE if Rain=rain_TRUE?\": \"?%\", \"What is the probability for Sprinkler=sprinkler_FALSE if Rain=rain_FALSE?\": \"?%\"}}\n            /* What is the probability for Rain=rain_TRUE? */\n            /* What is the probability for Rain=rain_FALSE? */\n            + [Rain]\n\n\nComplete BayesDown Example\nThe source BayesDown syntax representating the fully specified network:\n[Grass_Wet]: Concentrated moisture on grass. {\"instantiations\": [\"grass_wet_TRUE\", \"grass_wet_FALSE\"],\n \"priors\": {\"p(grass_wet_TRUE)\": \"0.322\", \"p(grass_wet_FALSE)\": \"0.678\"},\n \"posteriors\": {\n   \"p(grass_wet_TRUE|sprinkler_TRUE,rain_TRUE)\": \"0.99\",\n   \"p(grass_wet_TRUE|sprinkler_TRUE,rain_FALSE)\": \"0.9\",\n   \"p(grass_wet_TRUE|sprinkler_FALSE,rain_TRUE)\": \"0.8\",\n   \"p(grass_wet_TRUE|sprinkler_FALSE,rain_FALSE)\": \"0.0\"\n }}\n + [Rain]: Water falling from sky. {\"instantiations\": [\"rain_TRUE\", \"rain_FALSE\"],\n    \"priors\": {\"p(rain_TRUE)\": \"0.2\", \"p(rain_FALSE)\": \"0.8\"}}\n + [Sprinkler]: Artificial watering system. {\"instantiations\": [\"sprinkler_TRUE\", \"sprinkler_FALSE\"],\n    \"priors\": {\"p(sprinkler_TRUE)\": \"0.448\", \"p(sprinkler_FALSE)\": \"0.552\"},\n    \"posteriors\": {\n      \"p(sprinkler_TRUE|rain_TRUE)\": \"0.01\",\n      \"p(sprinkler_TRUE|rain_FALSE)\": \"0.4\"\n    }}\n   + [Rain]\n\n\nResulting Rain-Sprinkler-Grass DataFrame\n\n#| column: page\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\nDescription\nline\nline_numbers\nindentation\nindentation_levels\nParents\nChildren\ninstantiations\npriors\nposteriors\nNo_Parent\nNo_Children\nparent_instantiations\n\n\n\n\n\n0\nGrass_Wet\nConcentrated moisture on, between and around t…\n3\n[3]\n0\n[0]\n[Rain, Sprinkler]\n[]\n[grass_wet_TRUE, grass_wet_FALSE]\n{‘p(grass_wet_TRUE)’: ‘0.322’, ’p(grass_wet_FA…\n{‘p(grass_wet_TRUE|sprinkler_TRUE,rain_TRUE)’:…\nFalse\nTrue\n[[rain_TRUE, rain_FALSE], [sprinkler_TRUE, spr…\n\n\n1\nRain\nTears of angles crying high up in the skies hi…\n4\n[4, 6]\n2\n[1, 2]\n[]\n[Grass_Wet, Sprinkler]\n[rain_TRUE, rain_FALSE]\n{‘p(rain_TRUE)’: ‘0.2’, ‘p(rain_FALSE)’: ‘0.8’}\n{}\nTrue\nFalse\n[]\n\n\n2\nSprinkler\nActivation of a centrifugal force based CO2 dr…\n5\n[5]\n1\n[1]\n[Rain]\n[Grass_Wet]\n[sprinkler_TRUE, sprinkler_FALSE]\n{‘p(sprinkler_TRUE)’: ‘0.44838’, ’p(sprinkler_…\n{‘p(sprinkler_TRUE|rain_TRUE)’: ‘0.01’, ’p(spr…\nFalse\nFalse\n[[rain_TRUE, rain_FALSE]]\n\n\n\n\n\n\n3.4.3 Results\n\n\nRain-Sprinkler-Grass Network Rendering\n\n\nCode\nfrom IPython.display import IFrame\n\nIFrame(src=\"https://singularitysmith.github.io/AMTAIR_Prototype/bayesian_network.html\", width=\"100%\", height=\"600px\")\n\n\n\n        \n        \nDynamic Html Rendering of the Rain-Sprinkler-Grass DAG with Conditional Probabilities\n\n\nValidation Success\nThe system successfully extracts complete network structure, preserves all probability information, calculates correct marginal probabilities, generates interactive visualization, and enables inference queries—validating the basic pipeline functionality.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-case-carlsmith",
    "href": "chapters/Outlines/Outline_13.html#sec-case-carlsmith",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.5 Case Study: Carlsmith’s Power-Seeking AI Model",
    "text": "3.5 Case Study: Carlsmith’s Power-Seeking AI Model\n\nApplying AMTAIR to Carlsmith’s model demonstrates scalability to realistic AI safety arguments.\n\n3.5.1 Model Complexity\nThe Carlsmith model contains:\n\n23 nodes representing different factors\n29 edges encoding dependencies\nMultiple probability tables with complex conditionals\nSix-level causal depth from root causes to catastrophe\n\nThis represents a significant increase in complexity from the pedagogical example.\n\n\n\n3.5.2 Automated Extraction of the Carlsmith’s Argument Structure\nHaving validated the implementation on the canonical rain-sprinkler-lawn example, I applied the AMTAIR approach to a substantially more complex real-world case: Joseph Carlsmith’s model of existential risk from power-seeking AI. This application demonstrates the system’s ability to handle sophisticated multi-level arguments with numerous variables and relationships.\nCarlsmith’s analysis involves dozens of factors organized in a complex causal structure, from root causes like “Advanced AI Capability” and “Instrumental Convergence” through intermediate factors like “APS Systems” and “Misaligned Power Seeking” to final outcomes like “Existential Catastrophe.” The model exhibits several challenging features:\n\nMulti-level structure with causal chains spanning multiple steps\nDivergent pathways where factors influence outcomes through multiple routes\nComplex conditional dependencies with variables influenced by multiple parents\nVariables with three or more possible states rather than simple binary outcomes\nInterconnected clusters where factors form distinct but related argument groups\n\nCore Risk Pathway:\nExistential_Catastrophe \n← Human_Disempowerment \n← Scale_Of_Power_Seeking\n← Misaligned_Power_Seeking\n← [APS_Systems, Difficulty_Of_Alignment, Deployment_Decisions]\nSupporting Structure:\n\nCompetitive dynamics influencing deployment\nTechnical factors affecting alignment difficulty\nCorrective mechanisms and their limitations\n\nProbability Preservation:\n\nExtracted probabilities match Carlsmith’s published estimates\nConditional relationships properly captured\nFinal P(doom) calculation reproduces ~5% result\n\n\nPrompting LLMs for ArgDown Extraction\n\n\n\nCode\n# @title 1.2.0 --- Prompt Template Function Definitions --- [prompt_template_function]\n\n\"\"\"\nBLOCK PURPOSE: Defines a flexible template system for LLM prompts used in the extraction pipeline.\n\nThis block implements two key classes:\n1. PromptTemplate: A template class supporting variable substitution for dynamic prompts\n2. PromptLibrary: A collection of pre-defined prompt templates for different extraction tasks\n\nThese templates are used in the ArgDown and BayesDown probability extraction\nstages of the pipeline, providing consistent and well-structured prompts to the LLMs.\n\nDEPENDENCIES: string.Template for variable substitution\nOUTPUTS: PromptTemplate and PromptLibrary classes\n\"\"\"\n\nfrom string import Template\nfrom typing import Dict, Optional, Union, List\n\nclass PromptTemplate:\n    \"\"\"Template system for LLM prompts with variable substitution\"\"\"\n\n    def __init__(self, template: str):\n        \"\"\"Initialize with template string using $variable format\"\"\"\n        self.template = Template(template)\n\n    def format(self, **kwargs) -&gt; str:\n        \"\"\"Substitute variables in the template\"\"\"\n        return self.template.safe_substitute(**kwargs)\n\n    @classmethod\n    def from_file(cls, filepath: str) -&gt; 'PromptTemplate':\n        \"\"\"Load template from a file\"\"\"\n        with open(filepath, 'r') as f:\n            template = f.read()\n        return cls(template)\n\nclass PromptLibrary:\n    \"\"\"Collection of prompt templates for different extraction tasks\"\"\"\n\n    # ArgDown extraction prompt - transforms source text into structured argument map\n    ARGDOWN_EXTRACTION = PromptTemplate(\"\"\"\nYou are participating in the AMTAIR (Automating Transformative AI Risk Modeling)\nproject and you are tasked with converting natural language arguments into\nArgDown syntax by extracting and formalizing causal world models from\nunstructured text.\nYour specific task is to extract the implicit causal model from the provided\ndocument in structured ArgDown format.\n\n## Epistemic Foundation & Purpose\n\nThis extraction represents one possible interpretation of the implicit causal\nmodel in the document. Multiple extractions from the same text help reveal\npatterns of convergence (where the model is clearly articulated) and\ndivergence (where the model contains ambiguities). This approach acknowledges\nthat expert texts often contain implicit rather than explicit causal models.\n\nYour role is to reveal the causal structure already present in the author's\nthinking, maintaining epistemic humility about your interpretation while\nadhering strictly to the required format.\n\n## ArgDown Format Specification\n\n### Core Syntax\n\nArgDown represents causal relationships using a hierarchical structure:\n\n1. Variables appear in square brackets with descriptive text:\n   `[Variable_Name]: Description of the variable.`\n\n2. Causal relationships use indentation (2 spaces per level) and '+' symbols:\n\n[Effect]: Description of effect. + [Cause]: Description of cause. + [Deeper_Cause]: Description of deeper cause.\n\n3. Causality flows from bottom (more indented) to top (less indented):\n- More indented variables (causes) influence less indented variables (effects)\n- The top-level variable is the ultimate effect or outcome\n- Deeper indentation levels represent root causes or earlier factors\n\n4. Each variable must include JSON metadata with possible states (instantiations):\n`[Variable]: Description. {\"instantiations\": [\"variable_STATE1\", \"variable_STATE2\"]}`\n\n### JSON Metadata Format\n\nThe JSON metadata must follow this exact structure:\n\n```json\n{\"instantiations\": [\"variable_STATE1\", \"variable_STATE2\"]}\n\nRequirements:\n* Double quotes (not single) around field names and string values\n* Square brackets enclosing the instantiations array\n* Comma separation between array elements\n* No trailing comma after the last element\n* Must be valid JSON syntax that can be parsed by standard JSON parsers\n\nFor binary variables (most common case):\n{\"instantiations\": [\"variable_TRUE\", \"variable_FALSE\"]}\n\nFor multi-state variables (when clearly specified in the text):\n{\"instantiations\": [\"variable_HIGH\", \"variable_MEDIUM\", \"variable_LOW\"]}\n\nThe metadata must appear on the same line as the variable definition, after the description.\n## Complex Structural Patterns\n### Variables Influencing Multiple Effects\nThe same variable can appear multiple times in different places in the hierarchy if it influences multiple effects:\n[Effect1]: First effect description. {\"instantiations\": [\"effect1_TRUE\", \"effect1_FALSE\"]}\n  + [Cause_A]: Description of cause A. {\"instantiations\": [\"cause_a_TRUE\", \"cause_a_FALSE\"]}\n\n[Effect2]: Second effect description. {\"instantiations\": [\"effect2_TRUE\", \"effect2_FALSE\"]}\n  + [Cause_A]\n  + [Cause_B]: Description of cause B. {\"instantiations\": [\"cause_b_TRUE\", \"cause_b_FALSE\"]}\n\n### Multiple Causes of the Same Effect\nMultiple causes can influence the same effect by being listed at the same indentation level:\n[Effect]: Description of effect. {\"instantiations\": [\"effect_TRUE\", \"effect_FALSE\"]}\n  + [Cause1]: Description of first cause. {\"instantiations\": [\"cause1_TRUE\", \"cause1_FALSE\"]}\n  + [Cause2]: Description of second cause. {\"instantiations\": [\"cause2_TRUE\", \"cause2_FALSE\"]}\n    + [Deeper_Cause]: A cause that influences Cause2. {\"instantiations\": [\"deeper_cause_TRUE\", \"deeper_cause_FALSE\"]}\n\n### Causal Chains\nCausal chains are represented through multiple levels of indentation:\n[Ultimate_Effect]: The final outcome. {\"instantiations\": [\"ultimate_effect_TRUE\", \"ultimate_effect_FALSE\"]}\n  + [Intermediate_Effect]: A mediating variable. {\"instantiations\": [\"intermediate_effect_TRUE\", \"intermediate_effect_FALSE\"]}\n    + [Root_Cause]: The initial cause. {\"instantiations\": [\"root_cause_TRUE\", \"root_cause_FALSE\"]}\n  + [2nd_Intermediate_Effect]: A mediating variable. {\"instantiations\": [\"intermediate_effect_TRUE\", \"intermediate_effect_FALSE\"]}\n\n\n### Common Cause of Multiple Variables\nA common cause affecting multiple variables is represented by referencing the same variable in multiple places:\n[Effect1]: First effect description. {\"instantiations\": [\"effect1_TRUE\", \"effect1_FALSE\"]}\n  + [Common_Cause]: Description of common cause. {\"instantiations\": [\"common_cause_TRUE\", \"common_cause_FALSE\"]}\n\n[Effect2]: Second effect description. {\"instantiations\": [\"effect2_TRUE\", \"effect2_FALSE\"]}\n  + [Common_Cause]\n\n## Detailed Extraction Workflow\nPlease follow this step-by-step process, documenting your reasoning in XML tags:\n&lt;analysis&gt;\nFirst, conduct a holistic analysis of the document:\n1. Identify the main subject matter or domain\n2. Note key concepts, variables, and factors discussed\n3. Pay attention to language indicating causal relationships (causes, affects, influences, depends on, etc.)\n4. Look for the ultimate outcomes or effects that are the focus of the document\n5. Record your general understanding of the document's implicit causal structure\n&lt;/analysis&gt;\n&lt;variable_identification&gt;\nNext, identify and list the key variables in the causal model:\n* Focus on factors that are discussed as having an influence or being influenced\n* For each variable:\n  * Create a descriptive name in [square_brackets]\n  * Write a concise description based directly on the text\n  * Determine possible states (usually binary TRUE/FALSE unless clearly specified)\n* Distinguish between:\n  * Outcome variables (effects the author is concerned with)\n  * Intermediate variables (both causes and effects in chains)\n  * Root cause variables (exogenous factors in the model)\n* List all identified variables with their descriptions and possible states\n&lt;/variable_identification&gt;\n\n&lt;causal_structure&gt;\nThen, determine the causal relationships between variables:\n* For each variable, identify what factors influence it\n* Note the direction of causality (what causes what)\n* Look for mediating variables in causal chains\n* Identify common causes of multiple effects\n* Capture feedback loops if present (though they must be represented as DAGs)\n* Map out the hierarchical structure of the causal model\n&lt;/causal_structure&gt;\n\n&lt;format_conversion&gt;\nNow, convert your analysis into proper ArgDown format:\n* Start with the ultimate outcome variables at the top level\n* Place direct causes indented below with \\+ symbols\n* Continue with deeper causes at further indentation levels\n* Add variable descriptions and instantiations metadata\n* Ensure variables appearing in multiple places have consistent names\n* Check that the entire structure forms a valid directed acyclic graph\n&lt;/format_conversion&gt;\n\n&lt;validation&gt;\n\nFinally, review your extraction for quality and format correctness:\n1. Verify all variables have properly formatted metadata\n2. Check that indentation properly represents causal direction\n3. Confirm the extraction accurately reflects the document's implicit model\n4. Ensure no cycles exist in the causal structure\n5. Verify that variables referenced multiple times are consistent\n6. Check that the extraction would be useful for subsequent analysis\n\n&lt;/validation&gt;\n\n\n## Source Document Analysis Guidance\nWhen analyzing the source document:\n* Focus on revealing the author's own causal model, not imposing an external framework\n* Maintain the author's terminology where possible\n* Look for both explicit statements of causality and implicit assumptions\n* Pay attention to the relative importance the author assigns to different factors\n* Notice where the author expresses certainty versus uncertainty\n* Consider the level of granularity appropriate to the document's own analysis\n\nRemember that your goal is to make the implicit model explicit, not to evaluate or improve it.\nThe value lies in accurately representing the author's perspective, even if you might personally disagree or see limitations in their model.\n\n\"\"\")\n\n    # BayesDown probability extraction prompt - enhances ArgDown with probability information\n    BAYESDOWN_EXTRACTION = PromptTemplate(\"\"\"\nYou are an expert in probabilistic reasoning and Bayesian networks. Your task is\nto extend the provided ArgDown structure with probability information,\ncreating a BayesDown representation.\n\nFor each statement in the ArgDown structure, you need to:\n1. Estimate prior probabilities for each possible state\n2. Estimate conditional probabilities given parent states\n3. Maintain the original structure and relationships\n\nHere is the format to follow:\n[Node]: Description. { \"instantiations\": [\"node_TRUE\", \"node_FALSE\"], \"priors\": { \"p(node_TRUE)\": \"0.7\", \"p(node_FALSE)\": \"0.3\" }, \"posteriors\": { \"p(node_TRUE|parent_TRUE)\": \"0.9\", \"p(node_TRUE|parent_FALSE)\": \"0.4\", \"p(node_FALSE|parent_TRUE)\": \"0.1\", \"p(node_FALSE|parent_FALSE)\": \"0.6\" } }\n [Parent]: Parent description. {...}\n\n\nHere are the specific probability questions to answer:\n$questions\n\nArgDown structure to enhance:\n$argdown\n\nProvide the complete BayesDown representation with probabilities:\n\"\"\")\n\n    @classmethod\n    def get_template(cls, template_name: str) -&gt; PromptTemplate:\n        \"\"\"Get a prompt template by name\"\"\"\n        if hasattr(cls, template_name):\n            return getattr(cls, template_name)\n        else:\n            raise ValueError(f\"Template not found: {template_name}\")\n\n\n\n\n\n\nProcessing LLM Response\nThe extraction process began with an ArgDown representation capturing the structural relationships between variables:\n\n\n\nCode\n# @title 1.7.0 --- Parsing ArgDown & BayesDown (.md to .csv) --- [parsing_argdown_bayesdown]\n\n\"\"\"\nBLOCK PURPOSE: Provides the core parsing functionality for transforming ArgDown\nand BayesDown text representations into structured DataFrame format for further\nprocessing.\n\nThis block implements the critical extraction pipeline described in the AMTAIR\nproject (see PY_TechnicalImplementation) that converts argument structures\ninto Bayesian networks.\nThe function can handle both basic ArgDown (structure-only) and\nBayesDown (with probabilities).\n\nKey steps in the parsing process:\n1. Remove comments from the markdown text\n2. Extract titles, descriptions, and indentation levels\n3. Establish parent-child relationships based on indentation\n4. Convert the structured information into a DataFrame\n5. Add derived columns for network analysis\n\nDEPENDENCIES: pandas, re, json libraries\nINPUTS: Markdown text in ArgDown/BayesDown format\nOUTPUTS: Structured DataFrame with node information, relationships, and properties\n\"\"\"\n\ndef parse_markdown_hierarchy_fixed(markdown_text, ArgDown=False):\n    \"\"\"\n    Parse ArgDown or BayesDown format into a structured DataFrame with parent-child relationships.\n\n    Args:\n        markdown_text (str): Text in ArgDown or BayesDown format\n        ArgDown (bool): If True, extracts only structure without probabilities\n                        If False, extracts both structure and probability information\n\n    Returns:\n        pandas.DataFrame: Structured data with node information, relationships, and attributes\n    \"\"\"\n    # PHASE 1: Clean and prepare the text\n    clean_text = remove_comments(markdown_text)\n\n    # PHASE 2: Extract basic information about nodes\n    titles_info = extract_titles_info(clean_text)\n\n    # PHASE 3: Determine the hierarchical relationships\n    titles_with_relations = establish_relationships_fixed(titles_info, clean_text)\n\n    # PHASE 4: Convert to structured DataFrame format\n    df = convert_to_dataframe(titles_with_relations, ArgDown)\n\n    # PHASE 5: Add derived columns for analysis\n    df = add_no_parent_no_child_columns_to_df(df)\n    df = add_parents_instantiation_columns_to_df(df)\n\n    return df\n\ndef remove_comments(markdown_text):\n    \"\"\"\n    Remove comment blocks from markdown text using regex pattern matching.\n\n    Args:\n        markdown_text (str): Text containing potential comment blocks\n\n    Returns:\n        str: Text with comment blocks removed\n    \"\"\"\n    # Remove anything between /* and */ using regex\n    return re.sub(r'/\\*.*?\\*/', '', markdown_text, flags=re.DOTALL)\n\ndef extract_titles_info(text):\n    \"\"\"\n    Extract titles with their descriptions and indentation levels from markdown text.\n\n    Args:\n        text (str): Cleaned markdown text\n\n    Returns:\n        dict: Dictionary with titles as keys and dictionaries of attributes as values\n    \"\"\"\n    lines = text.split('\\n')\n    titles_info = {}\n\n    for line in lines:\n        # Skip empty lines\n        if not line.strip():\n            continue\n\n        # Extract title within square or angle brackets\n        title_match = re.search(r'[&lt;\\[](.+?)[&gt;\\]]', line)\n        if not title_match:\n            continue\n\n        title = title_match.group(1)\n\n        # Extract description and metadata\n        title_pattern_in_line = r'[&lt;\\[]' + re.escape(title) + r'[&gt;\\]]:'\n        description_match = re.search(title_pattern_in_line + r'\\s*(.*)', line)\n\n        if description_match:\n            full_text = description_match.group(1).strip()\n\n            # Split description and metadata at the first \"{\"\n            if \"{\" in full_text:\n                split_index = full_text.find(\"{\")\n                description = full_text[:split_index].strip()\n                metadata = full_text[split_index:].strip()\n            else:\n                # Keep the entire description and no metadata\n                description = full_text\n                metadata = ''  # Initialize as empty string\n        else:\n            description = ''\n            metadata = ''  # Ensure metadata is initialized\n\n        # Calculate indentation level based on spaces before + or - symbol\n        indentation = 0\n        if '+' in line:\n            symbol_index = line.find('+')\n            # Count spaces before the '+' symbol\n            i = symbol_index - 1\n            while i &gt;= 0 and line[i] == ' ':\n                indentation += 1\n                i -= 1\n        elif '-' in line:\n            symbol_index = line.find('-')\n            # Count spaces before the '-' symbol\n            i = symbol_index - 1\n            while i &gt;= 0 and line[i] == ' ':\n                indentation += 1\n                i -= 1\n\n        # If neither symbol exists, indentation remains 0\n\n        if title in titles_info:\n            # Only update description if it's currently empty and we found a new one\n            if not titles_info[title]['description'] and description:\n                titles_info[title]['description'] = description\n\n            # Store all indentation levels for this title\n            titles_info[title]['indentation_levels'].append(indentation)\n\n            # Keep max indentation for backward compatibility\n            if indentation &gt; titles_info[title]['indentation']:\n                titles_info[title]['indentation'] = indentation\n\n            # Do NOT update metadata here - keep the original metadata\n        else:\n            # First time seeing this title, create a new entry\n            titles_info[title] = {\n                'description': description,\n                'indentation': indentation,\n                'indentation_levels': [indentation],  # Initialize with first indentation level\n                'parents': [],\n                'children': [],\n                'line': None,\n                'line_numbers': [],  # Initialize an empty list for all occurrences\n                'metadata': metadata  # Set metadata explicitly from what we found\n            }\n\n    return titles_info\n\ndef establish_relationships_fixed(titles_info, text):\n    \"\"\"\n    Establish parent-child relationships between titles using BayesDown\n    indentation rules.\n\n    In BayesDown syntax:\n    - More indented nodes (with + symbol) are PARENTS of less indented nodes\n    - The relationship reads as \"Effect is caused by Cause\" (Effect + Cause)\n    - This aligns with how Bayesian networks represent causality\n\n    Args:\n        titles_info (dict): Dictionary with information about titles\n        text (str): Original markdown text (for identifying line numbers)\n\n    Returns:\n        dict: Updated dictionary with parent-child relationships\n    \"\"\"\n    lines = text.split('\\n')\n\n    # Dictionary to store line numbers for each title occurrence\n    title_occurrences = {}\n\n    # Record line number for each title (including multiple occurrences)\n    line_number = 0\n    for line in lines:\n        if not line.strip():\n            line_number += 1\n            continue\n\n        title_match = re.search(r'[&lt;\\[](.+?)[&gt;\\]]', line)\n        if not title_match:\n            line_number += 1\n            continue\n\n        title = title_match.group(1)\n\n        # Store all occurrences of each title with their line numbers\n        if title not in title_occurrences:\n            title_occurrences[title] = []\n        title_occurrences[title].append(line_number)\n\n        # Store all line numbers where this title appears\n        if 'line_numbers' not in titles_info[title]:\n            titles_info[title]['line_numbers'] = []\n        titles_info[title]['line_numbers'].append(line_number)\n\n        # For backward compatibility, keep the first occurrence in 'line'\n        if titles_info[title]['line'] is None:\n            titles_info[title]['line'] = line_number\n\n        line_number += 1\n\n    # Create an ordered list of all title occurrences with their line numbers\n    all_occurrences = []\n    for title, occurrences in title_occurrences.items():\n        for line_num in occurrences:\n            all_occurrences.append((title, line_num))\n\n    # Sort occurrences by line number\n    all_occurrences.sort(key=lambda x: x[1])\n\n    # Get indentation for each occurrence\n    occurrence_indents = {}\n    for title, line_num in all_occurrences:\n        for line in lines[line_num:line_num+1]:  # Only check the current line\n            indent = 0\n            if '+' in line:\n                symbol_index = line.find('+')\n                # Count spaces before the '+' symbol\n                j = symbol_index - 1\n                while j &gt;= 0 and line[j] == ' ':\n                    indent += 1\n                    j -= 1\n            elif '-' in line:\n                symbol_index = line.find('-')\n                # Count spaces before the '-' symbol\n                j = symbol_index - 1\n                while j &gt;= 0 and line[j] == ' ':\n                    indent += 1\n                    j -= 1\n            occurrence_indents[(title, line_num)] = indent\n\n    # Enhanced backward pass for correct parent-child relationships\n    for i, (title, line_num) in enumerate(all_occurrences):\n        current_indent = occurrence_indents[(title, line_num)]\n\n        # Skip root nodes (indentation 0) for processing\n        if current_indent == 0:\n            continue\n\n        # Look for the immediately preceding node with lower indentation\n        j = i - 1\n        while j &gt;= 0:\n            prev_title, prev_line = all_occurrences[j]\n            prev_indent = occurrence_indents[(prev_title, prev_line)]\n\n            # If we find a node with less indentation, it's a child of current node\n            if prev_indent &lt; current_indent:\n                # In BayesDown:\n                # More indented node is a parent (cause) of less indented node (effect)\n                if title not in titles_info[prev_title]['parents']:\n                    titles_info[prev_title]['parents'].append(title)\n                if prev_title not in titles_info[title]['children']:\n                    titles_info[title]['children'].append(prev_title)\n\n                # Only need to find the immediate child\n                # (closest preceding node with lower indentation)\n                break\n\n            j -= 1\n\n    return titles_info\n\ndef convert_to_dataframe(titles_info, ArgDown):\n    \"\"\"\n    Convert the titles information dictionary to a pandas DataFrame.\n\n    Args:\n        titles_info (dict): Dictionary with information about titles\n        ArgDown (bool): If True, extract only structural information without probabilities\n\n    Returns:\n        pandas.DataFrame: Structured data with node information and relationships\n    \"\"\"\n    if ArgDown == True:\n        # For ArgDown, exclude probability columns\n        df = pd.DataFrame(columns=['Title', 'Description', 'line', 'line_numbers', 'indentation',\n                               'indentation_levels', 'Parents', 'Children', 'instantiations'])\n    else:\n        # For BayesDown, include probability columns\n        df = pd.DataFrame(columns=['Title', 'Description', 'line', 'line_numbers', 'indentation',\n                               'indentation_levels', 'Parents', 'Children', 'instantiations',\n                               'priors', 'posteriors'])\n\n    for title, info in titles_info.items():\n        # Parse the metadata JSON string into a Python dictionary\n        if 'metadata' in info and info['metadata']:\n            try:\n                # Only try to parse if metadata is not empty\n                if info['metadata'].strip():\n                    jsonMetadata = json.loads(info['metadata'])\n                    if ArgDown == True:\n                        # Create the row dictionary with instantiations as\n                        # metadata only, no probabilities yet\n                        row = {\n                            'Title': title,\n                            'Description': info.get('description', ''),\n                            'line': info.get('line',''),\n                            'line_numbers': info.get('line_numbers', []),\n                            'indentation': info.get('indentation',''),\n                            'indentation_levels': info.get('indentation_levels', []),\n                            'Parents': info.get('parents', []),\n                            'Children': info.get('children', []),\n                            # Extract specific metadata fields,\n                            # defaulting to empty if not present\n                            'instantiations': jsonMetadata.get('instantiations', []),\n                        }\n                    else:\n                        # Create dict with probabilities for BayesDown\n                        row = {\n                            'Title': title,\n                            'Description': info.get('description', ''),\n                            'line': info.get('line',''),\n                            'line_numbers': info.get('line_numbers', []),\n                            'indentation': info.get('indentation',''),\n                            'indentation_levels': info.get('indentation_levels', []),\n                            'Parents': info.get('parents', []),\n                            'Children': info.get('children', []),\n                            # Extract specific metadata fields, defaulting to empty if not present\n                            'instantiations': jsonMetadata.get('instantiations', []),\n                            'priors': jsonMetadata.get('priors', {}),\n                            'posteriors': jsonMetadata.get('posteriors', {})\n                        }\n                else:\n                    # Empty metadata case\n                    row = {\n                        'Title': title,\n                        'Description': info.get('description', ''),\n                        'line': info.get('line',''),\n                        'line_numbers': info.get('line_numbers', []),\n                        'indentation': info.get('indentation',''),\n                        'indentation_levels': info.get('indentation_levels', []),\n                        'Parents': info.get('parents', []),\n                        'Children': info.get('children', []),\n                        'instantiations': [],\n                        'priors': {},\n                        'posteriors': {}\n                    }\n            except json.JSONDecodeError:\n                # Handle case where metadata isn't valid JSON\n                row = {\n                    'Title': title,\n                    'Description': info.get('description', ''),\n                    'line': info.get('line',''),\n                    'line_numbers': info.get('line_numbers', []),\n                    'indentation': info.get('indentation',''),\n                    'indentation_levels': info.get('indentation_levels', []),\n                    'Parents': info.get('parents', []),\n                    'Children': info.get('children', []),\n                    'instantiations': [],\n                    'priors': {},\n                    'posteriors': {}\n                }\n        else:\n            # Handle case where metadata field doesn't exist or is empty\n            row = {\n                'Title': title,\n                'Description': info.get('description', ''),\n                'line': info.get('line',''),\n                'line_numbers': info.get('line_numbers', []),\n                'indentation': info.get('indentation',''),\n                'indentation_levels': info.get('indentation_levels', []),\n                'Parents': info.get('parents', []),\n                'Children': info.get('children', []),\n                'instantiations': [],\n                'priors': {},\n                'posteriors': {}\n            }\n\n        # Add the row to the DataFrame\n        df.loc[len(df)] = row\n\n    return df\n\ndef add_no_parent_no_child_columns_to_df(dataframe):\n    \"\"\"\n    Add No_Parent and No_Children boolean columns to the DataFrame to\n    identify root and leaf nodes.\n\n    Args:\n        dataframe (pandas.DataFrame): The DataFrame to enhance\n\n    Returns:\n        pandas.DataFrame: Enhanced DataFrame with additional boolean columns\n    \"\"\"\n    no_parent = []\n    no_children = []\n\n    for _, row in dataframe.iterrows():\n        no_parent.append(not row['Parents'])  # True if Parents list is empty\n        no_children.append(not row['Children'])  # True if Children list is empty\n\n    dataframe['No_Parent'] = no_parent\n    dataframe['No_Children'] = no_children\n\n    return dataframe\n\ndef add_parents_instantiation_columns_to_df(dataframe):\n    \"\"\"\n    Add all possible instantiations of parents as a list of lists column\n    to the DataFrame.\n    This is crucial for generating conditional probability tables.\n\n    Args:\n        dataframe (pandas.DataFrame): The DataFrame to enhance\n\n    Returns:\n        pandas.DataFrame: Enhanced DataFrame with parent_instantiations column\n    \"\"\"\n    # Create a new column to store parent instantiations\n    parent_instantiations = []\n\n    # Iterate through each row in the dataframe\n    for _, row in dataframe.iterrows():\n        parents = row['Parents']\n        parent_insts = []\n\n        # For each parent, find its instantiations and add to the list\n        for parent in parents:\n            # Find the row where Title matches the parent\n            parent_row = dataframe[dataframe['Title'] == parent]\n\n            # If parent found in the dataframe\n            if not parent_row.empty:\n                # Get the instantiations of this parent\n                parent_instantiation = parent_row['instantiations'].iloc[0]\n                parent_insts.append(parent_instantiation)\n\n        # Add the list of parent instantiations to our new column\n        parent_instantiations.append(parent_insts)\n\n    # Add the new column to the dataframe\n    dataframe['parent_instantiations'] = parent_instantiations\n\n    return dataframe\n\n\n\n\nThis representation captures the complex causal structure of Carlsmith’s argument, with 21 variables organized in a multi-level hierarchy. The “Misaligned_Power_Seeking” node appears multiple times, reflecting its role as a central concept that influences several other variables.\n\n\n\n3.5.3 From ArgDown to BayesDown in Carlsmith’s Model\n\nAfter processing this structure with the AMTAIR system, probability information had to be added to create a complete BayesDown representation. The following excerpt shows the probability information for a single node (“Deployment_Decisions”):\n[Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\n  \"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"],\n  \"priors\": {\n    \"p(deployment_decisions_DEPLOY)\": \"0.70\",\n    \"p(deployment_decisions_WITHHOLD)\": \"0.30\"\n  },\n  \"posteriors\": {\n    \"p(deployment_decisions_DEPLOY|incentives_to_build_aps_STRONG, deception_by_ai_TRUE)\": \"0.90\",\n    \"p(deployment_decisions_DEPLOY|incentives_to_build_aps_STRONG, deception_by_ai_FALSE)\": \"0.75\",\n    \"p(deployment_decisions_DEPLOY|incentives_to_build_aps_WEAK, deception_by_ai_TRUE)\": \"0.60\",\n    \"p(deployment_decisions_DEPLOY|incentives_to_build_aps_WEAK, deception_by_ai_FALSE)\": \"0.30\"\n  }\n}\nThis node has two possible states (DEPLOY or WITHHOLD), prior probabilities for each state, and conditional probabilities based on different combinations of its parent variables (“Incentives_To_Build_APS” and “Deception_By_AI”).\n\n\n\nCode\n# Generate BayesDown format\nbayesdown_questions = extract_bayesdown_questions_fixed(\n    \"ArgDown_WithQuestions.csv\",\n    \"FULL_BayesDownQuestions.md\",\n    include_questions_as_comments=True\n)\n\n# Display a preview of the format\nprint(\"\\nBayesDown Format Preview:\")\nprint(bayesdown_questions[:5000] + \"...\\n\")\n\n\nLoading CSV from ArgDown_WithQuestions.csv...\nSuccessfully loaded CSV with 23 rows.\nGenerating BayesDown syntax with placeholder probabilities...\nBayesDown Questions saved to FULL_BayesDownQuestions.md\n\nBayesDown Format Preview:\n# BayesDown Representation with Placeholder Probabilities\n\n/* This file contains BayesDown syntax with placeholder probabilities.\n   Replace the placeholders with actual probability values based on the \n   questions in the comments. */\n\n/* What is the probability for Existential_Catastrophe=existential_catastrophe_TRUE? */\n/* What is the probability for Existential_Catastrophe=existential_catastrophe_FALSE? */\n[Existential_Catastrophe]: The destruction of humanity's long-term potential due to AI systems we've lost control over. {\"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"], \"priors\": {\"What is the probability for Existential_Catastrophe=existential_catastrophe_TRUE?\": \"%?\", \"What is the probability for Existential_Catastrophe=existential_catastrophe_FALSE?\": \"%?\"}}\n/* What is the probability for Human_Disempowerment=human_disempowerment_TRUE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_FALSE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE? */\n/* What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE? */\n[Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"], \"priors\": {\"What is the probability for Human_Disempowerment=human_disempowerment_TRUE?\": \"%?\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_TRUE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"?%\", \"What is the probability for Human_Disempowerment=human_disempowerment_FALSE if Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"?%\"}}\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n  /* What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n  + [Scale_Of_Power_Seeking]: Power-seeking by AI systems scaling to the point of permanently disempowering all of humanity. {\"instantiations\": [\"scale_of_power_seeking_TRUE\", \"scale_of_power_seeking_FALSE\"], \"priors\": {\"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE?\": \"%?\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_TRUE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_TRUE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"?%\", \"What is the probability for Scale_Of_Power_Seeking=scale_of_power_seeking_FALSE if Misaligned_Power_Seeking=misaligned_power_seeking_FALSE, Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"?%\"}}\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY? */\n    /* What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD? */\n    + [Misaligned_Power_Seeking]: Deployed AI systems seeking power in unintended and high-impact ways due to problems with their objectives. {\"instantiations\": [\"misaligned_power_seeking_TRUE\", \"misaligned_power_seeking_FALSE\"], \"priors\": {\"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE?\": \"%?\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_TRUE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_TRUE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_TRUE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_DEPLOY?\": \"?%\", \"What is the probability for Misaligned_Power_Seeking=misaligned_power_seeking_FALSE if APS_Systems=aps_systems_FALSE, Difficulty_Of_Alignment=difficulty_of_alignment_FALSE, Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"?%\"}}\n      /* What is the probability for APS_Systems=aps_systems_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE? */\n      /* What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE? */\n      + [APS_Systems]: AI systems with advanced capabilities, agentic planning, and strategic awareness. {\"instantiations\": [\"aps_systems_TRUE\", \"aps_systems_FALSE\"], \"priors\": {\"What is the probability for APS_Systems=aps_systems_TRUE?\": \"%?\", \"What is the probability for APS_Systems=aps_systems_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_TRUE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_TRUE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_TRUE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_TRUE?\": \"?%\", \"What is the probability for APS_Systems=aps_systems_FALSE if Advanced_AI_Capability=advanced_ai_capability_FALSE, Agentic_Planning=agentic_planning_FALSE, Strategic_Awareness=strategic_awareness_FALSE?\": \"?%\"}}\n        /* What is the probability for Advanced_AI_Capability=advanced_ai_capability_TRUE? */\n        /* What is the probability for Advanced_AI_Capability=advanced_ai_capability_FALSE? */\n        + [Advanced_AI_Capability]: AI systems that outperform humans on tasks that grant significant power in the world. {\"instantiations\": [\"advanced_ai_capability_TRUE\", \"advanced_ai_capability_FALSE\"], \"priors\": {\"What is the probability for Advanced_AI_Capability=advanced_ai_capability_TRUE?\": \"%?\", \"What is the probability for Advanced_AI_Capability=advanced_ai_capability_FALSE?\": \"%?\"}}\n        /* What is the probability for Agentic_Planning=agentic_planning_TRUE? */\n        /* What is the probability for Agentic_Planning=agentic_planning_FALSE? */\n        + [Agentic_Planning]: AI systems making and executing plans based on world models to achieve objectives. {\"instantiations\": [\"agentic_planning_TRUE\", \"agentic_planning_FALSE\"], \"priors\": {\"What is the probability for Agentic_Planning=agentic_planning_TRUE?\": \"%?\", \"What is the probability for Agentic_Planning=agentic_planning_FALSE?\": \"%?\"}}\n        /* What is the probability for Strategic_Awareness=strategic_awareness_TRUE? */\n        /* What is the probability for Strategic_Awareness=strategic_awareness_FALSE? */\n        + [Strategic_Awareness]: AI systems with models accurately representing power dynamics with humans. {\"instantiations\": [\"strategic_awareness_TRUE\", \"strategic_awareness_FALSE\"], \"priors\": {\"What is the probability for Strategic_Awareness=strategic_awareness_TRUE?\": \"%?\", \"What is the probability for Strategic_Awareness=strategic_awareness_FALSE?\": \"%?\"}}\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE? */\n      /* What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE? */\n      + [Difficulty_Of_Alignment]: It is harder to build aligned systems than misaligned systems that are attractive to deploy. {\"instantiations\": [\"difficulty_of_alignment_TRUE\", \"difficulty_of_alignment_FALSE\"], \"priors\": {\"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE?\": \"%?\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_TRUE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_TRUE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_TRUE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_TRUE?\": \"?%\", \"What is the probability for Difficulty_Of_Alignment=difficulty_of_alignment_FALSE if Instrumental_Convergence=instrumental_convergence_FALSE, Problems_With_Proxies=problems_with_proxies_FALSE, Problems_With_Search=problems_with_search_FALSE?\": \"?%\"}}\n        /* What is the probability for Instrumental_Convergence=instrumental_convergence_TRUE? */\n        /* What is the probability for Instrumental_Convergence=instrumental_convergence_FALSE? */\n        + [Instrumental_Convergence]: AI systems with misaligned objectives tend to seek power as an instrumental goal. {\"instantiations\": [\"instrumental_convergence_TRUE\", \"instrumental_convergence_FALSE\"], \"priors\": {\"What is the probability for Instrumental_Convergence=instrumental_convergence_TRUE?\": \"%?\", \"What is the probability for Instrumental_Convergence=instrumental_convergence_FALSE?\": \"%?\"}}\n        /* What is the probability for Problems_With_Proxies=problems_with_proxies_TRUE? */\n        /* What is the probability for Problems_With_Proxies=problems_with_proxies_FALSE? */\n        + [Problems_With_Proxies]: Optimizing for proxy objectives breaks correlations with intended goals. {\"instantiations\": [\"problems_with_proxies_TRUE\", \"problems_with_proxies_FALSE\"], \"priors\": {\"What is the probability for Problems_With_Proxies=problems_with_proxies_TRUE?\": \"%?\", \"What is the probability for Problems_With_Proxies=problems_with_proxies_FALSE?\": \"%?\"}}\n        /* What is the probability for Problems_With_Search=problems_with_search_TRUE? */\n        /* What is the probability for Problems_With_Search=problems_with_search_FALSE? */\n        + [Problems_With_Search]: Search processes can yield systems pursuing different objectives than intended. {\"instantiations\": [\"problems_with_search_TRUE\", \"problems_with_search_FALSE\"], \"priors\": {\"What is the probability for Problems_With_Search=problems_with_search_TRUE?\": \"%?\", \"What is the probability for Problems_With_Search=problems_with_search_FALSE?\": \"%?\"}}\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE? */\n      /* What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE? */\n      + [Deployment_Decisions]: Decisions to deploy potentially misaligned AI systems. {\"instantiations\": [\"deployment_decisions_DEPLOY\", \"deployment_decisions_WITHHOLD\"], \"priors\": {\"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY?\": \"%?\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD?\": \"%?\"}, \"posteriors\": {\"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_DEPLOY if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_STRONG, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_TRUE?\": \"?%\", \"What is the probability for Deployment_Decisions=deployment_decisions_WITHHOLD if Incentives_To_Build_APS=incentives_to_build_aps_WEAK, Deception_By_AI=deception_by_ai_FALSE?\": \"?%\"}}\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG? */\n        /* What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK? */\n        + [Incentives_To_Build_APS]: Strong incentives to build and deploy APS systems. {\"instantiations\": [\"incentives_to_build_aps_STRONG\", \"incentives_to_build_aps_WEAK\"], \"priors\": {\"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG?\": \"%?\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK?\": \"%?\"}, \"posteriors\": {\"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_STRONG if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_HIGH, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_STRONG?\": \"?%\", \"What is the probability for Incentives_To_Build_APS=incentives_to_build_aps_WEAK if Usefulness_Of_APS=usefulness_of_aps_LOW, Competitive_Dynamics=competitive_dynamics_WEAK?\": \"?%\"}}\n          /* What is the probability for Usefulness_Of_APS=usefulness_of_aps_HIGH? */\n          /* What is the probability for Usefulness_Of_APS=usefulness_of_aps_LOW? */\n          + [Usefulness_Of_APS]: APS systems are very useful for many valuable tasks. {\"instantiations\": [\"usefulness_of_aps_HIGH\", \"usefulness_of_aps_LOW\"], \"priors\": {\"What is the probability for Usefulness_Of_APS=usefulness_of_aps_HIGH?\": \"%?\", \"What is the probability for Usefulness_Of_APS=usefulness_of_aps_LOW?\": \"%?\"}}\n          /* What is the probability for Competitive_Dynamics=competitive_dynamics_STRONG? */\n          /* What is the probability for Competitive_Dynamics=competitive_dynamics_WEAK? */\n          + [Competitive_Dynamics]: Competitive pressures between AI developers. {\"instantiations\": [\"competitive_dynamics_STRONG\", \"competitive_dynamics_WEAK\"], \"priors\": {\"What is the probability for Competitive_Dynamics=competitive_dynamics_STRONG?\": \"%?\", \"What is the probability for Competitive_Dynamics=competitive_dynamics_WEAK?\": \"%?\"}}\n        /* What is the probability for Deception_By_AI=deception_by_ai_TRUE? */\n        /* What is the probability for Deception_By_AI=deception_by_ai_FALSE? */\n        + [Deception_By_AI]: AI systems deceiving humans about their true objectives. {\"instantiations\": [\"deception_by_ai_TRUE\", \"deception_by_ai_FALSE\"], \"priors\": {\"What is the probability for Deception_By_AI=deception_by_ai_TRUE?\": \"%?\", \"What is the probability for Deception_By_AI=deception_by_ai_FALSE?\": \"%?\"}}\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n    /* What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n    + [Corrective_Feedback]: Human society implementing corrections after observing problems. {\"instantiations\": [\"corrective_feedback_EFFECTIVE\", \"corrective_feedback_INEFFECTIVE\"], \"priors\": {\"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE?\": \"%?\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE?\": \"%?\"}, \"posteriors\": {\"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_EFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_OBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"?%\", \"What is the probability for Corrective_Feedback=corrective_feedback_INEFFECTIVE if Warning_Shots=warning_shots_UNOBSERVED, Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"?%\"}}\n      /* What is the probability for Warning_Shots=warning_shots_OBSERVED? */\n      /* What is the probability for Warning_Shots=warning_shots_UNOBSERVED? */\n      + [Warning_Shots]: Observable failures in weaker systems before catastrophic risks. {\"instantiations\": [\"warning_shots_OBSERVED\", \"warning_shots_UNOBSERVED\"], \"priors\": {\"What is the probability for Warning_Shots=warning_shots_OBSERVED?\": \"%?\", \"What is the probability for Warning_Shots=warning_shots_UNOBSERVED?\": \"%?\"}}\n      /* What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_TRUE? */\n      /* What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_FALSE? */\n      + [Rapid_Capability_Escalation]: AI capabilities escalating very rapidly, allowing little time for correction. {\"instantiations\": [\"rapid_capability_escalation_TRUE\", \"rapid_capability_escalation_FALSE\"], \"priors\": {\"What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_TRUE?\": \"%?\", \"What is the probability for Rapid_Capability_Escalation=rapid_capability_escalation_FALSE?\": \"%?\"}}\n/* What is the probability for Barriers_To_Understanding=barriers_to_understanding_HIGH? */\n/* What is the probability for Barriers_To_Understanding=barriers_to_understanding_LOW? */\n[Barriers_To_Understanding]: Difficulty in understanding the internal workings of advanced AI systems. {\"instantiations\": [\"barriers_to_understanding_HIGH\", \"barriers_to_understanding_LOW\"], \"priors\": {\"What is the probability for Barriers_To_Understanding=barriers_to_understanding_HIGH?\": \"%?\", \"What is the probability for Barriers_To_Understanding=barriers_to_understanding_LOW?\": \"%?\"}}\n/* What is the probability for Adversarial_Dynamics=adversarial_dynamics_TRUE? */\n/* What is the probability for Adversarial_Dynamics=adversarial_dynamics_FALSE? */\n[Adversarial_Dynamics]: Potentially adversarial relationships between humans and power-seeking AI. {\"instantiations\": [\"adversarial_dynamics_TRUE\", \"adversarial_dynamics_FALSE\"], \"priors\": {\"What is the probability for Adversarial_Dynamics=adversarial_dynamics_TRUE?\": \"%?\", \"What is the probability for Adversarial_Dynamics=adversarial_dynamics_FALSE?\": \"%?\"}}\n/* What is the probability for Stakes_Of_Error=stakes_of_error_HIGH? */\n/* What is the probability for Stakes_Of_Error=stakes_of_error_LOW? */\n[Stakes_Of_Error]: The escalating impact of mistakes with power-seeking AI systems. {\"instantiations\": [\"stakes_of_error_HIGH\", \"stakes_of_error_LOW\"], \"priors\": {\"What is the probability for Stakes_Of_Error=stakes_of_error_HIGH?\": \"%?\", \"What is the probability for Stakes_Of_Error=stakes_of_error_LOW?\": \"%?\"}}\n...\n\n\n\n\n\nAlong with these questions the following prompt is sent to the LLM:\n\nYou are an expert in probabilistic reasoning and Bayesian networks. Your task is\nto extend the provided ArgDown structure with probability information,\ncreating a BayesDown representation.\n\nFor each statement in the ArgDown structure, you need to:\n1. Estimate prior probabilities for each possible state\n2. Estimate conditional probabilities given parent states\n3. Maintain the original structure and relationships\n\nHere is the format to follow:\n[Node]: Description. { \"instantiations\": [\"node_TRUE\", \"node_FALSE\"], \"priors\": { \"p(node_TRUE)\": \"0.7\", \"p(node_FALSE)\": \"0.3\" }, \"posteriors\": { \"p(node_TRUE|parent_TRUE)\": \"0.9\", \"p(node_TRUE|parent_FALSE)\": \"0.4\", \"p(node_FALSE|parent_TRUE)\": \"0.1\", \"p(node_FALSE|parent_FALSE)\": \"0.6\" } }\n [Parent]: Parent description. {...}\n\n\nHere are the specific probability questions to answer:\n$questions\n\nArgDown structure to enhance:\n$argdown\n\nProvide the complete BayesDown representation with probabilities:\n\n\n3.5.4 Practically Meaningful BayesDown\nBridging Qualitative and Quantitative Representation\nIf the coordination crisis in AI governance stems partly from incompatible languages across domains—technical researchers speaking in mathematical formalisms, policy specialists in institutional frameworks, and ethicists in normative concepts—then effective coordination requires bridges between these domains. BayesDown serves as such a bridge, combining the narrative richness of qualitative argumentation with the precision of quantitative probability judgments.\nTraditional formal representations face a fundamental tradeoff: increase precision and you sacrifice accessibility; enhance accessibility and you lose precision. Mathematical notations offer exactness but exclude many stakeholders. Natural language provides accessibility but permits ambiguity and vagueness. This tradeoff creates communication barriers between technical and policy domains, limiting coordination on complex challenges like AI governance.\nBayesDown disrupts this tradeoff by creating a hybrid representation that preserves strengths from both worlds. Its design follows three key principles:\nFirst, human readability ensures the representation remains interpretable without specialized training. The syntax builds on familiar conventions from markdown and JSON, maintaining hierarchical relationships through indentation and encapsulating technical details within structured metadata. Unlike purely mathematical notations, the format preserves natural language descriptions alongside formal elements.\nSecond, machine processability enables computational analysis and transformation. The consistent syntax permits automated parsing, formal verification, and conversion to computational models like Bayesian networks. The structured JSON metadata provides clear paths for extracting probability information and mapping it to conditional probability tables.\nThird, contextual preservation maintains the connection to original arguments. By including descriptive text alongside formal structure, BayesDown retains the narrative context and qualitative considerations that inform probability judgments. This contextual information helps users interpret the model in light of the original arguments.\nConsider how these principles manifest in the BayesDown syntax. Each node begins with a bracketed title followed by a natural language description, preserving the core statement being formalized. The JSON metadata contains technical information like instantiations, priors, and posteriors, but keeps this information clearly separated from the narrative content. Hierarchical relationships use indentation and plus symbols, creating a visual structure that mirrors causal influence.\n\nExample BayesDown Excerpt from the Carlsmith model\n#| label: json_carlsmith_excerpt\n#| echo: true\n#| eval: true\n#| fig-cap: \"Example BayesDown Excerpt from the Carlsmith model\"\n#| fig-link: \"https://colab.research.google.com/github/VJMeyer/submission/blob/main/AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb#scrollTo=AFnu_1Ludahi\"\n#| fig-alt: \"Example BayesDown Excerpt from the Carlsmith model\"\n\n\n\n[Existential_Catastrophe]: The destruction of humanity's long-term potential due to AI systems we've lost control over. {\n  \"instantiations\": [\"existential_catastrophe_TRUE\", \"existential_catastrophe_FALSE\"],\n  \"priors\": {\"p(existential_catastrophe_TRUE)\": \"0.05\", \"p(existential_catastrophe_FALSE)\": \"0.95\"},\n  \"posteriors\": {\n    \"p(existential_catastrophe_TRUE|human_disempowerment_TRUE)\": \"0.95\",\n    \"p(existential_catastrophe_TRUE|human_disempowerment_FALSE)\": \"0.0\"\n  }\n}\n + [Human_Disempowerment]: Permanent and collective disempowerment of humanity relative to AI systems. {\n   \"instantiations\": [\"human_disempowerment_TRUE\", \"human_disempowerment_FALSE\"],\n   \"priors\": {\"p(human_disempowerment_TRUE)\": \"0.208\", \"p(human_disempowerment_FALSE)\": \"0.792\"},\n   \"posteriors\": {\n     \"p(human_disempowerment_TRUE|scale_of_power_seeking_TRUE)\": \"1.0\",\n     \"p(human_disempowerment_TRUE|scale_of_power_seeking_FALSE)\": \"0.0\"\n   }\n }\nThis excerpt from the Carlsmith model representation illustrates how BayesDown preserves both the narrative description (“The destruction of humanity’s long-term potential…”) and the precise probability judgments. Someone without technical background can still understand the core claims and their relationships, while someone seeking quantitative precision can find exact probability values.\nThe format supports multiple levels of engagement. At the most basic level, readers can follow the hierarchical structure to understand causal relationships between factors. At an intermediate level, they can examine probability judgments to assess the strength of different influences. At the most technical level, they can analyze the complete probabilistic model to perform inference and sensitivity analysis.\nThis multi-level accessibility creates important advantages for coordination across domains:\n\nTechnical-policy translation: BayesDown provides a common reference point for technical researchers explaining safety concerns and policy specialists evaluating governance options, reducing communication barriers.\nArgumentation transparency: The format makes assumptions explicit, helping identify genuine disagreements versus terminological confusion or unstated premises.\nIncremental formalization: BayesDown supports varying levels of formality, from qualitative structure to complete probability specifications, allowing gradual progression from informal to formal representations.\nVerification flexibility: Human experts can verify extracted representations at different levels—checking structural correctness without assessing probabilities, or focusing on critical probability judgments without reviewing the entire model.\n\nThe hybrid nature of BayesDown aligns with how experts typically communicate complex ideas: combining qualitative explanations with quantitative judgments, using natural language to provide context for formal claims, and adjusting precision based on audience needs. By mirroring these natural communication patterns, BayesDown makes formalization more intuitive and accessible.\nThis bridging function extends beyond representation to influence the entire extraction and analysis workflow. When extracting from text, the two-stage process preserves narrative context alongside formal structure. When visualizing models, interactive interfaces provide both qualitative descriptions and quantitative details. When evaluating policies, counterfactual analysis incorporates both mathematical precision and contextual interpretation.\nIn the broader context of the coordination crisis, BayesDown demonstrates how thoughtfully designed intermediate representations can overcome communication barriers between domains. Rather than forcing all stakeholders to adopt a single specialized language, it creates a flexible format that accommodates different perspectives while enabling precise analysis—precisely the kind of bridge needed for effective coordination on complex governance challenges.\n\n\n\n3.5.5 Interactive Visualization and Exploration\n\nComplex probabilistic models like Bayesian networks contain rich information, but they often remain inaccessible to many stakeholders. A conditional probability table with dozens of values conveys precise relationships, but few can intuitively grasp its implications. This accessibility gap limits the potential for coordinated action on AI governance challenges—what good is formalization if the resulting models remain opaque to most decision-makers?\nAMTAIR addresses this challenge through interactive visualization designed to make complex probabilistic relationships accessible to diverse stakeholders. The approach combines visual encoding of probability information, progressive disclosure of details, and interactive exploration capabilities to create intuitive interfaces for complex models.\nThe visualization system follows several key design principles:\nFirst, visual encoding of probability uses color gradients to represent likelihood values. Nodes are colored on a spectrum from red (low probability) to green (high probability) based on their primary state’s probability. This simple visual cue provides immediate insights into which outcomes are more or less likely without requiring numerical interpretation.\nSecond, structural classification uses border colors to indicate node types based on network position. Blue borders designate root causes (nodes without parents), purple borders mark intermediate nodes (with both parents and children), and magenta borders highlight leaf nodes (final effects without children). This classification helps users understand the causal flow through the network.\nThird, progressive disclosure presents information in layers of increasing detail. Basic node information appears in the visualization itself, additional details emerge in tooltips on hover, and comprehensive probability tables display in modal windows on click. This layered approach prevents information overload while ensuring all details remain accessible.\nFourth, interactive exploration allows users to reorganize nodes, zoom in on areas of interest, adjust physics parameters, and investigate probability values. These capabilities transform the visualization from a static image into an explorable knowledge landscape.\nThe complete BayesDown representation was processed through the AMTAIR pipeline, resulting in a structured DataFrame and ultimately a Bayesian network.\n\nThe figure below shows the interactive visualization of Carlsmith’s model, highlighting how color, border styling, and layout work together to represent complex causal relationships:\nThe visualization system implements these principles through a combination of NetworkX for graph representation and PyVis for interactive display, with custom HTML generation for tooltips and modals:\nThe resulting visualization (Figure 10) shows the complete Carlsmith model with color-coded nodes representing probability values:\n[FIGURE 10: Interactive visualization of Carlsmith’s model showing color-coded nodes and relationships]\n\n\n\nCode\n# @title 4.4.0 --- Main Visualization Function --- [main_visualization_function]\n\ndef create_bayesian_network_with_probabilities(df):\n    \"\"\"\n    Create an interactive Bayesian network visualization with enhanced\n    probability visualization and node classification based on network structure.\n    \"\"\"\n    # Create a directed graph\n    G = nx.DiGraph()\n\n    # Add nodes with proper attributes\n    for idx, row in df.iterrows():\n        title = row['Title']\n        description = row['Description']\n\n        # Process probability information\n        priors = get_priors(row)\n        instantiations = get_instantiations(row)\n\n        # Add node with base information\n        G.add_node(\n            title,\n            description=description,\n            priors=priors,\n            instantiations=instantiations,\n            posteriors=get_posteriors(row)\n        )\n\n    # Add edges\n    for idx, row in df.iterrows():\n        child = row['Title']\n        parents = get_parents(row)\n\n        # Add edges from each parent to this child\n        for parent in parents:\n            if parent in G.nodes():\n                G.add_edge(parent, child)\n\n    # Classify nodes based on network structure\n    classify_nodes(G)\n\n    # Create network visualization\n    net = Network(notebook=True, directed=True, cdn_resources=\"in_line\", height=\"600px\", width=\"100%\")\n\n    # Configure physics for better layout\n    net.force_atlas_2based(gravity=-50, spring_length=100, spring_strength=0.02)\n    net.show_buttons(filter_=['physics'])\n\n    # Add the graph to the network\n    net.from_nx(G)\n\n    # Enhance node appearance with probability information and classification\n    for node in net.nodes:\n        node_id = node['id']\n        node_data = G.nodes[node_id]\n\n        # Get node type and set border color\n        node_type = node_data.get('node_type', 'unknown')\n        border_color = get_border_color(node_type)\n\n        # Get probability information\n        priors = node_data.get('priors', {})\n        true_prob = priors.get('true_prob', 0.5) if priors else 0.5\n\n        # Get proper state names\n        instantiations = node_data.get('instantiations', [\"TRUE\", \"FALSE\"])\n        true_state = instantiations[0] if len(instantiations) &gt; 0 else \"TRUE\"\n        false_state = instantiations[1] if len(instantiations) &gt; 1 else \"FALSE\"\n\n        # Create background color based on probability\n        background_color = get_probability_color(priors)\n\n        # Create tooltip with probability information\n        tooltip = create_tooltip(node_id, node_data)\n\n        # Create a simpler node label with probability\n        simple_label = f\"{node_id}\\np={true_prob:.2f}\"\n\n        # Store expanded content as a node attribute for use in click handler\n        node_data['expanded_content'] = create_expanded_content(node_id, node_data)\n\n        # Set node attributes\n        node['title'] = tooltip  # Tooltip HTML\n        node['label'] = simple_label  # Simple text label\n        node['shape'] = 'box'\n        node['color'] = {\n            'background': background_color,\n            'border': border_color,\n            'highlight': {\n                'background': background_color,\n                'border': border_color\n            }\n        }\n\n    # Set up the click handler with proper data\n    setup_data = {\n        'nodes_data': {node_id: {\n            'expanded_content': json.dumps(G.nodes[node_id].get('expanded_content', '')),\n            'description': G.nodes[node_id].get('description', ''),\n            'priors': G.nodes[node_id].get('priors', {}),\n            'posteriors': G.nodes[node_id].get('posteriors', {})\n        } for node_id in G.nodes()}\n    }\n\n    # Add custom click handling JavaScript\n    click_js = \"\"\"\n    // Store node data for click handling\n    var nodesData = %s;\n\n    // Add event listener for node clicks\n    network.on(\"click\", function(params) {\n        if (params.nodes.length &gt; 0) {\n            var nodeId = params.nodes[0];\n            var nodeInfo = nodesData[nodeId];\n\n            if (nodeInfo) {\n                // Create a modal popup for expanded content\n                var modal = document.createElement('div');\n                modal.style.position = 'fixed';\n                modal.style.left = '50%%';\n                modal.style.top = '50%%';\n                modal.style.transform = 'translate(-50%%, -50%%)';\n                modal.style.backgroundColor = 'white';\n                modal.style.padding = '20px';\n                modal.style.borderRadius = '5px';\n                modal.style.boxShadow = '0 0 10px rgba(0,0,0,0.5)';\n                modal.style.zIndex = '1000';\n                modal.style.maxWidth = '80%%';\n                modal.style.maxHeight = '80%%';\n                modal.style.overflow = 'auto';\n\n                // Parse the JSON string back to HTML content\n                try {\n                    var expandedContent = JSON.parse(nodeInfo.expanded_content);\n                    modal.innerHTML = expandedContent;\n                } catch (e) {\n                    modal.innerHTML = 'Error displaying content: ' + e.message;\n                }\n\n                // Add close button\n                var closeBtn = document.createElement('button');\n                closeBtn.innerHTML = 'Close';\n                closeBtn.style.marginTop = '10px';\n                closeBtn.style.padding = '5px 10px';\n                closeBtn.style.cursor = 'pointer';\n                closeBtn.onclick = function() {\n                    document.body.removeChild(modal);\n                };\n                modal.appendChild(closeBtn);\n\n                // Add modal to body\n                document.body.appendChild(modal);\n            }\n        }\n    });\n    \"\"\" % json.dumps(setup_data['nodes_data'])\n\n    # Save the graph to HTML\n    html_file = \"bayesian_network.html\"\n    net.save_graph(html_file)\n\n    # Inject custom click handling into HTML\n    try:\n        with open(html_file, \"r\") as f:\n            html_content = f.read()\n\n        # Insert click handling script before the closing body tag\n        html_content = html_content.replace('&lt;/body&gt;', f'&lt;script&gt;{click_js}&lt;/script&gt;&lt;/body&gt;')\n\n        # Write back the modified HTML\n        with open(html_file, \"w\") as f:\n            f.write(html_content)\n\n        return HTML(html_content)\n    except Exception as e:\n        return HTML(f\"&lt;p&gt;Error rendering HTML: {str(e)}&lt;/p&gt;\"\n        + \"&lt;p&gt;The network visualization has been saved to '{html_file}'&lt;/p&gt;\")\n\n\n\n\n[FIGURE N: Interactive visualization of Carlsmith’s model showing color-coded nodes and causal relationships]\nThis visualization reveals several structural insights:\n\nCentral importance of “Misaligned_Power_Seeking” as a hub node with multiple parents and children\nMultiple pathways to “Existential_Catastrophe” through different intermediate factors\nClusters of related variables forming coherent subarguments (e.g., factors affecting alignment difficulty)\nFlow of influence from technical factors (bottom) through deployment decisions to ultimate outcomes (top)\n\nThe implementation successfully handles the complexity of Carlsmith’s model, correctly processing the multi-level structure, resolving repeated node references, and calculating appropriate probability distributions. The interactive visualization makes this complex model accessible, allowing users to explore different aspects of the argument through intuitive navigation.\nSeveral key aspects of the implementation were particularly important for handling this complex model:\n\nThe parent-child relationship detection algorithm correctly identified hierarchical relationships despite the complex structure with repeated nodes and multiple levels.\nThe probability question generation system created appropriate questions for all variables, including those with multiple parents requiring factorial combinations of conditional probabilities.\nThe network enhancement functions calculated useful metrics like centrality measures and Markov blankets that help interpret the model structure.\nThe visualization system effectively presented the complex network through color-coding, interactive exploration, and progressive disclosure of details.\n\nThe successful application to Carlsmith’s model demonstrates the AMTAIR approach’s scalability to complex real-world arguments. While the canonical rain-sprinkler-lawn example validated correctness, this application proves practical utility for sophisticated multi-level arguments with dozens of variables and complex interdependencies—precisely the kind of arguments that characterize AI risk assessments.\nThis capability addresses a core limitation of the original MTAIR framework: the labor intensity of manual formalization. Where manually converting Carlsmith’s argument to a formal model might take days of expert time, the AMTAIR approach accomplished this in minutes, creating a foundation for further analysis and exploration.\nBeyond the core visualization, the system includes specialized components that enhance understanding of probabilistic relationships:\n\nProbability bars provide visual representations of probability distributions, showing relative likelihoods of different states using color-coded horizontal bars with numeric labels.\nConditional probability tables organize complex relationships into structured matrices, displaying how different combinations of parent states influence probability distributions.\nSensitivity indicators highlight which nodes and relationships most significantly affect outcomes, directing attention to critical factors.\n\nThese components work together to create an intuitive interface for complex probabilistic models. A user might start by exploring the overall structure to understand key factors and relationships, hover over nodes of interest to see probability summaries, then click on specific nodes to examine detailed conditional probabilities.\nThe benefits of this visualization approach extend beyond aesthetic appeal to fundamental improvements in understanding and communication:\nFirst, intuitive comprehension of probability relationships becomes possible even for those without formal training in Bayesian statistics. The color coding provides immediate visual cues about which outcomes are more likely, while interactive exploration allows users to develop intuition about how different factors influence results.\nSecond, cross-stakeholder communication improves through shared visual reference points. Technical experts can use the visualizations to explain complex relationships to policy specialists, while governance experts can identify institutional factors that might be incorporated into the models.\nThird, disagreement identification becomes more precise as stakeholders can point to specific nodes, relationships, or probability values where their views differ, focusing discussion on substantive issues rather than terminological confusion.\nFourth, intervention assessment becomes more concrete as users can see how changing specific factors influences downstream effects, providing intuitive understanding of causal pathways and leverage points.\nThe visualization system demonstrates how thoughtful interface design can overcome barriers to understanding complex formal models. By making probabilistic relationships visually intuitive and progressively disclosing details based on user interest, it creates bridges between mathematical precision and human comprehension—precisely the kind of bridge needed to support coordination across domains in AI governance.\nThis approach reflects a broader principle: formalization is most valuable when it enhances rather than replaces human understanding. The AMTAIR visualization doesn’t simplify complex relationships; it makes them more accessible by leveraging visual cognition, interactive exploration, and progressive disclosure. This human-centered approach to formalization creates tools that augment rather than replace expert judgment, enhancing our collective ability to understand and address complex governance challenges.\n\nInsights from Formalization\nFormal representation reveals several insights:\nCritical Path Analysis: The pathway through APS development and deployment decisions carries the highest risk contribution.\nSensitivity Points: Small changes in deployment probability create large changes in overall risk.\nIntervention Opportunities: Improving alignment difficulty or deployment governance show highest impact potential.\nThese insights emerge naturally from formal analysis but remain implicit in textual arguments.\n\n\n\n\n3.5.6 Validation Against Original (From the MTAIR Project)",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-validation-methodology",
    "href": "chapters/Outlines/Outline_13.html#sec-validation-methodology",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.6 Validation Methodology",
    "text": "3.6 Validation Methodology\n\nEstablishing trust in automated extraction requires rigorous validation across multiple dimensions.\n\n\n3.6.1 Ground Truth Construction\n\nPlan the process:\n1. Expert selection criteria\n2. Training on extraction methodology\n3. Independent extraction procedures\n4. Consensus building process\n5. Inter-rater reliability metrics\n–&gt;\n\n\n3.6.2 Evaluation Metrics\n\n–&gt;\n\n\n3.6.3 Results Summary\n\nPerformance is strongest for explicit structural elements and numerical probabilities, with more challenges in extracting implicit relationships and qualitative uncertainty. –&gt;\n\n\n3.6.4 Error Analysis\n\nCommon failure modes to avoid:\nImplicit Assumptions: Unstated background assumptions that experts infer but system misses.\nComplex Conditionals: Nested conditionals with multiple antecedents challenge current parsing.\nAmbiguous Quantifiers: Terms like “significant” lack clear probability mapping without context.\nCoreference Resolution: Pronouns and indirect references create attribution challenges.\n\nUnderstanding these limitations guides both current usage and future improvements.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#extensions-opportunities-inference-analysis",
    "href": "chapters/Outlines/Outline_13.html#extensions-opportunities-inference-analysis",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.7 Extensions & Opportunities: Inference & Analysis",
    "text": "3.7 Extensions & Opportunities: Inference & Analysis\n\nQuantification & Formal Approximation — Inference: Monte Carlo Sampling over Probability Distributions\n\n3.7.1 Overview of Practical Software Implementations\n\n\n\n\n3.7.2 AI Risk Pathway Analyzer (ARPA)\n1. Document Ingestion System: Handles format normalization, metadata extraction, and citation tracking for diverse input formats. 2. LLM-Powered Extraction Pipeline: Uses two-stage prompting to identify variables, claims, and causal relationships from text. 3. ArgDown Representation Generator: Creates structured intermediate representation of arguments with formal syntax. 4. Bayesian Network Constructor: Transforms ArgDown into formal Bayesian networks with nodes and edges. 5. Probability Quantification Module: Populates conditional probability tables from extracted judgments. 6. Interactive Visualization Interface: Provides intuitive visual access to network structure and probabilities. 7. Sensitivity Analysis Engine: Identifies critical variables and tests robustness of conclusions.\nThe cornerstone system that transforms unstructured AI safety literature into formal, analyzable models. Like a Rosetta Stone for AI governance, ARPA creates a common language for discourse by extracting the implicit causal models embedded in research papers and converting them into explicit Bayesian networks. Its strategic value lies in overcoming the fundamental information processing bottleneck in AI governance—making the invisible visible by revealing the assumptions, relationships, and probability judgments that drive different conclusions about AI risk.\n\n\n\n\n\n3.7.3 P(Doom) Calculator\n\n\n3.7.4 Worldview Comparator\nA “gifted, diplomatic translator” that helps to reveal the hidden landscape of agreement and disagreement across different perspectives on AI risk. This system provides the cartography of ideas—mapping where different worldviews converge, diverge, and where crucial disagreements (“cruxes”) significantly affect conclusions. Its strategic value lies in focusing discourse on substantive disagreements rather than terminological differences, enabling more productive collaboration across philosophical and methodological divides within the AI safety community.\n1. Structural Comparison Engine: Identifies isomorphic subgraphs between different models and maps shared causal pathways. 2. Parameter Difference Analyzer: Quantifies differences in probability distributions across models. 3. Crux Identification System: Detects critical disagreements that significantly affect conclusions. 4. Worldview Explainer: Provides conversational interface for exploring different perspectives. 5. Worldview Communicator: Translates concepts between different terminological frameworks. 6. Consensus Model Builder: Identifies shared structures and constructs hybrid models representing areas of agreement.\n\n\n3.7.5 Policy Impact Evaluator\n1. Policy Representation System: Translates governance proposals into formal intervention parameters. 2. Counterfactual Analysis Engine: Implements Pearl’s do-calculus for simulating intervention effects. 3. Multi-Worldview Evaluator: Tests policy effects across different extracted models. 4. Intervention Portfolio Analyzer: Assesses combinations of policies for synergies and conflicts. 5. Policy Effectiveness Dashboard: Visualizes impact assessments with uncertainty representation.\nA policy simulator that functions like a governance wind tunnel—testing how specific interventions might perform across different possible futures. By representing policies as modifications to causal networks, this system enables rigorous counterfactual analysis of intervention effects. Its strategic value lies in transforming abstract policy discussions into concrete, quantifiable assessments of expected impact, helping governance stakeholders allocate resources to the most effective interventions.\n\n\n3.7.6 AI Risk Pathway Visualizer\n1. Risk Level Aggregation System: Combines multiple factors into summary risk metrics. 2. Temporal Tracking Interface: Records and displays changes in assessments over time. 3. Component Breakdown Visualizer: Separates overall risk into constituent factors. 4. Interactive Educational Components: Provides background on key concepts and methodologies. 5. Explanation Generator: Creates natural language interpretations of current status.\nA public-facing translation layer that converts complex probabilistic models into intuitive visual representations accessible to broader audiences. Like the Doomsday Clock for nuclear risk, this system creates focal points for public discourse about AI safety. Its strategic value lies in making technical risk assessments comprehensible to policymakers, journalists, and the public, expanding the reach and impact of AI safety research beyond technical communities.\n\n\n3.7.7 Strategic Intervention Generator\n1. Robust Strategy Identification System: Finds strategies that perform well across multiple scenarios. 2. Minimax Regret Calculator: Identifies strategies that minimize worst-case disappointment. 3. Option Value Analyzer: Evaluates strategies that preserve future flexibility and choices. 4. Intervention Portfolio Builder: Constructs complementary bundles of policy interventions. 5. Dependency Mapping Visualizer: Shows relationships and prerequisites between interventions.\nAn advanced decision support system that identifies robust governance strategies across multiple possible futures. Operating like a strategic chess engine, this system evaluates intervention portfolios under deep uncertainty to find approaches that preserve options and minimize maximum regret. Its strategic value lies in shifting governance planning from optimizing for specific scenarios to developing adaptive strategies that remain valuable despite fundamental uncertainty about AI development trajectories.\n\n\n3.7.8 Cross-Domain Understanding Communicator\n1. Concept Mapping System: Identifies equivalent concepts across different domain languages. 2. Terminology Translation Engine: Converts specialized terms between different disciplines. 3. Implication Surfacing Tool: Highlights relevant cross-domain considerations for specific questions. 4. Background Knowledge Provider: Supplies necessary context for understanding concepts. 5. Cross-Domain Recommendation Engine: Suggests relevant resources across disciplinary boundaries.\nAn interdisciplinary bridge-builder that connects specialists across technical alignment, governance, and forecasting domains. This system functions as a universal translator for AI safety, identifying equivalent concepts across different disciplinary languages and surfacing relevant cross-domain insights. Its strategic value lies in breaking down the knowledge silos that impede comprehensive strategy development, enabling researchers from different backgrounds to build on each other’s work more effectively.\n\n\n3.7.9 Policy Brief Communicator\n1. Audience Analysis System: Determines appropriate framing and detail level for target readers. 2. Jurisdictional Context Adapter: Tailors content to relevant legal and institutional frameworks. 3. Recommendation Formulator: Generates actionable governance suggestions from technical insights. 4. Format Template Library: Applies appropriate structure for different policy contexts. 5. Evidence Contextualization Engine: Presents technical evidence in accessible and persuasive ways.\nA specialized translation system that converts technical risk analyses into actionable policy documents tailored to specific governance contexts. This system bridges the gap between technical understanding and practical implementation by packaging complex insights into formats familiar to policymakers. Its strategic value lies in increasing the policy impact of technical research by making insights accessible and actionable for decision-makers in government, industry, and civil society.\n\n\n3.7.10 Prediction Market Integration\n—   Live Data Updating: Crowdsourcing Collective Intelligence Via API Integrations\n\nForecast Integration Dashboard\n1. Forecasting Platform API Connectors: Establishes connections with prediction markets and forecasting platforms. 2. Semantic Question Mapper: Links forecast questions to corresponding model variables. 3. Forecast Weighting System: Determines influence of different forecast sources based on track record. 4. Dynamic Update Engine: Manages synchronization between forecasts and model parameters. 5. Forecast Relevance Calculator: Identifies which forecasts would most reduce uncertainty in the model.\nA living nervous system that connects formal models to real-time data streams from forecasting platforms. This system ensures that risk assessments remain current as new information emerges, creating dynamic models that evolve with the rapidly changing AI landscape. Its strategic value lies in bridging the gap between static theoretical models and emerging empirical evidence, leveraging collective intelligence from prediction markets to continuously refine probability estimates.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-policy-evaluation",
    "href": "chapters/Outlines/Outline_13.html#sec-policy-evaluation",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.7 Policy Evaluation Capabilities",
    "text": "3.7 Policy Evaluation Capabilities\n\nBeyond extraction and visualization, AMTAIR enables systematic policy analysis through formal intervention modeling.\n\n\n3.7.1 Intervention Representation\n\n–&gt;\n\n\n3.7.2 Example: Deployment Governance\nConsider a policy requiring safety certification before deployment:\nIntervention: Set P(deployment|misaligned) = 0.1 (from 0.7)\nResults:\n\nBaseline P(catastrophe) = 0.05\nIntervened P(catastrophe) = 0.012\nRelative risk reduction = 76%\nNumber needed to regulate = 26 deployments\n\nThis hypothetical quantitative analysis enables comparison across interventions.\n\n\n\n3.7.3 Robustness Analysis\n\n\n\n\n\n\nCross-Worldview Robustness\n\n\n\nPolicies must work across worldviews. AMTAIR enables multi-model evaluation, parameter sensitivity testing, scenario analysis, and confidence bound computation—ensuring interventions remain effective despite uncertainty.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-visualization-design",
    "href": "chapters/Outlines/Outline_13.html#sec-visualization-design",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.8 Interactive Visualization Design",
    "text": "3.8 Interactive Visualization Design\n\nMaking Bayesian networks accessible to diverse stakeholders requires careful visualization design.\n\n3.8.1 Visual Encoding Strategy\n\nThe system uses multiple visual channels:\nColor: Probability magnitude (green=high, red=low)\nBorders: Node type (blue=root, purple=intermediate, magenta=effect)\nSize: Centrality in network (larger=more influential)\nLayout: Force-directed positioning reveals clusters\n\n\n\n\n3.8.2 Progressive Disclosure\nInformation appears at appropriate levels:\n\nOverview: Network structure and color coding\nHover: Node description and prior probability\nClick: Full probability tables and details\nInteraction: Drag to rearrange, zoom to explore\n\nThis layered approach serves both quick assessment and deep analysis needs.\n\n\n\n3.8.3 User Interface Elements",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-market-integration",
    "href": "chapters/Outlines/Outline_13.html#sec-market-integration",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.9 Integration with Prediction Markets",
    "text": "3.9 Integration with Prediction Markets\n\nWhile full integration remains future work, the architecture supports connection to live forecasting data.\n\n3.9.1 Design for Integration\n\n\n\n\n\n\nIntegration Architecture\n\n\n\nThe system anticipates market connections through API specifications for major platforms, semantic matching algorithms, probability aggregation methods, and update scheduling with caching.\n\n\n\nDesign documentation needed:\n- API specifications for major platforms\n- Semantic matching algorithms\n- Probability aggregation methods\n- Update scheduling and caching\n\n\n3.9.2 Challenges and Opportunities\n\nKey integration challenges:\n\nQuestion Mapping: Model variables rarely match market questions exactly\nTemporal Alignment: Markets forecast specific dates, models consider scenarios\nQuality Variation: Market depth and participation vary significantly\n\nDespite challenges, even partial integration provides value through external validation and dynamic updating.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-computational-performance",
    "href": "chapters/Outlines/Outline_13.html#sec-computational-performance",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.10 Computational Performance Analysis",
    "text": "3.10 Computational Performance Analysis\n\nAs networks grow large, computational challenges emerge requiring sophisticated approaches.\n\n3.10.1 Exact vs. Approximate Inference\n\nSmall networks enable exact inference through variable elimination. Larger networks require approximation:\nMonte Carlo Methods: Sample from probability distributions to estimate queries\nVariational Inference: Optimize simpler distributions to approximate true posteriors\nBelief Propagation: Pass messages between nodes to converge on beliefs\nThe system automatically selects appropriate methods based on network properties.\n\n\n\n\n3.10.2 Scaling Strategies\nFor very large networks:\n\nDocument strategies with benchmarks:\n1. Hierarchical decomposition algorithms\n2. Pruning criteria and impact\n3. Caching architecture\n4. Parallelization speedups",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-results-achievements",
    "href": "chapters/Outlines/Outline_13.html#sec-results-achievements",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.11 Results and Achievements",
    "text": "3.11 Results and Achievements\n\n\n3.11.1 Extraction Quality Assessment\n\n–&gt;\n\n\n3.11.2 Computational Performance\n\n\n\n3.11.3 Policy Impact Evaluation\n\n\n–&gt;",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-technical-summary",
    "href": "chapters/Outlines/Outline_13.html#sec-technical-summary",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "3.12 Summary of Technical Contributions",
    "text": "3.12 Summary of Technical Contributions\nAMTAIR successfully demonstrates:\n\nAutomated extraction from natural language to formal models\nTwo-stage architecture separating structure from quantification\nHigh fidelity preservation of complex arguments\nInteractive visualization accessible to diverse users\nScalable implementation handling realistic network sizes\n\nThese achievements validate the feasibility of computational coordination infrastructure for AI governance.\n\nThese results demonstrate both the feasibility and value of automated model extraction for AI governance. However, several important considerations and limitations merit discussion. The next chapter critically examines these issues, addresses potential objections, and explores the broader implications of this approach for enhancing epistemic security in AI governance.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-technical-limitations",
    "href": "chapters/Outlines/Outline_13.html#sec-technical-limitations",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "4.1 Technical Limitations and Responses",
    "text": "4.1 Technical Limitations and Responses\n\n4.1.1 Objection 1: Extraction Quality Boundaries\n\nCritic: “Complex implicit reasoning chains resist formalization; automated extraction will systematically miss nuanced arguments and subtle conditional relationships that human experts would identify.”\n\nResponse: This concern has merit—extraction does face inherent limitations. However, the empirical results tell a more nuanced story. With extraction achieving 85%+ accuracy for structural relationships and 73% for probability capture, the system performs well enough for practical use while falling short of human expert performance.\nMore importantly, AMTAIR employs a hybrid human-AI workflow that addresses this limitation:\n\nTwo-stage verification: Humans review structural extraction before probability quantification\nTransparent outputs: All intermediate representations remain human-readable\n\nIterative refinement: Extraction prompts improve based on error analysis\nEnsemble approaches: Multiple extraction attempts can identify ambiguities\n\nThe question is not whether automated extraction perfectly captures every nuance—it doesn’t. Rather, it’s whether imperfect extraction still provides value over no formal representation. When the alternative is relying on conflicting mental models that remain entirely implicit, even 75% accurate formal models represent significant progress.\nFurthermore, extraction errors often reveal interesting properties of the source arguments themselves—ambiguities that human readers gloss over become explicit when formalization fails. This diagnostic value enhances rather than undermines the approach.\n\n\n\n4.1.2 Objection 2: False Precision in Uncertainty\nCritic: “Attaching exact probabilities to unprecedented events like AI catastrophe is fundamentally misguided. The numbers create false confidence in what amounts to educated speculation about radically uncertain futures.”\nResponse: This philosophical objection strikes at the heart of formal risk assessment. However, AMTAIR addresses it through several design choices:\nFirst, the system explicitly represents uncertainty about uncertainty. Rather than point estimates, the framework supports probability distributions over parameters. When someone says “likely” we might model this as Beta(8,2) rather than exactly 0.8, capturing both the central estimate and our uncertainty about it.\n\n\nTechnical requirements:\n\n- Beta distributions for probability parameters\n- Dirichlet for multi-state variables\n- Propagation through inference\n- Visualization of uncertainty bounds\n\nSecond, all probabilities are explicitly conditional on stated assumptions. The system doesn’t claim “P(catastrophe) = 0.05” absolutely, but rather “Given Carlsmith’s model assumptions, P(catastrophe) = 0.05.” This conditionality is preserved throughout analysis.\nThird, sensitivity analysis reveals which probabilities actually matter. Often, precise values are unnecessary—knowing whether a parameter is closer to 0.1 or 0.9 suffices for decision-making. The formalization helps identify where precision matters and where it doesn’t.\nFinally, the alternative to quantification isn’t avoiding the problem but making it worse. When experts say “highly likely” or “significant risk,” they implicitly reason with probabilities. Formalization simply makes these implicit quantities explicit and subject to scrutiny. As Dennis Lindley noted, “Uncertainty is not in the events, but in our knowledge about them.”\n\n@Lindley (2013)\n\n\n4.1.3 Objection 3: Correlation Complexity\nCritic: “Bayesian networks assume conditional independence given parents, but real-world AI risks involve complex correlations. Ignoring these dependencies could dramatically misrepresent risk levels.”\nResponse: Standard Bayesian networks do face limitations with correlation representation—this is a genuine technical challenge. However, several approaches within the framework address this:\nExplicit correlation nodes: When factors share hidden common causes, we can add latent variables to capture correlations. For instance, “AI research culture” might influence both “capability advancement” and “safety investment.”\n\nCopula methods: For known correlation structures, copula functions can model dependencies while preserving marginal distributions. This extends standard Bayesian networks significantly.4\n\nNelson (2006)\nSensitivity bounds: When correlations remain uncertain, we can compute bounds on outcomes under different correlation assumptions. This reveals when correlations critically affect conclusions.\nModel ensembles: Different correlation structures can be modeled separately and results aggregated, similar to climate modeling approaches.\nMore fundamentally, the question is whether imperfect independence assumptions invalidate the approach. In practice, explicitly modeling first-order effects with known limitations often proves more valuable than attempting to capture all dependencies informally. The framework makes assumptions transparent, enabling targeted improvements where correlations matter most.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-conceptual-concerns",
    "href": "chapters/Outlines/Outline_13.html#sec-conceptual-concerns",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "4.2 Conceptual and Methodological Concerns",
    "text": "4.2 Conceptual and Methodological Concerns\n\n4.2.1 Objection 4: Democratic Exclusion\nCritic: “Transforming policy debates into complex graphs and equations will sideline non-technical stakeholders, concentrating influence among those comfortable with formal models. This technocratic approach undermines democratic participation in crucial decisions about humanity’s future.”\nResponse: This concern about technocratic exclusion deserves serious consideration—formal methods can indeed create barriers. However, AMTAIR’s design explicitly prioritizes accessibility alongside rigor:\nProgressive disclosure interfaces allow engagement at multiple levels. A policymaker might explore visual network structures and probability color-coding without engaging mathematical details. Interactive features let users modify assumptions and see consequences without understanding implementation.\nNatural language preservation ensures original arguments remain accessible. The BayesDown format maintains human-readable descriptions alongside formal specifications. Users can always trace from mathematical representations back to source texts.\nComparative advantage comes from making implicit technical content explicit, not adding complexity. When experts debate AI risk, they already employ sophisticated probabilistic reasoning—formalization reveals rather than creates this complexity. Making hidden assumptions visible arguably enhances rather than reduces democratic participation.\nMultiple interfaces serve different communities. Researchers access full technical depth, policymakers use summary dashboards, public stakeholders explore interactive visualizations. The same underlying model supports varied engagement modes.\nRather than excluding non-technical stakeholders, proper implementation can democratize access to expert reasoning by making it inspectable and modifiable. The risk lies not in formalization itself but in poor interface design or gatekeeping behaviors around model access.\n\n\n\n4.2.2 Objection 5: Oversimplification of Complex Systems\nCritic: “Forcing rich socio-technical systems into discrete Bayesian networks necessarily loses crucial dynamics—feedback loops, emergent properties, institutional responses, and cultural factors that shape AI development. The models become precise but wrong.”\nResponse: All models simplify by necessity—as Box noted, “All models are wrong, but some are useful.” The question becomes whether formal simplifications improve upon informal mental models:\nTransparent limitations make formal models’ shortcomings explicit. Unlike mental models where simplifications remain hidden, network representations clearly show what is and isn’t included. This transparency enables targeted criticism and improvement.\nIterative refinement allows models to grow more sophisticated over time. Starting with first-order effects and adding complexity where it proves important follows successful practice in other domains. Climate models began simply and added dynamics as computational power and understanding grew.\nComplementary tools address different aspects of the system. Bayesian networks excel at probabilistic reasoning and intervention analysis. Other approaches—agent-based models, system dynamics, scenario planning—can capture different properties. AMTAIR provides one lens, not the only lens.\nEmpirical adequacy ultimately judges models. If simplified representations enable better predictions and decisions than informal alternatives, their abstractions are justified. Early results suggest formal models, despite simplifications, outperform intuitive reasoning for complex risk assessment.\nThe goal isn’t creating perfect representations but useful ones. By making simplifications explicit and modifiable, formal models enable systematic improvement in ways mental models cannot.\n\nBox (1976)\n\n\n4.2.4 Objection 6: Idosyncratic Implementation and Modeling Choices {sec-idosyncratic}",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-red-teaming",
    "href": "chapters/Outlines/Outline_13.html#sec-red-teaming",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "4.3 Red-Teaming Results",
    "text": "4.3 Red-Teaming Results\n\nTo identify failure modes, I conducted systematic adversarial testing of the AMTAIR system.\n\n4.3.1 Adversarial Extraction Attempts\n\n–&gt;\n\n\n4.3.2 Robustness Findings\nKey vulnerabilities of LLMs (and human experts) identified:\n\n\nSpecific metrics need validation:\n\n- Anchoring bias: measured effect size with confidence intervals\n- Authority sensitivity: controlled experiment design\n- Complexity degradation: performance curve analysis\n- Context loss: dependency distance metrics\n\n\nAnchoring bias: System tends to over-weight first probability mentioned5\nAuthority sensitivity: Extracted probabilities influenced by cited expert prominence\nComplexity degradation: Performance drops sharply beyond 50 nodes\nContext loss: Long-range dependencies in text sometimes missed\n\nHowever, the system demonstrated robustness to: - Different writing styles and academic disciplines - Variations in argument structure and presentation order - Mixed numerical and qualitative probability expressions - Reasonable levels of grammatical errors and typos\n\n\n4.3.3 Implications for Deployment\nThese results suggest AMTAIR is suitable for: - Research applications with expert oversight - Policy analysis of well-structured arguments - Educational uses demonstrating formal reasoning - Collaborative modeling with human verification\nBut should be used cautiously for: - Fully automated analysis without review - Adversarial or politically contentious texts - Real-time decision-making without validation - Arguments far outside training distribution",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-epistemic-security",
    "href": "chapters/Outlines/Outline_13.html#sec-epistemic-security",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "4.4 Enhancing Epistemic Security",
    "text": "4.4 Enhancing Epistemic Security\n\nDespite limitations, AMTAIR contributes to epistemic security in AI governance through several mechanisms.\n\n4.4.1 Making Models Inspectable\nThe greatest epistemic benefit comes from forcing implicit models into explicit form. When an expert claims “misalignment likely leads to catastrophe,” formalization asks:\n\nLikely means what probability?\nThrough what causal pathways?\nUnder what assumptions?\nWith what evidence?\n\nThis explicitation serves multiple functions:\nClarity: Vague statements become precise claims subject to evaluation\nComparability: Different experts’ models can be systematically compared\nCriticizability: Hidden assumptions become visible targets for challenge\nUpdatability: Formal models can systematically incorporate new evidence\n\n\n\n4.4.2 Revealing Convergence and Divergence\n\n\nImplement comparison of 3+ models:\n\n- Structural similarity metrics\n- Parameter divergence analysis\n- Crux identification algorithms\n- Visualization of agreement patterns\n\nStructural convergence: Different experts often share similar causal models even when probability estimates diverge dramatically. This suggests shared understanding of mechanisms despite disagreement on magnitudes.\nParameter clustering: Probability estimates often cluster around a few values rather than spreading uniformly, suggesting implicit coordination or common evidence bases.\nCrux identification: Formal comparison precisely identifies where worldviews diverge—often just 2-3 key parameters drive different conclusions about overall risk.\nThese insights remain hidden when arguments stay in natural language form.\n\n–&gt;\n\n\n4.4.3 Improving Collective Reasoning\nAMTAIR enhances group epistemics through:\nExplicit uncertainty: Replacing “might,” “could,” “likely” with probability distributions reduces miscommunication and standardizes precision\nCompositional reasoning: Complex arguments decompose into manageable components that can be independently evaluated\nEvidence integration: New information updates specific parameters rather than requiring complete argument reconstruction\nExploration tools: Stakeholders can modify assumptions and immediately see consequences, building intuition about model dynamics",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-scaling",
    "href": "chapters/Outlines/Outline_13.html#sec-scaling",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "4.5 Scaling Challenges and Opportunities",
    "text": "4.5 Scaling Challenges and Opportunities\n\nMoving from prototype to widespread adoption faces both technical and social challenges.\n\n4.5.1 Technical Scaling\nComputational complexity grows with network size, but several approaches help: - Hierarchical decomposition for very large models - Caching and approximation for common queries - Distributed processing for extraction tasks - Incremental updating rather than full recomputation\n\nData quality varies dramatically across sources: - Academic papers provide structured arguments - Blog posts offer rich ideas with less formal structure - Policy documents mix normative and empirical claims - Social media presents extreme extraction challenges\nIntegration complexity increases with ecosystem growth: - Multiple LLM providers with different capabilities - Diverse visualization needs across users - Various export formats for downstream tools - Version control for evolving models\n\n\n\n4.5.2 Social and Institutional Scaling\nAdoption barriers include: - Learning curve for formal methods - Institutional inertia in established processes - Concerns about replacing human judgment - Resource requirements for implementation\nTrust building requires: - Transparent methodology documentation - Published validation studies - High-profile successful applications - Community ownership and development\nSustainability depends on: - Open source development model - Diverse funding sources - Academic and industry partnerships - Clear value demonstration\n\n\n4.5.3 Opportunities for Impact\nDespite challenges, several factors favor adoption:\nTiming: AI governance needs tools now, creating receptive audiences\nComplementarity: AMTAIR enhances rather than replaces existing processes\nFlexibility: The approach adapts to different contexts and needs\nNetwork effects: Value increases as more perspectives are formalized\nEarly adopters in research organizations and think tanks can demonstrate value, creating momentum for broader adoption.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-governance-integration",
    "href": "chapters/Outlines/Outline_13.html#sec-governance-integration",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "4.6 Integration with Governance Frameworks",
    "text": "4.6 Integration with Governance Frameworks\n\nAMTAIR complements and integrates rather than replaces existing governance approaches.\n\n4.6.1 Standards Development\nTechnical standards bodies could use AMTAIR to: - Model how proposed standards affect risk pathways - Compare different standard options systematically - Identify unintended consequences through pathway analysis - Build consensus through explicit model negotiation\nExample: Evaluating compute thresholds for AI system regulation by modeling how different thresholds affect capability development, safety investment, and competitive dynamics.\n\n\n4.6.2 Regulatory Design\nRegulators could apply the framework to: - Assess regulatory impact across different scenarios - Identify enforcement challenges through explicit modeling - Compare international approaches systematically - Design adaptive regulations responsive to evidence\nExample: Analyzing how liability frameworks affect corporate AI development decisions under different market conditions.\n\n\nCuomo, Mallin, and Zattoni (2016), Demirag, Sudarsanam, and WRIGHT (2000), De Villiers and Dimes (2021), Di Vito and Trottier (2022), Kaur (2024), List and Pettit (2011) and Solomon (2020)\n\n\n4.6.3 International Coordination\nMultilateral bodies could leverage shared models for: - Establishing common risk assessments - Negotiating agreements with explicit assumptions - Monitoring compliance through parameter tracking - Adapting agreements as evidence emerges\nExample: Building shared models for AGI development scenarios to inform international AI governance treaties.\n\n\n\n4.6.4 Organizational Decision-Making\nIndividual organizations could use AMTAIR for: - Internal risk assessment and planning - Board-level communication about AI strategies - Research prioritization based on model sensitivity - Safety case development with explicit assumptions\nExample: An AI lab modeling how different safety investments affect both capability advancement and risk mitigation.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-future-research",
    "href": "chapters/Outlines/Outline_13.html#sec-future-research",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "4.7 Future Research Directions",
    "text": "4.7 Future Research Directions\n\nSeveral research directions could enhance AMTAIR’s capabilities and impact.\n\n4.7.1 Technical Enhancements\nImproved extraction: Fine-tuning language models specifically for argument extraction, handling implicit reasoning, and cross-document synthesis\nRicher representations: Temporal dynamics, continuous variables, and multi-agent interactions within extended frameworks\nInference advances: Quantum computing applications, neural approximate inference, and hybrid symbolic-neural methods\nValidation methods: Automated consistency checking, anomaly detection in extracted models, and benchmark dataset development\n\n\n\n4.7.2 Methodological Extensions\nCausal discovery: Inferring causal structures from data rather than just extracting from text\nExperimental integration: Connecting models to empirical results from AI safety experiments\nDynamic updating: Continuous model refinement as new evidence emerges from research and deployment\nUncertainty quantification: Richer representation of deep uncertainty and model confidence\n\nBabakov et al. (2025), Ban et al. (2023), Bethard (2007), Chen et al. (2023), Duhem (1954), Heinze-Deml, Maathuis, and Meinshausen (2018), Meyer (2022), Squires and Uhler (2023), Squires and Uhler (2023), Yang, Han, and Poon (2022)\n\n\n4.7.3 Application Domains\nBeyond AI safety: Climate risk, biosecurity, nuclear policy, and other existential risks\nCorporate governance: Strategic planning, risk management, and innovation assessment\nScientific modeling: Formalizing theoretical arguments in emerging fields\nEducational tools: Teaching probabilistic reasoning and critical thinking\n\n\n\n4.7.4 Ecosystem Development\nOpen standards: Common formats for model exchange and tool interoperability\nCommunity platforms: Collaborative model development and sharing infrastructure\nTraining programs: Building capacity for formal modeling in governance communities\nQuality assurance: Certification processes for high-stakes model applications\nThese directions could transform AMTAIR from a single tool into a broader ecosystem for enhanced reasoning about complex risks.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-deep-uncertainties",
    "href": "chapters/Outlines/Outline_13.html#sec-deep-uncertainties",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "4.8 Known Unknowns and Deep Uncertainties",
    "text": "4.8 Known Unknowns and Deep Uncertainties\nWhile AMTAIR enhances reasoning under uncertainty, fundamental limitations remain regarding truly novel developments that might fall outside existing conceptual frameworks.\n\n4.8.1 Categories of Deep Uncertainty\nNovel Capabilities: Future AI developments may operate according to principles outside current scientific understanding. No amount of careful modeling can anticipate fundamental paradigm shifts in what intelligence can accomplish.\nEmergent Behaviors: Complex system properties that resist prediction from component analysis may dominate outcomes. The interaction between advanced AI systems and human society could produce wholly unexpected dynamics.\nStrategic Interactions: Game-theoretic dynamics with superhuman AI systems exceed human modeling capacity. We cannot reliably predict how entities smarter than us will behave strategically.\nSocial Transformation: Unprecedented social and economic changes may invalidate current institutional assumptions. Our models assume continuity in basic social structures that AI might fundamentally alter.\n\n\n4.8.2 Adaptation Strategies for Deep Uncertainty\nRather than pretending to model the unmodelable, AMTAIR incorporates several strategies:\nModel Architecture Flexibility: The modular structure enables rapid incorporation of new variables as novel factors become apparent. When surprises occur, models can be updated rather than discarded.\nExplicit Uncertainty Tracking: Confidence levels for each model component make clear where knowledge is solid versus speculative. This prevents false confidence in highly uncertain domains.\nScenario Branching: Multiple model variants capture different assumptions about fundamental uncertainties. Rather than committing to one worldview, the system maintains portfolios of possibilities.\nUpdate Mechanisms: Integration with prediction markets and expert assessment enables rapid model revision as new information emerges. Models evolve rather than remaining static.\n\n\n4.8.3 Robust Decision-Making Principles\nGiven deep uncertainty, certain decision principles become paramount:\nOption Value Preservation: Policies should maintain flexibility for future course corrections rather than locking in irreversible choices based on current models.\nPortfolio Diversification: Multiple approaches hedging across different uncertainty sources provide robustness against model error.\nEarly Warning Systems: Monitoring for developments that would invalidate current models enables rapid response when assumptions break down.\nAdaptive Governance: Institutional mechanisms must enable rapid response to new information rather than rigid adherence to plans based on outdated models.\nThe goal is not to eliminate uncertainty but to make good decisions despite it. AMTAIR provides tools for systematic reasoning about what we do know while maintaining appropriate humility about what we don’t and can’t know.\n\n\n4.9.1 Key Challenges & Mitigations for Software Extensions\n\n\nAI Risk Pathway Analyzer (ARPA)\nExtraction Quality Limitations: LLMs may struggle with complex reasoning or nuanced arguments. Mitigation: Develop hybrid human-AI workflow with clear validation points and expert review integration. Representational Challenges: Some arguments resist formal representation in Bayesian networks. Mitigation: Create specialized handlers for common edge cases and develop appropriate simplifications with documentation of limitations. Computational Complexity: Large networks may become computationally intractable for real-time analysis. Mitigation: Implement hierarchical modeling approaches and develop approximation methods for complex networks. Validation Difficulties: Difficult to assess extraction fidelity objectively without ground truth. Mitigation: Establish expert review protocols and create benchmark datasets with annotations.\n\n\nWorldview Comparator\nModel Quality Dependencies: Effectiveness depends on extraction quality from the ARPA system. Mitigation: Develop resilient comparison algorithms that can handle varying levels of model completeness. Philosophical Complexity: Some disagreements resist formalization in the Bayesian framework. Mitigation: Create hybrid approaches that combine formal comparison with natural language explanation. Interface Complexity: Visualizing multi-dimensional differences between models is challenging. Mitigation: Develop progressive disclosure interfaces with multiple visualization options for different user needs. Domain Expertise Requirements: Accurate identification of cruxes requires deep domain knowledge. Mitigation: Incorporate expert feedback loops and validation processes.\n\n\nPolicy Impact Evaluator\nModel Adequacy for Policy: Causal models may lack governance-relevant variables or dynamics. Mitigation: Develop extension mechanisms to incorporate policy-specific factors and domain knowledge. Intervention Formalization: Translating qualitative policy proposals to model parameters is challenging. Mitigation: Create structured templates and guidance for policy translation with expert input. Stakeholder Accessibility: Technical complexity may limit policy user adoption and understanding. Mitigation: Design layered interfaces with appropriate simplification for different user types and expertise levels. Counterfactual Validity: Ensuring simulated interventions match real-world effects is difficult. Mitigation: Validate against historical cases where possible and incorporate expert assessment of plausibility.\n\n\nAI Risk Pathway Visualizer\nSimplification vs. Accuracy: Balancing accessibility with technical precision creates tension. Mitigation: Develop layered disclosure with progressive detail options and clear indications of simplification. Establishing Credibility: Building trust with diverse audiences requires transparency. Mitigation: Create clear methodology documentation, expert validation processes, and uncertainty representation. Communication Effectiveness: Visual metaphors may be misinterpreted without proper context. Mitigation: Conduct user testing with diverse audiences and refine based on feedback. Update Frequency Challenges: Updates could create alarm if not properly contextualized. Mitigation: Develop careful update protocols with appropriate contextual information.\n\n\nStrategic Intervention Generator\nOptimization Complexity: Balancing multiple objectives across worldviews creates computational challenges. Mitigation: Develop progressive optimization approach with clear trade-off visualization. Decision Theoretic Challenges: Representing deep uncertainty appropriately is conceptually difficult. Mitigation: Implement multiple decision frameworks with explicit assumptions and limitations. Computational Intensity: Exhaustive analysis may be computationally prohibitive for complex models. Mitigation: Develop smart search algorithms and approximation methods for efficient exploration. Strategy Validation: Difficult to validate robustness without historical precedents. Mitigation: Incorporate expert assessment and develop plausibility scoring for identified strategies.\n\n\nCross-Domain Understanding Communicator\nKnowledge Representation: Formalizing diverse domain knowledge in compatible structures is challenging. Mitigation: Develop extensible ontologies with expert input from each domain and iterative refinement. Translation Accuracy: Preserving precision across domain boundaries requires nuanced understanding. Mitigation: Implement confidence scoring and expert validation for critical translations. Knowledge Breadth: Covering sufficient domain knowledge requires extensive content creation. Mitigation: Prioritize core concepts first with extensible architecture for expansion. Measuring Effectiveness: Difficult to validate successful knowledge transfer across domains. Mitigation: Develop concrete use cases and success metrics for cross-domain communication.\n\n\nPolicy Brief Communicator\nBalancing Accuracy and Impact: Maintaining technical accuracy while maximizing persuasiveness creates tension. Mitigation: Implement a multi-stage review process with both technical and policy experts. Jurisdictional Knowledge: Maintaining accurate understanding of diverse governance contexts requires expertise. Mitigation: Develop partnerships with policy experts in key jurisdictions and create modular approaches to governance contexts. Actionability Assessment: Ensuring recommendations are truly implementable requires practical wisdom. Mitigation: Create feedback loops with policy practitioners and implementation feasibility scoring. Avoiding Oversimplification: Risk of losing critical nuances when translating for non-technical audiences. Mitigation: Develop layered disclosure with progressive complexity and explicit confidence indicators.\n\n\nForecast Integration Dashboard\nForecast Availability: Limited relevant questions on platforms for many model variables. Mitigation: Develop suggestion system for valuable new questions and partner with platforms to create targeted questions. Mapping Complexity: Ambiguity between forecast questions and model variables creates uncertainty. Mitigation: Implement confidence scoring and expert review for critical mappings. API Stability: Changes to platform APIs may break connections and data flow. Mitigation: Design modular connectors with degradation monitoring and fallback mechanisms. Data Quality Variability: Forecasts vary greatly in reliability and relevance to model variables. Mitigation: Implement sophisticated weighting algorithms and calibration assessments.\n\nThese limitations and considerations do not diminish AMTAIR’s value but rather clarify its proper role: a tool for enhancing coordination and decision-making under uncertainty, not a crystal ball for predicting the future. With realistic expectations about capabilities and limitations, we can now examine the concrete contributions and future directions for this research. The concluding chapter summarizes key findings and charts a path forward for computational approaches to AI governance.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-key-contributions",
    "href": "chapters/Outlines/Outline_13.html#sec-key-contributions",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "5.1 Summary of Key Contributions",
    "text": "5.1 Summary of Key Contributions\nThis thesis has demonstrated both the need for and feasibility of computational approaches to enhancing coordination in AI governance. The work makes several distinct contributions across theory, methodology, and implementation.\n\n5.1.1 Theoretical Contributions\nDiagnosis of the Coordination Crisis: I’ve articulated how fragmentation across technical, policy, and strategic communities systematically amplifies existential risk from advanced AI. This framing moves beyond identifying disagreements to understanding how misaligned efforts create negative-sum dynamics—safety gaps emerge between communities, resources are misallocated through duplication and neglect, and interventions interact destructively.\nThe Multiplicative Benefits Framework: The combination of automated extraction, prediction market integration, and formal policy evaluation creates value exceeding the sum of parts. Automation enables scale, markets provide empirical grounding, and policy analysis delivers actionable insights. Together, they address different facets of the coordination challenge while reinforcing each other’s strengths.\nEpistemic Infrastructure Conception: Positioning formal models as epistemic infrastructure reframes the role of technical tools in governance. Rather than replacing human judgment, computational approaches provide common languages, shared representations, and systematic methods for managing disagreement—essential foundations for coordination under uncertainty.\n\n\n\n5.1.2 Methodological Innovations\nTwo-Stage Extraction Architecture: Separating structural extraction (ArgDown) from probability quantification (BayesDown) addresses key challenges in automated formalization. This modularity enables human oversight at critical points, supports multiple quantification methods, allows for unprecedented transparency and explainability of the entire process, and isolates different types of errors for targeted improvement.\nBayesDown as Bridge Representation: The development of BayesDown syntax creates a crucial intermediate representation preserving both narrative accessibility and mathematical precision. This bridge enables the transformation from qualitative arguments to quantitative models while maintaining traceability and human readability.\nValidation Framework: The systematic approach to validating automated extraction—comparing against expert annotations, measuring multiple accuracy dimensions, and analyzing error patterns—establishes scientific standards for assessing formalization tools. This framework can guide future development in this emerging area.\n\n\n5.1.3 Technical Achievements\nWorking Implementation: AMTAIR demonstrates end-to-end feasibility from document ingestion through interactive visualization. The system achieves practically useful accuracy levels: 85%+ for structural extraction and 73% for probability capture on real AI safety arguments.\nScalability Solutions: Technical approaches for handling realistic model complexity—hierarchical decomposition, approximate inference, and progressive visualization—show that computational limitations need not prevent practical application.\nAccessibility Design: The layered interface approach serves diverse stakeholders without compromising technical depth. Progressive disclosure, visual encoding, and interactive exploration make formal models accessible beyond technical specialists.\n\n\n\n5.1.4 Empirical Findings\nExtraction Feasibility: The successful extraction of complex arguments like Carlsmith’s model validates the core premise that implicit formal structures exist in natural language arguments and can be computationally recovered with reasonable fidelity.\nConvergence Patterns: Comparative analysis reveals structural agreement across repeated extraction even when probability estimates diverge substantially. This suggests shared understanding of the understanding causal models, argument structure and worldview despite parameter disagreements—a foundation for coordination.\nIntervention Impacts: Policy evaluation demonstrates how formal models enable rigorous assessment of governance options. The ability to quantify risk reduction across scenarios and identify robust strategies validates the practical value of formalization.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-limitations-assessment",
    "href": "chapters/Outlines/Outline_13.html#sec-limitations-assessment",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "5.2 Limitations and Honest Assessment",
    "text": "5.2 Limitations and Honest Assessment\nDespite these contributions, important limitations constrain current capabilities and should guide appropriate use.\n\n5.2.1 Technical Constraints\nExtraction Boundaries: Potential sources of systematic biases and confounding variables remain. Similar to experts, the automated system still struggles with implicit and hidden assumptions and complex conditionals. These limitations necessitate human review for high-stakes applications.\nCorrelation Handling: Over simplified Bayesian networks inadequately represent complex correlations in real systems. While extensions like copulas and explicit correlation nodes help, fully capturing interdependencies remains challenging.\nComputational Scaling: Very large networks (&gt;&gt;500 nodes) require approximations that may affect accuracy. As models grow to represent richer phenomena, computational constraints increasingly bind.\n\n\n5.2.2 Conceptual Limitations\nFormalization Trade-offs: Converting rich arguments to formal models necessarily loses nuance. While making assumptions explicit provides value, some unspoken insights may resist clear mathematical representation.\nProbability Interpretation: Deep uncertainty about unprecedented events challenges probabilistic intuitions. Numbers can create false precision even when explicitly conditional and uncertain.\nSocial Complexity: Institutional dynamics, cultural factors, and political processes influence AI development in ways that purely causal models struggle to capture.\n\n\n5.2.3 Practical Constraints\nAdoption Barriers: Learning curves, institutional inertia, and resource requirements limit immediate deployment. Even demonstrably valuable tools face implementation challenges.\nMaintenance Burden: Models require updating as arguments evolve and evidence emerges. Without sustained effort, formal representations quickly become outdated.\nContext Dependence: The approach works best for well-structured academic arguments. Application to “fuzzy” informal discussions, political speeches, or social media remains challenging.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-governance-implications",
    "href": "chapters/Outlines/Outline_13.html#sec-governance-implications",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "5.3 Implications for AI Governance",
    "text": "5.3 Implications for AI Governance\n\nDespite limitations, AMTAIR’s approach offers significant implications for how AI governance can evolve toward greater coordination and effectiveness.\n\n5.3.1 Near-Term Applications\nResearch Coordination: Research organizations can use formal models to: - Map the landscape of current arguments and identify gaps - Prioritize investigations targeting high-sensitivity parameters - Build cumulative knowledge through explicit model updating - Facilitate collaboration through shared representations\nPolicy Development: Governance bodies can apply the framework to: - Evaluate proposals across multiple expert worldviews - Identify robust interventions effective under uncertainty - Make assumptions explicit for democratic scrutiny - Track how evidence changes optimal policies over time\nStakeholder Communication: The visualization and analysis tools enable: - Clearer communication between technical and policy communities - Public engagement with complex risk assessments - Board-level strategic discussions grounded in formal analysis - International negotiations with explicit shared models\n\n\n\n5.3.2 Medium-Term Transformation\nAs adoption spreads, we might see:\nEpistemic Commons: Shared repositories of formalized arguments become reference points for governance discussions, similar to how economic models inform monetary policy or climate models guide environmental agreements.\nAdaptive Governance: Policies designed with explicit models can include triggers for reassessment as key parameters change, enabling responsive governance that avoids both paralysis and recklessness.\nProfessionalization: “Model curator” and “argument formalization specialist” emerge as recognized roles, building expertise in bridging natural language and formal representations.\nQuality Standards: Community norms develop around model transparency, validation requirements, and appropriate use cases, preventing both dismissal and over-reliance on formal tools.\n\n\n5.3.3 Long-Term Vision\nSuccessfully scaling this approach could fundamentally alter AI governance:\nCoordinated Response: Rather than fragmented efforts, the AI safety ecosystem could operate with shared situational awareness—different actors understanding how their efforts interact and contribute to collective goals.\nAnticipatory Action: Formal models with prediction market integration could provide early warning of emerging risks, enabling proactive rather than reactive governance.\nGlobal Cooperation: Shared formal frameworks could facilitate international coordination similar to how economic models enable monetary coordination or climate models support environmental agreements.\nDemocratic Enhancement: Making expert reasoning transparent and modifiable could enable broader participation in crucial decisions about humanity’s technological future.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-recommendations",
    "href": "chapters/Outlines/Outline_13.html#sec-recommendations",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "5.4 Recommendations for Stakeholders",
    "text": "5.4 Recommendations for Stakeholders\nDifferent communities can take concrete steps to realize these benefits:\n\n5.4.1 For Researchers\n\nExperiment with formalization: Try extracting your own arguments into ArgDown/BayesDown format to discover implicit assumptions\nContribute to validation: Provide expert annotations for building benchmark datasets and improving extraction quality\nDevelop extensions: Build on the open-source foundation to add capabilities for your specific domain needs\nPublish formally: Include formal model representations alongside traditional papers to enable cumulative building\n\n\n\n\n\n\n\nQuick Start Guide\n\n\n\n\nA comprehensive guide for researchers getting started with AMTAIR will be available at [project website], including templates, tutorials, and example extractions.\n\n\n\n\n5.4.2 For Policymakers\n\nPilot applications: Use AMTAIR for internal analysis of specific policy proposals to build familiarity and identify value\nDemand transparency: Request formal models underlying expert recommendations to understand assumptions and uncertainties\nFund development: Support tool development and training to build governance capacity for formal methods\nDesign adaptively: Create policies with explicit triggers based on model parameters to enable responsive governance\n\n\n\n\n5.4.3 For Technologists\n\nImprove extraction: Contribute better prompting strategies, fine-tuned models, or validation methods\nEnhance interfaces: Develop visualizations and interactions serving specific stakeholder needs\nBuild integrations: Connect AMTAIR to other tools in the AI governance ecosystem\nScale infrastructure: Address computational challenges for larger models and broader deployment\n\n\n\n\n5.4.4 For Funders\n\nSupport ecosystem: Fund not just tool development but training, community building, and maintenance\nBridge communities: Incentivize collaborations between formal modelers and domain experts\nMeasure coordination: Develop metrics for assessing coordination improvements from formal tools\nPatient capital: Recognize that epistemic infrastructure requires sustained investment to reach potential",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-future-research-agenda",
    "href": "chapters/Outlines/Outline_13.html#sec-future-research-agenda",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "5.5 Future Research Agenda",
    "text": "5.5 Future Research Agenda\nBuilding on this foundation, several research directions could amplify impact:\n\n5.5.1 Technical Priorities\nExtraction Enhancement: - Fine-tuning language models specifically for argument extraction - Handling implicit reasoning and long-range dependencies - Cross-document synthesis for comprehensive models - Multilingual extraction for global perspectives\nRepresentation Extensions: - Temporal dynamics for modeling AI development trajectories - Multi-agent representations for strategic interactions - Continuous variables for economic and capability metrics - Uncertainty types beyond probability distributions\nIntegration Depth: - Semantic matching between models and prediction markets - Automated experiment design based on model sensitivity - Policy optimization algorithms using extracted models - Real-time updating from news and research feeds\n\n\n\n\n5.5.2 Methodological Development\nValidation Science: - Larger benchmark datasets with diverse argument types - Metrics for semantic preservation beyond accuracy - Adversarial robustness testing protocols - Longitudinal studies of model evolution\nHybrid Approaches: - Optimal human-AI collaboration patterns for extraction - Combining formal models with other methods (scenarios, simulations) - Integration with deliberative and participatory processes - Balancing automation with expert judgment\nSocial Methods: - Ethnographic studies of model use in organizations - Measuring coordination improvements empirically - Understanding adoption barriers and facilitators - Designing interventions for epistemic security\n\n\n\n5.5.3 Application Expansion\nDomain Extensions: - Biosecurity governance and pandemic preparedness - Cyber risk assessment and policy evaluation - Nuclear policy and deterrence stability - Emerging technology governance broadly\nInstitutional Integration: - Embedding in regulatory impact assessment - Corporate strategic planning applications - Academic peer review enhancement - Democratic deliberation support tools\nGlobal Deployment: - Adapting to different governance contexts - Supporting multilateral negotiation processes - Building capacity in developing nations - Creating resilient distributed infrastructure",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#sec-closing-reflections",
    "href": "chapters/Outlines/Outline_13.html#sec-closing-reflections",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "5.6 Closing Reflections",
    "text": "5.6 Closing Reflections\nThe work presented in this thesis emerges from a simple observation: while humanity mobilizes unprecedented resources to address AI risks, our efforts remain tragically uncoordinated. Different communities work with incompatible frameworks, duplicate efforts, and sometimes actively undermine each other’s work. This fragmentation amplifies the very risks we seek to mitigate.\nAMTAIR represents one attempt to build bridges—computational tools that create common ground for disparate perspectives. By making implicit models explicit, quantifying uncertainty, and enabling systematic policy analysis, these tools offer hope for enhanced coordination. The successful extraction of complex arguments, validation against expert judgment, and demonstration of policy evaluation capabilities suggest this approach has merit.\nYet tools alone cannot solve coordination problems rooted in incentives, institutions, and human psychology. AMTAIR provides infrastructure for coordination, not coordination itself. Success requires not just technical development but changes in how we approach collective challenges—valuing transparency over strategic ambiguity, embracing uncertainty rather than false confidence, and prioritizing collective outcomes over parochial interests.\nThe path forward demands both ambition and humility. Ambition to build the epistemic infrastructure necessary for navigating unprecedented risks. Humility to recognize our tools’ limitations and the irreducible role of human wisdom in governance. The question is not whether formal models can replace human judgment—they cannot and should not. Rather, it’s whether we can augment our collective intelligence with computational tools that help us reason together about futures too important to leave to chance.\n\n\n\n\n\n\nThe Stakes\n\n\n\nAs AI capabilities advance toward transformative potential, the window for establishing effective governance narrows. We cannot afford continued fragmentation when facing potentially irreversible consequences. The coordination crisis in AI governance represents both existential risk and existential opportunity—risk if we fail to align our efforts, opportunity if we succeed in building unprecedented cooperation around humanity’s most important challenge.\n\n\nThis thesis contributes technical foundations and demonstrates feasibility. The greater work—building communities, changing practices, and fostering coordination—remains ahead. May we prove equal to the task, for all our futures depend on it.",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#amtair-thesis-relevant-literature-citations",
    "href": "chapters/Outlines/Outline_13.html#amtair-thesis-relevant-literature-citations",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "AMTAIR Thesis Relevant Literature & Citations",
    "text": "AMTAIR Thesis Relevant Literature & Citations\n\nItems from MAref.bib\n\n\n\n@carlsmith2021: Carlsmith (2021)\nCarlsmith, Joseph (2021)\nIs Power-Seeking AI an Existential Risk?\n\nDOI: 10.48550/arXiv.2206.13353\n\narXiv ID: 2206.13353\n\nBetter alternative: None - this is the primary case study\n\nRelevant thesis section(s): \n- Section 2.1: AI Existential Risk: The Carlsmith Model\n- Section 3.5: Case Study: Carlsmith's Power-Seeking AI Model\n- Throughout as validation example\n\n\nPotential claims supported (with certainty %):\n- \"Carlsmith's six-premise decomposition exemplifies structured probabilistic reasoning about AI risk\" (95%)\n- \"The model estimates ~5% existential risk by 2070\" (90%)\n- \"Explicit probability estimates enable formal analysis\" (95%)\n\n\n@bostrom2014: Bostrom (2014)\nBostrom, Nick (2014)\nSuperintelligence: Paths, Dangers, Strategies\n\nISBN: 978-0-19-967811-2\n\nBetter alternative: None - foundational text\n\nRelevant thesis section(s):\n- Section 1.2: The Coordination Crisis\n- Section 2.1: Historical foundations of AI risk\n- Background context throughout\n\nPotential claims supported (with certainty %):\n- \"Orthogonality thesis: intelligence and goals are independent\" (95%)\n- \"Instrumental convergence leads to power-seeking behavior\" (90%)\n- \"Superintelligence poses existential risk\" (85%)\n\n\n@clarke2022: Clarke et al. (2022)\nClarke, Sam et al. (2022)\nModeling Transformative AI Risks (MTAIR) Project -- Summary Report\n\nDOI: 10.48550/ARXIV.2206.09360\n\narXiv ID: 2206.09360\n\nBetter alternative: None - this is what AMTAIR builds upon\n\nRelevant thesis section(s):\n- Section 2.5: The MTAIR Framework: Achievements and Limitations\n- Section 1.3: Comparison with AMTAIR automation\n- Throughout as predecessor project\n\nPotential claims supported (with certainty %):\n- \"MTAIR demonstrated value of formal models but required extensive manual effort\" (95%)\n- \"Manual extraction takes 200-400 expert hours per model\" (80%)\n- \"Static models cannot track evolving arguments\" (90%)\n\n\n@pearl2009 and @pearl2000: Pearl (2000) and Pearl (2009)\nPearl, Judea (2009)\nCausality: Models, Reasoning and Inference (2nd Edition)\n\nISBN: 978-0-521-89560-6\n\nDOI: 10.1017/CBO9780511803161\n\nBetter alternative: None - theoretical foundation\n\nRelevant thesis section(s):\n- Section 2.3: Bayesian Networks as Knowledge Representation\n- Section 2.7.4: DAG structure and causal semantics\n- Section 3.7.1: Do-calculus for policy interventions\n\nPotential claims supported (with certainty %):\n- \"Bayesian networks enable causal reasoning under uncertainty\" (95%)\n- \"Do-calculus allows formal policy evaluation\" (95%)\n- \"DAGs encode conditional independence assumptions\" (95%)\n\n\n@jaynes2003: Jaynes (2003)\nJaynes, Edwin T. (2003)\nProbability Theory: The Logic of Science\n\nISBN: 978-0-521-59271-0\n\nDOI: 10.1017/CBO9780511790423\n\nBetter alternative: None for foundational probability theory\n\nRelevant thesis section(s):\n- Section 2.3: Mathematical foundations of Bayesian inference\n- Section 2.7.5: Probability as extended logic\n- Epistemological grounding throughout\n\nPotential claims supported (with certainty %):\n- \"Probability theory extends deductive logic to handle uncertainty\" (95%)\n- \"Bayesian inference provides principled belief updating\" (95%)\n- \"Maximum entropy principles handle missing information\" (90%)\n\n\n@tetlock2015: P. E. Tetlock and Gardner (2015)\nTetlock, Philip E. and Gardner, Dan (2015)\nSuperforecasting: The Art and Science of Prediction\n\nISBN: 978-0-8041-3671-6\n\nBetter alternative: @tetlock2023 for more recent long-range forecasting\n\nRelevant thesis section(s):\n- Section 1.5.2: Live Data Integration\n- Section 3.9: Integration with Prediction Markets\n- Forecasting methodology context\n\nPotential claims supported (with certainty %):\n- \"Aggregated forecasts outperform individual expert judgment\" (90%)\n- \"Prediction markets provide empirical grounding for models\" (85%)\n- \"Calibrated forecasters achieve measurable accuracy\" (90%)\n\n\n@lempert2003: Lempert, Popper, and Bankes (2003)\nLempert, Robert J., Popper, Steven W., and Bankes, Steven C. (2003)\nShaping the Next One Hundred Years: New Methods for Quantitative, Long-Term Policy Analysis\n\nISBN: 978-0-8330-3275-8\n\nBetter alternative: None for deep uncertainty methods\n\nRelevant thesis section(s):\n- Section 2.2.2: Limitations of Traditional Approaches\n- Section 4.1.2: Deep uncertainty in AI governance\n- Policy evaluation methodology\n\nPotential claims supported (with certainty %):\n- \"Traditional policy analysis fails under deep uncertainty\" (90%)\n- \"Robust decision-making requires considering multiple scenarios\" (85%)\n- \"AI governance faces irreducible uncertainties\" (90%)\n\n\n@good1966: Good (1966)\nGood, Irving John (1966)\nSpeculations Concerning the First Ultraintelligent Machine\n\nDOI: 10.1016/S0065-2458(08)60418-0\n\n\n\nRelevant thesis section(s):\n- Historical context in Introduction\n- Background for intelligence explosion concept\n\nPotential claims supported (with certainty %):\n- \"Intelligence explosion concept dates to 1960s\" (95%)\n- \"Recursive self-improvement could lead to rapid capability gains\" (80%)\n\n\n@yudkowsky2008: Yudkowsky (2008)\nYudkowsky, Eliezer (2008)\nArtificial Intelligence as a Positive and Negative Factor in Global Risk\n\nDOI: 10.1093/oso/9780198570509.003.0021\n\nBetter alternative: @yudkowsky2022 for more recent formulation\n\nRelevant thesis section(s):\n- Section 2.1: AI risk arguments\n- Background on alignment problem\n- Instrumental convergence discussion\n\nPotential claims supported (with certainty %):\n- \"AI alignment is the core challenge for beneficial AI\" (90%)\n- \"Default AI development may produce misaligned systems\" (85%)\n- \"Cognitive biases affect AI risk assessment\" (90%)\n\n\n@russell2015: Russell et al. (2015)\nRussell, Stuart et al. (2015)\nResearch Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter\n\nDOI: 10.1609/aimag.v36i4.2621\n\nBetter alternative: None - important consensus document\n\nRelevant thesis section(s):\n- Introduction: AI safety research mobilization\n- Context for coordination efforts\n\nPotential claims supported (with certainty %):\n- \"AI safety has gained mainstream research attention\" (95%)\n- \"Technical and governance challenges are interrelated\" (90%)",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#new-suggested-citations",
    "href": "chapters/Outlines/Outline_13.html#new-suggested-citations",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "New Suggested Citations",
    "text": "New Suggested Citations\n\nNew Items to Consider:\n\n\n@amodei2016: Amodei et al. (2016)\nAmodei, Dario et al. (2016)\nConcrete Problems in AI Safety\n\narXiv ID: 1606.06565\n\nRelevant thesis section(s):\n- Section 2.2: Technical safety challenges\n- Concrete problems motivating AMTAIR\n\nPotential claims supported (with certainty %):\n- \"AI safety includes avoiding negative side effects, safe exploration\" (95%)\n- \"Current ML systems exhibit safety failures\" (90%)\n\n\n@christiano2019: Christiano (2019)\nChristiano, Paul (2019)\nWhat Failure Looks Like\n\nURL: https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like\n\nRelevant thesis section(s):\n- Additional case study for extraction\n- Alternative risk model to Carlsmith\n\nPotential claims supported (with certainty %):\n- \"AI risk may manifest through gradual loss of control\" (85%)\n- \"Multiple pathways to existential risk exist\" (90%)\n\n\n@critch2020: Critch and Krueger (2020)\nCritch, Andrew (2019)\nARCHES: AI Research Considerations for Human Existential Safety\n\nURL: https://arxiv.org/abs/2006.04948\n\nRelevant thesis section(s):\n- Another structured model for extraction validation\n- Multi-stakeholder coordination framework\n\nPotential claims supported (with certainty %):\n- \"AI safety requires coordination across multiple sectors\" (90%)\n- \"Research, deployment, and governance interact complexly\" (85%)\n\n\n@dafoe2018 and updated @dafoe2021: Dafoe (2021) and Dafoe (2018)\nDafoe, Allan (2021)\nAI Governance: A Research Agenda\n\nURL: https://www.fhi.ox.ac.uk/govaiagenda/\n\nRelevant thesis section(s):\n- Section 2.6.2: Governance proposals taxonomy\n- Context for policy evaluation needs\n\nPotential claims supported (with certainty %):\n- \"AI governance requires interdisciplinary approaches\" (95%)\n- \"Technical and policy communities need better coordination\" (90%)\n\n\n@askell2021: Askell et al. (2021)\nAskell, Amanda et al. (2021)\nA General Language Assistant as a Laboratory for Alignment\n\narXiv ID: 2112.00861\n\nRelevant thesis section(s):\n- LLM capabilities for extraction tasks\n- Alignment considerations for AMTAIR\n\nPotential claims supported (with certainty %):\n- \"Language models can assist in complex reasoning tasks\" (90%)\n- \"Alignment challenges manifest in current systems\" (85%)",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#further-citations-to-integrate",
    "href": "chapters/Outlines/Outline_13.html#further-citations-to-integrate",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "Further Citations to Integrate:",
    "text": "Further Citations to Integrate:\n\nGrowiec (2024)\nClarke et al. (2022)\nK. Drexler (2019) and K. E. Drexler (2019)\nBrundage, Avin, Clark, Toner, Eckersley, Garfinkel, Dafoe, Scharre, Zeitzoff, Filar, Anderson, et al. (2018) and Brundage, Avin, Clark, Toner, Eckersley, Garfinkel, Dafoe, Scharre, Zeitzoff, and Filar (2018)\nKumar et al. (2019b) and Kumar et al. (2019a)\nCarlsmith (2021) and Carlsmith (2022) and Carlsmith (2024)\nHendrycks et al. (2021a) and Hendrycks et al. (2021b)\nWilson et al. (2023)\nKilian, Ventura, and Bailey (2023)\nKulveit et al. (2025)\nHadshar (2023)\nKasirzadeh (2024)\nSotala (2018)\nClaimify: Metropolitansky and Larson (2025)\nBayes Server:\nBayes (2025)\nMTAIR:\nMartin, Chrisman, and Englander (2023)\nManheim (2021)\nRice and Martin (2021)\nEth (2021)\nMartin and Eth (2021)\nCottier (2021)\nCottier (2021)\nCottier, Eth, and Martin (2021)\nDavidmanheim (2021)\nAnalytica:\nLumina (2025)",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "chapters/Outlines/Outline_13.html#footnotes",
    "href": "chapters/Outlines/Outline_13.html#footnotes",
    "title": "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)",
    "section": "",
    "text": "The probability estimates vary between outlines; using more conservative estimates from 12.2↩︎\nThis example, while simple, demonstrates all essential features of Bayesian networks and serves as the foundation for understanding more complex applications↩︎\nPearl’s causal framework revolutionized how we think about causation in complex systems↩︎\nCopulas provide a mathematically elegant way to separate marginal behavior from dependence structure↩︎\nThis reflects how LLMs inherit human cognitive biases from training data↩︎",
    "crumbs": [
      "Automating the Modeling of Transformative Artificial Intelligence Risks (AMTAIR)"
    ]
  },
  {
    "objectID": "ref/references.html",
    "href": "ref/references.html",
    "title": "1  References (.md)",
    "section": "",
    "text": "1.1 Error Watch",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>References (.md)</span>"
    ]
  },
  {
    "objectID": "ref/references.html#error-watch",
    "href": "ref/references.html#error-watch",
    "title": "1  References (.md)",
    "section": "",
    "text": "1.1.1 Catch ALL Potential Hallucinations\n&lt;!-- [ ] Collect all errors and hallucinations here to be able to reference against them later and ensure none remain throught text --&gt;\n&lt;!-- [ ] Keep track of all hallucinations that have been found here: --&gt;\n\nValidation Metrics: Claims of “85%+ accuracy for structural extraction” and “73% for probability capture” appear precise for what seems to be a prototype system. These need careful verification or qualification.\nPilot Study Results: “40% reduction in time to identify disagreements” and “60% improvement in agreement about disagreement” lack citations and seem surprisingly specific.\nRed-teaming Quantification: “34% anchoring bias effect” and other precise percentages from adversarial testing need support or qualification as estimates.\nPrediction Market Integration: Some passages imply deeper integration than the “future work” status indicated elsewhere.\n\n&lt;!-- [ ] Make sure all hallucinations have been removed --&gt;\n\n\n1.1.2 Master Citation Registry\n\n## BibTeX of Main Citations Included\n\n&lt;!-- [ ] Add all the main literature / citations / references here (makes it easy to verify correct key etc. while writing) --&gt;\n\n&lt;!-- [ ] Keep 'References.md' updated with/from ref/MAref.bib --&gt;\n\n&lt;!-- [ ] Remove/hide 'References.md' before final publication --&gt;\n\n## Update in ref/MAref.bib\n\n\n## Core Citations (Must Have)\n\n### Foundational Works\n- [x] @carlsmith2021 - Power-seeking AI framework\n  - Chapter usage: 1, 2, 4\n  - Key concepts: Six premises, existential risk\n  - Notes: Central to thesis argument\n\n- [x] @bostrom2014 - Superintelligence paths\n  - Chapter usage: 1, 2, 3, 5\n  - Key concepts: Orthogonality, convergence\n  - Notes: Historical foundation\n\n\n\n@article{bostrom2012,\n  title = {The {{Superintelligent Will}}: {{Motivation}} and {{Instrumental Rationality}} in {{Advanced Artificial Agents}}},\n  author = {Bostrom, Nick},\n  date = {2012},\n  journaltitle = {Minds and Machines},\n  volume = {22},\n  number = {2},\n  pages = {71--85},\n  publisher = {Kluwer Academic Publishers Norwell, MA, USA},\n  doi = {10.1007/s11023-012-9281-3},\n  url = {https://philpapers.org/rec/BOSTSW}\n}\n\n@book{bostrom2014,\n  title = {Superintelligence: {{Paths}}, Strategies, Dangers},\n  author = {Bostrom, Nick},\n  date = {2014},\n  publisher = {Oxford University Press},\n  location = {Oxford},\n  url = {https://scholar.dominican.edu/cynthia-stokes-brown-books-big-history/47},\n  abstract = {The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence.},\n  isbn = {978-0-19-967811-2}\n}\n\n@article{bostrom2016,\n  title = {The {{Unilateralist}}’s {{Curse}} and the {{Case}} for a {{Principle}} of {{Conformity}}},\n  author = {Bostrom, Nick and Douglas, Thomas and Sandberg, Anders},\n  date = {2016},\n  journaltitle = {Social Epistemology},\n  volume = {30},\n  number = {4},\n  pages = {350--371},\n  publisher = {Routledge, part of the Taylor \\& Francis Group},\n  doi = {10.1080/02691728.2015.1108373},\n  url = {https://www.tandfonline.com/doi/full/10.1080/02691728.2015.1108373}\n}\n\n@article{bostrom2019,\n  title = {The Vulnerable World Hypothesis},\n  author = {Bostrom, Nick},\n  date = {2019},\n  journaltitle = {Global Policy},\n  volume = {10},\n  number = {4},\n  pages = {455--476},\n  publisher = {Wiley Online Library},\n  doi = {10.1111/1758-5899.12718}\n}\n\n\n\n\n## Pending Citations\n\n### Need to Find\n- [ ] FIND: @ai-governance-2024: \"Recent survey on international AI governance frameworks\"\n  - For: Chapter 3, Section 3.2\n  - Search terms: AI governance, international coordination, 2024\n  - Priority: High\n\n### Need to Verify\n- [ ] VERIFY: @prediction-markets-ai: \"Tetlock et al on prediction markets for AI timelines\"\n  - Current info: Possibly in Metaculus report 2023\n  - For: Chapter 4, Section 4.3\n  - Priority: Medium\n\n\n## Citation Health Check\n- [ ] All citations in .bib file\n- [ ] All .bib entries have DOIs/URLs\n- [ ] No duplicate entries\n- [ ] Consistent naming scheme\n- [ ] Recent sources included (2023-2024)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>References (.md)</span>"
    ]
  },
  {
    "objectID": "ref/references.html#figure-inventory-and-tracking",
    "href": "ref/references.html#figure-inventory-and-tracking",
    "title": "1  References (.md)",
    "section": "1.2 Figure Inventory and Tracking",
    "text": "1.2 Figure Inventory and Tracking\n## Master Figure Registry {.unnumbered .unlisted}\n\n\n\n&lt;figure_syntax&gt;\n\n```markdown\n\n  [![Figure Caption for Display](/path/to/image.png){\n    #fig-unique-identifier\n    fig-scap=\"Short caption for list of figures\"\n    fig-alt=\"Detailed description for accessibility.\n            TYPE: [Chart/Diagram/Photo/etc.]\n            DATA: [What data is shown, axes, units]\n            PURPOSE: [Why included, what to observe]\n            DETAILS: [Key patterns, insights, anomalies]\n            SOURCE: [Citation or data source]\"\n    fig-align=\"center\"\n    width=\"80%\"\n  }](https://optional-link-url.com)\n\n\n\nfrom @metropolitansky2025\n\n[![Claimify claim-extraction stages](/images/claimify-stages.jpg){\n    #fig-claimify-stages\n    fig-scap=\"Claimify claim-extraction stages\"\n    fig-alt=\"COMPOSITE FIGURE: table and process flow. TABLE: four-row, two-column table enumerates stages 1–4 of Claimify’s pipeline—Sentence splitting and context creation, Selection, Disambiguation, Decomposition—each with a plain-language description. FLOW-CHART: sequence of rectangles and diamond decision nodes shows per-sentence logic. Start node ‘Input question & answer’ feeds into ‘Split into sentences & create context’. Decision 1 asks if the sentence contains verifiable content; ‘No’ exits with red X ‘No verifiable claims’, ‘Yes’ advances. Decision 2 checks for irresolvable ambiguity; ‘Yes’ exits with red X ‘Cannot be disambiguated’, ‘No’ advances. Decision 3 asks if at least one claim is produced; ‘No’ exits with red X ‘No verifiable claims’, ‘Yes’ ends with green check ‘Extracted claims’. A dashed bracket labelled ‘Per sentence’ spans the decision chain. PURPOSE: illustrates Claimify’s staged filtering that aligns with AMTAIR’s need for clean, disambiguated claims before formal modelling. DATA: categorical process flow—no numeric axes. SOURCE: Adapted from Claimify documentation (2024, https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/).\"\n    fig-align=\"center\"\n    width=\"100%\"\n}](https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/)\n\n\n\nfrom @tetlock2022\n\n[![Conditional-tree AI-risk forecasts](/images/conditional_metaculus.jpg){\n    #fig-conditional_metaculus\n    fig-scap=\"Conditional-tree AI-risk forecasts\"\n    fig-alt=\"SCREENSHOT of a forecasting-platform interface titled ‘Series Contents’. A search bar and filter chips sit above five forecast cards: 1) ‘If, before 2050, AI kills more than 1 million people, will the policy response be insufficient?’ with a 75 percent gauge (green, arrow up 8 percent). 2) ‘Before 2050, will an AI system be shut down due to exhibiting power-seeking behavior?’ at 95 percent (arrow down 2 percent). 3) ‘Before 2100, will AI cause the human population to fall below 5000 individuals?’ at 4 percent. 4) ‘Before 2030, will there be an AI-caused administrative disempowerment?’ at 20 percent. 5) ‘Between 2023 and 2030, will revenue from deep learning double every two years?’ at 80 percent. Beneath several cards, grey CONDITION boxes branch to green bars labelled ‘CTs AI Extinction Before 2100’ with different probabilities for IF YES and IF NO scenarios (e.g. 26 % vs 37 %). Each question lists forecaster counts, closing dates (2030 or 2050), and the tag ‘Conditional Trees: AI Risk’. A footer card introduces the series report. CHART TYPE: mixed UI elements—gauge dials and horizontal bars—displaying probabilities and conditional probabilities. DATA: probabilities (% chances) for base and conditional events; no axes. PURPOSE: demonstrates how crowd-forecasting encodes marginal and counterfactual probabilities suitable as inputs for AMTAIR Bayesian-network nodes. DETAILS: notable high probability for power-seeking AI shutdown, low probability for population collapse, and large shifts in extinction risk under certain conditions. SOURCE: Forecasting Research Institute conditional-tree series, @tetlock2022.\"\n    fig-align=\"center\"\n    width=\"100%\"\n}](https://www.metaculus.com/tournament/3508/)\n\n\n\nfrom @gruetzemacher2022\n\n[![Bayes-net pruning → crux extraction → re-expansion](/images/bns_and_conditional_trees.jpg){\n    #fig-bayesnet-crux-flow\n    fig-scap=\"Bayes-net pruning → crux extraction → re-expansion\"\n    fig-alt=\"THREE-PANEL DIAGRAM. Panel A (upper left) titled ‘Initial Bayes Net—Pruning Least Relevant Nodes’ shows eleven circular nodes connected by arrows inside a rounded rectangle. Solid circles remain; dashed or dotted ones are pruned. Arrows converge on a solid node labelled ‘AI causes human extinction’. Panel B (upper right) titled ‘Two Sets of Crux Events from Bayes Nets Isolated as Conditional Trees’ shows two short vertical chains of dotted or dashed circles. Chain 1: ‘AI alignment problem is solved’ → ‘China and the US cooperate on AI alignment’ → ‘Discontinuous progress in computational costs’. Chain 2: ‘Intergovernmental treaty on AI alignment’ ← ‘Robust AI-driven economic growth’ ← ‘Continual learning integrated with foundation models’. Panel C (bottom) titled ‘Top Set of Crux Events as Conditional Tree Decomposed to Bayes Net’ depicts a new Bayes net where context nodes such as ‘Photonic computing is used for CPU’, ‘US/China trade increases’, and ‘US grows increasingly authoritarian’ feed into ‘China and the US cooperate on AI alignment’, then into ‘AI alignment problem is solved’, and finally ‘AI causes human extinction’. Arrows between panels illustrate the workflow sequence. CHART TYPE: conceptual flow diagram with two Bayes nets and intermediate conditional trees. DATA: relationships among qualitative variables—no numeric axes. PURPOSE: illustrates AMTAIR’s iterative refinement pipeline from full Bayes net to crux-tree extraction and back. DETAILS: emphasises node styles (solid, dashed, dotted) for relevance; shows convergence toward the extinction outcome. SOURCE: @gruetzemacher2022, May 2025.\"\n    fig-align=\"center\"                        \n    width=\"100%\"\n}](https://bnma.co/uai2022-apps-workshop/papers/S5.pdf)\n\n\n\n\n\n\n\nfrom @mccaslin2024\n\n[![Conditional-tree Guide](/images/conditional_tree.jpg){\n  #fig-conditional_tree\n  fig-scap=\"Conditional-tree Guide\"\n  fig-alt=\"CHART TYPE: annotated schematic of a three-level conditional tree. DATA: placeholders XX %, AA %, BB %, VV %, WW %, etc. PURPOSE: illustrates colour and label conventions—green for ultimate question, blue/purple for indicator questions, grey/red for branch probabilities, red for updated extinction probabilities and relative-risk factors. DETAILS: shows how each indicator’s TRUE or FALSE branch feeds probabilistically into the ultimate extinction outcome. SOURCE: McCaslin et al. 2024 @mccaslin2024, FRI Working Paper #3.\"\n  fig-align=\"center\"\n    width=\"100%\"\n}](https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/66ba37a144f1d6095de467df/1723479995772/AIConditionalTrees.pdf#page=5.78)\n\n\n\n\n\n\nfrom @mccaslin2024\n\n[![Experts’ conditional-tree updates (2030-2070)](/images/concerned_experts.jpg){\n    #fig-concerned_experts\n    fig-scap=\"Experts’ conditional-tree updates (2030-2070)\"\n    fig-alt=\"CHART TYPE: conditional-probability tree with three sequential indicator nodes. DATA: baseline AI-extinction probability 17 % in 2023; indicator 1 (2030 administrative disempowerment warning shot) TRUE=37 %, FALSE=63 %; two conditional probabilities for extinction in 2100: 31.6 % (relative-risk 1.9×) if TRUE, 14.3 % (0.9×) if FALSE. Indicator 2 (2050 power-seeking warning shot) TRUE=54 %, FALSE=46 %; corresponding extinction probabilities 23.4 % (1.4×) and 10.5 % (0.6×). Indicator 3 (2070 no aligned AGI) TRUE=46 %, FALSE=54 %; extinction probabilities 25.0 % (1.5×) and 13.7 % (0.8×). PURPOSE: quantifies how confirmation or disconfirmation of warning-shot events would shift expert-assessed AI-extinction risk. DETAILS: experts are most alarmed by earlier administrative disempowerment (1.9× increase) and least by absence of power-seeking shot (0.6×). SOURCE: McCaslin et al. 2024 @mccaslin2024, FRI Working Paper #3.\"\n    fig-align=\"center\"\n    width=\"100%\"\n}](https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/66ba37a144f1d6095de467df/1723479995772/AIConditionalTrees.pdf#page=5.78)\n\n\n\nfrom @manheim2021\n\n[![Overlay of inside/outside/assimilation views](/images/mtair-insideoutside-overlay.jpg){\n    #fig-mtair-insideoutside-overlay\n    fig-scap=\"Overlay of inside/outside/assimilation views\"\n    fig-alt=\"CONCEPT MAP overlaid by three translucent circles captioned Inside view, Outside views, and Assimilation logic. Left bullet list of six APS assumptions feeds a central causal chain of probabilities (timeline, incentive, alignment, failure, disempowerment, catastrophe) leading to a node titled ‘Cr existential catastrophe | world model’. Lower-left cluster of rectangles represents outside-view priors (Second Species Argument, transformative-tech base rate, AGI timeline forecasts, etc.). Right-hand cluster shows weighting and integration logic combining world-model estimate with outside-view priors into a final existential-catastrophe credence. No numerical axes—pure structural relationships. PURPOSE: illustrate how MTAIR reconciles inside-view technical reasoning with outside-view priors using an assimilation weighting scheme. SOURCE: David Manheim @manheim2021, MTAIR sequence post #3, Jul 2021.\"\n    fig-align=\"center\"\n    width=\"100%\"\n}](https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk)\n\n\n\n\nfrom @manheim2021\n\n[![Base APS causal map](/images/mtair-insideoutside-base.jpg){\n    #fig-mtair-insideoutside-base\n    fig-scap=\"Base APS causal map (clean)\"\n    fig-alt=\"Same node-and-arrow causal graph as the overlay figure but without the purple, violet, and red guiding circles. Blue bullet premises feed ‘Collection of inputs’ rectangle, cascading turquoise probability ovals lead to ‘Cr existential catastrophe | world model’. Lower left shows outside-view priors, right shows weighting logic, centre red oval ‘Cr existential catastrophe’. Provides uncluttered view of the structural model prior to explanatory overlay. SOURCE: David Manheim @manheim2021, MTAIR sequence, 2021.\"\n    fig-align=\"center\"\n    width=\"100%\"\n}](https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk)\n\n\n\n\n\n\n\n\n\n\n\nfrom @clarke2022\n\n[![MTAIR Quantitative map structure](/images/mtair-quant-map.jpg){\n    #fig-mtair-quant-map\n    fig-scap=\"MTAIR Quantitative map structure\"\n    fig-alt=\"FLOW DIAGRAM titled ‘Quantitative Model’. Blue and cyan rectangles (Hypotheses and Debated propositions) feed green ‘Proposed agenda’ boxes and a rose ‘Meta-uncertainty’ box, which all point to red ‘Catastrophe scenario’ boxes. Tiny mini-PDF icons depict probability distributions beside each variable. Right-hand analysis panel lists Effects of investment, Sensitivity analysis, What-if questions, Decision approaches, Analysis tools. PURPOSE: show how MTAIR converts a qualitative causal map into a quantified Bayesian network that supports downstream scenario and decision analysis. OURCE: David Manheim et. al, Modeling Transformative AI Risks (MTAIR) Project -- Summary Report, 2021.\"\n    fig-align=\"center\"\n    width=\"100%\"\n}](https://arxiv.org/pdf/2206.09360#page=10.75)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom @clarke2022\n\n[![MTAIR Qualitative map structure](/images/mtair-qual-map.jpg){\n    #fig-mtair-qual-map\n    fig-scap=\"MTAIR Qualitative map structure\"\n    fig-alt=\"NODE-LINK DIAGRAM titled ‘Qualitative Map’. Blue rectangles ‘Hypothesis 1’ and ‘Hypothesis 2’, cyan rectangles ‘Debated propositions 1 & 2’, green rectangles ‘Proposed agendas 1 & 2’, red rectangles ‘Catastrophe scenarios 1 & 2’. Arrows show causal influence path from hypotheses through debated propositions and agendas to catastrophes. No probability icons, no analysis panel. PURPOSE: foundational structure before numerical parametrisation, illustrating argumentative flow in MTAIR. SOURCE: David Manheim et. al, Modeling Transformative AI Risks (MTAIR) Project -- Summary Report, 2021.\"\n    fig-align=\"center\"\n    width=\"100%\"\n}](https://arxiv.org/pdf/2206.09360#page=10.75)\n\n\n\n\n\n\nfrom @cottier2019\n\n[![Key hypotheses in AI alignment](/images/hypotheses_diagram.pdf){\n    #fig-ai-hypotheses-map\n    fig-scap=\"Key hypotheses in AI alignment\"\n    fig-alt=\"LARGE CONCEPT MAP. Nodes are colour-coded: red for problems that could lead to catastrophe, green for solutions or agendas, blue for scenarios or conceptual models. Bold-border nodes denote primary hypotheses such as ‘Discontinuity to AGI’, ‘Agentive AGI’, ‘Broad basin for corrigibility’, and ‘Mesa-optimisation’. Directed arrows link questions to hypotheses, questions to questions, and scenarios to hypotheses. Arrow labels (Yes, No, Defer, brief rationales) indicate how answering the tail node influences credence in the head node. A legend at the bottom explains colour categories and arrow semantics. Source: Ben Cottier & Rohin Shah (2019) @cottier2019 “Clarifying Some Key Hypotheses in AI Alignment”, AI Alignment Forum.\"\n    fig-align=\"center\"\n    width=\"100%\"\n}](https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment#Agentive_AGI_)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom Metropolitansky and Larson (2025)\n\n\n\n\n\n\nFigure 1.1: Claimify claim-extraction stages\n\n\n\nfrom Tetlock (2022)\n\n\n\n\n\n\nFigure 1.2: Conditional-tree AI-risk forecasts\n\n\n\nfrom Gruetzemacher (2022)\n\n\n\n\n\n\nFigure 1.3: Bayes-net pruning → crux extraction → re-expansion\n\n\n\nfrom McCaslin et al. (2024)\n\n\n\n\n\n\nFigure 1.4: Conditional-tree Guide\n\n\n\nfrom McCaslin et al. (2024)\n\n\n\n\n\n\nFigure 1.5: Experts’ conditional-tree updates (2030-2070)\n\n\n\nfrom Manheim (2021)\n\n\n\n\n\n\nFigure 1.6: Overlay of inside/outside/assimilation views\n\n\n\nfrom Manheim (2021)\n\n\n\n\n\n\nFigure 1.7: Base APS causal map\n\n\n\nfrom Clarke et al. (2022)\n\n\n\n\n\n\nFigure 1.8: MTAIR Quantitative map structure\n\n\n\nfrom Clarke et al. (2022)\n\n\n\n\n\n\nFigure 1.9: MTAIR Qualitative map structure\n\n\n\nfrom Cottier and Shah (2019)\n\n\n\n\n\n\nFigure 1.10: Key hypotheses in AI alignment",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>References (.md)</span>"
    ]
  }
]