<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; 1. Introduction: The Coordination Crisis in AI Governance – Automating the Modelling of Transformative Artificial Intelligence Risks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/2.0.Context.html" rel="next">
<link href="../chapters/0.Frontmatter.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/1.Introduction.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1. Introduction: The Coordination Crisis in AI Governance</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Automating the Modelling of Transformative Artificial Intelligence Risks</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/VJMeyer/submission" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../Automating-the-Modelling-of-Transformative-Artificial-Intelligence-Risks.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../Automating-the-Modelling-of-Transformative-Artificial-Intelligence-Risks.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Abstract</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/0.Frontmatter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Preface</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/1.Introduction.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1. Introduction: The Coordination Crisis in AI Governance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/2.0.Context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2. Context and Theoretical Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/3.0.AMTAIR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3. AMTAIR: Design and Implementation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/4.Discussion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4. Discussion: Implications and Limitations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/5.Conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5. Conclusion: Toward Coordinated AI Governance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ref/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/Appendix-K.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix K: From Prototype to Platform: A Research Program Roadmap</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/Appendix-L.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix L: Prompt Engineering - The Hidden Art</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/Appendix-M.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix M: The Validation Frontier - Measuring Truth in Argument Extraction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/Appendix-N.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix N: Bucknall Case Study - Near-Term AI as Existential Risk Factor</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title"></span></span></a><a href="https://colab.research.google.com/github/SingularitySmith/AMTAIR_Prototype/blob/main/version_history/AMTAIR_Prototype_0_1.3.ipynb#scrollTo=lt8-AnebGUXr">AMTAIR Prototype Demonstration (Public Colab Notebook)</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#opening-scenario-the-policymakers-dilemma" id="toc-opening-scenario-the-policymakers-dilemma" class="nav-link active" data-scroll-target="#opening-scenario-the-policymakers-dilemma">1.1 Opening Scenario: The Policymaker’s Dilemma</a></li>
  <li><a href="#the-coordination-crisis-in-ai-governance" id="toc-the-coordination-crisis-in-ai-governance" class="nav-link" data-scroll-target="#the-coordination-crisis-in-ai-governance">1.2 The Coordination Crisis in AI Governance</a>
  <ul>
  <li><a href="#safety-gaps-from-misaligned-efforts" id="toc-safety-gaps-from-misaligned-efforts" class="nav-link" data-scroll-target="#safety-gaps-from-misaligned-efforts">1.2.1 Safety Gaps from Misaligned Efforts</a></li>
  <li><a href="#resource-misallocation" id="toc-resource-misallocation" class="nav-link" data-scroll-target="#resource-misallocation">1.2.2 Resource Misallocation</a></li>
  <li><a href="#negative-sum-dynamics" id="toc-negative-sum-dynamics" class="nav-link" data-scroll-target="#negative-sum-dynamics">1.2.3 Negative-Sum Dynamics</a></li>
  </ul></li>
  <li><a href="#historical-parallels-and-temporal-urgency" id="toc-historical-parallels-and-temporal-urgency" class="nav-link" data-scroll-target="#historical-parallels-and-temporal-urgency">1.3 Historical Parallels and Temporal Urgency</a></li>
  <li><a href="#research-question-and-scope" id="toc-research-question-and-scope" class="nav-link" data-scroll-target="#research-question-and-scope">1.4 Research Question and Scope</a></li>
  <li><a href="#the-multiplicative-benefits-framework" id="toc-the-multiplicative-benefits-framework" class="nav-link" data-scroll-target="#the-multiplicative-benefits-framework">1.5 The Multiplicative Benefits Framework</a>
  <ul>
  <li><a href="#automated-worldview-extraction" id="toc-automated-worldview-extraction" class="nav-link" data-scroll-target="#automated-worldview-extraction">1.5.1 Automated Worldview Extraction</a></li>
  <li><a href="#live-data-integration" id="toc-live-data-integration" class="nav-link" data-scroll-target="#live-data-integration">1.5.2 Live Data Integration</a></li>
  <li><a href="#formal-policy-evaluation" id="toc-formal-policy-evaluation" class="nav-link" data-scroll-target="#formal-policy-evaluation">1.5.3 Formal Policy Evaluation</a></li>
  <li><a href="#the-synergy" id="toc-the-synergy" class="nav-link" data-scroll-target="#the-synergy">1.5.4 The Synergy</a></li>
  </ul></li>
  <li><a href="#thesis-structure-and-roadmap" id="toc-thesis-structure-and-roadmap" class="nav-link" data-scroll-target="#thesis-structure-and-roadmap">1.6 Thesis Structure and Roadmap</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/VJMeyer/submission/edit/main/chapters/1.Introduction.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div><div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="https://colab.research.google.com/github/VJMeyer/submission/blob/main/AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb"><i class="bi bi-file-code"></i>Colab Notebook</a></li><li><a href="https://github.com/VJMeyer/submission"><i class="bi bi-github"></i>GitHub Repository</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1. Introduction: The Coordination Crisis in AI Governance</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- 
**Chapter Overview**  
**Grade Weight**: 10% | **Target Length**: ~14% of text (~4,200 words)  
**Requirements**: Introduces and motivates the core question, provides context, states precise thesis, provides roadmap
-->
<!-- ## 1.1 Opening Scenario: The Policymaker's Dilemma -->
<!-- [-] TODO: Polish opening scenario for maximum impact -->
<!-- 
Consider the following scenario, one that has become distressingly common in regulatory offices worldwide. A policy advisor—let's call her Sarah—faces an impossible task. Her desk groans under the weight of competing analyses about artificial intelligence risks. Each report carries impressive credentials. Each reaches fundamentally different conclusions.

The first document, authored by a consortium of computer scientists, warns of imminent existential threat. Its pages bristle with technical terminology: mesa-optimization, instrumental convergence, orthogonality thesis. The mathematics appears sound, yet the conclusions seem almost fantastical. The second report, penned by economists and policy veterans, dismisses such concerns as speculative fear-mongering. Where the computer scientists see existential risk, the economists see manageable externalities. A third analysis, straddling both worlds, proposes technical standards so hedged with caveats as to be nearly meaningless.

Sarah's dilemma extends beyond mere intellectual disagreement. The clock ticks toward a legislative deadline. Real decisions with profound consequences await. Yet the expert community offers not clarity but cacophony. Each perspective seems internally coherent, even compelling, yet they cannot all be correct. The technical arguments require PhD-level mathematics to evaluate properly. The timescales range from "urgent action needed yesterday" to "let's wait and see how the technology develops." 

This scenario—Sarah's scenario—plays out with minor variations in Washington, Brussels, Beijing, and dozens of other capitals. It represents what I've come to understand as the coordination crisis in AI governance: a systematic failure of our epistemic infrastructure at precisely the moment we need it most. -->
<section id="opening-scenario-the-policymakers-dilemma" class="level2">
<h2 class="anchored" data-anchor-id="opening-scenario-the-policymakers-dilemma">1.1 Opening Scenario: The Policymaker’s Dilemma</h2>
<!-- [-] TODO: Polish opening scenario for maximum impact -->
<!-- [-] ADD: @todd2024 add as reference for more resources towards AI safety -->
<!-- [ ] ADD: CITATIONS for orthogonality thesis and instrumental convergence thesis -->
<p>A senior policy advisor sits at her desk, drowning in reports. Twelve different documents from AI safety researchers, each compelling, each contradictory. One warns of existential catastrophe within the decade, citing concepts she half-understands—orthogonality, instrumental convergence. Another dismisses these fears as overblown. A third proposes technical standards but hedges with so many caveats it might as well propose nothing. The clock’s ticking. Legislation needs drafting. Yet these experts, brilliant as they are individually, seem to inhabit different universes. The technical arguments involve mathematical formalism she lacks time to parse. The policy recommendations conflict at fundamental levels. She needs synthesis, not more analysis. She needs a way to see where these worldviews actually diverge versus where they’re using different words for the same fears. This scenario plays out daily across Washington, Brussels, Beijing—wherever humans grapple with governing something that doesn’t exist yet but might remake everything when it does.</p>
<!-- [-] EXPLAIN: in footnotes: a) Orthogonality Thesis: Intelligence and goals are independent; an AI can have any set of objectives regardless of its intelligence level. b) Instrumental Convergence Thesis: Different AI systems may adopt similar instrumental goals (e.g., self-preservation, resource acquisition) to achieve their objectives. -->
<p>This scenario<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> plays out daily across government offices, corporate boardrooms, and research institutions worldwide. It exemplifies what I term the “coordination crisis” in AI governance: despite unprecedented attention and resources directed toward AI safety, we lack the epistemic infrastructure to synthesize diverse expert knowledge into actionable governance strategies <span class="citation" data-cites="todd2024">Todd (<a href="../ref/references.html#ref-todd2024" role="doc-biblioref">2024</a>)</span>.</p>
<!-- [-] CREATE: {#fig-policymaker-dilemma}: "Visual representation of conflicting expert reports on advisor's desk" 
Show Image
-->
</section>
<section id="the-coordination-crisis-in-ai-governance" class="level2">
<h2 class="anchored" data-anchor-id="the-coordination-crisis-in-ai-governance">1.2 The Coordination Crisis in AI Governance</h2>
<!-- [-] TODO: Frame the problem as coordination failure rather than merely technical challenge -->
<!-- [-] ADD: @maslej2025 Add citation for page 85 as evidence for accelerating capabilities -->
<!-- [-] ADD: @samborska2025 Add as citation for accelerating capabilities -->
<p>As AI capabilities advance at an accelerating pace—demonstrated by the rapid progression from GPT-3 to GPT-4, Claude, and emerging multimodal systems <span class="citation" data-cites="maslej2025">Maslej (<a href="../ref/references.html#ref-maslej2025" role="doc-biblioref">2025</a>)</span> <span class="citation" data-cites="samborska2025">Samborska (<a href="../ref/references.html#ref-samborska2025" role="doc-biblioref">2025</a>)</span>—humanity faces a governance challenge unlike any in history. The task of ensuring increasingly powerful AI systems remain aligned with human values and beneficial to our long-term flourishing grows more urgent with each capability breakthrough. This challenge becomes particularly acute when considering transformative AI systems that could drastically alter civilization’s trajectory, potentially including existential risks from misaligned systems pursuing objectives counter to human welfare.</p>
<p>Despite unprecedented investment in AI safety research, rapidly growing awareness among key stakeholders, and proliferating frameworks for responsible AI development, we face what I’ll term the “coordination crisis” in AI governance—a systemic failure to align diverse efforts across technical, policy, and strategic domains into a coherent response proportionate to the risks we face.</p>
<p>The current state of AI governance presents a striking paradox. On one hand, we witness extraordinary mobilization: billions in research funding, proliferating safety initiatives, major tech companies establishing alignment teams, and governments worldwide developing AI strategies. The Asilomar AI Principles garnered thousands of signatures <span class="citation" data-cites="tegmark2024">Tegmark (<a href="../ref/references.html#ref-tegmark2024" role="doc-biblioref">2024</a>)</span>, the EU advances comprehensive AI regulation <span class="citation" data-cites="european2024">European (<a href="../ref/references.html#ref-european2024" role="doc-biblioref">2024</a>)</span>, and technical researchers produce increasingly sophisticated work on alignment, interpretability, and robustness.</p>
<p>Yet alongside this activity, we observe systematic coordination failures that may prove catastrophic. Technical safety researchers develop sophisticated alignment techniques without clear implementation pathways. Policy specialists craft regulatory frameworks lacking technical grounding to ensure practical efficacy. Ethicists articulate normative principles that lack operational specificity. Strategy researchers identify critical uncertainties but struggle to translate these into actionable guidance. International bodies convene without shared frameworks for assessing interventions.</p>
<!-- [-] CREATE: {#fig-coordination-crisis}: "Systems diagram showing fragmentation between AI governance communities" 
Show Image
-->
<div id="fig-ai-hypotheses-map" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" data-fig-scap="Key hypotheses in AI alignment" alt="LARGE CONCEPT MAP. Nodes are colour-coded: red for problems that could lead to catastrophe, green for solutions or agendas, blue for scenarios or conceptual models. Bold-border nodes denote primary hypotheses such as ‘Discontinuity to AGI’, ‘Agentive AGI’, ‘Broad basin for corrigibility’, and ‘Mesa-optimisation’. Directed arrows link questions to hypotheses, questions to questions, and scenarios to hypotheses. Arrow labels (Yes, No, Defer, brief rationales) indicate how answering the tail node influences credence in the head node. A legend at the bottom explains colour categories and arrow semantics. Source: Ben Cottier &amp; Rohin Shah (2019) @cottier2019 “Clarifying Some Key Hypotheses in AI Alignment”, AI Alignment Forum." width="110%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ai-hypotheses-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment#Agentive_AGI_"><embed src="../images/hypotheses_diagram.pdf" class="img-fluid quarto-figure quarto-figure-center" style="width:110.0%" data-fig-scap="Key hypotheses in AI alignment" alt="LARGE CONCEPT MAP. Nodes are colour-coded: red for problems that could lead to catastrophe, green for solutions or agendas, blue for scenarios or conceptual models. Bold-border nodes denote primary hypotheses such as ‘Discontinuity to AGI’, ‘Agentive AGI’, ‘Broad basin for corrigibility’, and ‘Mesa-optimisation’. Directed arrows link questions to hypotheses, questions to questions, and scenarios to hypotheses. Arrow labels (Yes, No, Defer, brief rationales) indicate how answering the tail node influences credence in the head node. A legend at the bottom explains colour categories and arrow semantics. Source: Ben Cottier &amp; Rohin Shah (2019) @cottier2019 “Clarifying Some Key Hypotheses in AI Alignment”, AI Alignment Forum."></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ai-hypotheses-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: from <span class="citation" data-cites="cottier2019">Cottier and Shah (<a href="../ref/references.html#ref-cottier2019" role="doc-biblioref">2019</a>)</span>: Key hypotheses across the AI alignment ecosystem
</figcaption>
</figure>
</div>
<section id="safety-gaps-from-misaligned-efforts" class="level3">
<h3 class="anchored" data-anchor-id="safety-gaps-from-misaligned-efforts">1.2.1 Safety Gaps from Misaligned Efforts</h3>
<!-- [-] TODO: Document how fragmentation systematically increases risk through with specific, good examples -->
<p>The fragmentation problem manifests in incompatible frameworks between technical researchers, policy specialists, and strategic analysts. Each community develops sophisticated approaches within their domain, yet translation between domains remains primitive. This creates systematic blind spots where risks emerge at the interfaces between technical capabilities, institutional responses, and strategic dynamics.</p>
<p>When different communities operate with incompatible frameworks, critical risks fall through the cracks. Technical researchers may solve alignment problems under assumptions that policymakers’ decisions invalidate. Regulations optimized for current systems may inadvertently incentivize dangerous development patterns. Without shared models of the risk landscape, our collective efforts resemble the parable of blind men describing an elephant—each accurate within their domain but missing the complete picture <span class="citation" data-cites="paul2023">Paul (<a href="../ref/references.html#ref-paul2023" role="doc-biblioref">2023</a>)</span>.</p>
<!-- [-] FIND: @coordination-failure-examples: "Specific historical examples of coordination failures in technology governance" -- arms races to nobodies benefit -- technological race to ?? -->
<p>Historical precedents demonstrate how coordination failures in technology governance can lead to dangerous dynamics. The nuclear arms race exemplifies how lack of coordination can create negative-sum outcomes where all parties become less secure despite massive investments in safety measures. Similar dynamics may emerge in AI development without proper coordination infrastructure.</p>
</section>
<section id="resource-misallocation" class="level3">
<h3 class="anchored" data-anchor-id="resource-misallocation">1.2.2 Resource Misallocation</h3>
<!-- [-] TODO: Explain that duplicative efforts absorbing research funding, publications, and initiatives might sometimes improve reliability (think reproducing results) but tend to waste resources in expectation (opportuntity cost) -- change the tone of the paragraph accordingly -->
<p>The AI safety community faces a complex tradeoff in resource allocation. While some duplication of efforts can improve reliability through independent verification—akin to reproducing scientific results—the current level of fragmentation often leads to wasteful redundancy. Multiple teams independently develop similar frameworks without building on each other’s work, creating opportunity costs where critical but unglamorous research areas remain understaffed. Funders struggle to identify high-impact opportunities across technical and governance domains, lacking the epistemic infrastructure to assess where marginal resources would have the greatest impact. This misallocation becomes more costly as the window for establishing effective governance narrows with accelerating AI development.</p>
<!-- [-] CREATE: {#tbl-resource-duplication}: "Examples of duplicated AI safety efforts across organizations" -->
<div id="tbl-resource-duplication" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-resource-duplication-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Table 1.2.2: Examples of duplicated AI safety efforts across organizations
</figcaption>
<div aria-describedby="tbl-resource-duplication-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Research Area</th>
<th>Organization A</th>
<th>Organization B</th>
<th>Duplication Level</th>
<th>Opportunity Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Interpretability Methods</td>
<td>Anthropic’s mechanistic interpretability</td>
<td>DeepMind’s concept activation vectors</td>
<td>Medium</td>
<td>Reduced focus on multi-agent safety</td>
</tr>
<tr class="even">
<td>Alignment Frameworks</td>
<td>MIRI’s embedded agency</td>
<td>FHI’s comprehensive AI services</td>
<td>High</td>
<td>Limited work on institutional design</td>
</tr>
<tr class="odd">
<td>Risk Assessment Models</td>
<td>GovAI’s policy models</td>
<td>CSER’s existential risk frameworks</td>
<td>High</td>
<td>Insufficient capability benchmarking</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="negative-sum-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="negative-sum-dynamics">1.2.3 Negative-Sum Dynamics</h3>
<!-- [-] TODO: Address capability-governance gaps widening with accelerating development -->
<p>Perhaps most concerning, uncoordinated interventions can actively increase risk. Safety standards that advantage established players may accelerate risky development elsewhere. Partial transparency requirements might enable capability advances without commensurate safety improvements. International agreements lacking shared technical understanding may lock in dangerous practices. Without coordination, our cure risks becoming worse than the disease.</p>
<p>The game-theoretic structure of AI development creates particularly pernicious dynamics. Armstrong et al. <span class="citation" data-cites="armstrong2016">Armstrong, Bostrom, and Shulman (<a href="../ref/references.html#ref-armstrong2016" role="doc-biblioref">2016</a>)</span> demonstrate how uncoordinated policies can incentivize a “race to the precipice” where competitive pressures override safety considerations. The situation resembles a multi-player prisoner’s dilemma or stag hunt where individually rational decisions lead to collectively catastrophic outcomes <span class="citation" data-cites="samuel2023">Samuel (<a href="../ref/references.html#ref-samuel2023" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="hunt2025">Hunt (<a href="../ref/references.html#ref-hunt2025" role="doc-biblioref">2025</a>)</span>.</p>
<!-- [-] LATER ADD: Citation for unpublished "anybody builds it everyone dies (soares & yudkowsky)" -->
</section>
</section>
<section id="historical-parallels-and-temporal-urgency" class="level2">
<h2 class="anchored" data-anchor-id="historical-parallels-and-temporal-urgency">1.3 Historical Parallels and Temporal Urgency</h2>
<!-- [-] TODO: Draw connections to nuclear governance, climate change, and biosecurity -->
<p>History offers instructive parallels. The nuclear age began with scientists racing to understand and control forces that could destroy civilization. Early coordination failures—competing national programs, scientist-military tensions, public-expert divides—nearly led to catastrophe multiple times. Only through developing shared frameworks (deterrence theory) <span class="citation" data-cites="schelling1960">Schelling (<a href="../ref/references.html#ref-schelling1960" role="doc-biblioref">1960</a>)</span>, institutions (IAEA), and communication channels (hotlines, treaties) did humanity navigate the nuclear precipice <span class="citation" data-cites="rehman2025">Rehman (<a href="../ref/references.html#ref-rehman2025" role="doc-biblioref">2025</a>)</span>.</p>
<p>Yet AI presents unique coordination challenges that compress our response timeline:</p>
<p><strong>Accelerating Development</strong>: Unlike nuclear weapons requiring massive infrastructure, AI development proceeds in corporate labs and academic departments worldwide. Capability improvements come through algorithmic insights and computational scale, both advancing exponentially.</p>
<p><strong>Dual-Use Ubiquity</strong>: Every AI advance potentially contributes to both beneficial applications and catastrophic risks. The same language model architectures enabling scientific breakthroughs could facilitate dangerous manipulation or deception at scale.</p>
<p><strong>Comprehension Barriers</strong>: Nuclear risks were viscerally understandable—cities vaporized, radiation sickness, nuclear winter. AI risks involve abstract concepts like optimization processes, goal misspecification, and emergent capabilities that resist intuitive understanding.</p>
<p><strong>Governance Lag</strong>: Traditional governance mechanisms—legislation, international treaties, professional standards—operate on timescales of years to decades. AI capabilities advance on timescales of months to years, creating an ever-widening capability-governance gap.</p>
<!-- [-] CREATE: {#fig-governance-lag}: "Timeline comparison of AI capability vs governance development" 
Show Image
-->
</section>
<section id="research-question-and-scope" class="level2">
<h2 class="anchored" data-anchor-id="research-question-and-scope">1.4 Research Question and Scope</h2>
<!-- [-] TODO: Clearly articulate the primary research question with precision -->
<p>This thesis addresses a specific dimension of the coordination challenge by investigating the question:</p>
<p><strong>Can frontier AI technologies be utilized to automate the modeling of transformative AI risks, enabling robust prediction of policy impacts across diverse worldviews?</strong></p>
<p>More specifically, I explore whether frontier language models can automate the extraction and formalization of probabilistic world models from AI safety literature, creating a scalable computational framework that enhances coordination in AI governance through systematic policy evaluation under uncertainty.</p>
<p>To break this down into its components:</p>
<ul>
<li><strong>Frontier AI Technologies</strong>: Today’s most capable language models (GPT-4, Claude-3 level systems)</li>
<li><strong>Automated Modeling</strong>: Using these systems to extract and formalize argument structures from natural language</li>
<li><strong>Transformative AI Risks</strong>: Potentially catastrophic outcomes from advanced AI systems, particularly existential risks</li>
<li><strong>Policy Impact Prediction</strong>: Evaluating how governance interventions might alter probability distributions over outcomes</li>
<li><strong>Diverse Worldviews</strong>: Accounting for fundamental disagreements about AI development trajectories and risk factors</li>
</ul>
<p>The investigation encompasses both theoretical development and practical implementation, focusing specifically on existential risks from misaligned AI systems rather than broader AI ethics concerns. This narrowed scope enables deep technical development while addressing the highest-stakes coordination challenges.</p>
<!-- [-] LATER TODO: Refine thesis statement based on advisor feedback -->
</section>
<section id="the-multiplicative-benefits-framework" class="level2">
<h2 class="anchored" data-anchor-id="the-multiplicative-benefits-framework">1.5 The Multiplicative Benefits Framework</h2>
<!-- [-] TODO: Establish central thesis about synergistic combination of three elements -->
<p>The central thesis of this work is that combining three elements—automated worldview extraction, prediction market integration, and formal policy evaluation—creates multiplicative rather than merely additive benefits for AI governance. Each component enhances the others, creating a system more valuable than the sum of its parts.</p>
<!-- [-] CREATE: {#fig-multiplicative-benefits}: "Venn diagram showing synergies between extraction, markets, and evaluation": Automation × Live Prediction Market Integrations × Policy Impact Evaluations
Show Image
-->
<section id="automated-worldview-extraction" class="level3">
<h3 class="anchored" data-anchor-id="automated-worldview-extraction">1.5.1 Automated Worldview Extraction</h3>
<p>Current approaches to AI risk modeling, exemplified by the Modeling Transformative AI Risks (MTAIR) project, demonstrate the value of formal representation but require extensive manual effort. Creating a single model demands dozens of expert-hours to translate qualitative arguments into quantitative frameworks. This bottleneck severely limits the number of perspectives that can be formalized and the speed of model updates as new arguments emerge.</p>
<p>Automation using frontier language models addresses this scaling challenge. By developing systematic methods to extract causal structures and probability judgments from natural language, we can:</p>
<ul>
<li>Process orders of magnitude more content</li>
<li>Incorporate diverse perspectives rapidly</li>
<li>Maintain models that evolve with the discourse</li>
<li>Reduce barriers to entry for contributing worldviews</li>
</ul>
</section>
<section id="live-data-integration" class="level3">
<h3 class="anchored" data-anchor-id="live-data-integration">1.5.2 Live Data Integration</h3>
<p>Static models, however well-constructed, quickly become outdated in fast-moving domains. Prediction markets and forecasting platforms aggregate distributed knowledge about uncertain futures, providing continuously updated probability estimates. By connecting formal models to these live data sources, we create dynamic assessments that incorporate the latest collective intelligence <span class="citation" data-cites="tetlock2015">Tetlock and Gardner (<a href="../ref/references.html#ref-tetlock2015" role="doc-biblioref">2015</a>)</span>.</p>
<p>This integration serves multiple purposes:</p>
<ul>
<li>Grounding abstract models in empirical forecasts</li>
<li>Identifying which uncertainties most affect outcomes</li>
<li>Revealing when model assumptions diverge from collective expectations</li>
<li>Generating new questions for forecasting communities</li>
</ul>
</section>
<section id="formal-policy-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="formal-policy-evaluation">1.5.3 Formal Policy Evaluation</h3>
<p><strong>Formal policy evaluation</strong> transforms static risk assessments into actionable guidance by modeling how specific interventions alter critical parameters. Using causal inference techniques <span class="citation" data-cites="pearl2000">Pearl (<a href="../ref/references.html#ref-pearl2000" role="doc-biblioref">2000</a>)</span> <span class="citation" data-cites="pearl2009">Pearl (<a href="../ref/references.html#ref-pearl2009" role="doc-biblioref">2009</a>)</span>, we can assess not just the probability of adverse outcomes but how those probabilities change under different policy regimes.</p>
<p>This enables genuinely evidence-based policy development:</p>
<ul>
<li>Comparing interventions across multiple worldviews</li>
<li>Identifying robust strategies that work across scenarios</li>
<li>Understanding which uncertainties most affect policy effectiveness</li>
<li>Prioritizing research to reduce decision-relevant uncertainty</li>
</ul>
</section>
<section id="the-synergy" class="level3">
<h3 class="anchored" data-anchor-id="the-synergy">1.5.4 The Synergy</h3>
<p>The multiplicative benefits emerge from the interactions between components:</p>
<ul>
<li>Automation enables comprehensive coverage, making prediction market integration more valuable by connecting to more perspectives</li>
<li>Market data validates and calibrates automated extractions, improving quality</li>
<li>Policy evaluation gains precision from both comprehensive models and live probability updates</li>
<li>The complete system creates feedback loops where policy analysis identifies critical uncertainties for market attention</li>
</ul>
<p>This synergistic combination addresses the coordination crisis by providing common ground for disparate communities, translating between technical and policy languages, quantifying previously implicit disagreements, and enabling evidence-based compromise.</p>
</section>
</section>
<section id="thesis-structure-and-roadmap" class="level2">
<h2 class="anchored" data-anchor-id="thesis-structure-and-roadmap">1.6 Thesis Structure and Roadmap</h2>
<!-- [-] TODO: Preview the logical progression of the thesis -->
<p>The remainder of this thesis develops the multiplicative benefits framework from theoretical foundations to practical implementation:</p>
<p><strong>Chapter 2: Context and Theoretical Foundations</strong> establishes the intellectual groundwork, examining the epistemic challenges unique to AI governance, Bayesian networks as formal tools for uncertainty representation, argument mapping as a bridge from natural language to formal models, the MTAIR project’s achievements and limitations, and requirements for effective coordination infrastructure.</p>
<p><strong>Chapter 3: AMTAIR Design and Implementation</strong> presents the technical system including overall architecture and design principles, the two-stage extraction pipeline (ArgDown → BayesDown), validation methodology and results, case studies from simple examples to complex AI risk models, and integration with prediction markets and policy evaluation.</p>
<p><strong>Chapter 4: Discussion - Implications and Limitations</strong> critically examines technical limitations and failure modes, conceptual concerns about formalization, integration with existing governance frameworks, scaling challenges and opportunities, and broader implications for epistemic security.</p>
<p><strong>Chapter 5: Conclusion</strong> synthesizes key contributions and charts paths forward with a summary of theoretical and practical achievements, concrete recommendations for stakeholders, research agenda for community development, and vision for AI governance with proper coordination infrastructure.</p>
<p>Throughout this progression, I maintain dual focus on theoretical sophistication and practical utility. The framework aims not merely to advance academic understanding but to provide actionable tools for improving coordination in AI governance during this critical period.</p>
<!-- [-] CREATE: {#fig-thesis-roadmap}: "Visual flowchart of thesis structure and chapter connections"
Show Image
-->
<p>Having established the coordination crisis and outlined how automated modeling can address it, we now turn to the theoretical foundations that make this approach possible. The next chapter examines the unique epistemic challenges of AI governance and introduces the formal tools—particularly Bayesian networks—that enable rigorous reasoning under deep uncertainty.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-armstrong2016" class="csl-entry" role="listitem">
Armstrong, Stuart, Nick Bostrom, and Carl Shulman. 2016. <span>“Racing to the Precipice: A Model of Artificial Intelligence Development.”</span> <em>AI &amp; SOCIETY</em> 31 (2): 201–6. <a href="https://doi.org/10.1007/s00146-015-0590-y">https://doi.org/10.1007/s00146-015-0590-y</a>.
</div>
<div id="ref-cottier2019" class="csl-entry" role="listitem">
Cottier, Ben, and Rohin Shah. 2019. <span>“Clarifying Some Key Hypotheses in <span>AI</span> Alignment,”</span> August. <a href="https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment">https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment</a>.
</div>
<div id="ref-european2024" class="csl-entry" role="listitem">
European, Union. 2024. <span>“The <span>Act Texts</span> | <span>EU Artificial Intelligence Act</span>.”</span> 2024. <a href="https://artificialintelligenceact.eu/the-act/">https://artificialintelligenceact.eu/the-act/</a>.
</div>
<div id="ref-hunt2025" class="csl-entry" role="listitem">
Hunt, Tam. 2025. <span>“The Insane <span>‘Logic’</span> of the <span>AI</span> Arms Race.”</span> Medium. March 3, 2025. <a href="https://tamhunt.medium.com/the-insane-logic-of-the-ai-arms-race-45a5f79f4c0e">https://tamhunt.medium.com/the-insane-logic-of-the-ai-arms-race-45a5f79f4c0e</a>.
</div>
<div id="ref-maslej2025" class="csl-entry" role="listitem">
Maslej, Nestor. 2025. <span>“Artificial <span>Intelligence Index Report</span> 2025.”</span> <em>Artificial Intelligence</em>.
</div>
<div id="ref-paul2023" class="csl-entry" role="listitem">
Paul. 2023. <span>“The <span class="nocase">elephAInt</span> – <span>Are</span> We All Like the Six Blind Men When It Comes to <span>AI</span>? | <span>PRISMAGuard LLC</span>.”</span> 2023. <a href="https://www.prismaguard.com/the-elephaint-are-we-all-like-the-six-blind-men-when-it-comes-to-ai/">https://www.prismaguard.com/the-elephaint-are-we-all-like-the-six-blind-men-when-it-comes-to-ai/</a>.
</div>
<div id="ref-pearl2000" class="csl-entry" role="listitem">
Pearl, Judea. 2000. <em>Causality: Models, Reasoning, and Inference</em>. Cambridge, U.K. ; New York: Cambridge University Press.
</div>
<div id="ref-pearl2009" class="csl-entry" role="listitem">
———. 2009. <em>Causality: <span>Models</span>, Reasoning and Inference</em>. 2nd ed. Cambridge University Press.
</div>
<div id="ref-rehman2025" class="csl-entry" role="listitem">
Rehman, Iskander. 2025. <span>“The <span>Battle</span> for <span>Brilliant Minds</span>: <span>From</span> the <span>Nuclear Age</span> to <span>AI</span>.”</span> War on the Rocks. January 13, 2025. <a href="https://warontherocks.com/2025/01/the-battle-for-brilliant-minds-from-the-nuclear-age-to-ai/">https://warontherocks.com/2025/01/the-battle-for-brilliant-minds-from-the-nuclear-age-to-ai/</a>.
</div>
<div id="ref-samborska2025" class="csl-entry" role="listitem">
Samborska, Veronika. 2025. <span>“Scaling up: How Increasing Inputs Has Made Artificial Intelligence More Capable.”</span> <em>Our World in Data</em>, January. <a href="https://ourworldindata.org/scaling-up-ai">https://ourworldindata.org/scaling-up-ai</a>.
</div>
<div id="ref-samuel2023" class="csl-entry" role="listitem">
Samuel, Sigal. 2023. <span>“<span>AI</span> Is a <span>‘Tragedy of the Commons.’</span> <span>We</span>’ve Got Solutions for That.”</span> Vox. July 7, 2023. <a href="https://www.vox.com/future-perfect/2023/7/7/23787011/ai-arms-race-tragedy-commons-risk-safety">https://www.vox.com/future-perfect/2023/7/7/23787011/ai-arms-race-tragedy-commons-risk-safety</a>.
</div>
<div id="ref-schelling1960" class="csl-entry" role="listitem">
Schelling, Thomas C. 1960. <span>“I960. <span>The</span> Strategy of Conflict.”</span> <em>Cambridge, Mass</em>.
</div>
<div id="ref-tegmark2024" class="csl-entry" role="listitem">
Tegmark, Max. 2024. <span>“Asilomar <span>AI Principles</span>.”</span> Future of Life Institute. 2024. <a href="https://futureoflife.org/open-letter/ai-principles/">https://futureoflife.org/open-letter/ai-principles/</a>.
</div>
<div id="ref-tetlock2015" class="csl-entry" role="listitem">
Tetlock, Philip E., and Dan Gardner. 2015. <em>Superforecasting: The Art and Science of Prediction</em>. First paperback edition. New York: Broadway Books.
</div>
<div id="ref-todd2024" class="csl-entry" role="listitem">
Todd, Benjamin. 2024. <span>“It Looks Like There Are Some Good Funding Opportunities in <span>AI</span> Safety Right Now.”</span> Substack newsletter. Benjamin Todd. December 21, 2024. <a href="https://benjamintodd.substack.com/p/looks-like-there-are-some-good-funding">https://benjamintodd.substack.com/p/looks-like-there-are-some-good-funding</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The orthogonality thesis posits that intelligence and goals are independent—an AI can have any set of objectives regardless of its intelligence level. The instrumental convergence thesis suggests that different AI systems may adopt similar instrumental goals (e.g., self-preservation, resource acquisition) to achieve their objectives.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/0.Frontmatter.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Preface</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/2.0.Context.html" class="pagination-link" aria-label="2. Context and Theoretical Foundations">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2. Context and Theoretical Foundations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/VJMeyer/submission/edit/main/chapters/1.Introduction.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>