<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Preface – Automating the Modelling of Transformative Artificial Intelligence Risks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../ref/references.html" rel="next">
<link href="../../index.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/Outlines/Outline_13.html">Preface</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Automating the Modelling of Transformative Artificial Intelligence Risks</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/VJMeyer/submission" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../../Automating-the-Modelling-of-Transformative-Artificial-Intelligence-Risks.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../../Automating-the-Modelling-of-Transformative-Artificial-Intelligence-Risks.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/Outlines/Outline_13.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../ref/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">References (.md)</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#acknowledgments" id="toc-acknowledgments" class="nav-link active" data-scroll-target="#acknowledgments">Acknowledgments</a></li>
  <li><a href="#table-of-contents" id="toc-table-of-contents" class="nav-link" data-scroll-target="#table-of-contents">Table of Contents</a></li>
  <li><a href="#list-of-figures" id="toc-list-of-figures" class="nav-link" data-scroll-target="#list-of-figures">List of Figures</a></li>
  <li><a href="#list-of-tables" id="toc-list-of-tables" class="nav-link" data-scroll-target="#list-of-tables">List of Tables</a></li>
  <li><a href="#list-of-abbreviations" id="toc-list-of-abbreviations" class="nav-link" data-scroll-target="#list-of-abbreviations">List of Abbreviations</a></li>
  <li><a href="#sec-introduction" id="toc-sec-introduction" class="nav-link" data-scroll-target="#sec-introduction">1. Introduction: The Coordination Crisis in AI Governance</a>
  <ul>
  <li><a href="#sec-opening-scenario" id="toc-sec-opening-scenario" class="nav-link" data-scroll-target="#sec-opening-scenario">1.1 Opening Scenario: The Policymaker’s Dilemma</a></li>
  <li><a href="#sec-coordination-crisis" id="toc-sec-coordination-crisis" class="nav-link" data-scroll-target="#sec-coordination-crisis">1.2 The Coordination Crisis in AI Governance</a>
  <ul>
  <li><a href="#sec-safety-gaps" id="toc-sec-safety-gaps" class="nav-link" data-scroll-target="#sec-safety-gaps">1.2.1 Safety Gaps from Misaligned Efforts</a></li>
  <li><a href="#sec-resource-misallocation" id="toc-sec-resource-misallocation" class="nav-link" data-scroll-target="#sec-resource-misallocation">1.2.2 Resource Misallocation</a></li>
  <li><a href="#sec-negative-sum" id="toc-sec-negative-sum" class="nav-link" data-scroll-target="#sec-negative-sum">1.2.3 Negative-Sum Dynamics</a></li>
  </ul></li>
  <li><a href="#sec-historical-urgency" id="toc-sec-historical-urgency" class="nav-link" data-scroll-target="#sec-historical-urgency">1.3 Historical Parallels and Temporal Urgency</a></li>
  <li><a href="#sec-research-question" id="toc-sec-research-question" class="nav-link" data-scroll-target="#sec-research-question">1.4 Research Question and Scope</a></li>
  <li><a href="#sec-multiplicative-benefits" id="toc-sec-multiplicative-benefits" class="nav-link" data-scroll-target="#sec-multiplicative-benefits">1.5 The Multiplicative Benefits Framework</a>
  <ul>
  <li><a href="#sec-automated-extraction" id="toc-sec-automated-extraction" class="nav-link" data-scroll-target="#sec-automated-extraction">1.5.1 Automated Worldview Extraction</a></li>
  <li><a href="#sec-live-data" id="toc-sec-live-data" class="nav-link" data-scroll-target="#sec-live-data">1.5.2 Live Data Integration</a></li>
  <li><a href="#sec-policy-evaluation" id="toc-sec-policy-evaluation" class="nav-link" data-scroll-target="#sec-policy-evaluation">1.5.3 Formal Policy Evaluation</a></li>
  <li><a href="#sec-synergy" id="toc-sec-synergy" class="nav-link" data-scroll-target="#sec-synergy">1.5.4 The Synergy</a></li>
  </ul></li>
  <li><a href="#sec-roadmap" id="toc-sec-roadmap" class="nav-link" data-scroll-target="#sec-roadmap">1.6 Thesis Structure and Roadmap</a></li>
  </ul></li>
  <li><a href="#sec-context" id="toc-sec-context" class="nav-link" data-scroll-target="#sec-context">2. Context and Theoretical Foundations</a>
  <ul>
  <li><a href="#sec-carlsmith-model" id="toc-sec-carlsmith-model" class="nav-link" data-scroll-target="#sec-carlsmith-model">2.1 AI Existential Risk: The Carlsmith Model</a>
  <ul>
  <li><a href="#sec-six-premise" id="toc-sec-six-premise" class="nav-link" data-scroll-target="#sec-six-premise">2.1.1 Six-Premise Decomposition</a></li>
  <li><a href="#sec-carlsmith-formalizable" id="toc-sec-carlsmith-formalizable" class="nav-link" data-scroll-target="#sec-carlsmith-formalizable">2.1.2 Why Carlsmith Exemplifies Formalizable Arguments</a></li>
  </ul></li>
  <li><a href="#sec-epistemic-challenge" id="toc-sec-epistemic-challenge" class="nav-link" data-scroll-target="#sec-epistemic-challenge">2.2 The Epistemic Challenge of Policy Evaluation</a>
  <ul>
  <li><a href="#sec-ai-governance-unique" id="toc-sec-ai-governance-unique" class="nav-link" data-scroll-target="#sec-ai-governance-unique">2.2.1 Unique Characteristics of AI Governance</a></li>
  <li><a href="#sec-traditional-limitations" id="toc-sec-traditional-limitations" class="nav-link" data-scroll-target="#sec-traditional-limitations">2.2.2 Limitations of Traditional Approaches</a></li>
  <li><a href="#sec-new-epistemic-tools" id="toc-sec-new-epistemic-tools" class="nav-link" data-scroll-target="#sec-new-epistemic-tools">2.2.3 Toward New Epistemic Tools</a></li>
  </ul></li>
  <li><a href="#sec-bayesian-networks" id="toc-sec-bayesian-networks" class="nav-link" data-scroll-target="#sec-bayesian-networks">2.3 Bayesian Networks as Knowledge Representation</a>
  <ul>
  <li><a href="#sec-mathematical-foundations" id="toc-sec-mathematical-foundations" class="nav-link" data-scroll-target="#sec-mathematical-foundations">2.3.1 Mathematical Foundations</a></li>
  <li><a href="#sec-rain-sprinkler-example" id="toc-sec-rain-sprinkler-example" class="nav-link" data-scroll-target="#sec-rain-sprinkler-example">2.3.2 The Rain-Sprinkler-Grass Example</a></li>
  <li><a href="#sec-modeling-advantages" id="toc-sec-modeling-advantages" class="nav-link" data-scroll-target="#sec-modeling-advantages">2.3.3 Advantages for AI Risk Modeling</a></li>
  </ul></li>
  <li><a href="#sec-argument-mapping" id="toc-sec-argument-mapping" class="nav-link" data-scroll-target="#sec-argument-mapping">2.4 Argument Mapping and Formal Representations</a>
  <ul>
  <li><a href="#sec-natural-to-structure" id="toc-sec-natural-to-structure" class="nav-link" data-scroll-target="#sec-natural-to-structure">2.4.1 From Natural Language to Structure</a></li>
  <li><a href="#sec-argdown-notation" id="toc-sec-argdown-notation" class="nav-link" data-scroll-target="#sec-argdown-notation">2.4.2 ArgDown: Structured Argument Notation</a></li>
  <li><a href="#sec-bayesdown" id="toc-sec-bayesdown" class="nav-link" data-scroll-target="#sec-bayesdown">2.4.3 BayesDown: The Bridge to Bayesian Networks</a></li>
  </ul></li>
  <li><a href="#sec-mtair-framework" id="toc-sec-mtair-framework" class="nav-link" data-scroll-target="#sec-mtair-framework">2.5 The MTAIR Framework: Achievements and Limitations</a>
  <ul>
  <li><a href="#sec-mtair-approach" id="toc-sec-mtair-approach" class="nav-link" data-scroll-target="#sec-mtair-approach">2.5.1 MTAIR’s Approach</a></li>
  <li><a href="#sec-mtair-achievements" id="toc-sec-mtair-achievements" class="nav-link" data-scroll-target="#sec-mtair-achievements">2.5.2 Key Achievements</a></li>
  <li><a href="#sec-mtair-limitations" id="toc-sec-mtair-limitations" class="nav-link" data-scroll-target="#sec-mtair-limitations">2.5.3 Fundamental Limitations</a></li>
  <li><a href="#sec-automation-opportunity" id="toc-sec-automation-opportunity" class="nav-link" data-scroll-target="#sec-automation-opportunity">2.5.4 The Automation Opportunity</a></li>
  </ul></li>
  <li><a href="#sec-literature-review" id="toc-sec-literature-review" class="nav-link" data-scroll-target="#sec-literature-review">2.6 Literature Review: Content and Technical Levels</a>
  <ul>
  <li><a href="#sec-risk-models-evolution" id="toc-sec-risk-models-evolution" class="nav-link" data-scroll-target="#sec-risk-models-evolution">2.6.1 AI Risk Models Evolution</a></li>
  <li><a href="#sec-governance-taxonomy" id="toc-sec-governance-taxonomy" class="nav-link" data-scroll-target="#sec-governance-taxonomy">2.6.2 Governance Proposals Taxonomy</a></li>
  <li><a href="#sec-bn-theory" id="toc-sec-bn-theory" class="nav-link" data-scroll-target="#sec-bn-theory">2.6.3 Bayesian Network Theory and Applications</a></li>
  <li><a href="#sec-software-tools" id="toc-sec-software-tools" class="nav-link" data-scroll-target="#sec-software-tools">2.6.4 Software Tools Landscape</a></li>
  <li><a href="#sec-formalization" id="toc-sec-formalization" class="nav-link" data-scroll-target="#sec-formalization">2.6.5 Formalization Approaches</a></li>
  <li><a href="#sec-correlation-methods" id="toc-sec-correlation-methods" class="nav-link" data-scroll-target="#sec-correlation-methods">2.6.6 Correlation Accounting Methods</a></li>
  </ul></li>
  <li><a href="#sec-methodology" id="toc-sec-methodology" class="nav-link" data-scroll-target="#sec-methodology">2.7 Methodology</a>
  <ul>
  <li><a href="#sec-research-design" id="toc-sec-research-design" class="nav-link" data-scroll-target="#sec-research-design">2.7.1 Research Design Overview</a></li>
  <li><a href="#sec-formalizing-world-models" id="toc-sec-formalizing-world-models" class="nav-link" data-scroll-target="#sec-formalizing-world-models">2.7.2 Formalizing World Models from AI Safety Literature</a></li>
  <li><a href="#sec-natural-to-computational" id="toc-sec-natural-to-computational" class="nav-link" data-scroll-target="#sec-natural-to-computational">2.7.3 From Natural Language to Computational Models</a></li>
  <li><a href="#sec-dag-structure" id="toc-sec-dag-structure" class="nav-link" data-scroll-target="#sec-dag-structure">2.7.4 Directed Acyclic Graphs: Structure and Semantics</a></li>
  <li><a href="#sec-quantification" id="toc-sec-quantification" class="nav-link" data-scroll-target="#sec-quantification">2.7.5 Quantification of Probabilistic Judgments</a></li>
  <li><a href="#sec-inference-techniques" id="toc-sec-inference-techniques" class="nav-link" data-scroll-target="#sec-inference-techniques">2.7.6 Inference Techniques for Complex Networks</a></li>
  <li><a href="#sec-prediction-markets" id="toc-sec-prediction-markets" class="nav-link" data-scroll-target="#sec-prediction-markets">2.7.7 Integration with Prediction Markets and Forecasting Platforms</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-amtair" id="toc-sec-amtair" class="nav-link" data-scroll-target="#sec-amtair">3. AMTAIR: Design and Implementation</a>
  <ul>
  <li><a href="#sec-system-architecture" id="toc-sec-system-architecture" class="nav-link" data-scroll-target="#sec-system-architecture">3.1 System Architecture Overview</a>
  <ul>
  <li><a href="#sec-five-stage-pipeline" id="toc-sec-five-stage-pipeline" class="nav-link" data-scroll-target="#sec-five-stage-pipeline">3.1.1 Five-Stage Pipeline Architecture</a></li>
  <li><a href="#sec-design-principles" id="toc-sec-design-principles" class="nav-link" data-scroll-target="#sec-design-principles">3.1.2 Design Principles</a></li>
  </ul></li>
  <li><a href="#sec-two-stage-extraction" id="toc-sec-two-stage-extraction" class="nav-link" data-scroll-target="#sec-two-stage-extraction">3.2 The Two-Stage Extraction Process</a>
  <ul>
  <li><a href="#sec-stage1-argdown" id="toc-sec-stage1-argdown" class="nav-link" data-scroll-target="#sec-stage1-argdown">3.2.1 Stage 1: Structural Extraction (ArgDown)</a></li>
  <li><a href="#sec-stage2-bayesdown" id="toc-sec-stage2-bayesdown" class="nav-link" data-scroll-target="#sec-stage2-bayesdown">3.2.2 Stage 2: Probability Integration (BayesDown)</a></li>
  <li><a href="#sec-why-two-stages" id="toc-sec-why-two-stages" class="nav-link" data-scroll-target="#sec-why-two-stages">3.2.3 Why Two Stages?</a></li>
  </ul></li>
  <li><a href="#sec-implementation-tech" id="toc-sec-implementation-tech" class="nav-link" data-scroll-target="#sec-implementation-tech">3.3 Implementation Technologies</a>
  <ul>
  <li><a href="#sec-tech-stack" id="toc-sec-tech-stack" class="nav-link" data-scroll-target="#sec-tech-stack">3.3.1 Technology Stack</a></li>
  <li><a href="#sec-key-algorithms" id="toc-sec-key-algorithms" class="nav-link" data-scroll-target="#sec-key-algorithms">3.3.2 Key Algorithms</a></li>
  <li><a href="#sec-performance" id="toc-sec-performance" class="nav-link" data-scroll-target="#sec-performance">3.3.3 (Expected) Performance Characteristics</a></li>
  </ul></li>
  <li><a href="#sec-case-rain-sprinkler" id="toc-sec-case-rain-sprinkler" class="nav-link" data-scroll-target="#sec-case-rain-sprinkler">3.4 Case Study: Rain-Sprinkler-Grass</a>
  <ul>
  <li><a href="#sec-rsg-input" id="toc-sec-rsg-input" class="nav-link" data-scroll-target="#sec-rsg-input">3.4.1 Input Representation</a></li>
  <li><a href="#sec-rsg-processing" id="toc-sec-rsg-processing" class="nav-link" data-scroll-target="#sec-rsg-processing">3.4.2 Processing Steps</a></li>
  <li><a href="#sec-rsg-results" id="toc-sec-rsg-results" class="nav-link" data-scroll-target="#sec-rsg-results">3.4.3 Results</a></li>
  </ul></li>
  <li><a href="#sec-case-carlsmith" id="toc-sec-case-carlsmith" class="nav-link" data-scroll-target="#sec-case-carlsmith">3.5 Case Study: Carlsmith’s Power-Seeking AI Model</a>
  <ul>
  <li><a href="#sec-carlsmith-complexity" id="toc-sec-carlsmith-complexity" class="nav-link" data-scroll-target="#sec-carlsmith-complexity">3.5.1 Model Complexity</a></li>
  <li><a href="#sec-carlsmith-extraction" id="toc-sec-carlsmith-extraction" class="nav-link" data-scroll-target="#sec-carlsmith-extraction">3.5.2 Extraction Results</a></li>
  <li><a href="#sec-carlsmith-validation" id="toc-sec-carlsmith-validation" class="nav-link" data-scroll-target="#sec-carlsmith-validation">3.5.3 Validation Against Original</a></li>
  <li><a href="#sec-carlsmith-insights" id="toc-sec-carlsmith-insights" class="nav-link" data-scroll-target="#sec-carlsmith-insights">3.5.4 Insights from Formalization</a></li>
  </ul></li>
  <li><a href="#sec-validation-methodology" id="toc-sec-validation-methodology" class="nav-link" data-scroll-target="#sec-validation-methodology">3.6 Validation Methodology</a>
  <ul>
  <li><a href="#sec-ground-truth" id="toc-sec-ground-truth" class="nav-link" data-scroll-target="#sec-ground-truth">3.6.1 Ground Truth Construction</a></li>
  <li><a href="#sec-evaluation-metrics" id="toc-sec-evaluation-metrics" class="nav-link" data-scroll-target="#sec-evaluation-metrics">3.6.2 Evaluation Metrics</a></li>
  </ul></li>
  <li><a href="#sec-policy-evaluation" id="toc-sec-policy-evaluation" class="nav-link" data-scroll-target="#sec-policy-evaluation">3.7 Policy Evaluation Capabilities</a>
  <ul>
  <li><a href="#sec-intervention-representation" id="toc-sec-intervention-representation" class="nav-link" data-scroll-target="#sec-intervention-representation">3.7.1 Intervention Representation</a></li>
  <li><a href="#sec-deployment-example" id="toc-sec-deployment-example" class="nav-link" data-scroll-target="#sec-deployment-example">3.7.2 Example: Deployment Governance</a></li>
  <li><a href="#sec-robustness" id="toc-sec-robustness" class="nav-link" data-scroll-target="#sec-robustness">3.7.3 Robustness Analysis</a></li>
  </ul></li>
  <li><a href="#sec-visualization-design" id="toc-sec-visualization-design" class="nav-link" data-scroll-target="#sec-visualization-design">3.8 Interactive Visualization Design</a>
  <ul>
  <li><a href="#sec-visual-encoding" id="toc-sec-visual-encoding" class="nav-link" data-scroll-target="#sec-visual-encoding">3.8.1 Visual Encoding Strategy</a></li>
  <li><a href="#sec-progressive-disclosure" id="toc-sec-progressive-disclosure" class="nav-link" data-scroll-target="#sec-progressive-disclosure">3.8.2 Progressive Disclosure</a></li>
  <li><a href="#sec-ui-elements" id="toc-sec-ui-elements" class="nav-link" data-scroll-target="#sec-ui-elements">3.8.3 User Interface Elements</a></li>
  </ul></li>
  <li><a href="#sec-market-integration" id="toc-sec-market-integration" class="nav-link" data-scroll-target="#sec-market-integration">3.9 Integration with Prediction Markets</a>
  <ul>
  <li><a href="#sec-integration-design" id="toc-sec-integration-design" class="nav-link" data-scroll-target="#sec-integration-design">3.9.1 Design for Integration</a></li>
  <li><a href="#sec-market-challenges" id="toc-sec-market-challenges" class="nav-link" data-scroll-target="#sec-market-challenges">3.9.2 Challenges and Opportunities</a></li>
  </ul></li>
  <li><a href="#sec-computational-performance" id="toc-sec-computational-performance" class="nav-link" data-scroll-target="#sec-computational-performance">3.10 Computational Performance Analysis</a>
  <ul>
  <li><a href="#sec-exact-approximate" id="toc-sec-exact-approximate" class="nav-link" data-scroll-target="#sec-exact-approximate">3.10.1 Exact vs.&nbsp;Approximate Inference</a></li>
  <li><a href="#sec-scaling-strategies" id="toc-sec-scaling-strategies" class="nav-link" data-scroll-target="#sec-scaling-strategies">3.10.2 Scaling Strategies</a></li>
  </ul></li>
  <li><a href="#sec-results-achievements" id="toc-sec-results-achievements" class="nav-link" data-scroll-target="#sec-results-achievements">3.11 Results and Achievements</a>
  <ul>
  <li><a href="#sec-extraction-quality" id="toc-sec-extraction-quality" class="nav-link" data-scroll-target="#sec-extraction-quality">3.11.1 Extraction Quality Assessment</a></li>
  <li><a href="#sec-computational-performance" id="toc-sec-computational-performance" class="nav-link" data-scroll-target="#sec-computational-performance">3.11.2 Computational Performance</a></li>
  <li><a href="#sec-policy-impact" id="toc-sec-policy-impact" class="nav-link" data-scroll-target="#sec-policy-impact">3.11.3 Policy Impact Evaluation</a></li>
  </ul></li>
  <li><a href="#sec-technical-summary" id="toc-sec-technical-summary" class="nav-link" data-scroll-target="#sec-technical-summary">3.12 Summary of Technical Contributions</a></li>
  </ul></li>
  <li><a href="#sec-discussion" id="toc-sec-discussion" class="nav-link" data-scroll-target="#sec-discussion">4. Discussion: Implications and Limitations</a>
  <ul>
  <li><a href="#sec-technical-limitations" id="toc-sec-technical-limitations" class="nav-link" data-scroll-target="#sec-technical-limitations">4.1 Technical Limitations and Responses</a>
  <ul>
  <li><a href="#sec-extraction-boundaries" id="toc-sec-extraction-boundaries" class="nav-link" data-scroll-target="#sec-extraction-boundaries">4.1.1 Objection 1: Extraction Quality Boundaries</a></li>
  <li><a href="#sec-false-precision" id="toc-sec-false-precision" class="nav-link" data-scroll-target="#sec-false-precision">4.1.2 Objection 2: False Precision in Uncertainty</a></li>
  <li><a href="#sec-correlation-complexity" id="toc-sec-correlation-complexity" class="nav-link" data-scroll-target="#sec-correlation-complexity">4.1.3 Objection 3: Correlation Complexity</a></li>
  </ul></li>
  <li><a href="#sec-conceptual-concerns" id="toc-sec-conceptual-concerns" class="nav-link" data-scroll-target="#sec-conceptual-concerns">4.2 Conceptual and Methodological Concerns</a>
  <ul>
  <li><a href="#sec-democratic-exclusion" id="toc-sec-democratic-exclusion" class="nav-link" data-scroll-target="#sec-democratic-exclusion">4.2.1 Objection 4: Democratic Exclusion</a></li>
  <li><a href="#sec-oversimplification" id="toc-sec-oversimplification" class="nav-link" data-scroll-target="#sec-oversimplification">4.2.2 Objection 5: Oversimplification of Complex Systems</a></li>
  </ul></li>
  <li><a href="#sec-red-teaming" id="toc-sec-red-teaming" class="nav-link" data-scroll-target="#sec-red-teaming">4.3 Red-Teaming Results</a>
  <ul>
  <li><a href="#sec-adversarial-extraction" id="toc-sec-adversarial-extraction" class="nav-link" data-scroll-target="#sec-adversarial-extraction">4.3.1 Adversarial Extraction Attempts</a></li>
  <li><a href="#sec-robustness-findings" id="toc-sec-robustness-findings" class="nav-link" data-scroll-target="#sec-robustness-findings">4.3.2 Robustness Findings</a></li>
  <li><a href="#sec-deployment-implications" id="toc-sec-deployment-implications" class="nav-link" data-scroll-target="#sec-deployment-implications">4.3.3 Implications for Deployment</a></li>
  </ul></li>
  <li><a href="#sec-epistemic-security" id="toc-sec-epistemic-security" class="nav-link" data-scroll-target="#sec-epistemic-security">4.4 Enhancing Epistemic Security</a>
  <ul>
  <li><a href="#sec-inspectable-models" id="toc-sec-inspectable-models" class="nav-link" data-scroll-target="#sec-inspectable-models">4.4.1 Making Models Inspectable</a></li>
  <li><a href="#sec-convergence-divergence" id="toc-sec-convergence-divergence" class="nav-link" data-scroll-target="#sec-convergence-divergence">4.4.2 Revealing Convergence and Divergence</a></li>
  <li><a href="#sec-collective-reasoning" id="toc-sec-collective-reasoning" class="nav-link" data-scroll-target="#sec-collective-reasoning">4.4.3 Improving Collective Reasoning</a></li>
  </ul></li>
  <li><a href="#sec-scaling" id="toc-sec-scaling" class="nav-link" data-scroll-target="#sec-scaling">4.5 Scaling Challenges and Opportunities</a>
  <ul>
  <li><a href="#sec-technical-scaling" id="toc-sec-technical-scaling" class="nav-link" data-scroll-target="#sec-technical-scaling">4.5.1 Technical Scaling</a></li>
  <li><a href="#sec-social-scaling" id="toc-sec-social-scaling" class="nav-link" data-scroll-target="#sec-social-scaling">4.5.2 Social and Institutional Scaling</a></li>
  <li><a href="#sec-impact-opportunities" id="toc-sec-impact-opportunities" class="nav-link" data-scroll-target="#sec-impact-opportunities">4.5.3 Opportunities for Impact</a></li>
  </ul></li>
  <li><a href="#sec-governance-integration" id="toc-sec-governance-integration" class="nav-link" data-scroll-target="#sec-governance-integration">4.6 Integration with Governance Frameworks</a>
  <ul>
  <li><a href="#sec-standards-integration" id="toc-sec-standards-integration" class="nav-link" data-scroll-target="#sec-standards-integration">4.6.1 Standards Development</a></li>
  <li><a href="#sec-regulatory-integration" id="toc-sec-regulatory-integration" class="nav-link" data-scroll-target="#sec-regulatory-integration">4.6.2 Regulatory Design</a></li>
  <li><a href="#sec-international-integration" id="toc-sec-international-integration" class="nav-link" data-scroll-target="#sec-international-integration">4.6.3 International Coordination</a></li>
  <li><a href="#sec-organizational-integration" id="toc-sec-organizational-integration" class="nav-link" data-scroll-target="#sec-organizational-integration">4.6.4 Organizational Decision-Making</a></li>
  </ul></li>
  <li><a href="#sec-future-research" id="toc-sec-future-research" class="nav-link" data-scroll-target="#sec-future-research">4.7 Future Research Directions</a>
  <ul>
  <li><a href="#sec-technical-future" id="toc-sec-technical-future" class="nav-link" data-scroll-target="#sec-technical-future">4.7.1 Technical Enhancements</a></li>
  <li><a href="#sec-methodological-future" id="toc-sec-methodological-future" class="nav-link" data-scroll-target="#sec-methodological-future">4.7.2 Methodological Extensions</a></li>
  <li><a href="#sec-application-future" id="toc-sec-application-future" class="nav-link" data-scroll-target="#sec-application-future">4.7.3 Application Domains</a></li>
  <li><a href="#sec-ecosystem-future" id="toc-sec-ecosystem-future" class="nav-link" data-scroll-target="#sec-ecosystem-future">4.7.4 Ecosystem Development</a></li>
  </ul></li>
  <li><a href="#sec-deep-uncertainties" id="toc-sec-deep-uncertainties" class="nav-link" data-scroll-target="#sec-deep-uncertainties">4.8 Known Unknowns and Deep Uncertainties</a>
  <ul>
  <li><a href="#sec-uncertainty-categories" id="toc-sec-uncertainty-categories" class="nav-link" data-scroll-target="#sec-uncertainty-categories">4.8.1 Categories of Deep Uncertainty</a></li>
  <li><a href="#sec-adaptation-strategies" id="toc-sec-adaptation-strategies" class="nav-link" data-scroll-target="#sec-adaptation-strategies">4.8.2 Adaptation Strategies for Deep Uncertainty</a></li>
  <li><a href="#sec-robust-principles" id="toc-sec-robust-principles" class="nav-link" data-scroll-target="#sec-robust-principles">4.8.3 Robust Decision-Making Principles</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-conclusion" id="toc-sec-conclusion" class="nav-link" data-scroll-target="#sec-conclusion">5. Conclusion: Toward Coordinated AI Governance</a>
  <ul>
  <li><a href="#sec-key-contributions" id="toc-sec-key-contributions" class="nav-link" data-scroll-target="#sec-key-contributions">5.1 Summary of Key Contributions</a>
  <ul>
  <li><a href="#sec-theoretical-contributions" id="toc-sec-theoretical-contributions" class="nav-link" data-scroll-target="#sec-theoretical-contributions">5.1.1 Theoretical Contributions</a></li>
  <li><a href="#sec-methodological-innovations" id="toc-sec-methodological-innovations" class="nav-link" data-scroll-target="#sec-methodological-innovations">5.1.2 Methodological Innovations</a></li>
  <li><a href="#sec-technical-achievements" id="toc-sec-technical-achievements" class="nav-link" data-scroll-target="#sec-technical-achievements">5.1.3 Technical Achievements</a></li>
  <li><a href="#sec-empirical-findings" id="toc-sec-empirical-findings" class="nav-link" data-scroll-target="#sec-empirical-findings">5.1.4 Empirical Findings</a></li>
  </ul></li>
  <li><a href="#sec-limitations-assessment" id="toc-sec-limitations-assessment" class="nav-link" data-scroll-target="#sec-limitations-assessment">5.2 Limitations and Honest Assessment</a>
  <ul>
  <li><a href="#sec-technical-constraints" id="toc-sec-technical-constraints" class="nav-link" data-scroll-target="#sec-technical-constraints">5.2.1 Technical Constraints</a></li>
  <li><a href="#sec-conceptual-limitations" id="toc-sec-conceptual-limitations" class="nav-link" data-scroll-target="#sec-conceptual-limitations">5.2.2 Conceptual Limitations</a></li>
  <li><a href="#sec-practical-constraints" id="toc-sec-practical-constraints" class="nav-link" data-scroll-target="#sec-practical-constraints">5.2.3 Practical Constraints</a></li>
  </ul></li>
  <li><a href="#sec-governance-implications" id="toc-sec-governance-implications" class="nav-link" data-scroll-target="#sec-governance-implications">5.3 Implications for AI Governance</a>
  <ul>
  <li><a href="#sec-near-term-applications" id="toc-sec-near-term-applications" class="nav-link" data-scroll-target="#sec-near-term-applications">5.3.1 Near-Term Applications</a></li>
  <li><a href="#sec-medium-term" id="toc-sec-medium-term" class="nav-link" data-scroll-target="#sec-medium-term">5.3.2 Medium-Term Transformation</a></li>
  <li><a href="#sec-long-term-vision" id="toc-sec-long-term-vision" class="nav-link" data-scroll-target="#sec-long-term-vision">5.3.3 Long-Term Vision</a></li>
  </ul></li>
  <li><a href="#sec-recommendations" id="toc-sec-recommendations" class="nav-link" data-scroll-target="#sec-recommendations">5.4 Recommendations for Stakeholders</a>
  <ul>
  <li><a href="#sec-researcher-recommendations" id="toc-sec-researcher-recommendations" class="nav-link" data-scroll-target="#sec-researcher-recommendations">5.4.1 For Researchers</a></li>
  <li><a href="#sec-policymaker-recommendations" id="toc-sec-policymaker-recommendations" class="nav-link" data-scroll-target="#sec-policymaker-recommendations">5.4.2 For Policymakers</a></li>
  <li><a href="#sec-technologist-recommendations" id="toc-sec-technologist-recommendations" class="nav-link" data-scroll-target="#sec-technologist-recommendations">5.4.3 For Technologists</a></li>
  <li><a href="#sec-funder-recommendations" id="toc-sec-funder-recommendations" class="nav-link" data-scroll-target="#sec-funder-recommendations">5.4.4 For Funders</a></li>
  </ul></li>
  <li><a href="#sec-future-research-agenda" id="toc-sec-future-research-agenda" class="nav-link" data-scroll-target="#sec-future-research-agenda">5.5 Future Research Agenda</a>
  <ul>
  <li><a href="#sec-technical-priorities" id="toc-sec-technical-priorities" class="nav-link" data-scroll-target="#sec-technical-priorities">5.5.1 Technical Priorities</a></li>
  <li><a href="#sec-methodological-development" id="toc-sec-methodological-development" class="nav-link" data-scroll-target="#sec-methodological-development">5.5.2 Methodological Development</a></li>
  <li><a href="#sec-application-expansion" id="toc-sec-application-expansion" class="nav-link" data-scroll-target="#sec-application-expansion">5.5.3 Application Expansion</a></li>
  </ul></li>
  <li><a href="#sec-closing-reflections" id="toc-sec-closing-reflections" class="nav-link" data-scroll-target="#sec-closing-reflections">5.6 Closing Reflections</a></li>
  </ul></li>
  <li><a href="#sec-references" id="toc-sec-references" class="nav-link" data-scroll-target="#sec-references">References</a></li>
  <li><a href="#sec-appendices" id="toc-sec-appendices" class="nav-link" data-scroll-target="#sec-appendices">Appendices</a>
  <ul>
  <li><a href="#sec-appendix-technical" id="toc-sec-appendix-technical" class="nav-link" data-scroll-target="#sec-appendix-technical">Appendix A: Technical Implementation Details</a>
  <ul>
  <li><a href="#sec-data-structures" id="toc-sec-data-structures" class="nav-link" data-scroll-target="#sec-data-structures">A.1 Core Data Structures</a></li>
  <li><a href="#sec-extraction-details" id="toc-sec-extraction-details" class="nav-link" data-scroll-target="#sec-extraction-details">A.2 Extraction Algorithm Details</a></li>
  <li><a href="#sec-api-specs" id="toc-sec-api-specs" class="nav-link" data-scroll-target="#sec-api-specs">A.3 API Specifications</a></li>
  </ul></li>
  <li><a href="#sec-appendix-validation" id="toc-sec-appendix-validation" class="nav-link" data-scroll-target="#sec-appendix-validation">Appendix B: Validation Datasets and Procedures</a>
  <ul>
  <li><a href="#sec-annotation-protocol" id="toc-sec-annotation-protocol" class="nav-link" data-scroll-target="#sec-annotation-protocol">B.1 Expert Annotation Protocol</a></li>
  <li><a href="#sec-benchmark-construction" id="toc-sec-benchmark-construction" class="nav-link" data-scroll-target="#sec-benchmark-construction">B.2 Benchmark Dataset Construction</a></li>
  <li><a href="#sec-validation-results" id="toc-sec-validation-results" class="nav-link" data-scroll-target="#sec-validation-results">B.3 Validation Results</a></li>
  </ul></li>
  <li><a href="#sec-appendix-cases" id="toc-sec-appendix-cases" class="nav-link" data-scroll-target="#sec-appendix-cases">Appendix C: Extended Case Studies</a>
  <ul>
  <li><a href="#sec-christiano-extraction" id="toc-sec-christiano-extraction" class="nav-link" data-scroll-target="#sec-christiano-extraction">C.1 Christiano’s “What Failure Looks Like” Extraction</a></li>
  <li><a href="#sec-critch-extraction" id="toc-sec-critch-extraction" class="nav-link" data-scroll-target="#sec-critch-extraction">C.2 Critch’s ARCHES Model</a></li>
  <li><a href="#sec-narrow-path-evaluation" id="toc-sec-narrow-path-evaluation" class="nav-link" data-scroll-target="#sec-narrow-path-evaluation">C.3 Policy Evaluation: A Narrow Path</a></li>
  </ul></li>
  <li><a href="#sec-appendix-bayesdown" id="toc-sec-appendix-bayesdown" class="nav-link" data-scroll-target="#sec-appendix-bayesdown">Appendix D: BayesDown Syntax Specification</a></li>
  <li><a href="#sec-appendix-prompts" id="toc-sec-appendix-prompts" class="nav-link" data-scroll-target="#sec-appendix-prompts">Appendix E: Prompt Engineering Details</a></li>
  <li><a href="#sec-appendix-userguide" id="toc-sec-appendix-userguide" class="nav-link" data-scroll-target="#sec-appendix-userguide">Appendix F: User Guide</a></li>
  <li><a href="#sec-appendix-notebook" id="toc-sec-appendix-notebook" class="nav-link" data-scroll-target="#sec-appendix-notebook">Appendix G: Jupyter Notebook Implementation</a></li>
  <li><a href="#sec-appendix-ethical" id="toc-sec-appendix-ethical" class="nav-link" data-scroll-target="#sec-appendix-ethical">Appendix H: Ethical Considerations and Governance</a>
  <ul>
  <li><a href="#sec-misuse-scenarios" id="toc-sec-misuse-scenarios" class="nav-link" data-scroll-target="#sec-misuse-scenarios">H.1 Potential Misuse Scenarios</a></li>
  <li><a href="#sec-democratic-frameworks" id="toc-sec-democratic-frameworks" class="nav-link" data-scroll-target="#sec-democratic-frameworks">H.2 Democratic Participation Frameworks</a></li>
  <li><a href="#sec-responsibility" id="toc-sec-responsibility" class="nav-link" data-scroll-target="#sec-responsibility">H.3 Responsibility Assignment</a></li>
  </ul></li>
  <li><a href="#sec-appendix-examples" id="toc-sec-appendix-examples" class="nav-link" data-scroll-target="#sec-appendix-examples">Appendix I: Full Extraction Examples</a></li>
  <li><a href="#sec-appendix-software" id="toc-sec-appendix-software" class="nav-link" data-scroll-target="#sec-appendix-software">Appendix J: Software Installation and Usage Guide</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/VJMeyer/submission/edit/main/chapters/Outlines/Outline_13.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div><div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="https://colab.research.google.com/github/VJMeyer/submission/blob/main/AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb"><i class="bi bi-file-code"></i>Colab Notebook (Manual Link in .yml)</a></li><li><a href="https://github.com/VJMeyer/submission"><i class="bi bi-github"></i>GitHub Repository</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Preface</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="sourceCode" id="cb1"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Automating the Modeling of Transformative Artificial Intelligence Risks" subtitle: "An Epistemic Framework for Leveraging Frontier AI Systems to Upscale Conditional Policy Assessments in Bayesian Networks on a Narrow Path toward Existential Safety" author:</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>name: Valentin Jakob Meyer orcid: 0009-0006-0889-5269 corresponding: true email: <span class="co">[</span><span class="ot">Valentin.Meyer@uni-bayreuth.de</span><span class="co">](mailto:Valentin.Meyer@uni-bayreuth.de)</span> roles:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Graduate Author affiliations:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>University of Bayreuth</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>MCMP — LMU Munich</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>name: Dr. Timo Speith orcid: 0000-0002-6675-154X corresponding: false roles:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Supervisor affiliations:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>University of Bayreuth keywords:</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>AMTAIR</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>AI Governance</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bayesian Networks</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transformative AI</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Risk Assessment</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Argument Extraction</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Existential Risk</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Coordination Crisis</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Epistemic Security</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Policy Evaluation abstract: | This thesis addresses coordination failures in AI safety by creating computational tools that automatically extract and formalize probabilistic world models from AI safety literature using frontier language models. The AMTAIR (Automating Transformative AI Risk Modeling) system implements an end-to-end pipeline transforming unstructured arguments into interactive Bayesian networks through a novel two-stage extraction process: first capturing argument structure in ArgDown format, then enhancing it with probability information in BayesDown.</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>Applied to canonical examples and real AI safety arguments, the system demonstrates extraction accuracy exceeding 85% for structural relationships and 73% for probability capture. By making implicit models explicit, enabling cross-worldview comparison, and supporting rigorous policy evaluation, AMTAIR bridges communication gaps between technical researchers, policy specialists, and other stakeholders working to address existential risks from advanced AI.</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>The thesis contributes both theoretical foundations and practical implementation, validated through expert comparison and real-world case studies including Carlsmith's power-seeking AI model. While current limitations include correlation handling and extraction ambiguities, the approach provides essential epistemic infrastructure for coordinated AI governance. plain-language-summary: | This thesis develops software tools that automatically extract and visualize the hidden assumptions and probability estimates in AI safety arguments. By transforming complex written arguments into interactive diagrams showing relationships and probabilities, AMTAIR helps different groups working on AI safety—researchers, policymakers, and others—understand each other better and coordinate their efforts to address risks from advanced AI systems. key-points:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A novel two-stage extraction pipeline transforms argument structures into Bayesian networks through ArgDown and BayesDown intermediate representations</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interactive visualizations make complex probabilistic relationships accessible to diverse stakeholders</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Formal representation enables systematic comparison across different worldviews and assumptions</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Validated extraction achieves &gt;85% accuracy for structure and &gt;73% for probabilities</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The approach addresses coordination failures by creating a common language for AI risk assessment date: "2025-05-26" bibliography: ref/MAref.bib citation: container-title: University of Bayreuth Master's Thesis number-sections: true toc: true toc-depth: 4 lof: true lot: true</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<!-- [ ] TODO: Expand preface to include personal journey and acknowledgments -->
<p>This thesis represents the culmination of interdisciplinary research at the intersection of AI safety, formal epistemology, and computational social science. The work emerged from recognizing a fundamental challenge in AI governance: while investment in AI safety research has grown exponentially, coordination between different stakeholder communities remains fragmented, potentially increasing existential risk through misaligned efforts.</p>
<div class="merge-candidate" data-merge-with="Alternative preface from 11.7">
<p>The journey from initial concept to working implementation involved iterative refinement based on feedback from advisors, domain experts, and potential users. What began as a technical exercise in automated extraction evolved into a broader framework for enhancing epistemic security in one of humanity’s most critical coordination challenges.</p>
</div>
<p>I hope this work contributes to building the intellectual and technical infrastructure necessary for humanity to navigate the transition to transformative AI safely. The tools and frameworks presented here are offered in the spirit of collaborative problem-solving, recognizing that the challenges we face require unprecedented cooperation across disciplines, institutions, and worldviews.</p>
<section id="acknowledgments" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="acknowledgments">Acknowledgments</h2>
<!-- [ ] TODO: Add specific names and contributions -->
<!-- [ ] ADD CONDITIONAL ON INCLUSION OF EXTRA EXTRACTION SECTION: Mention/Acknowledgement of Johannes Meyer and Jelena Meyer for their help to verify the automated extraction procedure by manually extracting the argdown and bayesdown data from the carlsmith paper (AND extra, other second paper) -->
<p>I thank my supervisor Dr.&nbsp;Timo Speith for guidance throughout this project, the MTAIR team for pioneering the manual approach that inspired automation, and the AI safety community for creating the rich literature that made this work possible. Special recognition goes to technical advisors who provided invaluable feedback and Coleman Snell for his partnership and research collaboration with the AMTAIR project. Any errors or limitations remain my own responsibility.</p>
</section>
<section id="table-of-contents" class="level1 unnumbered">
<h1 class="unnumbered">Table of Contents</h1>
<!-- Quarto auto-generates TOC -->
</section>
<section id="list-of-figures" class="level1 unnumbered">
<h1 class="unnumbered">List of Figures</h1>
<!-- [ ] TODO: Verify all figures are properly labeled and captioned -->
<!-- Quarto auto-generates LOF -->
</section>
<section id="list-of-tables" class="level1 unnumbered">
<h1 class="unnumbered">List of Tables</h1>
<!-- [ ] TODO: Ensure all tables have proper captions -->
<!-- Quarto auto-generates LOT -->
</section>
<section id="list-of-abbreviations" class="level1 unnumbered">
<h1 class="unnumbered">List of Abbreviations</h1>
<!-- [ ] TODO: Review and expand abbreviation list -->
<p>AI - Artificial Intelligence<br>
AGI - Artificial General Intelligence<br>
AMTAIR - Automating Transformative AI Risk Modeling<br>
API - Application Programming Interface<br>
APS - Advanced, Planning, Strategic (AI systems)<br>
BN - Bayesian Network<br>
CPT - Conditional Probability Table<br>
DAG - Directed Acyclic Graph<br>
LLM - Large Language Model<br>
ML - Machine Learning<br>
MTAIR - Modeling Transformative AI Risks<br>
NLP - Natural Language Processing<br>
P&amp;E - Philosophy &amp; Economics<br>
PDF - Portable Document Format<br>
TAI - Transformative Artificial Intelligence</p>
</section>
<section id="sec-introduction" class="level1">
<h1>1. Introduction: The Coordination Crisis in AI Governance</h1>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Grade Weight</strong>: 10% | <strong>Target Length</strong>: ~14% of text (~4,200 words)<br>
<strong>Requirements</strong>: Introduces and motivates the core question, provides context, states precise thesis, provides roadmap</p>
</div>
</div>
<section id="sec-opening-scenario" class="level2">
<h2 class="anchored" data-anchor-id="sec-opening-scenario">1.1 Opening Scenario: The Policymaker’s Dilemma</h2>
<!-- [ ] TODO: Polish opening scenario for maximum impact -->
<!-- [ ] ADD: @todd2024 add as reference for more resources towards AI safety -->
<p><span class="citation" data-cites="todd2024">Todd (<a href="Outline_13.html#ref-todd2024" role="doc-biblioref">2024</a>)</span></p>
<p>Imagine a senior policy advisor preparing recommendations for AI governance legislation. On her desk lie a dozen reports from leading AI safety researchers, each painting a different picture of the risks ahead. One argues that misaligned AI could pose existential risks within the decade, citing complex technical arguments about instrumental convergence and orthogonality. Another suggests these concerns are overblown, emphasizing uncertainty and the strength of existing institutions. A third proposes specific technical standards but acknowledges deep uncertainty about their effectiveness.</p>
<div class="redundant-content" data-better-version="Outline_12.2#sec-opening">
<p>Each report seems compelling in isolation, written by credentialed experts with sophisticated arguments. Yet they reach dramatically different conclusions about both the magnitude of risk and appropriate interventions. The technical arguments involve unfamiliar concepts—mesa-optimization, corrigibility, capability amplification—expressed through different frameworks and implicit assumptions. Time is limited, stakes are high, and the legislation could shape humanity’s trajectory for decades.</p>
</div>
<p>This scenario plays out daily across government offices, corporate boardrooms, and research institutions worldwide. It exemplifies what I term the “coordination crisis” in AI governance: despite unprecedented attention and resources directed toward AI safety, we lack the epistemic infrastructure to synthesize diverse expert knowledge into actionable governance strategies.</p>
<!-- [ ] CREATE: {#fig-policymaker-dilemma}: "Visual representation of conflicting expert reports on advisor's desk" -->
</section>
<section id="sec-coordination-crisis" class="level2">
<h2 class="anchored" data-anchor-id="sec-coordination-crisis">1.2 The Coordination Crisis in AI Governance</h2>
<!-- [ ] TODO: Frame the problem as coordination failure rather than merely technical challenge -->
<!-- [ ] ADD: @maslej2025  Add citation for page 85 as evidence for accelerating capabilities -->
<p><span class="citation" data-cites="maslej2025">Maslej (<a href="Outline_13.html#ref-maslej2025" role="doc-biblioref">2025</a>)</span></p>
<!-- [ ] ADD: @samborska2025  Add as citation for accelerating capabilities -->
<p><span class="citation" data-cites="samborska2025">Samborska (<a href="Outline_13.html#ref-samborska2025" role="doc-biblioref">2025</a>)</span></p>
<p>As AI capabilities advance at an accelerating pace—demonstrated by the rapid progression from GPT-3 to GPT-4, Claude, and emerging multimodal systems—humanity faces a governance challenge unlike any in history. The task of ensuring increasingly powerful AI systems remain aligned with human values and beneficial to our long-term flourishing grows more urgent with each capability breakthrough. This challenge becomes particularly acute when considering transformative AI systems that could drastically alter civilization’s trajectory, potentially including existential risks from misaligned systems pursuing objectives counter to human welfare.</p>
<blockquote class="blockquote">
<p>Despite unprecedented investment in AI safety research, rapidly growing awareness among key stakeholders, and proliferating frameworks for responsible AI development, we face what I’ll term the “coordination crisis” in AI governance—a systemic failure to align diverse efforts across technical, policy, and strategic domains into a coherent response proportionate to the risks we face.</p>
</blockquote>
<p>The current state of AI governance presents a striking paradox. On one hand, we witness extraordinary mobilization: billions in research funding, proliferating safety initiatives, major tech companies establishing alignment teams, and governments worldwide developing AI strategies. The <a href="https://futureoflife.org/open-letter/ai-principles/">Asilomar AI Principles</a> garnered thousands of signatures, the EU advances <a href="https://artificialintelligenceact.eu/the-act/">comprehensive AI regulation</a> , and technical researchers produce increasingly sophisticated work on alignment, interpretability, and robustness.</p>
<!-- [ ] ADD: @tegmark2024  add citation for the Asilomar Principles Open Letter (as footnote?) -->
<p><span class="citation" data-cites="tegmark2024">Tegmark (<a href="Outline_13.html#ref-tegmark2024" role="doc-biblioref">2024</a>)</span></p>
<!-- [ ] ADD: @european2024    add citation for the EU AI (as footnote) -->
<p><span class="citation" data-cites="european2024">European (<a href="Outline_13.html#ref-european2024" role="doc-biblioref">2024</a>)</span></p>
<div class="duplicate-content" data-source="Outline_11.7#sec-coordination-crisis">
<p>Yet alongside this activity, we observe systematic coordination failures that may prove catastrophic. Technical safety researchers develop sophisticated alignment techniques without clear implementation pathways. Policy specialists craft regulatory frameworks lacking technical grounding to ensure practical efficacy. Ethicists articulate normative principles that lack operational specificity. Strategy researchers identify critical uncertainties but struggle to translate these into actionable guidance. International bodies convene without shared frameworks for assessing interventions.</p>
</div>
<!-- [ ] CREATE: {#fig-coordination-crisis}: "Systems diagram showing fragmentation between AI governance communities" -->
<section id="sec-safety-gaps" class="level3">
<h3 class="anchored" data-anchor-id="sec-safety-gaps">1.2.1 Safety Gaps from Misaligned Efforts</h3>
<!-- [ ] TODO: Document how fragmentation systematically increases risk through specific examples -->
<p>The fragmentation problem manifests in incompatible frameworks between technical researchers, policy specialists, and strategic analysts. Each community develops sophisticated approaches within their domain, yet translation between domains remains primitive. This creates systematic blind spots where risks emerge at the interfaces between technical capabilities, institutional responses, and strategic dynamics.</p>
<p>When different communities operate with incompatible frameworks, critical risks fall through the cracks. Technical researchers may solve alignment problems under assumptions that policymakers’ decisions invalidate. Regulations optimized for current systems may inadvertently incentivize dangerous development patterns. Without shared models of the risk landscape, our collective efforts resemble the parable of blind men describing an elephant—each accurate within their domain but missing the complete picture.</p>
<!-- [ ] FIND: @coordination-failure-examples: "Specific historical examples of coordination failures in technology governance" -->
<!-- [ ] ADD: @paul2023 from Url: https://www.prismaguard.com/the-elephaint-are-we-all-like-the-six-blind-men-when-it-comes-to-ai/ for blind men elephant vs ai risk analogy-->
<p><span class="citation" data-cites="paul2023">Paul (<a href="Outline_13.html#ref-paul2023" role="doc-biblioref">2023</a>)</span></p>
</section>
<section id="sec-resource-misallocation" class="level3">
<h3 class="anchored" data-anchor-id="sec-resource-misallocation">1.2.2 Resource Misallocation</h3>
<!-- [ ] TODO: Provide concrete statistics on research funding, publications, and initiatives -->
<p>The AI safety community duplicates efforts while leaving critical areas underexplored. Multiple teams independently develop similar frameworks without building on each other’s work. Funders struggle to identify high-impact opportunities across technical and governance domains. Talent flows toward well-publicized approaches while neglected strategies remain understaffed. This misallocation becomes more costly as the window for establishing effective governance narrows.</p>
<!-- [ ] CREATE: {#tbl-resource-duplication}: "Examples of duplicated AI safety efforts across organizations" -->
</section>
<section id="sec-negative-sum" class="level3">
<h3 class="anchored" data-anchor-id="sec-negative-sum">1.2.3 Negative-Sum Dynamics</h3>
<!-- [ ] TODO: Address capability-governance gaps widening with accelerating development -->
<p>Perhaps most concerning, uncoordinated interventions can actively increase risk. Safety standards that advantage established players may accelerate risky development elsewhere. Partial transparency requirements might enable capability advances without commensurate safety improvements. International agreements lacking shared technical understanding may lock in dangerous practices. Without coordination, our cure risks becoming worse than the disease.</p>
<div class="merge-candidate" data-merge-with="Outline_11.7#systematic-risk">
<p>Coordination failures systematically amplify existential risk through multiple pathways. Safety gaps emerge when technical solutions lack policy implementation pathways. Resource misallocation occurs when multiple teams unknowingly duplicate efforts while critical areas remain unaddressed. Most perniciously, locally optimized decisions by individual actors can create negative-sum dynamics that increase overall risk—an AI governance tragedy of the commons.</p>
</div>
<!-- [ ] FIND: @negative-sum-policy: "Game-theoretic analysis of how uncoordinated AI policies can backfire" -->
</section>
</section>
<section id="sec-historical-urgency" class="level2">
<h2 class="anchored" data-anchor-id="sec-historical-urgency">1.3 Historical Parallels and Temporal Urgency</h2>
<!-- [ ] TODO: Draw connections to nuclear governance, climate change, and biosecurity -->
<p>History offers instructive parallels. The nuclear age began with scientists racing to understand and control forces that could destroy civilization. Early coordination failures—competing national programs, scientist-military tensions, public-expert divides—nearly led to catastrophe multiple times. Only through developing shared frameworks (deterrence theory), institutions (IAEA), and communication channels (hotlines, treaties) did humanity navigate the nuclear precipice.</p>
<!-- [ ] ADD: @schelling1960: "Schelling, T. (1960). The Strategy of Conflict" -->
<p><span class="citation" data-cites="schelling1960">Schelling (<a href="Outline_13.html#ref-schelling1960" role="doc-biblioref">1960</a>)</span></p>
<!-- [ ] ADD: @rehman2025 from: https://warontherocks.com/2025/01/the-battle-for-brilliant-minds-from-the-nuclear-age-to-ai/ -->
<p><span class="citation" data-cites="rehman2025">Rehman (<a href="Outline_13.html#ref-rehman2025" role="doc-biblioref">2025</a>)</span></p>
<div class="redundant-content" data-better-version="Outline_12.2#temporal-urgency">
<p>Yet AI presents unique coordination challenges that compress our response timeline:</p>
<p><strong>Accelerating Development</strong>: Unlike nuclear weapons requiring massive infrastructure, AI development proceeds in corporate labs and academic departments worldwide. Capability improvements come through algorithmic insights and computational scale, both advancing exponentially.</p>
<p><strong>Dual-Use Ubiquity</strong>: Every AI advance potentially contributes to both beneficial applications and catastrophic risks. The same language model architectures enabling scientific breakthroughs could facilitate dangerous manipulation or deception at scale.</p>
<p><strong>Comprehension Barriers</strong>: Nuclear risks were viscerally understandable—cities vaporized, radiation sickness, nuclear winter. AI risks involve abstract concepts like optimization processes, goal misspecification, and emergent capabilities that resist intuitive understanding.</p>
<p><strong>Governance Lag</strong>: Traditional governance mechanisms—legislation, international treaties, professional standards—operate on timescales of years to decades. AI capabilities advance on timescales of months to years, creating an ever-widening capability-governance gap.</p>
</div>
<!-- [ ] CREATE: {#fig-governance-lag}: "Timeline comparison of AI capability vs governance development" -->
</section>
<section id="sec-research-question" class="level2">
<h2 class="anchored" data-anchor-id="sec-research-question">1.4 Research Question and Scope</h2>
<!-- [ ] TODO: Clearly articulate the primary research question with precision -->
<p>This thesis addresses a specific dimension of the coordination challenge by investigating the question:</p>
<p><strong>Can frontier AI technologies be utilized to automate the modeling of transformative AI risks, enabling robust prediction of policy impacts across diverse worldviews?</strong></p>
<div class="merge-candidate" data-merge-with="Outline_11.7#research-question">
<p>More specifically, I explore whether frontier language models can automate the extraction and formalization of probabilistic world models from AI safety literature, creating a scalable computational framework that enhances coordination in AI governance through systematic policy evaluation under uncertainty.</p>
</div>
<p>To break this down into its components:</p>
<ul>
<li><strong>Frontier AI Technologies</strong>: Today’s most capable language models (GPT-4, Claude-3 level systems)</li>
<li><strong>Automated Modeling</strong>: Using these systems to extract and formalize argument structures from natural language</li>
<li><strong>Transformative AI Risks</strong>: Potentially catastrophic outcomes from advanced AI systems, particularly existential risks</li>
<li><strong>Policy Impact Prediction</strong>: Evaluating how governance interventions might alter probability distributions over outcomes</li>
<li><strong>Diverse Worldviews</strong>: Accounting for fundamental disagreements about AI development trajectories and risk factors</li>
</ul>
<p>The investigation encompasses both theoretical development and practical implementation, focusing specifically on existential risks from misaligned AI systems rather than broader AI ethics concerns. This narrowed scope enables deep technical development while addressing the highest-stakes coordination challenges.</p>
<!-- [ ] TODO: Refine thesis statement based on advisor feedback -->
</section>
<section id="sec-multiplicative-benefits" class="level2">
<h2 class="anchored" data-anchor-id="sec-multiplicative-benefits">1.5 The Multiplicative Benefits Framework</h2>
<!-- [ ] TODO: Establish central thesis about synergistic combination of three elements -->
<p>The central thesis of this work is that combining three elements—automated worldview extraction, prediction market integration, and formal policy evaluation—creates multiplicative rather than merely additive benefits for AI governance. Each component enhances the others, creating a system more valuable than the sum of its parts.</p>
<!-- [ ] CREATE: {#fig-multiplicative-benefits}: "Venn diagram showing synergies between extraction, markets, and evaluation" -->
<section id="sec-automated-extraction" class="level3">
<h3 class="anchored" data-anchor-id="sec-automated-extraction">1.5.1 Automated Worldview Extraction</h3>
<div class="duplicate-content" data-source="Outline_11.7#automated-extraction">
<p><strong>Automated worldview extraction</strong> using frontier language models addresses the scaling bottleneck in current approaches to AI risk modeling. The Modeling Transformative AI Risks (MTAIR) project demonstrated the value of formal representation but required extensive manual effort to translate qualitative arguments into quantitative models. Automation enables processing orders of magnitude more content, incorporating diverse perspectives, and maintaining models in near real-time as new arguments emerge.</p>
</div>
<p>Current approaches to AI risk modeling, exemplified by the Modeling Transformative AI Risks (MTAIR) project, demonstrate the value of formal representation but require extensive manual effort. Creating a single model demands dozens of expert-hours to translate qualitative arguments into quantitative frameworks. This bottleneck severely limits the number of perspectives that can be formalized and the speed of model updates as new arguments emerge.</p>
<!-- [-] VERIFIED FALSE: @mtair-effort: "Bucknall et al. (2022) - specific hour estimates for manual modeling" NO SUCH SOURCE EXIST. REMOVE ANY MENTIONS! -->
<p>Automation using frontier language models addresses this scaling challenge. By developing systematic methods to extract causal structures and probability judgments from natural language, we can:</p>
<ul>
<li>Process orders of magnitude more content</li>
<li>Incorporate diverse perspectives rapidly</li>
<li>Maintain models that evolve with the discourse</li>
<li>Reduce barriers to entry for contributing worldviews</li>
</ul>
</section>
<section id="sec-live-data" class="level3">
<h3 class="anchored" data-anchor-id="sec-live-data">1.5.2 Live Data Integration</h3>
<div class="redundant-content" data-better-version="Outline_12.2#prediction-markets">
<p><strong>Prediction market integration</strong> grounds these models in collective forecasting intelligence. By connecting formal representations to live forecasting platforms, the system can incorporate timely judgments about critical uncertainties from calibrated forecasters. This creates a dynamic feedback loop where models inform forecasters and forecasts update models.</p>
</div>
<p>Static models, however well-constructed, quickly become outdated in fast-moving domains. Prediction markets and forecasting platforms aggregate distributed knowledge about uncertain futures, providing continuously updated probability estimates. By connecting formal models to these live data sources, we create dynamic assessments that incorporate the latest collective intelligence.</p>
<p>This integration serves multiple purposes:</p>
<ul>
<li>Grounding abstract models in empirical forecasts</li>
<li>Identifying which uncertainties most affect outcomes</li>
<li>Revealing when model assumptions diverge from collective expectations</li>
<li>Generating new questions for forecasting communities</li>
</ul>
<!-- [ ] ADD: @tetlock2015 and introduce prediction markets for a general audience, explain how they can be used in forecasting technological developments and impacts -->
<p><span class="citation" data-cites="tetlock2015">Tetlock and Gardner (<a href="Outline_13.html#ref-tetlock2015" role="doc-biblioref">2015</a>)</span></p>
</section>
<section id="sec-policy-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="sec-policy-evaluation">1.5.3 Formal Policy Evaluation</h3>
<p><strong>Formal policy evaluation</strong> transforms static risk assessments into actionable guidance by modeling how specific interventions alter critical parameters. Using causal inference techniques, we can assess not just the probability of adverse outcomes but how those probabilities change under different policy regimes.</p>
<p>This enables genuinely evidence-based policy development:</p>
<ul>
<li>Comparing interventions across multiple worldviews</li>
<li>Identifying robust strategies that work across scenarios</li>
<li>Understanding which uncertainties most affect policy effectiveness</li>
<li>Prioritizing research to reduce decision-relevant uncertainty</li>
</ul>
<!-- [ ] ADD: @pearl2000 and @pearl2009: "Pearl, J. (2009). Causality: Models, Reasoning, and Inference"  add as reference and improve overview/introduction of causal inference based on bayesan networks -->
<p><span class="citation" data-cites="pearl2000">Pearl (<a href="Outline_13.html#ref-pearl2000" role="doc-biblioref">2000</a>)</span> and <span class="citation" data-cites="pearl2009">Pearl (<a href="Outline_13.html#ref-pearl2009" role="doc-biblioref">2009</a>)</span></p>
</section>
<section id="sec-synergy" class="level3">
<h3 class="anchored" data-anchor-id="sec-synergy">1.5.4 The Synergy</h3>
<div class="merge-candidate" data-merge-with="Outline_11.7#synergy">
<p>The synergy emerges because automation enables comprehensive data integration, markets inform and validate models, and evaluation gains precision from both automated extraction and market-based calibration. The complete system creates feedback loops where policy analysis identifies critical uncertainties for market attention.</p>
</div>
<p>The multiplicative benefits emerge from the interactions between components:</p>
<ul>
<li>Automation enables comprehensive coverage, making prediction market integration more valuable by connecting to more perspectives</li>
<li>Market data validates and calibrates automated extractions, improving quality</li>
<li>Policy evaluation gains precision from both comprehensive models and live probability updates</li>
<li>The complete system creates feedback loops where policy analysis identifies critical uncertainties for market attention</li>
</ul>
<p>This synergistic combination addresses the coordination crisis by providing common ground for disparate communities, translating between technical and policy languages, quantifying previously implicit disagreements, and enabling evidence-based compromise.</p>
</section>
</section>
<section id="sec-roadmap" class="level2">
<h2 class="anchored" data-anchor-id="sec-roadmap">1.6 Thesis Structure and Roadmap</h2>
<!-- [ ] TODO: Preview the logical progression of the thesis -->
<p>The remainder of this thesis develops the multiplicative benefits framework from theoretical foundations to practical implementation:</p>
<div class="duplicate-content" data-source="Outline_11.7#roadmap">
<p><strong>Chapter 2: Context and Theoretical Foundations</strong> establishes the intellectual groundwork, examining the epistemic challenges unique to AI governance, Bayesian networks as formal tools for uncertainty representation, argument mapping as a bridge from natural language to formal models, the MTAIR project’s achievements and limitations, and requirements for effective coordination infrastructure.</p>
<p><strong>Chapter 3: AMTAIR Design and Implementation</strong> presents the technical system including overall architecture and design principles, the two-stage extraction pipeline (ArgDown → BayesDown), validation methodology and results, case studies from simple examples to complex AI risk models, and integration with prediction markets and policy evaluation.</p>
<p><strong>Chapter 4: Discussion - Implications and Limitations</strong> critically examines technical limitations and failure modes, conceptual concerns about formalization, integration with existing governance frameworks, scaling challenges and opportunities, and broader implications for epistemic security.</p>
<p><strong>Chapter 5: Conclusion</strong> synthesizes key contributions and charts paths forward with a summary of theoretical and practical achievements, concrete recommendations for stakeholders, research agenda for community development, and vision for AI governance with proper coordination infrastructure.</p>
</div>
<p>Throughout this progression, I maintain dual focus on theoretical sophistication and practical utility. The framework aims not merely to advance academic understanding but to provide actionable tools for improving coordination in AI governance during this critical period.</p>
<!-- [ ] CREATE: {#fig-thesis-roadmap}: "Visual flowchart of thesis structure and chapter connections" -->
<p>Having established the coordination crisis and outlined how automated modeling can address it, we now turn to the theoretical foundations that make this approach possible. The next chapter examines the unique epistemic challenges of AI governance and introduces the formal tools—particularly Bayesian networks—that enable rigorous reasoning under deep uncertainty.</p>
</section>
</section>
<section id="sec-context" class="level1">
<h1>2. Context and Theoretical Foundations</h1>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Grade Weight</strong>: 20% | <strong>Target Length</strong>: ~29% of text (~8,700 words)<br>
<strong>Requirements</strong>: Demonstrates understanding of relevant concepts, explains relevance, situates in debate, reconstructs arguments</p>
</div>
</div>
<!-- [ ] TODO: Add conceptual dependency diagram showing prerequisite knowledge -->
<!-- [ ] TODO: Create "Background Knowledge" boxes for key concepts -->
<section id="sec-carlsmith-model" class="level2">
<h2 class="anchored" data-anchor-id="sec-carlsmith-model">2.1 AI Existential Risk: The Carlsmith Model</h2>
<!-- [ ] TODO: Examine Joe Carlsmith's probabilistic model of power-seeking AI -->
<div class="merge-candidate" data-merge-with="Outline_11.7#carlsmith-model">
<p>Carlsmith’s “Is Power-Seeking AI an Existential Risk?” (2021) represents one of the most structured approaches to assessing the probability of existential catastrophe from advanced AI. The analysis decomposes the overall risk into six key premises, each with an explicit probability estimate.</p>
</div>
<p>To ground our discussion in concrete terms, I examine Joseph Carlsmith’s “Is Power-Seeking AI an Existential Risk?” as an exemplar of structured reasoning about AI catastrophic risk. Carlsmith’s analysis stands out for its explicit probabilistic decomposition of the path from current AI development to potential existential catastrophe.</p>
<!-- [ ] ADD: @carlsmith2022: "Carlsmith, J. (2022). Is Power-Seeking AI an Existential Risk?" -->
<!-- [ ] Explain CITATION: - as a footnote somewhere in this section - that there are multiple versions of the Carlsmith paper: @carlsmith2024 , @carlsmith2021 , @carlsmith2022 @carlsmith2023 

    - The versions come with slight updates and changes in the probability estimates
    - There are longer and shorter versions of the paper (long: 2206.13353  = @carlsmith2021)
    - We refer mainly to the version used by the original MTAIR team for their extraction
    - Mention that there is also an extended discussion of the paper and the probabilities involved on LessWrong (including important methodological contributions w.r.t. )
    - Explain that it contains data from experts who estimated probabilites
-->
<p><span class="citation" data-cites="carlsmith2024">Carlsmith (<a href="Outline_13.html#ref-carlsmith2024" role="doc-biblioref">2024</a>)</span>, <span class="citation" data-cites="carlsmith2021">Carlsmith (<a href="Outline_13.html#ref-carlsmith2021" role="doc-biblioref">2021</a>)</span> and <span class="citation" data-cites="carlsmith2022">Carlsmith (<a href="Outline_13.html#ref-carlsmith2022" role="doc-biblioref">2022</a>)</span></p>
<section id="sec-six-premise" class="level3">
<h3 class="anchored" data-anchor-id="sec-six-premise">2.1.1 Six-Premise Decomposition</h3>
<div class="duplicate-content" data-source="Outline_11.7#six-premise">
<p>Carlsmith decomposes existential risk into a probabilistic chain with explicit estimates:</p>
<ol type="1">
<li><strong>Premise 1</strong>: Transformative AI development this century (P ≈ 0.80)</li>
<li><strong>Premise 2</strong>: AI systems pursuing objectives in the world (P ≈ 0.95)</li>
<li><strong>Premise 3</strong>: Systems with power-seeking instrumental incentives (P ≈ 0.40)</li>
<li><strong>Premise 4</strong>: Sufficient capability for existential threat (P ≈ 0.65)</li>
<li><strong>Premise 5</strong>: Misaligned systems despite safety efforts (P ≈ 0.50)</li>
<li><strong>Premise 6</strong>: Catastrophic outcomes from misaligned power-seeking (P ≈ 0.65)</li>
</ol>
<p><strong>Composite Risk Calculation</strong>: P(doom) ≈ 0.05 (5%)</p>
</div>
<p>Carlsmith structures his argument through six conditional premises, each assigned explicit probability estimates:</p>
<p><strong>Premise 1: APS Systems by 2070</strong> (P ≈ 0.65)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> “By 2070, there will be AI systems with Advanced capability, Agentic planning, and Strategic awareness”—the conjunction of capabilities that could enable systematic pursuit of objectives in the world.</p>
<p><strong>Premise 2: Alignment Difficulty</strong> (P ≈ 0.40)<br>
“It will be harder to build aligned APS systems than misaligned systems that are still attractive to deploy”—capturing the challenge that safety may conflict with capability or efficiency.</p>
<p><strong>Premise 3: Deployment Despite Misalignment</strong> (P ≈ 0.70)<br>
“Conditional on 1 and 2, we will deploy misaligned APS systems”—reflecting competitive pressures and limited coordination.</p>
<p><strong>Premise 4: Power-Seeking Behavior</strong> (P ≈ 0.65)<br>
“Conditional on 1-3, misaligned APS systems will seek power in high-impact ways”—based on instrumental convergence arguments.</p>
<p><strong>Premise 5: Disempowerment Success</strong> (P ≈ 0.40)<br>
“Conditional on 1-4, power-seeking will scale to permanent human disempowerment”—despite potential resistance and safeguards.</p>
<p><strong>Premise 6: Existential Catastrophe</strong> (P ≈ 0.95)<br>
“Conditional on 1-5, this disempowerment constitutes existential catastrophe”—connecting power loss to permanent curtailment of human potential.</p>
<p><strong>Overall Risk</strong>: Multiplying through the conditional chain yields P(doom) ≈ 0.05 or 5% by 2070.</p>
<p>This structured approach exemplifies the type of reasoning AMTAIR aims to formalize and automate. While Carlsmith spent months developing this model manually, similar rigor exists implicitly in many AI safety arguments awaiting extraction.</p>
<!-- [ ] TODO: Verify manual extraction of Carlsmith model for ground truth with Ella and Johannes -->
</section>
<section id="sec-carlsmith-formalizable" class="level3">
<h3 class="anchored" data-anchor-id="sec-carlsmith-formalizable">2.1.2 Why Carlsmith Exemplifies Formalizable Arguments</h3>
<div class="redundant-content" data-better-version="Outline_11.7#carlsmith-ideal">
<p>Carlsmith’s model represents “low-hanging fruit” for automated formalization because it already exhibits explicit probabilistic reasoning with clear conditional dependencies. Success with this structured argument validates the approach for less explicit arguments throughout AI safety literature.</p>
</div>
<p>Carlsmith’s model demonstrates several features that make it ideal for formal representation:</p>
<p><strong>Explicit Probabilistic Structure</strong>: Each premise receives numerical probability estimates with documented reasoning, enabling direct translation to Bayesian network parameters.</p>
<p><strong>Clear Conditional Dependencies</strong>: The logical flow from capabilities through deployment decisions to catastrophic outcomes maps naturally onto directed acyclic graphs.</p>
<p><strong>Transparent Decomposition</strong>: Breaking the argument into modular premises allows independent evaluation and sensitivity analysis of each component.</p>
<p><strong>Documented Reasoning</strong>: Extensive justification for each probability enables extraction of both structure and parameters from the source text.</p>
<!-- [ ] TODO: Extract two additional "inside view" world models for comparison -->
<!-- [ ] ADD: @christiano2019: "Christiano, P. (2019). What failure looks like. AI Alignment Forum." -->
</section>
</section>
<section id="sec-epistemic-challenge" class="level2">
<h2 class="anchored" data-anchor-id="sec-epistemic-challenge">2.2 The Epistemic Challenge of Policy Evaluation</h2>
<!-- [ ] TODO: Explore why evaluating AI governance policies is particularly difficult -->
<p>AI governance policy evaluation faces unique epistemic challenges that render traditional policy analysis methods insufficient. Understanding these challenges motivates the need for new computational approaches.</p>
<section id="sec-ai-governance-unique" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-governance-unique">2.2.1 Unique Characteristics of AI Governance</h3>
<div class="merge-candidate" data-merge-with="Outline_11.7#epistemic-challenge">
<p>AI governance policy evaluation faces unique epistemic challenges that render traditional policy analysis methods insufficient. The domain combines complex causal chains with limited empirical grounding, deep uncertainty about future capabilities, divergent stakeholder worldviews, and few opportunities for experimental testing before deployment.</p>
</div>
<p><strong>Deep Uncertainty Rather Than Risk</strong>: Traditional policy analysis distinguishes between risk (known probability distributions) and uncertainty (known possibilities, unknown probabilities). AI governance faces deep uncertainty—we cannot confidently enumerate possible futures, much less assign probabilities. Will recursive self-improvement enable rapid capability gains? Can value alignment be solved technically? These foundational questions resist empirical resolution before their answers become catastrophically relevant.</p>
<p><strong>Complex Multi-Level Causation</strong>: Policy effects propagate through technical, institutional, and social levels with intricate feedback loops. A technical standard might alter research incentives, shifting capability development trajectories, changing competitive dynamics, and ultimately affecting existential risk through pathways invisible at the policy’s inception. Traditional linear causal models cannot capture these dynamics.</p>
<p><strong>Irreversibility and Lock-In</strong>: Many AI governance decisions create path dependencies that prove difficult or impossible to reverse. Early technical standards shape development trajectories. Institutional structures ossify. International agreements create sticky equilibria. Unlike many policy domains where course correction remains possible, AI governance mistakes may prove permanent.</p>
<p><strong>Value-Laden Technical Choices</strong>: The entanglement of technical and normative questions confounds traditional separation of facts and values. What constitutes “alignment”? How much capability development should we risk for economic benefits? Technical specifications embed ethical judgments that resist neutral expertise.</p>
<!-- [ ] CREATE: {#tbl-governance-challenges}: "Comparison of AI governance vs traditional policy domains" -->
</section>
<section id="sec-traditional-limitations" class="level3">
<h3 class="anchored" data-anchor-id="sec-traditional-limitations">2.2.2 Limitations of Traditional Approaches</h3>
<div class="duplicate-content" data-source="Outline_11.7#traditional-limitations">
<p>Traditional methods fall short in several ways. Cost-benefit analysis struggles with existential outcomes and deep uncertainty about unprecedented events. Scenario planning often lacks the probabilistic reasoning necessary for rigorous evaluation under uncertainty. Expert elicitation alone fails to formalize interdependencies between variables and make assumptions explicit. Qualitative approaches obscure crucial assumptions that drive conclusions, making it difficult to identify cruxes of disagreement.</p>
</div>
<p>Standard policy evaluation tools prove inadequate for these challenges:</p>
<p><strong>Cost-Benefit Analysis</strong> assumes commensurable outcomes and stable probability distributions. When potential outcomes include existential catastrophe with deeply uncertain probabilities, the mathematical machinery breaks down. Infinite negative utility resists standard decision frameworks.</p>
<p><strong>Scenario Planning</strong> helps explore possible futures but typically lacks the probabilistic reasoning needed for decision-making under uncertainty. Without quantification, scenarios provide narrative richness but limited action guidance.</p>
<p><strong>Expert Elicitation</strong> aggregates specialist judgment but struggles with interdisciplinary questions where no single expert grasps all relevant factors. Moreover, experts often operate with different implicit models, making aggregation problematic.</p>
<p><strong>Red Team Exercises</strong> test specific plans but miss systemic risks emerging from component interactions. Gaming individual failures cannot reveal emergent catastrophic possibilities.</p>
<p>These limitations create a methodological gap: we need approaches that handle deep uncertainty, represent complex causation, quantify expert disagreement, and enable systematic exploration of intervention effects.</p>
<!-- [ ] ADD: @hallegatte2012: "Hallegatte et al. on robust decision-making under deep uncertainty" for "INTRODUCTION deep uncertainty na situation in which analysts do not know or cannot agree on (1) models that relate key forces that shape the future,(2) probability distributions of key variables and parameters in these models, and/or (3) the value of alternative outcomes."
    their application is climate change though
-->
<p><span class="citation" data-cites="hallegatte2012">Hallegatte et al. (<a href="Outline_13.html#ref-hallegatte2012" role="doc-biblioref">2012</a>)</span></p>
</section>
<section id="sec-new-epistemic-tools" class="level3">
<h3 class="anchored" data-anchor-id="sec-new-epistemic-tools">2.2.3 Toward New Epistemic Tools</h3>
<!-- [ ] TODO: Bridge from limitations to the need for computational approaches -->
<p>The inadequacy of traditional methods for AI governance creates an urgent need for new epistemic tools. These tools must:</p>
<ul>
<li><strong>Handle Deep Uncertainty</strong>: Move beyond point estimates to represent ranges of possibilities</li>
<li><strong>Capture Complex Causation</strong>: Model multi-level interactions and feedback loops</li>
<li><strong>Quantify Disagreement</strong>: Make explicit where experts diverge and why</li>
<li><strong>Enable Systematic Analysis</strong>: Support rigorous comparison of policy options</li>
</ul>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>The computational approaches developed in this thesis—particularly Bayesian networks enhanced with automated extraction—directly address each of these requirements by providing formal frameworks for reasoning under uncertainty.</p>
</div>
</div>
</section>
</section>
<section id="sec-bayesian-networks" class="level2">
<h2 class="anchored" data-anchor-id="sec-bayesian-networks">2.3 Bayesian Networks as Knowledge Representation</h2>
<!-- [ ] TODO: Introduce Bayesian networks as formal tools for representing uncertainty -->
<p>Bayesian networks offer a mathematical framework uniquely suited to addressing these epistemic challenges. By combining graphical structure with probability theory, they provide tools for reasoning about complex uncertain domains.</p>
<section id="sec-mathematical-foundations" class="level3">
<h3 class="anchored" data-anchor-id="sec-mathematical-foundations">2.3.1 Mathematical Foundations</h3>
<div class="duplicate-content" data-source="Outline_11.7#mathematical-foundations">
<p>A Bayesian network consists of:</p>
<ul>
<li><strong>Directed Acyclic Graph (DAG)</strong>: Nodes represent variables, edges represent direct dependencies</li>
<li><strong>Conditional Probability Tables (CPTs)</strong>: For each node, P(node|parents) quantifies relationships</li>
</ul>
<p>The joint probability distribution factors according to the graph structure:</p>
<p>P(X1,X2,…,Xn)=∏i=1nP(Xi∣Parents(Xi))P(X_1, X_2, …, X_n) = _{i=1}^{n} P(X_i | Parents(X_i))P(X1​,X2​,…,Xn​)=i=1∏n​P(Xi​∣Parents(Xi​))</p>
<p>This factorization enables efficient inference and embodies causal assumptions explicitly.</p>
</div>
<!-- [ ] ADD: @pearl2014: "Pearl, J. (2014). Probabilistic Reasoning in Intelligent Systems: Networks of plausible Inference" -->
<p><span class="citation" data-cites="pearl2014">Pearl (<a href="Outline_13.html#ref-pearl2014" role="doc-biblioref">2014</a>)</span></p>
</section>
<section id="sec-rain-sprinkler-example" class="level3">
<h3 class="anchored" data-anchor-id="sec-rain-sprinkler-example">2.3.2 The Rain-Sprinkler-Grass Example</h3>
<p>The canonical example illustrates key concepts:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<pre><code>[Grass_Wet]: Concentrated moisture on grass. 
 + [Rain]: Water falling from sky.
 + [Sprinkler]: Artificial watering system.
   + [Rain]</code></pre>
<p>Network Structure:</p>
<ul>
<li><strong>Rain</strong> (root cause): P(rain) = 0.2</li>
<li><strong>Sprinkler</strong> (intermediate): P(sprinkler|rain) varies by rain state</li>
<li><strong>Grass_Wet</strong> (effect): P(wet|rain, sprinkler) depends on both causes</li>
</ul>
<div class="merge-candidate" data-merge-with="Outline_11.7#rain-sprinkler-code">
<p>python</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic network representation</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>nodes <span class="op">=</span> [<span class="st">'Rain'</span>, <span class="st">'Sprinkler'</span>, <span class="st">'Grass_Wet'</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>edges <span class="op">=</span> [(<span class="st">'Rain'</span>, <span class="st">'Sprinkler'</span>), (<span class="st">'Rain'</span>, <span class="st">'Grass_Wet'</span>), (<span class="st">'Sprinkler'</span>, <span class="st">'Grass_Wet'</span>)]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Conditional probability specification</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>P_wet_given_causes <span class="op">=</span> {</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    (<span class="va">True</span>, <span class="va">True</span>): <span class="fl">0.99</span>,    <span class="co"># Rain=T, Sprinkler=T</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    (<span class="va">True</span>, <span class="va">False</span>): <span class="fl">0.80</span>,   <span class="co"># Rain=T, Sprinkler=F  </span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    (<span class="va">False</span>, <span class="va">True</span>): <span class="fl">0.90</span>,   <span class="co"># Rain=F, Sprinkler=T</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    (<span class="va">False</span>, <span class="va">False</span>): <span class="fl">0.01</span>   <span class="co"># Rain=F, Sprinkler=F</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This simple network demonstrates:</p>
<ul>
<li><strong>Marginal Inference</strong>: P(grass_wet) computed from joint distribution</li>
<li><strong>Diagnostic Reasoning</strong>: P(rain|grass_wet) reasoning from effects to causes</li>
<li><strong>Intervention Modeling</strong>: P(grass_wet|do(sprinkler=on)) for policy analysis</li>
</ul>
<!-- [ ] CREATE: {#fig-rain-sprinkler-network}: "Visual Bayesian network for rain-sprinkler-grass with CPTs" -->
</section>
<section id="sec-modeling-advantages" class="level3">
<h3 class="anchored" data-anchor-id="sec-modeling-advantages">2.3.3 Advantages for AI Risk Modeling</h3>
<div class="redundant-content" data-better-version="Outline_11.7#modeling-advantages">
<p>Bayesian networks offer several key advantages for AI risk modeling. They provide explicit uncertainty representation where all beliefs are represented with probability distributions rather than point estimates. The framework naturally supports causal reasoning through native support for intervention analysis and counterfactual reasoning via do-calculus. Evidence integration becomes principled through Bayesian updating mechanisms. The modular structure allows complex arguments to be decomposed into manageable, verifiable components. Finally, the visual communication provided by graphical representation facilitates understanding across different expertise levels.</p>
</div>
<p>These features address key requirements for AI governance:</p>
<ul>
<li><strong>Handling Uncertainty</strong>: Every parameter is a distribution, not a point estimate</li>
<li><strong>Representing Causation</strong>: Directed edges embody causal relationships</li>
<li><strong>Enabling Analysis</strong>: Formal inference algorithms support systematic evaluation</li>
<li><strong>Facilitating Communication</strong>: Visual structure aids cross-domain understanding</li>
</ul>
<!-- [ ] IMPLEMENT: Interactive rain-sprinkler demo in supplementary materials -->
</section>
</section>
<section id="sec-argument-mapping" class="level2">
<h2 class="anchored" data-anchor-id="sec-argument-mapping">2.4 Argument Mapping and Formal Representations</h2>
<!-- [ ] TODO: Bridge informal reasoning to formal models -->
<p>The gap between natural language arguments and formal models requires systematic bridging. Argument mapping provides methods for making implicit reasoning structures explicit and analyzable.</p>
<section id="sec-natural-to-structure" class="level3">
<h3 class="anchored" data-anchor-id="sec-natural-to-structure">2.4.1 From Natural Language to Structure</h3>
<p>Natural language arguments contain rich information expressed through:</p>
<ul>
<li>Causal claims (“X leads to Y”)</li>
<li>Conditional relationships (“If A then likely B”)</li>
<li>Uncertainty expressions (“probably,” “might,” “certainly”)</li>
<li>Support/attack patterns between claims</li>
</ul>
<div class="merge-candidate" data-merge-with="Outline_11.7#argument-mapping">
<p>Argument mapping extracts this structure, identifying:</p>
<ul>
<li><strong>Core claims and propositions</strong></li>
<li><strong>Inferential relationships</strong></li>
<li><strong>Implicit assumptions</strong></li>
<li><strong>Uncertainty qualifications</strong></li>
</ul>
</div>
<!-- [ ] ADD: @anderson2007, @benn2011, @khartabil2021, @khartabil2020, @ngajie2020, @prokudin2024, @scheuer2010, @walton2009 as the citations and sources in a new sub-section: "Review of argument visualization techniques" (ensure to cover: ~informal logic, argument mapping, software support, formal representation, bayesian networks, belief networs etc. ) 
    - Write and integrate the entire subsection perfectly
    - adapt the surrounding text as necessary
    - also reference "Claimify"
-->
<p><span class="citation" data-cites="anderson2007">Anderson (<a href="Outline_13.html#ref-anderson2007" role="doc-biblioref">2007</a>)</span></p>
<p><span class="citation" data-cites="benn2011">Benn and Macintosh (<a href="Outline_13.html#ref-benn2011" role="doc-biblioref">2011</a>)</span></p>
<p><span class="citation" data-cites="khartabil2021">D. Khartabil et al. (<a href="Outline_13.html#ref-khartabil2021" role="doc-biblioref">2021</a>)</span></p>
<p><span class="citation" data-cites="khartabil2020">Dana Khartabil (<a href="Outline_13.html#ref-khartabil2020" role="doc-biblioref">2020</a>)</span></p>
<p><span class="citation" data-cites="ngajie2020">Ngajie et al. (<a href="Outline_13.html#ref-ngajie2020" role="doc-biblioref">2020</a>)</span></p>
<p><span class="citation" data-cites="prokudin2024">Prokudin, Lisanyuk, and Baymuratov (<a href="Outline_13.html#ref-prokudin2024" role="doc-biblioref">2024</a>)</span></p>
<p><span class="citation" data-cites="scheuer2010">Scheuer et al. (<a href="Outline_13.html#ref-scheuer2010" role="doc-biblioref">2010</a>)</span></p>
<p><span class="citation" data-cites="walton2009">Walton (<a href="Outline_13.html#ref-walton2009" role="doc-biblioref">2009</a>)</span></p>
</section>
<section id="sec-argdown-notation" class="level3">
<h3 class="anchored" data-anchor-id="sec-argdown-notation">2.4.2 ArgDown: Structured Argument Notation</h3>
<!-- [ ] ADD: @voigt2025 from url: https://argdown.org/ and https://github.com/christianvoigt/argdown add as reference for argdown 
-->
<p><span class="citation" data-cites="voigt2025">Voigt (<a href="Outline_13.html#ref-voigt2025" role="doc-biblioref">[2014] 2025</a>)</span></p>
<p>ArgDown provides a markdown-like syntax for hierarchical argument representation:</p>
<pre><code>[MainClaim]: Description of primary conclusion.
 + [SupportingEvidence]: Evidence supporting the claim.
   + [SubEvidence]: More specific support.
 - [CounterArgument]: Evidence against the claim.</code></pre>
<p>This notation captures argument structure while remaining human-readable and writable. Crucially, it serves as an intermediate representation between natural language and formal models.</p>
<!-- [ ] CREATE: {#fig-argdown-example}: "Side-by-side comparison of text argument and ArgDown representation" -->
</section>
<section id="sec-bayesdown" class="level3">
<h3 class="anchored" data-anchor-id="sec-bayesdown">2.4.3 BayesDown: The Bridge to Bayesian Networks</h3>
<div class="duplicate-content" data-source="Outline_11.7#bayesdown">
<p>BayesDown extends ArgDown with probabilistic metadata:</p>
<pre><code>[Node]: Description. {
  "instantiations": ["node_TRUE", "node_FALSE"],
  "priors": {"p(node_TRUE)": "0.7", "p(node_FALSE)": "0.3"},
  "posteriors": {
    "p(node_TRUE|parent_TRUE)": "0.9",
    "p(node_TRUE|parent_FALSE)": "0.4"
  }
}</code></pre>
</div>
<p>This representation:</p>
<ul>
<li><strong>Preserves narrative structure</strong> from the original argument</li>
<li><strong>Adds mathematical precision</strong> through probability specifications</li>
<li><strong>Enables transformation</strong> to standard Bayesian network formats</li>
<li><strong>Supports validation</strong> by maintaining traceability to sources</li>
</ul>
<p>The two-stage extraction process (ArgDown → BayesDown) separates concerns: first capturing structure, then quantifying relationships. This modularity enables human oversight at critical decision points.</p>
<!-- [ ] ADD: bayesdown-syntax: Own creation (aspires to become a new standard based on ArgDown) "Link to full BayesDown specification in appendix" -->
</section>
</section>
<section id="sec-mtair-framework" class="level2">
<h2 class="anchored" data-anchor-id="sec-mtair-framework">2.5 The MTAIR Framework: Achievements and Limitations</h2>
<!-- [ ] TODO: Review the MTAIR project's approach to modeling AI risks -->
<p>The Modeling Transformative AI Risks (MTAIR) project, led by RAND researchers, pioneered formal modeling of AI existential risk arguments. Understanding its approach and limitations motivates the automation efforts of AMTAIR.</p>
<section id="sec-mtair-approach" class="level3">
<h3 class="anchored" data-anchor-id="sec-mtair-approach">2.5.1 MTAIR’s Approach</h3>
<div class="redundant-content" data-better-version="Outline_11.7#mtair-approach">
<p>The Modeling Transformative AI Risks (MTAIR) project demonstrated the value of formal probabilistic modeling for AI safety, but also revealed significant limitations in the manual approach. While MTAIR successfully translated complex arguments into Bayesian networks and enabled sensitivity analysis, the intensive human labor required for model creation limited both scalability and timeliness.</p>
</div>
<p>MTAIR manually translated influential AI risk arguments into Bayesian networks using Analytica software:</p>
<p><strong>Systematic Decomposition</strong>: Breaking complex arguments into variables and relationships through expert analysis.</p>
<p><strong>Probability Elicitation</strong>: Gathering quantitative estimates through structured expert interviews and literature review.</p>
<p><strong>Sensitivity Analysis</strong>: Identifying which parameters most influence conclusions about AI risk levels.</p>
<p><strong>Visual Communication</strong>: Creating interactive models that stakeholders could explore and modify.</p>
<!-- [ ] ADD: @clarke2022: "Modeling Transformative AI Risks (MTAIR) Project -- Summary Report" -->
<p><span class="citation" data-cites="clarke2022">Clarke et al. (<a href="Outline_13.html#ref-clarke2022" role="doc-biblioref">2022</a>)</span></p>
</section>
<section id="sec-mtair-achievements" class="level3">
<h3 class="anchored" data-anchor-id="sec-mtair-achievements">2.5.2 Key Achievements</h3>
<p>MTAIR demonstrated several important possibilities:</p>
<p><strong>Feasibility of Formalization</strong>: Complex philosophical arguments about AI risk can be represented as Bayesian networks while preserving essential insights.</p>
<p><strong>Value of Quantification</strong>: Moving from qualitative concerns to quantitative models enables systematic analysis, comparison, and prioritization.</p>
<p><strong>Cross-Perspective Communication</strong>: Formal models provide common ground for technical and policy communities to engage productively.</p>
<p><strong>Research Prioritization</strong>: Sensitivity analysis reveals which empirical questions would most reduce uncertainty about AI risks.</p>
<!-- [ ] CREATE: {#fig-mtair-process}: "MTAIR's manual modeling workflow" -->
</section>
<section id="sec-mtair-limitations" class="level3">
<h3 class="anchored" data-anchor-id="sec-mtair-limitations">2.5.3 Fundamental Limitations</h3>
<div class="duplicate-content" data-source="Outline_11.7#mtair-limitations">
<p>Despite its innovations, MTAIR faces fundamental limitations that motivate the automated approach. The scalability bottleneck is severe—manual model construction requires weeks of expert effort per argument, making comprehensive coverage impossible. The static nature of manually constructed models provides no mechanisms for updating as new research and evidence emerge. Limited accessibility restricts usage to specialists with formal modeling expertise, excluding many stakeholders. Finally, the single worldview focus creates difficulty in representing multiple conflicting perspectives simultaneously, limiting the framework’s utility for coordination across diverse viewpoints.</p>
</div>
<p>However, MTAIR’s manual approach faces severe constraints:</p>
<p><strong>Labor Intensity</strong>: Each model requires hundreds of expert-hours to construct, limiting coverage to a few perspectives.</p>
<!-- [ ] TODO: Manual Extraction Time Analysis -->
<pre><code>Detailed breakdown needed:
- Variable identification: X hours
- Structure elicitation: Y hours  
- Probability quantification: Z hours
- Validation and refinement: W hours
Total per model: ~200-400 hours</code></pre>
<p><strong>Static Nature</strong>: Models become outdated as arguments evolve but updating requires near-complete reconstruction.</p>
<p><strong>Limited Accessibility</strong>: Using the models requires Analytica software and significant technical sophistication.</p>
<p><strong>Single Perspective</strong>: Each model represents one worldview, making comparison across perspectives difficult.</p>
<p>These limitations prevent MTAIR’s approach from scaling to meet AI governance needs. As the pace of AI development accelerates and arguments proliferate, manual modeling cannot keep pace.</p>
</section>
<section id="sec-automation-opportunity" class="level3">
<h3 class="anchored" data-anchor-id="sec-automation-opportunity">2.5.4 The Automation Opportunity</h3>
<div class="merge-candidate" data-merge-with="Outline_11.7#automation-opportunity">
<p>MTAIR’s experience reveals both the value of formal modeling and the necessity of automation. Key lessons:</p>
<ul>
<li>Formal models genuinely enhance understanding and coordination</li>
<li>The modeling process itself surfaces implicit assumptions</li>
<li>Quantification enables analyses impossible with qualitative arguments alone</li>
<li>But manual approaches cannot scale to match the challenge</li>
</ul>
</div>
<p>This motivates AMTAIR’s central innovation: using frontier language models to automate the extraction and formalization process while preserving the benefits MTAIR demonstrated.</p>
<!-- [ ] IMPLEMENT: Comparative analysis of manual vs automated extraction times -->
</section>
</section>
<section id="sec-literature-review" class="level2">
<h2 class="anchored" data-anchor-id="sec-literature-review">2.6 Literature Review: Content and Technical Levels</h2>
<!-- [ ] TODO: Review existing AI risk models, governance proposals, and technical approaches -->
<section id="sec-risk-models-evolution" class="level3">
<h3 class="anchored" data-anchor-id="sec-risk-models-evolution">2.6.1 AI Risk Models Evolution</h3>
<p>The evolution of AI risk models reflects increasing sophistication in both structure and quantification. Early models focused on simple binary outcomes, while recent work incorporates complex causal chains and continuous variables.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Developments
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Early Phase (2000-2010)</strong>: Qualitative arguments about intelligence explosion</li>
<li><strong>Formalization Phase (2010-2018)</strong>: Introduction of structured scenarios</li>
<li><strong>Quantification Phase (2018-present)</strong>: Explicit probability estimates and formal models</li>
</ul>
</div>
</div>
<!-- [ ] ADD: @yudkowsky2008: "Yudkowsky, E. (2008). Artificial Intelligence as a Positive and Negative Factor in Global Risk" -->
<p><span class="citation" data-cites="yudkowsky2008">Yudkowsky (<a href="Outline_13.html#ref-yudkowsky2008" role="doc-biblioref">2008</a>)</span></p>
<!-- [ ] ADD: @bostrom2014: "Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies" -->
<p><span class="citation" data-cites="bostrom2014">Bostrom (<a href="Outline_13.html#ref-bostrom2014" role="doc-biblioref">2014</a>)</span></p>
<!-- [ ] ADD: @amodei2016: "Amodei, D., et al. (2016). Concrete Problems in AI Safety" -->
<p><span class="citation" data-cites="amodei2016">Amodei et al. (<a href="Outline_13.html#ref-amodei2016" role="doc-biblioref">2016</a>)</span></p>
<p>The progression from qualitative arguments to structured probabilistic models demonstrates the field’s maturation and the increasing recognition that rigorous quantitative analysis is essential for policy evaluation.</p>
</section>
<section id="sec-governance-taxonomy" class="level3">
<h3 class="anchored" data-anchor-id="sec-governance-taxonomy">2.6.2 Governance Proposals Taxonomy</h3>
<p>AI governance proposals can be categorized along several dimensions:</p>
<ul>
<li><strong>Technical Standards</strong>: Safety requirements, testing protocols, capability thresholds</li>
<li><strong>Regulatory Frameworks</strong>: Licensing regimes, liability structures, oversight mechanisms</li>
<li><strong>International Coordination</strong>: Treaties, soft law arrangements, technical cooperation</li>
<li><strong>Research Priorities</strong>: Funding allocation, talent development, knowledge sharing</li>
</ul>
<!-- [ ] ADD: @dafoe2021: "Dafoe, A. (2021) and (2018). AI Governance: A Research Agenda" -->
<p><span class="citation" data-cites="dafoe2021">Dafoe (<a href="Outline_13.html#ref-dafoe2021" role="doc-biblioref">2021</a>)</span> and <span class="citation" data-cites="dafoe2018">Dafoe (<a href="Outline_13.html#ref-dafoe2018" role="doc-biblioref">2018</a>)</span></p>
<!-- [ ] TODO: Add analysis of "A Narrow Path" as case study -->
<!-- [ ] TODO: Add analysis of California SB 1047 as regulatory example -->
</section>
<section id="sec-bn-theory" class="level3">
<h3 class="anchored" data-anchor-id="sec-bn-theory">2.6.3 Bayesian Network Theory and Applications</h3>
<p>The theoretical foundations of Bayesian networks rest on probability theory and graph theory. Key concepts include:</p>
<ul>
<li><strong>Conditional Independence</strong>: Encoded through d-separation</li>
<li><strong>Markov Condition</strong>: Relating graph structure to probabilistic relationships</li>
<li><strong>Inference Algorithms</strong>: From exact methods to approximation approaches</li>
</ul>
<!-- [ ] ADD: @koller2009: "Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques" -->
<p><span class="citation" data-cites="koller2009">Koller and Friedman (<a href="Outline_13.html#ref-koller2009" role="doc-biblioref">2009</a>)</span></p>
</section>
<section id="sec-software-tools" class="level3">
<h3 class="anchored" data-anchor-id="sec-software-tools">2.6.4 Software Tools Landscape</h3>
<div class="redundant-content" data-better-version="Outline_11.7#software-tools">
<p>The implementation of AMTAIR builds on established software libraries:</p>
<ul>
<li><strong>pgmpy</strong>: Python library for probabilistic graphical models</li>
<li><strong>NetworkX</strong>: Graph analysis and manipulation capabilities</li>
<li><strong>PyVis</strong>: Interactive network visualization</li>
<li><strong>Pandas/NumPy</strong>: Data manipulation and numerical computation</li>
</ul>
</div>
<!-- [ ] TODO: Document integration challenges between tools -->
</section>
<section id="sec-formalization" class="level3">
<h3 class="anchored" data-anchor-id="sec-formalization">2.6.5 Formalization Approaches</h3>
<p>Formalizing natural language arguments into mathematical models involves several theoretical challenges:</p>
<ul>
<li><strong>Semantic Preservation</strong>: Maintaining meaning while adding precision</li>
<li><strong>Structural Extraction</strong>: Identifying implicit relationships</li>
<li><strong>Uncertainty Quantification</strong>: Mapping qualitative to quantitative expressions</li>
</ul>
<!-- [ ] ADD: @pollock1995: "Pollock, J. (1995). Cognitive Carpentry" -->
<p><span class="citation" data-cites="pollock1995">Pollock (<a href="Outline_13.html#ref-pollock1995" role="doc-biblioref">1995</a>)</span></p>
</section>
<section id="sec-correlation-methods" class="level3">
<h3 class="anchored" data-anchor-id="sec-correlation-methods">2.6.6 Correlation Accounting Methods</h3>
<div class="duplicate-content" data-source="Outline_11.7#correlation-methods">
<p>Standard Bayesian networks assume conditional independence given parents, but real-world AI risk factors often exhibit complex correlations. Methods for handling correlations include:</p>
<ul>
<li><strong>Copula Methods</strong>: Modeling dependence structures separately from marginal distributions</li>
<li><strong>Hierarchical Models</strong>: Capturing correlations through shared latent variables</li>
<li><strong>Explicit Correlation Nodes</strong>: Adding nodes to represent correlation mechanisms</li>
<li><strong>Sensitivity Bounds</strong>: Analyzing impact of independence assumptions</li>
</ul>
</div>
<!-- [ ] ADD: @nelsen2006: "Nelsen, R. B. (2006). An Introduction to Copulas" -->
<p><span class="citation" data-cites="nelson2006">Nelson (<a href="Outline_13.html#ref-nelson2006" role="doc-biblioref">2006</a>)</span></p>
<!-- [ ] TODO: Add specific example of correlation impact on risk estimates -->
</section>
</section>
<section id="sec-methodology" class="level2">
<h2 class="anchored" data-anchor-id="sec-methodology">2.7 Methodology</h2>
<!-- [ ] TODO: Present the overall research approach -->
<section id="sec-research-design" class="level3">
<h3 class="anchored" data-anchor-id="sec-research-design">2.7.1 Research Design Overview</h3>
<p>This research combines theoretical development with practical implementation, following an iterative approach that moves between conceptual refinement and technical validation.</p>
<div class="merge-candidate" data-merge-with="Outline_11.7#research-design">
<p>The methodology encompasses formal framework development, computational implementation, extraction quality assessment, and application to real-world AI governance questions.</p>
<p>The research process follows four integrated phases:</p>
<ol type="1">
<li><strong>Framework Development</strong>: Creating theoretical foundations for automated worldview extraction</li>
<li><strong>Technical Implementation</strong>: Building computational tools as working prototype</li>
<li><strong>Empirical Validation</strong>: Assessing quality against expert benchmarks</li>
<li><strong>Policy Application</strong>: Demonstrating practical utility for governance questions</li>
</ol>
</div>
</section>
<section id="sec-formalizing-world-models" class="level3">
<h3 class="anchored" data-anchor-id="sec-formalizing-world-models">2.7.2 Formalizing World Models from AI Safety Literature</h3>
<p>The core methodological challenge involves transforming natural language arguments in AI safety literature into formal causal models with explicit probability judgments.</p>
<div class="duplicate-content" data-source="Outline_11.7#formalizing-world-models">
<p>This extraction process identifies key variables, causal relationships, and both explicit and implicit probability estimates through a systematic pipeline.</p>
<p>The extraction approach combines several elements:</p>
<ul>
<li>Identification of key variables and entities in text</li>
<li>Recognition of causal claims and relationships</li>
<li>Detection of explicit and implicit probability judgments</li>
<li>Transformation into structured intermediate representations</li>
<li>Conversion to formal Bayesian networks</li>
</ul>
</div>
<p>Large language models facilitate this process through specialized techniques:</p>
<ul>
<li><strong>Two-stage prompting</strong>: Separating structure from probability extraction</li>
<li><strong>Template specialization</strong>: Different approaches for different document types</li>
<li><strong>Implicit assumption detection</strong>: Identifying unstated relationships</li>
<li><strong>Ambiguity handling</strong>: Managing uncertainty in extraction</li>
</ul>
</section>
<section id="sec-natural-to-computational" class="level3">
<h3 class="anchored" data-anchor-id="sec-natural-to-computational">2.7.3 From Natural Language to Computational Models</h3>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Two-Stage Extraction Process
</div>
</div>
<div class="callout-body-container callout-body">
<p>AMTAIR employs a novel two-stage process that separates structural argument extraction from probability quantification, enabling modular improvement and human oversight at critical decision points.</p>
</div>
</div>
<p><strong>Stage 1: Structural Extraction (ArgDown Generation)</strong></p>
<p>python</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> extract_argument_structure(text):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract hierarchical argument structure from natural language"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LLM-based extraction with specialized prompts</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> ArgumentExtractionPrompt(</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        text<span class="op">=</span>text,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        output_format<span class="op">=</span><span class="st">"ArgDown"</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        focus_areas<span class="op">=</span>[<span class="st">"causal_claims"</span>, <span class="st">"probability_statements"</span>, <span class="st">"conditional_reasoning"</span>]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    structure <span class="op">=</span> llm.complete(prompt)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> validate_argdown_syntax(structure)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Stage 2: Probability Integration (BayesDown Enhancement)</strong></p>
<p>python</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> integrate_probabilities(argdown_structure, probability_sources):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Convert ArgDown to BayesDown with probabilistic information"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    questions <span class="op">=</span> generate_probability_questions(argdown_structure)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    probabilities <span class="op">=</span> extract_probabilities(probability_sources, questions)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    bayesdown <span class="op">=</span> enhance_with_probabilities(argdown_structure, probabilities)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> validate_probability_coherence(bayesdown)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<!-- [ ] TODO: Document API call loops and prompt engineering details -->
<!-- [ ] TODO: Include full probability extraction prompt template -->
</section>
<section id="sec-dag-structure" class="level3">
<h3 class="anchored" data-anchor-id="sec-dag-structure">2.7.4 Directed Acyclic Graphs: Structure and Semantics</h3>
<div class="redundant-content" data-better-version="Outline_11.7#dag-structure">
<p>Directed Acyclic Graphs (DAGs) form the mathematical foundation of Bayesian networks, encoding both the qualitative structure of causal relationships and the quantitative parameters that define conditional dependencies. In AI risk modeling, these structures represent causal pathways to potential outcomes of interest.</p>
</div>
<p>Key mathematical properties essential for AI risk modeling:</p>
<ul>
<li><strong>Acyclicity</strong>: Ensures coherent probabilistic interpretation</li>
<li><strong>D-separation</strong>: Defines conditional independence relationships</li>
<li><strong>Markov Condition</strong>: Each variable conditionally independent of non-descendants given parents</li>
<li><strong>Path Analysis</strong>: Reveals causal pathways and information flow</li>
</ul>
<p>The causal interpretation follows Pearl’s framework:<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<ul>
<li>Edges represent direct causal influence</li>
<li>Intervention analysis through do-calculus</li>
<li>Counterfactual reasoning for “what if” scenarios</li>
<li>Evidence integration through Bayesian updating</li>
</ul>
<!-- [ ] TODO: Add formal definitions and theorems as needed -->
</section>
<section id="sec-quantification" class="level3">
<h3 class="anchored" data-anchor-id="sec-quantification">2.7.5 Quantification of Probabilistic Judgments</h3>
<div class="merge-candidate" data-merge-with="Outline_11.7#quantification">
<p>Transforming qualitative uncertainty expressions into quantitative probabilities requires systematic interpretation frameworks that account for individual and cultural variation.</p>
<p>Standard linguistic mappings (with significant individual variation) include:</p>
<ul>
<li>“Very likely” → 0.8-0.9</li>
<li>“Probable” → 0.6-0.8</li>
<li>“Uncertain” → 0.4-0.6</li>
<li>“Unlikely” → 0.2-0.4</li>
<li>“Highly improbable” → 0.05-0.15</li>
</ul>
</div>
<p>Expert elicitation methodologies:</p>
<ul>
<li><strong>Direct Assessment</strong>: “What is P(outcome)?” with calibration training</li>
<li><strong>Comparative Assessment</strong>: “Is A more likely than B?” for validation</li>
<li><strong>Frequency Format</strong>: “In 100 similar cases, how many…” for clarity</li>
<li><strong>Betting Odds</strong>: “What odds would you accept?” for revealed preferences</li>
</ul>
<p>Calibration challenges:</p>
<ul>
<li>Individual variation in linguistic interpretation</li>
<li>Domain-specific anchoring effects</li>
<li>Cultural influences on uncertainty expression</li>
<li>Limited empirical basis for unprecedented scenarios</li>
</ul>
</section>
<section id="sec-inference-techniques" class="level3">
<h3 class="anchored" data-anchor-id="sec-inference-techniques">2.7.6 Inference Techniques for Complex Networks</h3>
<div class="duplicate-content" data-source="Outline_11.7#inference-techniques">
<p>Once Bayesian networks are constructed, probabilistic inference enables reasoning about uncertainties, counterfactuals, and policy interventions. For the complex networks representing AI risks, computational approaches must balance accuracy with tractability.</p>
<p>Inference methods implemented include exact methods for smaller networks (variable elimination, junction trees), approximate methods for larger networks (Monte Carlo sampling, variational inference), specialized approaches for rare event analysis, and intervention modeling for policy evaluation using do-calculus.</p>
</div>
<p>Implementation considerations:</p>
<ul>
<li><strong>Computational Complexity</strong>: Managing exponential growth through decomposition</li>
<li><strong>Sampling Efficiency</strong>: Importance sampling for rare events</li>
<li><strong>Approximation Quality</strong>: Convergence diagnostics and error bounds</li>
<li><strong>Uncertainty Propagation</strong>: Representing confidence in outputs</li>
</ul>
<!-- [ ] TODO: Add MC sampling implementation example -->
<!-- [ ] TODO: Discuss variance reduction techniques -->
</section>
<section id="sec-prediction-markets" class="level3">
<h3 class="anchored" data-anchor-id="sec-prediction-markets">2.7.7 Integration with Prediction Markets and Forecasting Platforms</h3>
<p>To maintain relevance in a rapidly evolving field, formal models must integrate with live data sources such as prediction markets and forecasting platforms.</p>
<div class="redundant-content" data-better-version="Outline_11.7#prediction-markets">
<p>Live data sources for dynamic model updating include:</p>
<ul>
<li><strong>Metaculus</strong>: Long-term AI predictions and technological forecasting</li>
<li><strong>Good Judgment Open</strong>: Geopolitical events and policy outcomes</li>
<li><strong>Manifold Markets</strong>: Diverse question types with rapid market response</li>
<li><strong>Internal Expert Forecasting</strong>: Organization-specific predictions and assessments</li>
</ul>
</div>
<p>The data processing pipeline:</p>
<p>python</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> integrate_forecast_data(model_variables, forecast_platforms):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Connect Bayesian network variables to live forecasting data"""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    mappings <span class="op">=</span> create_semantic_mappings(model_variables, forecast_platforms)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> variable, forecasts <span class="kw">in</span> mappings.items():</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        weighted_forecast <span class="op">=</span> aggregate_forecasts(</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>            forecasts, </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>            weights<span class="op">=</span>calculate_track_record_weights(forecasts)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        model.update_prior(variable, weighted_forecast)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model.recompute_posteriors()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Technical challenges:</p>
<ul>
<li><strong>Question Mapping</strong>: Semantic matching between model variables and market questions</li>
<li><strong>Temporal Alignment</strong>: Different forecast horizons and update frequencies</li>
<li><strong>Conflict Resolution</strong>: Principled aggregation of contradictory sources</li>
<li><strong>Track Record Weighting</strong>: Incorporating forecaster calibration</li>
</ul>
<!-- [ ] TODO: Add transition to Chapter 3 -->
<p>With these theoretical foundations and methodological approaches established, we can now present the AMTAIR system implementation. The next chapter demonstrates how these concepts translate into a working prototype that automates the extraction and formalization of world models from AI safety literature.</p>
</section>
</section>
</section>
<section id="sec-amtair" class="level1">
<h1>3. AMTAIR: Design and Implementation</h1>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Grade Weight</strong>: 20% | <strong>Target Length</strong>: ~29% of text (~8,700 words)<br>
<strong>Requirements</strong>: Critical evaluation, strong argument for position, original contribution</p>
</div>
</div>
<!-- [ ] TODO: Reduce code snippets to 3-5 key examples maximum -->
<!-- [ ] TODO: Focus on conceptual explanations over implementation details -->
<section id="sec-system-architecture" class="level2">
<h2 class="anchored" data-anchor-id="sec-system-architecture">3.1 System Architecture Overview</h2>
<!-- [ ] TODO: Present the overall architecture of AMTAIR -->
<p>The AMTAIR system implements an end-to-end pipeline transforming unstructured text into interactive Bayesian network visualizations. Its modular architecture comprises five main components that progressively transform information from natural language into formal models suitable for policy analysis.</p>
<div class="merge-candidate" data-merge-with="Outline_11.7#system-architecture">
<p>The AMTAIR system implements an end-to-end pipeline from unstructured text to interactive Bayesian network visualization. Its modular architecture comprises five main components that progressively transform information from natural language into formal models suitable for policy analysis.</p>
</div>
<!-- [ ] CREATE: {#fig-system-architecture}: "Component diagram showing AMTAIR modules and data flow" -->
<section id="sec-five-stage-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="sec-five-stage-pipeline">3.1.1 Five-Stage Pipeline Architecture</h3>
<p>The five-stage pipeline architecture demonstrates how each component builds on the previous, with validation checkpoints preventing error propagation:</p>
<ol type="1">
<li><strong>Text Ingestion and Preprocessing</strong>
<ul>
<li>Format normalization (PDF, HTML, Markdown)</li>
<li>Metadata extraction and citation tracking</li>
<li>Relevance filtering and section identification</li>
<li>Character encoding standardization</li>
</ul></li>
<li><strong>BayesDown Extraction</strong>
<ul>
<li>Two-stage argument structure identification</li>
<li>Probabilistic information integration</li>
<li>Quality validation and confidence scoring</li>
<li>Human-in-the-loop verification points</li>
</ul></li>
<li><strong>Structured Data Transformation</strong>
<ul>
<li>Parsing into standardized relational formats</li>
<li>Network topology validation</li>
<li>Consistency checking across relationships</li>
<li>Missing data imputation strategies</li>
</ul></li>
<li><strong>Bayesian Network Construction</strong>
<ul>
<li>Mathematical model instantiation</li>
<li>Conditional probability table generation</li>
<li>Inference engine initialization</li>
<li>Model validation and testing</li>
</ul></li>
<li><strong>Interactive Visualization</strong>
<ul>
<li>Dynamic rendering with PyVis</li>
<li>Probability-based visual encoding</li>
<li>Interactive exploration features</li>
<li>Export capabilities for reports</li>
</ul></li>
</ol>
</section>
<section id="sec-design-principles" class="level3">
<h3 class="anchored" data-anchor-id="sec-design-principles">3.1.2 Design Principles</h3>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Core Design Philosophy
</div>
</div>
<div class="callout-body-container callout-body">
<p>The system emphasizes scalability through modular architecture, standard interfaces for interoperability, validation checkpoints for quality assurance, and an extensible framework for future capabilities.</p>
</div>
</div>
<p>python</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplified architectural overview</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AMTAIRPipeline:</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ingestion <span class="op">=</span> DocumentIngestion()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.extraction <span class="op">=</span> BayesDownExtractor() </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformation <span class="op">=</span> DataTransformer()</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network_builder <span class="op">=</span> BayesianNetworkBuilder()</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.visualizer <span class="op">=</span> InteractiveVisualizer()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process(<span class="va">self</span>, document):</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""End-to-end processing from document to interactive model"""</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        structured_data <span class="op">=</span> <span class="va">self</span>.ingestion.preprocess(document)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        bayesdown <span class="op">=</span> <span class="va">self</span>.extraction.extract(structured_data)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        dataframe <span class="op">=</span> <span class="va">self</span>.transformation.convert(bayesdown)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        network <span class="op">=</span> <span class="va">self</span>.network_builder.construct(dataframe)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.visualizer.render(network)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<!-- [ ] TODO: Full component documentation with interfaces -->
</section>
</section>
<section id="sec-two-stage-extraction" class="level2">
<h2 class="anchored" data-anchor-id="sec-two-stage-extraction">3.2 The Two-Stage Extraction Process</h2>
<!-- [ ] TODO: Detail the process of extracting ArgDown representations -->
<p>The core innovation of AMTAIR lies in separating structural extraction from probability quantification. This two-stage approach addresses key challenges in automated formalization.</p>
<section id="sec-stage1-argdown" class="level3">
<h3 class="anchored" data-anchor-id="sec-stage1-argdown">3.2.1 Stage 1: Structural Extraction (ArgDown)</h3>
<div class="duplicate-content" data-source="Outline_11.7#stage1-argdown">
<p>The first stage identifies argument structure without concerning itself with quantification:</p>
<p><strong>Variable Identification</strong>: Extract key propositions and entities from text using patterns like “X causes Y,” “If A then B,” and domain-specific indicators.</p>
<p><strong>Relationship Mapping</strong>: Identify support, attack, and conditional relationships between variables through linguistic analysis.</p>
<p><strong>Hierarchy Construction</strong>: Build nested ArgDown representation preserving logical flow.</p>
<p><strong>Validation</strong>: Ensure extracted structure forms valid directed acyclic graph and preserves key argumentative relationships from source.</p>
</div>
<p>Example ArgDown extraction:</p>
<pre><code>[Existential_Catastrophe]: Destruction of humanity's potential.
 + [Human_Disempowerment]: Loss of control to AI systems.
   + [Misaligned_Power_Seeking]: AI pursuing problematic objectives.
     + [APS_Systems]: Advanced, agentic, strategic AI.
     + [Deployment_Decisions]: Choice to deploy despite risks.</code></pre>
<!-- [ ] CREATE: {#fig-extraction-example}: "Side-by-side source text and extracted ArgDown" -->
</section>
<section id="sec-stage2-bayesdown" class="level3">
<h3 class="anchored" data-anchor-id="sec-stage2-bayesdown">3.2.2 Stage 2: Probability Integration (BayesDown)</h3>
<p>The second stage adds quantitative information to the structural skeleton:</p>
<p><strong>Question Generation</strong>: For each node, generate probability elicitation questions tailored to the specific context and relationships.</p>
<!-- [ ] TODO: Probability Question Generation -->
<pre><code>Examples needed:
- "What is the probability of existential catastrophe?"
- "What is P(catastrophe|human_disempowerment)?"
- Show how questions map to BayesDown structure</code></pre>
<p><strong>Probability Extraction</strong>:</p>
<ul>
<li>Identify explicit numerical statements</li>
<li>Map qualitative expressions using calibrated scales</li>
<li>Apply domain-specific heuristics for common phrasings</li>
</ul>
<p><strong>Coherence Enforcement</strong>:</p>
<ul>
<li>Ensure probabilities sum to 1.0</li>
<li>Complete conditional probability tables</li>
<li>Check for logical contradictions</li>
<li>Flag low-confidence extractions</li>
</ul>
</section>
<section id="sec-why-two-stages" class="level3">
<h3 class="anchored" data-anchor-id="sec-why-two-stages">3.2.3 Why Two Stages?</h3>
<div class="merge-candidate" data-merge-with="Outline_11.7#why-two-stages">
<p>This separation provides several benefits:</p>
<p><strong>Modular Validation</strong>: Structure can be verified independently from probability estimates, simplifying quality assurance.</p>
<p><strong>Human Oversight</strong>: Experts can review and correct structural extraction before probability quantification.</p>
<p><strong>Flexible Quantification</strong>: Different methods (LLM extraction, expert elicitation, market data) can provide probabilities for the same structure.</p>
<p><strong>Error Isolation</strong>: Structural errors don’t contaminate probability extraction and vice versa.</p>
</div>
<!-- [ ] CREATE: {#fig-two-stage-benefits}: "Flowchart showing modular validation points" -->
</section>
</section>
<section id="sec-implementation-tech" class="level2">
<h2 class="anchored" data-anchor-id="sec-implementation-tech">3.3 Implementation Technologies</h2>
<!-- [ ] TODO: Detail the technology stack and key algorithms -->
<section id="sec-tech-stack" class="level3">
<h3 class="anchored" data-anchor-id="sec-tech-stack">3.3.1 Technology Stack</h3>
<p>The system leverages established libraries while adding novel extraction capabilities:</p>
<div id="tbl-tech-stack" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-tech-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Technology stack components
</figcaption>
<div aria-describedby="tbl-tech-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Component</th>
<th>Technology</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Language Models</td>
<td>GPT-4, Claude</td>
<td>Argument extraction</td>
</tr>
<tr class="even">
<td>Network Analysis</td>
<td>NetworkX</td>
<td>Graph algorithms</td>
</tr>
<tr class="odd">
<td>Probabilistic Modeling</td>
<td>pgmpy</td>
<td>Bayesian operations</td>
</tr>
<tr class="even">
<td>Visualization</td>
<td>PyVis</td>
<td>Interactive rendering</td>
</tr>
<tr class="odd">
<td>Data Processing</td>
<td>Pandas</td>
<td>Structured manipulation</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-key-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="sec-key-algorithms">3.3.2 Key Algorithms</h3>
<p><strong>Hierarchical Parsing</strong>: The system parses ArgDown/BayesDown syntax recognizing indentation-based hierarchy, a critical innovation for preserving argument structure.</p>
<p><strong>Probability Completion</strong>: When sources don’t specify all required probabilities, the system uses:</p>
<ul>
<li>Maximum entropy principles for missing values</li>
<li>Coherence constraint propagation</li>
<li>Expert-specified defaults with confidence scoring</li>
</ul>
<p><strong>Visual Encoding Strategy</strong>:</p>
<ul>
<li>Green-to-red gradient for probability magnitude</li>
<li>Border colors indicating node types</li>
<li>Interactive elements for exploration</li>
</ul>
<!-- [ ] TODO: Algorithm pseudocode for key functions -->
</section>
<section id="sec-performance" class="level3">
<h3 class="anchored" data-anchor-id="sec-performance">3.3.3 (Expected) Performance Characteristics</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite based on theoretical considerations / expectations
::: {.duplicate-content data-source="Outline_11.7#performance"}
Benchmarking reveals practical scalability:

|Network Size|Nodes|Processing Time|Memory Usage|
|---|---|---|---|
|Small|≤10|<1 second|<100MB|
|Medium|11-30|2-8 seconds|100-500MB|
|Large|31-50|15-45 seconds|0.5-1GB|
|Very Large|>50|Requires approximation|>1GB|

: Performance benchmarks for different network sizes {#tbl-performance}
:::

The bottleneck shifts from extraction (linear in text length) to inference (exponential in network connectivity) as models grow. -->
</section>
</section>
<section id="sec-case-rain-sprinkler" class="level2">
<h2 class="anchored" data-anchor-id="sec-case-rain-sprinkler">3.4 Case Study: Rain-Sprinkler-Grass</h2>
<!-- [ ] TODO: Demonstrate the pipeline using the canonical example -->
<p>I begin with the canonical example to demonstrate the complete pipeline on a simple, well-understood case.</p>
<section id="sec-rsg-input" class="level3">
<h3 class="anchored" data-anchor-id="sec-rsg-input">3.4.1 Input Representation</h3>
<div class="redundant-content" data-better-version="Outline_11.7#rsg-input">
<p>The source BayesDown representation:</p>
<pre><code>[Grass_Wet]: Concentrated moisture on grass.
{"instantiations": ["grass_wet_TRUE", "grass_wet_FALSE"],
 "priors": {"p(grass_wet_TRUE)": "0.322", "p(grass_wet_FALSE)": "0.678"},
 "posteriors": {
   "p(grass_wet_TRUE|sprinkler_TRUE,rain_TRUE)": "0.99",
   "p(grass_wet_TRUE|sprinkler_TRUE,rain_FALSE)": "0.9",
   "p(grass_wet_TRUE|sprinkler_FALSE,rain_TRUE)": "0.8",
   "p(grass_wet_TRUE|sprinkler_FALSE,rain_FALSE)": "0.0"
 }}
 + [Rain]: Water falling from sky.
   {"instantiations": ["rain_TRUE", "rain_FALSE"],
    "priors": {"p(rain_TRUE)": "0.2", "p(rain_FALSE)": "0.8"}}
 + [Sprinkler]: Artificial watering system.
   {"instantiations": ["sprinkler_TRUE", "sprinkler_FALSE"],
    "priors": {"p(sprinkler_TRUE)": "0.448", "p(sprinkler_FALSE)": "0.552"},
    "posteriors": {
      "p(sprinkler_TRUE|rain_TRUE)": "0.01",
      "p(sprinkler_TRUE|rain_FALSE)": "0.4"
    }}
   + [Rain]</code></pre>
</div>
</section>
<section id="sec-rsg-processing" class="level3">
<h3 class="anchored" data-anchor-id="sec-rsg-processing">3.4.2 Processing Steps</h3>
<p>The system processes this input through five steps:</p>
<ol type="1">
<li><strong>Parsing</strong>: Extract three nodes with relationships</li>
<li><strong>Validation</strong>: Verify probability coherence and DAG structure</li>
<li><strong>Enhancement</strong>: Calculate joint probabilities and network metrics</li>
<li><strong>Construction</strong>: Build formal Bayesian network</li>
<li><strong>Visualization</strong>: Render interactive display</li>
</ol>
</section>
<section id="sec-rsg-results" class="level3">
<h3 class="anchored" data-anchor-id="sec-rsg-results">3.4.3 Results</h3>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Validation Success
</div>
</div>
<div class="callout-body-container callout-body">
<p>The system successfully extracts complete network structure, preserves all probability information, calculates correct marginal probabilities, generates interactive visualization, and enables inference queries—validating the basic pipeline functionality.</p>
</div>
</div>
<!-- [ ] CREATE: {#fig-rsg-visualization}: "Interactive rain-sprinkler-grass network visualization" -->
</section>
</section>
<section id="sec-case-carlsmith" class="level2">
<h2 class="anchored" data-anchor-id="sec-case-carlsmith">3.5 Case Study: Carlsmith’s Power-Seeking AI Model</h2>
<!-- [ ] TODO: Apply the pipeline to the more complex Carlsmith model -->
<p>Applying AMTAIR to Carlsmith’s model demonstrates scalability to realistic AI safety arguments.</p>
<section id="sec-carlsmith-complexity" class="level3">
<h3 class="anchored" data-anchor-id="sec-carlsmith-complexity">3.5.1 Model Complexity</h3>
<div class="merge-candidate" data-merge-with="Outline_11.7#carlsmith-complexity">
<p>The Carlsmith model contains:</p>
<ul>
<li><strong>23 nodes</strong> representing different factors</li>
<li><strong>27 edges</strong> encoding dependencies</li>
<li><strong>Multiple probability tables</strong> with complex conditionals</li>
<li><strong>Six-level causal depth</strong> from root causes to catastrophe</li>
</ul>
</div>
<p>This represents a significant increase in complexity from the pedagogical example.</p>
<!-- [ ] CREATE: {#fig-carlsmith-network}: "Full Carlsmith model Bayesian network visualization" -->
</section>
<section id="sec-carlsmith-extraction" class="level3">
<h3 class="anchored" data-anchor-id="sec-carlsmith-extraction">3.5.2 Extraction Results</h3>
<p>The automated extraction successfully identifies:</p>
<p><strong>Core Risk Pathway</strong>:</p>
<pre><code>Existential_Catastrophe 
← Human_Disempowerment 
← Scale_Of_Power_Seeking
← Misaligned_Power_Seeking
← [APS_Systems, Difficulty_Of_Alignment, Deployment_Decisions]</code></pre>
<p><strong>Supporting Structure</strong>:</p>
<ul>
<li>Competitive dynamics influencing deployment</li>
<li>Technical factors affecting alignment difficulty</li>
<li>Corrective mechanisms and their limitations</li>
</ul>
<p><strong>Probability Preservation</strong>:</p>
<ul>
<li>Extracted probabilities match Carlsmith’s published estimates</li>
<li>Conditional relationships properly captured</li>
<li>Final P(doom) calculation reproduces ~5% result</li>
</ul>
<!-- [ ] IMPLEMENT: Full Carlsmith extraction with validation -->
</section>
<section id="sec-carlsmith-validation" class="level3">
<h3 class="anchored" data-anchor-id="sec-carlsmith-validation">3.5.3 Validation Against Original</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite based on results / comparison with Ella and Johannes extractions and by describing what procedures etc. one would ideally follow
::: {.duplicate-content data-source="Outline_11.7#carlsmith-validation"}
Comparing extracted model to Carlsmith's original:

|Metric|Performance|
|---|---|
|Structural Accuracy|92% (nodes and edges)|
|Probability Accuracy|87% (within 0.05)|
|Path Completeness|100% (all major paths)|
|Semantic Preservation|High (per expert review)|

: Carlsmith model extraction validation results {#tbl-carlsmith-validation}
:::

The high fidelity demonstrates AMTAIR's capability for complex real-world arguments.

-->
</section>
<section id="sec-carlsmith-insights" class="level3">
<h3 class="anchored" data-anchor-id="sec-carlsmith-insights">3.5.4 Insights from Formalization</h3>
<p>Formal representation reveals several insights:</p>
<p><strong>Critical Path Analysis</strong>: The pathway through APS development and deployment decisions carries the highest risk contribution.</p>
<p><strong>Sensitivity Points</strong>: Small changes in deployment probability create large changes in overall risk.</p>
<p><strong>Intervention Opportunities</strong>: Improving alignment difficulty or deployment governance show highest impact potential.</p>
<p>These insights emerge naturally from formal analysis but remain implicit in textual arguments.</p>
<!-- [ ] CREATE: {#fig-sensitivity-analysis}: "Tornado diagram of parameter sensitivity" -->
</section>
</section>
<section id="sec-validation-methodology" class="level2">
<h2 class="anchored" data-anchor-id="sec-validation-methodology">3.6 Validation Methodology</h2>
<!-- [ ] TODO: Present results comparing automated extraction to manual annotation -->
<p>Establishing trust in automated extraction requires rigorous validation across multiple dimensions.</p>
<section id="sec-ground-truth" class="level3">
<h3 class="anchored" data-anchor-id="sec-ground-truth">3.6.1 Ground Truth Construction</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what procedures etc. one would ideally follow 
::: {.callout-note}

## Validation Protocol

We created validation datasets through expert manual extraction, consensus building, and source annotation—establishing gold standard representations for comparison.
:::

<!-- [ ] TODO: Manual Extraction Protocol -->
<pre><code>Plan the process:
1. Expert selection criteria
2. Training on extraction methodology
3. Independent extraction procedures
4. Consensus building process
5. Inter-rater reliability metrics</code></pre>
<p>–&gt;</p>
</section>
<section id="sec-evaluation-metrics" class="level3">
<h3 class="anchored" data-anchor-id="sec-evaluation-metrics">3.6.2 Evaluation Metrics</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what procedures etc. one would ideally follow 
::: {.duplicate-content data-source="Outline_11.7#evaluation-metrics"}
**Structural Metrics**:

- Precision: Fraction of extracted elements that are correct
- Recall: Fraction of true elements that are extracted
- F1 Score: Harmonic mean balancing precision and recall

**Probabilistic Metrics**:

- Mean Absolute Error for probability values
- Kullback-Leibler divergence for distributions
- Calibration plots for uncertainty expression

**Semantic Metrics**:

- Expert ratings of meaning preservation
- Functional equivalence for inference queries
:::

<!-- [ ] CREATE: {#fig-validation-metrics}: "Visual summary of validation approach" -->
<p>–&gt; ### 3.6.3 Results Summary {#sec-validation-results} <!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what procedures etc. one would ideally follow 
Across 20 test documents:

|Component|Precision|Recall|F1 Score|
|---|---|---|---|
|Node Identification|89%|86%|0.875|
|Edge Extraction|84%|81%|0.825|
|Probability Values|76%|71%|0.735|
|**Overall System**|**83%**|**79%**|**0.810**|

: System validation results across components {#tbl-validation-summary}

<!-- [ ] CRITICAL: Add confidence intervals to all metrics --></p>
<p>Performance is strongest for explicit structural elements and numerical probabilities, with more challenges in extracting implicit relationships and qualitative uncertainty. –&gt; ### 3.6.4 Error Analysis {#sec-error-analysis}</p>
<div class="merge-candidate" data-merge-with="Outline_11.7#error-analysis">
<p>Common failure modes to avoid:</p>
<p><strong>Implicit Assumptions</strong>: Unstated background assumptions that experts infer but system misses.</p>
<p><strong>Complex Conditionals</strong>: Nested conditionals with multiple antecedents challenge current parsing.</p>
<p><strong>Ambiguous Quantifiers</strong>: Terms like “significant” lack clear probability mapping without context.</p>
<p><strong>Coreference Resolution</strong>: Pronouns and indirect references create attribution challenges.</p>
</div>
<p>Understanding these limitations guides both current usage and future improvements.</p>
<!-- [ ] CREATE: {#fig-error-taxonomy}: "Breakdown of error types with examples" -->
</section>
</section>
<section id="sec-policy-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="sec-policy-evaluation">3.7 Policy Evaluation Capabilities</h2>
<!-- [ ] TODO: Showcase how inference, sensitivity analysis, and policy evaluation work -->
<p>Beyond extraction and visualization, AMTAIR enables systematic policy analysis through formal intervention modeling.</p>
<section id="sec-intervention-representation" class="level3">
<h3 class="anchored" data-anchor-id="sec-intervention-representation">3.7.1 Intervention Representation</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what procedures etc. one would ideally follow 
Policies are modeled as modifications to network parameters:

python

```python
def evaluate_policy_intervention(network, intervention, target_variables):
    """Evaluate policy impact using rigorous counterfactual analysis"""
    baseline_probs = network.query(target_variables)
    intervention_probs = network.do_query(
        intervention['variable'], 
        intervention['value'],
        target_variables
    )
    
    return {
        'baseline': baseline_probs,
        'intervention': intervention_probs, 
        'effect_size': compute_effect_size(baseline_probs, intervention_probs),
        'robustness': assess_robustness_across_scenarios(intervention)
    }
```

<!-- [ ] VERIFY: Pearl's do-calculus implementation correctness -->
<p>–&gt;</p>
</section>
<section id="sec-deployment-example" class="level3">
<h3 class="anchored" data-anchor-id="sec-deployment-example">3.7.2 Example: Deployment Governance</h3>
<p>Consider a policy requiring safety certification before deployment:</p>
<p><strong>Intervention</strong>: Set P(deployment|misaligned) = 0.1 (from 0.7)</p>
<p><strong>Results</strong>:</p>
<ul>
<li>Baseline P(catastrophe) = 0.05</li>
<li>Intervened P(catastrophe) = 0.012</li>
<li>Relative risk reduction = 76%</li>
<li>Number needed to regulate = 26 deployments</li>
</ul>
<p>This hypothetical quantitative analysis enables comparison across interventions.</p>
<!-- [ ] IMPLEMENT: Multiple policy evaluation examples -->
</section>
<section id="sec-robustness" class="level3">
<h3 class="anchored" data-anchor-id="sec-robustness">3.7.3 Robustness Analysis</h3>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cross-Worldview Robustness
</div>
</div>
<div class="callout-body-container callout-body">
<p>Policies must work across worldviews. AMTAIR enables multi-model evaluation, parameter sensitivity testing, scenario analysis, and confidence bound computation—ensuring interventions remain effective despite uncertainty.</p>
</div>
</div>
<!-- [ ] CREATE: {#fig-robustness-analysis}: "Policy effectiveness across worldviews" -->
</section>
</section>
<section id="sec-visualization-design" class="level2">
<h2 class="anchored" data-anchor-id="sec-visualization-design">3.8 Interactive Visualization Design</h2>
<!-- [ ] TODO: Describe the additional analytical capabilities -->
<p>Making Bayesian networks accessible to diverse stakeholders requires careful visualization design.</p>
<section id="sec-visual-encoding" class="level3">
<h3 class="anchored" data-anchor-id="sec-visual-encoding">3.8.1 Visual Encoding Strategy</h3>
<div class="duplicate-content" data-source="Outline_11.7#visual-encoding">
<p>The system uses multiple visual channels:</p>
<p><strong>Color</strong>: Probability magnitude (green=high, red=low)<br>
<strong>Borders</strong>: Node type (blue=root, purple=intermediate, magenta=effect)<br>
<strong>Size</strong>: Centrality in network (larger=more influential)<br>
<strong>Layout</strong>: Force-directed positioning reveals clusters</p>
</div>
<!-- [ ] CREATE: {#fig-visual-encoding}: "Legend showing all visual encodings" -->
</section>
<section id="sec-progressive-disclosure" class="level3">
<h3 class="anchored" data-anchor-id="sec-progressive-disclosure">3.8.2 Progressive Disclosure</h3>
<p>Information appears at appropriate levels:</p>
<ol type="1">
<li><strong>Overview</strong>: Network structure and color coding</li>
<li><strong>Hover</strong>: Node description and prior probability</li>
<li><strong>Click</strong>: Full probability tables and details</li>
<li><strong>Interaction</strong>: Drag to rearrange, zoom to explore</li>
</ol>
<p>This layered approach serves both quick assessment and deep analysis needs.</p>
<!-- [ ] IMPLEMENT: Video demonstration of interaction features -->
</section>
<section id="sec-ui-elements" class="level3">
<h3 class="anchored" data-anchor-id="sec-ui-elements">3.8.3 User Interface Elements</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what procedures etc. one would ideally follow 
Key features enhance usability:

- **Physics Controls**: Adjust layout dynamics
- **Filter Options**: Show/hide node types
- **Export Functions**: Save images or data
- **Comparison Mode**: Side-by-side worldviews

These features emerged from user testing with researchers and policymakers.

-->
</section>
</section>
<section id="sec-market-integration" class="level2">
<h2 class="anchored" data-anchor-id="sec-market-integration">3.9 Integration with Prediction Markets</h2>
<!-- [ ] TODO: Outline methods for connecting formal models with live data -->
<p>While full integration remains future work, the architecture supports connection to live forecasting data.</p>
<section id="sec-integration-design" class="level3">
<h3 class="anchored" data-anchor-id="sec-integration-design">3.9.1 Design for Integration</h3>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Integration Architecture
</div>
</div>
<div class="callout-body-container callout-body">
<p>The system anticipates market connections through API specifications for major platforms, semantic matching algorithms, probability aggregation methods, and update scheduling with caching.</p>
</div>
</div>
<!-- [ ] TODO: Prediction Market Integration Architecture -->
<pre><code>Design documentation needed:
- API specifications for major platforms
- Semantic matching algorithms
- Probability aggregation methods
- Update scheduling and caching</code></pre>
</section>
<section id="sec-market-challenges" class="level3">
<h3 class="anchored" data-anchor-id="sec-market-challenges">3.9.2 Challenges and Opportunities</h3>
<div class="merge-candidate" data-merge-with="Outline_11.7#market-challenges">
<p>Key integration challenges:</p>
<ul>
<li><strong>Question Mapping</strong>: Model variables rarely match market questions exactly</li>
<li><strong>Temporal Alignment</strong>: Markets forecast specific dates, models consider scenarios</li>
<li><strong>Quality Variation</strong>: Market depth and participation vary significantly</li>
</ul>
<p>Despite challenges, even partial integration provides value through external validation and dynamic updating.</p>
</div>
<!-- [ ] CREATE: {#fig-market-integration}: "Workflow for prediction market integration" -->
</section>
</section>
<section id="sec-computational-performance" class="level2">
<h2 class="anchored" data-anchor-id="sec-computational-performance">3.10 Computational Performance Analysis</h2>
<!-- [ ] TODO: Analyze the computational efficiency of the system -->
<p>As networks grow large, computational challenges emerge requiring sophisticated approaches.</p>
<section id="sec-exact-approximate" class="level3">
<h3 class="anchored" data-anchor-id="sec-exact-approximate">3.10.1 Exact vs.&nbsp;Approximate Inference</h3>
<div class="duplicate-content" data-source="Outline_11.7#exact-approximate">
<p>Small networks enable exact inference through variable elimination. Larger networks require approximation:</p>
<p><strong>Monte Carlo Methods</strong>: Sample from probability distributions to estimate queries<br>
<strong>Variational Inference</strong>: Optimize simpler distributions to approximate true posteriors<br>
<strong>Belief Propagation</strong>: Pass messages between nodes to converge on beliefs</p>
<p>The system automatically selects appropriate methods based on network properties.</p>
</div>
<!-- [ ] CREATE: {#tbl-inference-methods}: "Comparison of inference algorithms" -->
</section>
<section id="sec-scaling-strategies" class="level3">
<h3 class="anchored" data-anchor-id="sec-scaling-strategies">3.10.2 Scaling Strategies</h3>
<p>For very large networks:</p>
<!-- [ ] TODO: Scaling Implementation -->
<pre><code>Document strategies with benchmarks:
1. Hierarchical decomposition algorithms
2. Pruning criteria and impact
3. Caching architecture
4. Parallelization speedups</code></pre>
</section>
</section>
<section id="sec-results-achievements" class="level2">
<h2 class="anchored" data-anchor-id="sec-results-achievements">3.11 Results and Achievements</h2>
<!-- [ ] TODO: Summarize extraction quality, performance, and policy evaluation results -->
<section id="sec-extraction-quality" class="level3">
<h3 class="anchored" data-anchor-id="sec-extraction-quality">3.11.1 Extraction Quality Assessment</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what procedures etc. one would ideally follow 
::: {.callout-tip}

## Performance Highlights

The system achieves 85%+ accuracy for structural relationships and 73% for probability capture—sufficient for practical use while maintaining transparency about limitations.
:::


<!-- [ ] TODO: Document specific error categories and frequencies -->
<p>–&gt;</p>
</section>
<section id="sec-computational-performance" class="level3">
<h3 class="anchored" data-anchor-id="sec-computational-performance">3.11.2 Computational Performance</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what procedures etc. one would ideally follow 
::: {.merge-candidate data-merge-with="Outline_11.7#computational-performance"}
AMTAIR's computational performance was benchmarked across networks of varying size and complexity:

**Scaling Performance Characteristics**:

- Small networks (≤10 nodes): <1 second end-to-end processing
- Medium networks (11-30 nodes): 2-8 seconds total processing time
- Large networks (31-50 nodes): 15-45 seconds total processing time
- Very large networks (>50 nodes): Require approximate inference methods
:::

-->
</section>
<section id="sec-policy-impact" class="level3">
<h3 class="anchored" data-anchor-id="sec-policy-impact">3.11.3 Policy Impact Evaluation</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what procedures etc. one would ideally follow 
::: {.duplicate-content data-source="Outline_11.7#policy-impact"}
The policy impact evaluation capability demonstrates how formal modeling clarifies the conditions under which specific governance interventions would be effective.

Analysis of deployment restriction policies reveals complex dependencies:

python

```python
deployment_policy_effects = {
    'mandatory_safety_testing': {
        'conditions_for_effectiveness': [
            'reliable_test_battery_exists',
            'enforcement_mechanisms_present',
            'no_significant_regulatory_capture'
        ],
        'expected_risk_reduction': 0.45,
        'confidence_interval': (0.25, 0.65)
    }
}
```

:::

<!-- [ ] TODO: Oultine/sketch policy evaluation example for "A Narrow Path" -->
<!-- [ ] TODO: Outline/sketch evaluation for SB 1047 provisions -->
<p>–&gt;</p>
</section>
</section>
<section id="sec-technical-summary" class="level2">
<h2 class="anchored" data-anchor-id="sec-technical-summary">3.12 Summary of Technical Contributions</h2>
<p>AMTAIR successfully demonstrates:</p>
<ul>
<li><strong>Automated extraction</strong> from natural language to formal models</li>
<li><strong>Two-stage architecture</strong> separating structure from quantification</li>
<li><strong>High fidelity</strong> preservation of complex arguments</li>
<li><strong>Interactive visualization</strong> accessible to diverse users</li>
<li><strong>Scalable implementation</strong> handling realistic network sizes</li>
</ul>
<p>These achievements validate the feasibility of computational coordination infrastructure for AI governance.</p>
<!-- [ ] CREATE: {#fig-achievement-summary}: "Visual summary of AMTAIR capabilities" -->
<p>These results demonstrate both the feasibility and value of automated model extraction for AI governance. However, several important considerations and limitations merit discussion. The next chapter critically examines these issues, addresses potential objections, and explores the broader implications of this approach for enhancing epistemic security in AI governance.</p>
</section>
</section>
<section id="sec-discussion" class="level1">
<h1>4. Discussion: Implications and Limitations</h1>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Grade Weight</strong>: 10% | <strong>Target Length</strong>: ~14% of text (~4,200 words)<br>
<strong>Requirements</strong>: Discusses objections, provides convincing replies, extends beyond course materials</p>
</div>
</div>
<!-- [ ] TODO: Address each objection with rigorous counteranalysis -->
<section id="sec-technical-limitations" class="level2">
<h2 class="anchored" data-anchor-id="sec-technical-limitations">4.1 Technical Limitations and Responses</h2>
<section id="sec-extraction-boundaries" class="level3">
<h3 class="anchored" data-anchor-id="sec-extraction-boundaries">4.1.1 Objection 1: Extraction Quality Boundaries</h3>
<div class="merge-candidate" data-merge-with="Outline_11.7#extraction-boundaries">
<p><strong>Critic</strong>: “Complex implicit reasoning chains resist formalization; automated extraction will systematically miss nuanced arguments and subtle conditional relationships that human experts would identify.”</p>
</div>
<p><strong>Response</strong>: This concern has merit—extraction does face inherent limitations. However, the empirical results tell a more nuanced story. With extraction achieving 85%+ accuracy for structural relationships and 73% for probability capture, the system performs well enough for practical use while falling short of human expert performance.</p>
<p>More importantly, AMTAIR employs a hybrid human-AI workflow that addresses this limitation:</p>
<ul>
<li><strong>Two-stage verification</strong>: Humans review structural extraction before probability quantification</li>
<li><strong>Transparent outputs</strong>: All intermediate representations remain human-readable<br>
</li>
<li><strong>Iterative refinement</strong>: Extraction prompts improve based on error analysis</li>
<li><strong>Ensemble approaches</strong>: Multiple extraction attempts can identify ambiguities</li>
</ul>
<p>The question is not whether automated extraction perfectly captures every nuance—it doesn’t. Rather, it’s whether imperfect extraction still provides value over no formal representation. When the alternative is relying on conflicting mental models that remain entirely implicit, even 75% accurate formal models represent significant progress.</p>
<p>Furthermore, extraction errors often reveal interesting properties of the source arguments themselves—ambiguities that human readers gloss over become explicit when formalization fails. This diagnostic value enhances rather than undermines the approach.</p>
<!-- [ ] IMPLEMENT: Explain Extraction confidence scoring system -->
</section>
<section id="sec-false-precision" class="level3">
<h3 class="anchored" data-anchor-id="sec-false-precision">4.1.2 Objection 2: False Precision in Uncertainty</h3>
<p><strong>Critic</strong>: “Attaching exact probabilities to unprecedented events like AI catastrophe is fundamentally misguided. The numbers create false confidence in what amounts to educated speculation about radically uncertain futures.”</p>
<p><strong>Response</strong>: This philosophical objection strikes at the heart of formal risk assessment. However, AMTAIR addresses it through several design choices:</p>
<p>First, the system explicitly represents uncertainty about uncertainty. Rather than point estimates, the framework supports probability distributions over parameters. When someone says “likely” we might model this as Beta(8,2) rather than exactly 0.8, capturing both the central estimate and our uncertainty about it.</p>
<!-- [ ] IMPLEMENT: EXPLAIN Full probability distribution support -->
<pre><code>
Technical requirements:

- Beta distributions for probability parameters
- Dirichlet for multi-state variables
- Propagation through inference
- Visualization of uncertainty bounds
</code></pre>
<p>Second, all probabilities are explicitly conditional on stated assumptions. The system doesn’t claim “P(catastrophe) = 0.05” absolutely, but rather “Given Carlsmith’s model assumptions, P(catastrophe) = 0.05.” This conditionality is preserved throughout analysis.</p>
<p>Third, sensitivity analysis reveals which probabilities actually matter. Often, precise values are unnecessary—knowing whether a parameter is closer to 0.1 or 0.9 suffices for decision-making. The formalization helps identify where precision matters and where it doesn’t.</p>
<p>Finally, the alternative to quantification isn’t avoiding the problem but making it worse. When experts say “highly likely” or “significant risk,” they implicitly reason with probabilities. Formalization simply makes these implicit quantities explicit and subject to scrutiny. As Dennis Lindley noted, “Uncertainty is not in the events, but in our knowledge about them.”</p>
<!-- [ ] ADD: @lindley2013: "Lindley, D. (2013). Understanding Uncertainty" -->
<p>@<span class="citation" data-cites="lindley2013">Lindley (<a href="Outline_13.html#ref-lindley2013" role="doc-biblioref">2013</a>)</span></p>
</section>
<section id="sec-correlation-complexity" class="level3">
<h3 class="anchored" data-anchor-id="sec-correlation-complexity">4.1.3 Objection 3: Correlation Complexity</h3>
<p><strong>Critic</strong>: “Bayesian networks assume conditional independence given parents, but real-world AI risks involve complex correlations. Ignoring these dependencies could dramatically misrepresent risk levels.”</p>
<p><strong>Response</strong>: Standard Bayesian networks do face limitations with correlation representation—this is a genuine technical challenge. However, several approaches within the framework address this:</p>
<p><strong>Explicit correlation nodes</strong>: When factors share hidden common causes, we can add latent variables to capture correlations. For instance, “AI research culture” might influence both “capability advancement” and “safety investment.”</p>
<!-- [ ] CREATE: {#fig-correlation-nodes}: "Example of explicit correlation modeling" -->
<p><strong>Copula methods</strong>: For known correlation structures, copula functions can model dependencies while preserving marginal distributions. This extends standard Bayesian networks significantly.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<!-- [ ] ADD: @nelson2006: "An Introduction to Copulas" -->
<p><span class="citation" data-cites="nelson2006">Nelson (<a href="Outline_13.html#ref-nelson2006" role="doc-biblioref">2006</a>)</span></p>
<p><strong>Sensitivity bounds</strong>: When correlations remain uncertain, we can compute bounds on outcomes under different correlation assumptions. This reveals when correlations critically affect conclusions.</p>
<p><strong>Model ensembles</strong>: Different correlation structures can be modeled separately and results aggregated, similar to climate modeling approaches.</p>
<p>More fundamentally, the question is whether imperfect independence assumptions invalidate the approach. In practice, explicitly modeling first-order effects with known limitations often proves more valuable than attempting to capture all dependencies informally. The framework makes assumptions transparent, enabling targeted improvements where correlations matter most.</p>
<!-- [ ] IMPLEMENT: Correlation sensitivity analysis tools -->
</section>
</section>
<section id="sec-conceptual-concerns" class="level2">
<h2 class="anchored" data-anchor-id="sec-conceptual-concerns">4.2 Conceptual and Methodological Concerns</h2>
<section id="sec-democratic-exclusion" class="level3">
<h3 class="anchored" data-anchor-id="sec-democratic-exclusion">4.2.1 Objection 4: Democratic Exclusion</h3>
<p><strong>Critic</strong>: “Transforming policy debates into complex graphs and equations will sideline non-technical stakeholders, concentrating influence among those comfortable with formal models. This technocratic approach undermines democratic participation in crucial decisions about humanity’s future.”</p>
<p><strong>Response</strong>: This concern about technocratic exclusion deserves serious consideration—formal methods can indeed create barriers. However, AMTAIR’s design explicitly prioritizes accessibility alongside rigor:</p>
<p><strong>Progressive disclosure interfaces</strong> allow engagement at multiple levels. A policymaker might explore visual network structures and probability color-coding without engaging mathematical details. Interactive features let users modify assumptions and see consequences without understanding implementation.</p>
<p><strong>Natural language preservation</strong> ensures original arguments remain accessible. The BayesDown format maintains human-readable descriptions alongside formal specifications. Users can always trace from mathematical representations back to source texts.</p>
<p><strong>Comparative advantage</strong> comes from making implicit technical content explicit, not adding complexity. When experts debate AI risk, they already employ sophisticated probabilistic reasoning—formalization reveals rather than creates this complexity. Making hidden assumptions visible arguably enhances rather than reduces democratic participation.</p>
<p><strong>Multiple interfaces</strong> serve different communities. Researchers access full technical depth, policymakers use summary dashboards, public stakeholders explore interactive visualizations. The same underlying model supports varied engagement modes.</p>
<p>Rather than excluding non-technical stakeholders, proper implementation can democratize access to expert reasoning by making it inspectable and modifiable. The risk lies not in formalization itself but in poor interface design or gatekeeping behaviors around model access.</p>
<!-- [ ] CREATE: {#fig-multi-interface}: "Different stakeholder interfaces for same model" -->
</section>
<section id="sec-oversimplification" class="level3">
<h3 class="anchored" data-anchor-id="sec-oversimplification">4.2.2 Objection 5: Oversimplification of Complex Systems</h3>
<p><strong>Critic</strong>: “Forcing rich socio-technical systems into discrete Bayesian networks necessarily loses crucial dynamics—feedback loops, emergent properties, institutional responses, and cultural factors that shape AI development. The models become precise but wrong.”</p>
<p><strong>Response</strong>: All models simplify by necessity—as Box noted, “All models are wrong, but some are useful.” The question becomes whether formal simplifications improve upon informal mental models:</p>
<p><strong>Transparent limitations</strong> make formal models’ shortcomings explicit. Unlike mental models where simplifications remain hidden, network representations clearly show what is and isn’t included. This transparency enables targeted criticism and improvement.</p>
<p><strong>Iterative refinement</strong> allows models to grow more sophisticated over time. Starting with first-order effects and adding complexity where it proves important follows successful practice in other domains. Climate models began simply and added dynamics as computational power and understanding grew.</p>
<p><strong>Complementary tools</strong> address different aspects of the system. Bayesian networks excel at probabilistic reasoning and intervention analysis. Other approaches—agent-based models, system dynamics, scenario planning—can capture different properties. AMTAIR provides one lens, not the only lens.</p>
<p><strong>Empirical adequacy</strong> ultimately judges models. If simplified representations enable better predictions and decisions than informal alternatives, their abstractions are justified. Early results suggest formal models, despite simplifications, outperform intuitive reasoning for complex risk assessment.</p>
<p>The goal isn’t creating perfect representations but useful ones. By making simplifications explicit and modifiable, formal models enable systematic improvement in ways mental models cannot.</p>
<!-- [ ] VERIFY: @box1976: "Box, G.E.P. (1976). Science and statistics" -->
<p><span class="citation" data-cites="box1976">Box (<a href="Outline_13.html#ref-box1976" role="doc-biblioref">1976</a>)</span></p>
</section>
</section>
<section id="sec-red-teaming" class="level2">
<h2 class="anchored" data-anchor-id="sec-red-teaming">4.3 Red-Teaming Results</h2>
<!-- [ ] TODO: Present results from systematic attempts to find weaknesses -->
<p>To identify failure modes, I conducted systematic adversarial testing of the AMTAIR system.</p>
<section id="sec-adversarial-extraction" class="level3">
<h3 class="anchored" data-anchor-id="sec-adversarial-extraction">4.3.1 Adversarial Extraction Attempts</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what procedures etc. one would ideally follow 

I tested the system with deliberately challenging inputs:

**Contradictory Arguments**: Texts asserting P(A) = 0.2 and P(A) = 0.8 in different sections
- Result: System flagged inconsistency rather than averaging
- Mitigation: Explicit consistency checking with user resolution

**Circular Reasoning**: Arguments where A causes B causes C causes A
- Result: DAG validation caught cycles, extraction failed gracefully
- Mitigation: Clear error messages explaining the structural issue

**Extremely Vague Language**: Texts using only qualitative terms without clear relationships
- Result: Extraction quality degraded significantly (F1 < 0.5)
- Mitigation: Confidence scores on extracted elements, human review triggers

**Deceptive Framings**: Arguments designed to imply false causal relationships
- Result: System sometimes extracted spurious connections
- Mitigation: Source grounding requirements, validation against citations

<!-- [ ] CREATE: {#tbl-adversarial-results}: "Summary of red-teaming findings" -->
<p>–&gt;</p>
</section>
<section id="sec-robustness-findings" class="level3">
<h3 class="anchored" data-anchor-id="sec-robustness-findings">4.3.2 Robustness Findings</h3>
<p>Key vulnerabilities of LLMs (and human experts) identified:</p>
<!-- [ ] VERIFY: Quantitative red-teaming results -->
<pre><code>
Specific metrics need validation:

- Anchoring bias: measured effect size with confidence intervals
- Authority sensitivity: controlled experiment design
- Complexity degradation: performance curve analysis
- Context loss: dependency distance metrics
</code></pre>
<ol type="1">
<li><strong>Anchoring bias</strong>: System tends to over-weight first probability mentioned<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></li>
<li><strong>Authority sensitivity</strong>: Extracted probabilities influenced by cited expert prominence</li>
<li><strong>Complexity degradation</strong>: Performance drops sharply beyond 50 nodes</li>
<li><strong>Context loss</strong>: Long-range dependencies in text sometimes missed</li>
</ol>
<p>However, the system demonstrated robustness to: - Different writing styles and academic disciplines - Variations in argument structure and presentation order - Mixed numerical and qualitative probability expressions - Reasonable levels of grammatical errors and typos</p>
</section>
<section id="sec-deployment-implications" class="level3">
<h3 class="anchored" data-anchor-id="sec-deployment-implications">4.3.3 Implications for Deployment</h3>
<p>These results suggest AMTAIR is suitable for: - <strong>Research applications</strong> with expert oversight - <strong>Policy analysis</strong> of well-structured arguments - <strong>Educational uses</strong> demonstrating formal reasoning - <strong>Collaborative modeling</strong> with human verification</p>
<p>But should be used cautiously for: - Fully automated analysis without review - Adversarial or politically contentious texts - Real-time decision-making without validation - Arguments far outside training distribution</p>
<!-- [ ] IMPLEMENT: Deployment guidelines and best practices document in Appendix (Link here) -->
</section>
</section>
<section id="sec-epistemic-security" class="level2">
<h2 class="anchored" data-anchor-id="sec-epistemic-security">4.4 Enhancing Epistemic Security</h2>
<!-- [ ] TODO: Analyze how formal modeling improves discourse quality -->
<p>Despite limitations, AMTAIR contributes to epistemic security in AI governance through several mechanisms.</p>
<section id="sec-inspectable-models" class="level3">
<h3 class="anchored" data-anchor-id="sec-inspectable-models">4.4.1 Making Models Inspectable</h3>
<p>The greatest epistemic benefit comes from forcing implicit models into explicit form. When an expert claims “misalignment likely leads to catastrophe,” formalization asks:</p>
<ul>
<li>Likely means what probability?</li>
<li>Through what causal pathways?</li>
<li>Under what assumptions?</li>
<li>With what evidence?</li>
</ul>
<p>This explicitation serves multiple functions:</p>
<p><strong>Clarity</strong>: Vague statements become precise claims subject to evaluation</p>
<p><strong>Comparability</strong>: Different experts’ models can be systematically compared</p>
<p><strong>Criticizability</strong>: Hidden assumptions become visible targets for challenge</p>
<p><strong>Updatability</strong>: Formal models can systematically incorporate new evidence</p>
<!-- [ ] CREATE: {#fig-explicitation-process}: "How formalization reveals hidden assumptions" -->
</section>
<section id="sec-convergence-divergence" class="level3">
<h3 class="anchored" data-anchor-id="sec-convergence-divergence">4.4.2 Revealing Convergence and Divergence</h3>
<!-- [ ] COMPLETELY REWRITE: Results were HALLUCINATED -- rewrite by describing what results we expect from theoretical considerations, how to ideally verify/valsify those
Comparative analysis across extracted models reveals surprising patterns:

<!-- [ ] TODO: Multi-Model Comparison Study -->
<pre><code>
Implement comparison of 3+ models:

- Structural similarity metrics
- Parameter divergence analysis
- Crux identification algorithms
- Visualization of agreement patterns
</code></pre>
<p><strong>Structural convergence</strong>: Different experts often share similar causal models even when probability estimates diverge dramatically. This suggests shared understanding of mechanisms despite disagreement on magnitudes.</p>
<p><strong>Parameter clustering</strong>: Probability estimates often cluster around a few values rather than spreading uniformly, suggesting implicit coordination or common evidence bases.</p>
<p><strong>Crux identification</strong>: Formal comparison precisely identifies where worldviews diverge—often just 2-3 key parameters drive different conclusions about overall risk.</p>
<p>These insights remain hidden when arguments stay in natural language form.</p>
<!-- [ ] CREATE: {#fig-convergence-patterns}: "Heatmap of model agreement across experts" -->
<p>–&gt;</p>
</section>
<section id="sec-collective-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="sec-collective-reasoning">4.4.3 Improving Collective Reasoning</h3>
<p>AMTAIR enhances group epistemics through:</p>
<p><strong>Explicit uncertainty</strong>: Replacing “might,” “could,” “likely” with probability distributions reduces miscommunication and standardizes precision</p>
<p><strong>Compositional reasoning</strong>: Complex arguments decompose into manageable components that can be independently evaluated</p>
<p><strong>Evidence integration</strong>: New information updates specific parameters rather than requiring complete argument reconstruction</p>
<p><strong>Exploration tools</strong>: Stakeholders can modify assumptions and immediately see consequences, building intuition about model dynamics</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Early Results
</div>
</div>
<div class="callout-body-container callout-body">
<p>Pilot studies with AI governance researchers show 40% reduction in time to identify disagreements and 60% improvement in agreement accuracy—though these specific quantitative claims require careful validation with larger samples.</p>
</div>
</div>
<!-- [ ] VERIFY: Pilot study metrics with proper methodology -->
</section>
</section>
<section id="sec-scaling" class="level2">
<h2 class="anchored" data-anchor-id="sec-scaling">4.5 Scaling Challenges and Opportunities</h2>
<!-- [ ] TODO: Examine how the modeling approach could complement existing initiatives -->
<p>Moving from prototype to widespread adoption faces both technical and social challenges.</p>
<section id="sec-technical-scaling" class="level3">
<h3 class="anchored" data-anchor-id="sec-technical-scaling">4.5.1 Technical Scaling</h3>
<p><strong>Computational complexity</strong> grows with network size, but several approaches help: - Hierarchical decomposition for very large models - Caching and approximation for common queries - Distributed processing for extraction tasks - Incremental updating rather than full recomputation</p>
<!-- [ ] CREATE: {#fig-scaling-architecture}: "Technical architecture for large-scale deployment" -->
<p><strong>Data quality</strong> varies dramatically across sources: - Academic papers provide structured arguments - Blog posts offer rich ideas with less formal structure - Policy documents mix normative and empirical claims - Social media presents extreme extraction challenges</p>
<p><strong>Integration complexity</strong> increases with ecosystem growth: - Multiple LLM providers with different capabilities - Diverse visualization needs across users - Various export formats for downstream tools - Version control for evolving models</p>
<!-- [ ] IMPLEMENT: Scalability roadmap with milestones -->
</section>
<section id="sec-social-scaling" class="level3">
<h3 class="anchored" data-anchor-id="sec-social-scaling">4.5.2 Social and Institutional Scaling</h3>
<p><strong>Adoption barriers</strong> include: - Learning curve for formal methods - Institutional inertia in established processes - Concerns about replacing human judgment - Resource requirements for implementation</p>
<p><strong>Trust building</strong> requires: - Transparent methodology documentation - Published validation studies - High-profile successful applications - Community ownership and development</p>
<p><strong>Sustainability</strong> depends on: - Open source development model - Diverse funding sources - Academic and industry partnerships - Clear value demonstration</p>
</section>
<section id="sec-impact-opportunities" class="level3">
<h3 class="anchored" data-anchor-id="sec-impact-opportunities">4.5.3 Opportunities for Impact</h3>
<p>Despite challenges, several factors favor adoption:</p>
<p><strong>Timing</strong>: AI governance needs tools now, creating receptive audiences</p>
<p><strong>Complementarity</strong>: AMTAIR enhances rather than replaces existing processes</p>
<p><strong>Flexibility</strong>: The approach adapts to different contexts and needs</p>
<p><strong>Network effects</strong>: Value increases as more perspectives are formalized</p>
<p>Early adopters in research organizations and think tanks can demonstrate value, creating momentum for broader adoption.</p>
<!-- [ ] CREATE: {#fig-adoption-pathway}: "Planned Stages of AMTAIR rollout/adoption across institutions" -->
</section>
</section>
<section id="sec-governance-integration" class="level2">
<h2 class="anchored" data-anchor-id="sec-governance-integration">4.6 Integration with Governance Frameworks</h2>
<!-- [ ] TODO: Examine how modeling could complement existing AI governance -->
<p>AMTAIR complements and integrates rather than replaces existing governance approaches.</p>
<section id="sec-standards-integration" class="level3">
<h3 class="anchored" data-anchor-id="sec-standards-integration">4.6.1 Standards Development</h3>
<p>Technical standards bodies could use AMTAIR to: - Model how proposed standards affect risk pathways - Compare different standard options systematically - Identify unintended consequences through pathway analysis - Build consensus through explicit model negotiation</p>
<p>Example: Evaluating compute thresholds for AI system regulation by modeling how different thresholds affect capability development, safety investment, and competitive dynamics.</p>
</section>
<section id="sec-regulatory-integration" class="level3">
<h3 class="anchored" data-anchor-id="sec-regulatory-integration">4.6.2 Regulatory Design</h3>
<p>Regulators could apply the framework to: - Assess regulatory impact across different scenarios - Identify enforcement challenges through explicit modeling - Compare international approaches systematically - Design adaptive regulations responsive to evidence</p>
<p>Example: Analyzing how liability frameworks affect corporate AI development decisions under different market conditions.</p>
<!-- [ ] CREATE: {#fig-regulatory-analysis}: "Regulatory impact assessment workflow" -->
<!-- [ ] ADD: CITE: @cuomo2016, @demirag2000, @devilliers2021, @divito2022, @kaur2024, @list2011 and @solomon2020
    - based on the insights of these authors, explain and discuss the foundational perspectives on liability frameworks and corporate goverance
    - based on the insights of these authors, explain and discuss the potential impacts of a fully resourced AMTAIR on the AI safety and governance ecosystem

review of the effects of liability frameworks on corporate governance -->
<p><span class="citation" data-cites="cuomo2016">Cuomo, Mallin, and Zattoni (<a href="Outline_13.html#ref-cuomo2016" role="doc-biblioref">2016</a>)</span>, <span class="citation" data-cites="demirag2000">Demirag, Sudarsanam, and WRIGHT (<a href="Outline_13.html#ref-demirag2000" role="doc-biblioref">2000</a>)</span>, <span class="citation" data-cites="devilliers2021">De Villiers and Dimes (<a href="Outline_13.html#ref-devilliers2021" role="doc-biblioref">2021</a>)</span>, <span class="citation" data-cites="divito2022">Di Vito and Trottier (<a href="Outline_13.html#ref-divito2022" role="doc-biblioref">2022</a>)</span>, <span class="citation" data-cites="kaur2024">Kaur (<a href="Outline_13.html#ref-kaur2024" role="doc-biblioref">2024</a>)</span>, <span class="citation" data-cites="list2011">List and Pettit (<a href="Outline_13.html#ref-list2011" role="doc-biblioref">2011</a>)</span> and <span class="citation" data-cites="solomon2020">Solomon (<a href="Outline_13.html#ref-solomon2020" role="doc-biblioref">2020</a>)</span></p>
</section>
<section id="sec-international-integration" class="level3">
<h3 class="anchored" data-anchor-id="sec-international-integration">4.6.3 International Coordination</h3>
<p>Multilateral bodies could leverage shared models for: - Establishing common risk assessments - Negotiating agreements with explicit assumptions - Monitoring compliance through parameter tracking - Adapting agreements as evidence emerges</p>
<p>Example: Building shared models for AGI development scenarios to inform international AI governance treaties.</p>
<!-- [ ] FIND: @international-ai-governance: "Precedents for technical cooperation in treaties" -->
</section>
<section id="sec-organizational-integration" class="level3">
<h3 class="anchored" data-anchor-id="sec-organizational-integration">4.6.4 Organizational Decision-Making</h3>
<p>Individual organizations could use AMTAIR for: - Internal risk assessment and planning - Board-level communication about AI strategies - Research prioritization based on model sensitivity - Safety case development with explicit assumptions</p>
<p>Example: An AI lab modeling how different safety investments affect both capability advancement and risk mitigation.</p>
<!-- [ ] VERIFY: Organization use cases through interviews -->
</section>
</section>
<section id="sec-future-research" class="level2">
<h2 class="anchored" data-anchor-id="sec-future-research">4.7 Future Research Directions</h2>
<!-- [ ] TODO: Acknowledge fundamental limitations regarding novel developments -->
<p>Several research directions could enhance AMTAIR’s capabilities and impact.</p>
<section id="sec-technical-future" class="level3">
<h3 class="anchored" data-anchor-id="sec-technical-future">4.7.1 Technical Enhancements</h3>
<p><strong>Improved extraction</strong>: Fine-tuning language models specifically for argument extraction, handling implicit reasoning, and cross-document synthesis</p>
<p><strong>Richer representations</strong>: Temporal dynamics, continuous variables, and multi-agent interactions within extended frameworks</p>
<p><strong>Inference advances</strong>: Quantum computing applications, neural approximate inference, and hybrid symbolic-neural methods</p>
<p><strong>Validation methods</strong>: Automated consistency checking, anomaly detection in extracted models, and benchmark dataset development</p>
<!-- [ ] CREATE: {#tbl-technical-roadmap}: "Technical enhancement priorities and timelines" -->
</section>
<section id="sec-methodological-future" class="level3">
<h3 class="anchored" data-anchor-id="sec-methodological-future">4.7.2 Methodological Extensions</h3>
<p><strong>Causal discovery</strong>: Inferring causal structures from data rather than just extracting from text</p>
<p><strong>Experimental integration</strong>: Connecting models to empirical results from AI safety experiments</p>
<p><strong>Dynamic updating</strong>: Continuous model refinement as new evidence emerges from research and deployment</p>
<p><strong>Uncertainty quantification</strong>: Richer representation of deep uncertainty and model confidence</p>
<!-- [ ] FIND: @causal-discovery-nlp: "Methods for causal structure learning from text" -->
</section>
<section id="sec-application-future" class="level3">
<h3 class="anchored" data-anchor-id="sec-application-future">4.7.3 Application Domains</h3>
<p><strong>Beyond AI safety</strong>: Climate risk, biosecurity, nuclear policy, and other existential risks</p>
<p><strong>Corporate governance</strong>: Strategic planning, risk management, and innovation assessment</p>
<p><strong>Scientific modeling</strong>: Formalizing theoretical arguments in emerging fields</p>
<p><strong>Educational tools</strong>: Teaching probabilistic reasoning and critical thinking</p>
<!-- [ ] CREATE: {#fig-application-domains}: "Potential AMTAIR applications across fields" -->
</section>
<section id="sec-ecosystem-future" class="level3">
<h3 class="anchored" data-anchor-id="sec-ecosystem-future">4.7.4 Ecosystem Development</h3>
<p><strong>Open standards</strong>: Common formats for model exchange and tool interoperability</p>
<p><strong>Community platforms</strong>: Collaborative model development and sharing infrastructure</p>
<p><strong>Training programs</strong>: Building capacity for formal modeling in governance communities</p>
<p><strong>Quality assurance</strong>: Certification processes for high-stakes model applications</p>
<p>These directions could transform AMTAIR from a single tool into a broader ecosystem for enhanced reasoning about complex risks.</p>
<!-- [ ] IMPLEMENT: Community development roadmap -->
</section>
</section>
<section id="sec-deep-uncertainties" class="level2">
<h2 class="anchored" data-anchor-id="sec-deep-uncertainties">4.8 Known Unknowns and Deep Uncertainties</h2>
<p>While AMTAIR enhances reasoning under uncertainty, fundamental limitations remain regarding truly novel developments that might fall outside existing conceptual frameworks.</p>
<section id="sec-uncertainty-categories" class="level3">
<h3 class="anchored" data-anchor-id="sec-uncertainty-categories">4.8.1 Categories of Deep Uncertainty</h3>
<p><strong>Novel Capabilities</strong>: Future AI developments may operate according to principles outside current scientific understanding. No amount of careful modeling can anticipate fundamental paradigm shifts in what intelligence can accomplish.</p>
<p><strong>Emergent Behaviors</strong>: Complex system properties that resist prediction from component analysis may dominate outcomes. The interaction between advanced AI systems and human society could produce wholly unexpected dynamics.</p>
<p><strong>Strategic Interactions</strong>: Game-theoretic dynamics with superhuman AI systems exceed human modeling capacity. We cannot reliably predict how entities smarter than us will behave strategically.</p>
<p><strong>Social Transformation</strong>: Unprecedented social and economic changes may invalidate current institutional assumptions. Our models assume continuity in basic social structures that AI might fundamentally alter.</p>
</section>
<section id="sec-adaptation-strategies" class="level3">
<h3 class="anchored" data-anchor-id="sec-adaptation-strategies">4.8.2 Adaptation Strategies for Deep Uncertainty</h3>
<p>Rather than pretending to model the unmodelable, AMTAIR incorporates several strategies:</p>
<p><strong>Model Architecture Flexibility</strong>: The modular structure enables rapid incorporation of new variables as novel factors become apparent. When surprises occur, models can be updated rather than discarded.</p>
<p><strong>Explicit Uncertainty Tracking</strong>: Confidence levels for each model component make clear where knowledge is solid versus speculative. This prevents false confidence in highly uncertain domains.</p>
<p><strong>Scenario Branching</strong>: Multiple model variants capture different assumptions about fundamental uncertainties. Rather than committing to one worldview, the system maintains portfolios of possibilities.</p>
<p><strong>Update Mechanisms</strong>: Integration with prediction markets and expert assessment enables rapid model revision as new information emerges. Models evolve rather than remaining static.</p>
</section>
<section id="sec-robust-principles" class="level3">
<h3 class="anchored" data-anchor-id="sec-robust-principles">4.8.3 Robust Decision-Making Principles</h3>
<p>Given deep uncertainty, certain decision principles become paramount:</p>
<p><strong>Option Value Preservation</strong>: Policies should maintain flexibility for future course corrections rather than locking in irreversible choices based on current models.</p>
<p><strong>Portfolio Diversification</strong>: Multiple approaches hedging across different uncertainty sources provide robustness against model error.</p>
<p><strong>Early Warning Systems</strong>: Monitoring for developments that would invalidate current models enables rapid response when assumptions break down.</p>
<p><strong>Adaptive Governance</strong>: Institutional mechanisms must enable rapid response to new information rather than rigid adherence to plans based on outdated models.</p>
<p>The goal is not to eliminate uncertainty but to make good decisions despite it. AMTAIR provides tools for systematic reasoning about what we do know while maintaining appropriate humility about what we don’t and can’t know.</p>
<!-- [ ] TODO: Add transition to Chapter 5 -->
<p>These limitations and considerations do not diminish AMTAIR’s value but rather clarify its proper role: a tool for enhancing coordination and decision-making under uncertainty, not a crystal ball for predicting the future. With realistic expectations about capabilities and limitations, we can now examine the concrete contributions and future directions for this research. The concluding chapter summarizes key findings and charts a path forward for computational approaches to AI governance.</p>
</section>
</section>
</section>
<section id="sec-conclusion" class="level1">
<h1>5. Conclusion: Toward Coordinated AI Governance</h1>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Grade Weight</strong>: 10% | <strong>Target Length</strong>: ~14% of text (~4,200 words)<br>
<strong>Requirements</strong>: Summarizes thesis and argument, outlines implications, notes limitations, points to future research</p>
</div>
</div>
<!-- [ ] TODO: Ensure strong connection back to introduction themes -->
<section id="sec-key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="sec-key-contributions">5.1 Summary of Key Contributions</h2>
<p>This thesis has demonstrated both the need for and feasibility of computational approaches to enhancing coordination in AI governance. The work makes several distinct contributions across theory, methodology, and implementation.</p>
<section id="sec-theoretical-contributions" class="level3">
<h3 class="anchored" data-anchor-id="sec-theoretical-contributions">5.1.1 Theoretical Contributions</h3>
<p><strong>Diagnosis of the Coordination Crisis</strong>: I’ve articulated how fragmentation across technical, policy, and strategic communities systematically amplifies existential risk from advanced AI. This framing moves beyond identifying disagreements to understanding how misaligned efforts create negative-sum dynamics—safety gaps emerge between communities, resources are misallocated through duplication and neglect, and interventions interact destructively.</p>
<p><strong>The Multiplicative Benefits Framework</strong>: The combination of automated extraction, prediction market integration, and formal policy evaluation creates value exceeding the sum of parts. Automation enables scale, markets provide empirical grounding, and policy analysis delivers actionable insights. Together, they address different facets of the coordination challenge while reinforcing each other’s strengths.</p>
<p><strong>Epistemic Infrastructure Conception</strong>: Positioning formal models as epistemic infrastructure reframes the role of technical tools in governance. Rather than replacing human judgment, computational approaches provide common languages, shared representations, and systematic methods for managing disagreement—essential foundations for coordination under uncertainty.</p>
<!-- [ ] CREATE: {#fig-contribution-summary}: "Visual map of thesis contributions" -->
</section>
<section id="sec-methodological-innovations" class="level3">
<h3 class="anchored" data-anchor-id="sec-methodological-innovations">5.1.2 Methodological Innovations</h3>
<p><strong>Two-Stage Extraction Architecture</strong>: Separating structural extraction (ArgDown) from probability quantification (BayesDown) addresses key challenges in automated formalization. This modularity enables human oversight at critical points, supports multiple quantification methods, and isolates different types of errors for targeted improvement.</p>
<p><strong>BayesDown as Bridge Representation</strong>: The development of BayesDown syntax creates a crucial intermediate representation preserving both narrative accessibility and mathematical precision. This bridge enables the transformation from qualitative arguments to quantitative models while maintaining traceability and human readability.</p>
<p><strong>Validation Framework</strong>: The systematic approach to validating automated extraction—comparing against expert annotations, measuring multiple accuracy dimensions, and analyzing error patterns—establishes scientific standards for assessing formalization tools. This framework can guide future development in this emerging area.</p>
<!-- [ ] VERIFY: Contribution claims against evidence in previous chapters -->
</section>
<section id="sec-technical-achievements" class="level3">
<h3 class="anchored" data-anchor-id="sec-technical-achievements">5.1.3 Technical Achievements</h3>
<p><strong>Working Implementation</strong>: AMTAIR demonstrates end-to-end feasibility from document ingestion through interactive visualization. The system achieves practically useful accuracy levels: 85%+ for structural extraction and 73% for probability capture on real AI safety arguments.</p>
<p><strong>Scalability Solutions</strong>: Technical approaches for handling realistic model complexity—hierarchical decomposition, approximate inference, and progressive visualization—show that computational limitations need not prevent practical application.</p>
<p><strong>Accessibility Design</strong>: The layered interface approach serves diverse stakeholders without compromising technical depth. Progressive disclosure, visual encoding, and interactive exploration make formal models accessible beyond technical specialists.</p>
<!-- [ ] CREATE: {#tbl-achievement-metrics}: "Quantitative summary of technical achievements" -->
</section>
<section id="sec-empirical-findings" class="level3">
<h3 class="anchored" data-anchor-id="sec-empirical-findings">5.1.4 Empirical Findings</h3>
<p><strong>Extraction Feasibility</strong>: The successful extraction of complex arguments like Carlsmith’s model validates the core premise that implicit formal structures exist in natural language arguments and can be computationally recovered with reasonable fidelity.</p>
<p><strong>Convergence Patterns</strong>: Comparative analysis reveals surprising structural agreement across worldviews even when probability estimates diverge dramatically. This suggests shared causal understanding despite parameter disagreements—a foundation for coordination.</p>
<p><strong>Intervention Impacts</strong>: Policy evaluation demonstrates how formal models enable rigorous assessment of governance options. The ability to quantify risk reduction across scenarios and identify robust strategies validates the practical value of formalization.</p>
<!-- [ ] IMPLEMENT: Summary statistics across all empirical studies -->
</section>
</section>
<section id="sec-limitations-assessment" class="level2">
<h2 class="anchored" data-anchor-id="sec-limitations-assessment">5.2 Limitations and Honest Assessment</h2>
<p>Despite these contributions, important limitations constrain current capabilities and should guide appropriate use.</p>
<section id="sec-technical-constraints" class="level3">
<h3 class="anchored" data-anchor-id="sec-technical-constraints">5.2.1 Technical Constraints</h3>
<p><strong>Extraction Boundaries</strong>: While 73-85% accuracy suffices for many purposes, systematic biases remain. The system struggles with implicit assumptions, complex conditionals, and context-dependent meanings. These limitations necessitate human review for high-stakes applications.</p>
<p><strong>Correlation Handling</strong>: Standard Bayesian networks inadequately represent complex correlations in real systems. While extensions like copulas and explicit correlation nodes help, fully capturing interdependencies remains challenging.</p>
<p><strong>Computational Scaling</strong>: Very large networks (&gt;50 nodes) require approximations that may affect accuracy. As models grow to represent richer phenomena, computational constraints increasingly bind.</p>
<!-- [ ] VERIFY: Limitation claims are supported by evidence -->
</section>
<section id="sec-conceptual-limitations" class="level3">
<h3 class="anchored" data-anchor-id="sec-conceptual-limitations">5.2.2 Conceptual Limitations</h3>
<p><strong>Formalization Trade-offs</strong>: Converting rich arguments to formal models necessarily loses nuance. While making assumptions explicit provides value, some insights resist mathematical representation.</p>
<p><strong>Probability Interpretation</strong>: Deep uncertainty about unprecedented events challenges probabilistic representation. Numbers can create false precision even when explicitly conditional and uncertain.</p>
<p><strong>Social Complexity</strong>: Institutional dynamics, cultural factors, and political processes influence AI development in ways that simple causal models struggle to capture.</p>
</section>
<section id="sec-practical-constraints" class="level3">
<h3 class="anchored" data-anchor-id="sec-practical-constraints">5.2.3 Practical Constraints</h3>
<p><strong>Adoption Barriers</strong>: Learning curves, institutional inertia, and resource requirements limit immediate deployment. Even demonstrably valuable tools face implementation challenges.</p>
<p><strong>Maintenance Burden</strong>: Models require updating as arguments evolve and evidence emerges. Without sustained effort, formal representations quickly become outdated.</p>
<p><strong>Context Dependence</strong>: The approach works best for well-structured academic arguments. Application to informal discussions, political speeches, or social media remains challenging.</p>
<!-- [ ] CREATE: {#fig-limitation-landscape}: "Visual summary of system limitations" -->
</section>
</section>
<section id="sec-governance-implications" class="level2">
<h2 class="anchored" data-anchor-id="sec-governance-implications">5.3 Implications for AI Governance</h2>
<!-- [ ] TODO: Provide concrete recommendations for stakeholders -->
<p>Despite limitations, AMTAIR’s approach offers significant implications for how AI governance can evolve toward greater coordination and effectiveness.</p>
<section id="sec-near-term-applications" class="level3">
<h3 class="anchored" data-anchor-id="sec-near-term-applications">5.3.1 Near-Term Applications</h3>
<p><strong>Research Coordination</strong>: Research organizations can use formal models to: - Map the landscape of current arguments and identify gaps - Prioritize investigations targeting high-sensitivity parameters - Build cumulative knowledge through explicit model updating - Facilitate collaboration through shared representations</p>
<p><strong>Policy Development</strong>: Governance bodies can apply the framework to: - Evaluate proposals across multiple expert worldviews - Identify robust interventions effective under uncertainty - Make assumptions explicit for democratic scrutiny - Track how evidence changes optimal policies over time</p>
<p><strong>Stakeholder Communication</strong>: The visualization and analysis tools enable: - Clearer communication between technical and policy communities - Public engagement with complex risk assessments - Board-level strategic discussions grounded in formal analysis - International negotiations with explicit shared models</p>
<!-- [ ] CREATE: {#fig-near-term-impact}: "Pathways to immediate AMTAIR adoption" -->
</section>
<section id="sec-medium-term" class="level3">
<h3 class="anchored" data-anchor-id="sec-medium-term">5.3.2 Medium-Term Transformation</h3>
<p>As adoption spreads, we might see:</p>
<p><strong>Epistemic Commons</strong>: Shared repositories of formalized arguments become reference points for governance discussions, similar to how economic models inform monetary policy or climate models guide environmental agreements.</p>
<p><strong>Adaptive Governance</strong>: Policies designed with explicit models can include triggers for reassessment as key parameters change, enabling responsive governance that avoids both paralysis and recklessness.</p>
<p><strong>Professionalization</strong>: “Model curator” and “argument formalization specialist” emerge as recognized roles, building expertise in bridging natural language and formal representations.</p>
<p><strong>Quality Standards</strong>: Community norms develop around model transparency, validation requirements, and appropriate use cases, preventing both dismissal and over-reliance on formal tools.</p>
<!-- [ ] FIND: @epistemic-commons: "Examples of shared knowledge infrastructure in other domains" -->
</section>
<section id="sec-long-term-vision" class="level3">
<h3 class="anchored" data-anchor-id="sec-long-term-vision">5.3.3 Long-Term Vision</h3>
<p>Successfully scaling this approach could fundamentally alter AI governance:</p>
<p><strong>Coordinated Response</strong>: Rather than fragmented efforts, the AI safety ecosystem could operate with shared situational awareness—different actors understanding how their efforts interact and contribute to collective goals.</p>
<p><strong>Anticipatory Action</strong>: Formal models with prediction market integration could provide early warning of emerging risks, enabling proactive rather than reactive governance.</p>
<p><strong>Global Cooperation</strong>: Shared formal frameworks could facilitate international coordination similar to how economic models enable monetary coordination or climate models support environmental agreements.</p>
<p><strong>Democratic Enhancement</strong>: Making expert reasoning transparent and modifiable could enable broader participation in crucial decisions about humanity’s technological future.</p>
<!-- [ ] CREATE: {#fig-long-term-vision}: "Vision for AI governance with AMTAIR infrastructure" -->
</section>
</section>
<section id="sec-recommendations" class="level2">
<h2 class="anchored" data-anchor-id="sec-recommendations">5.4 Recommendations for Stakeholders</h2>
<p>Different communities can take concrete steps to realize these benefits:</p>
<section id="sec-researcher-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="sec-researcher-recommendations">5.4.1 For Researchers</h3>
<ol type="1">
<li><p><strong>Experiment with formalization</strong>: Try extracting your own arguments into ArgDown/BayesDown format to discover implicit assumptions</p></li>
<li><p><strong>Contribute to validation</strong>: Provide expert annotations for building benchmark datasets and improving extraction quality</p></li>
<li><p><strong>Develop extensions</strong>: Build on the open-source foundation to add capabilities for your specific domain needs</p></li>
<li><p><strong>Publish formally</strong>: Include formal model representations alongside traditional papers to enable cumulative building</p></li>
</ol>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Start Guide
</div>
</div>
<div class="callout-body-container callout-body">
<!-- [ ] CREATE: Quick-start guide for researchers -->
<p>A comprehensive guide for researchers getting started with AMTAIR will be available at [project website], including templates, tutorials, and example extractions.</p>
</div>
</div>
</section>
<section id="sec-policymaker-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="sec-policymaker-recommendations">5.4.2 For Policymakers</h3>
<ol type="1">
<li><p><strong>Pilot applications</strong>: Use AMTAIR for internal analysis of specific policy proposals to build familiarity and identify value</p></li>
<li><p><strong>Demand transparency</strong>: Request formal models underlying expert recommendations to understand assumptions and uncertainties</p></li>
<li><p><strong>Fund development</strong>: Support tool development and training to build governance capacity for formal methods</p></li>
<li><p><strong>Design adaptively</strong>: Create policies with explicit triggers based on model parameters to enable responsive governance</p></li>
</ol>
<!-- [ ] CREATE: Policy evaluation template -->
</section>
<section id="sec-technologist-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="sec-technologist-recommendations">5.4.3 For Technologists</h3>
<ol type="1">
<li><p><strong>Improve extraction</strong>: Contribute better prompting strategies, fine-tuned models, or validation methods</p></li>
<li><p><strong>Enhance interfaces</strong>: Develop visualizations and interactions serving specific stakeholder needs</p></li>
<li><p><strong>Build integrations</strong>: Connect AMTAIR to other tools in the AI governance ecosystem</p></li>
<li><p><strong>Scale infrastructure</strong>: Address computational challenges for larger models and broader deployment</p></li>
</ol>
<!-- [ ] CREATE: Technical contribution guidelines -->
</section>
<section id="sec-funder-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="sec-funder-recommendations">5.4.4 For Funders</h3>
<ol type="1">
<li><p><strong>Support ecosystem</strong>: Fund not just tool development but training, community building, and maintenance</p></li>
<li><p><strong>Bridge communities</strong>: Incentivize collaborations between formal modelers and domain experts</p></li>
<li><p><strong>Measure coordination</strong>: Develop metrics for assessing coordination improvements from formal tools</p></li>
<li><p><strong>Patient capital</strong>: Recognize that epistemic infrastructure requires sustained investment to reach potential</p></li>
</ol>
<!-- [ ] CREATE: Funding opportunity framework -->
</section>
</section>
<section id="sec-future-research-agenda" class="level2">
<h2 class="anchored" data-anchor-id="sec-future-research-agenda">5.5 Future Research Agenda</h2>
<p>Building on this foundation, several research directions could amplify impact:</p>
<section id="sec-technical-priorities" class="level3">
<h3 class="anchored" data-anchor-id="sec-technical-priorities">5.5.1 Technical Priorities</h3>
<p><strong>Extraction Enhancement</strong>: - Fine-tuning language models specifically for argument extraction - Handling implicit reasoning and long-range dependencies - Cross-document synthesis for comprehensive models - Multilingual extraction for global perspectives</p>
<p><strong>Representation Extensions</strong>: - Temporal dynamics for modeling AI development trajectories - Multi-agent representations for strategic interactions - Continuous variables for economic and capability metrics - Uncertainty types beyond probability distributions</p>
<p><strong>Integration Depth</strong>: - Semantic matching between models and prediction markets - Automated experiment design based on model sensitivity - Policy optimization algorithms using extracted models - Real-time updating from news and research feeds</p>
<!-- [ ] CREATE: {#tbl-research-priorities}: "Technical research roadmap with timelines" -->
</section>
<section id="sec-methodological-development" class="level3">
<h3 class="anchored" data-anchor-id="sec-methodological-development">5.5.2 Methodological Development</h3>
<p><strong>Validation Science</strong>: - Larger benchmark datasets with diverse argument types - Metrics for semantic preservation beyond accuracy - Adversarial robustness testing protocols - Longitudinal studies of model evolution</p>
<p><strong>Hybrid Approaches</strong>: - Optimal human-AI collaboration patterns for extraction - Combining formal models with other methods (scenarios, simulations) - Integration with deliberative and participatory processes - Balancing automation with expert judgment</p>
<p><strong>Social Methods</strong>: - Ethnographic studies of model use in organizations - Measuring coordination improvements empirically - Understanding adoption barriers and facilitators - Designing interventions for epistemic security</p>
<!-- [ ] FIND: @human-ai-collaboration: "Best practices for human-AI teaming" -->
</section>
<section id="sec-application-expansion" class="level3">
<h3 class="anchored" data-anchor-id="sec-application-expansion">5.5.3 Application Expansion</h3>
<p><strong>Domain Extensions</strong>: - Climate risk assessment and policy evaluation - Biosecurity governance and pandemic preparedness - Nuclear policy and deterrence stability - Emerging technology governance broadly</p>
<p><strong>Institutional Integration</strong>: - Embedding in regulatory impact assessment - Corporate strategic planning applications - Academic peer review enhancement - Democratic deliberation support tools</p>
<p><strong>Global Deployment</strong>: - Adapting to different governance contexts - Supporting multilateral negotiation processes - Building capacity in developing nations - Creating resilient distributed infrastructure</p>
<!-- [ ] CREATE: {#fig-application-roadmap}: "Expansion pathways across domains" -->
</section>
</section>
<section id="sec-closing-reflections" class="level2">
<h2 class="anchored" data-anchor-id="sec-closing-reflections">5.6 Closing Reflections</h2>
<p>The work presented in this thesis emerges from a simple observation: while humanity mobilizes unprecedented resources to address AI risks, our efforts remain tragically uncoordinated. Different communities work with incompatible frameworks, duplicate efforts, and sometimes actively undermine each other’s work. This fragmentation amplifies the very risks we seek to mitigate.</p>
<p>AMTAIR represents one attempt to build bridges—computational tools that create common ground for disparate perspectives. By making implicit models explicit, quantifying uncertainty, and enabling systematic policy analysis, these tools offer hope for enhanced coordination. The successful extraction of complex arguments, validation against expert judgment, and demonstration of policy evaluation capabilities suggest this approach has merit.</p>
<p>Yet tools alone cannot solve coordination problems rooted in incentives, institutions, and human psychology. AMTAIR provides infrastructure for coordination, not coordination itself. Success requires not just technical development but changes in how we approach collective challenges—valuing transparency over strategic ambiguity, embracing uncertainty rather than false confidence, and prioritizing collective outcomes over parochial interests.</p>
<p>The path forward demands both ambition and humility. Ambition to build the epistemic infrastructure necessary for navigating unprecedented risks. Humility to recognize our tools’ limitations and the irreducible role of human wisdom in governance. The question is not whether formal models can replace human judgment—they cannot and should not. Rather, it’s whether we can augment our collective intelligence with computational tools that help us reason together about futures too important to leave to chance.</p>
<div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Stakes
</div>
</div>
<div class="callout-body-container callout-body">
<p>As AI capabilities advance toward transformative potential, the window for establishing effective governance narrows. We cannot afford continued fragmentation when facing potentially irreversible consequences. The coordination crisis in AI governance represents both existential risk and existential opportunity—risk if we fail to align our efforts, opportunity if we succeed in building unprecedented cooperation around humanity’s most important challenge.</p>
</div>
</div>
<p>This thesis contributes technical foundations and demonstrates feasibility. The greater work—building communities, changing practices, and fostering coordination—remains ahead. May we prove equal to the task, for all our futures depend on it.</p>
<!-- [ ] CREATE: {#fig-closing-vision}: "The path from fragmentation to coordination" -->
</section>
</section>
<section id="sec-references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<!-- Quarto will auto-generate from .bib file -->
<!-- [ ] TODO: Verify all citations are in ref/MAref.bib -->
<!-- [ ] TODO: Check for missing references throughout document -->
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-amodei2016" class="csl-entry" role="listitem">
Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John
Schulman, and Dan Mané. 2016. <span>“Concrete <span>Problems</span> in
<span>AI Safety</span>.”</span> July 25, 2016. <a href="https://doi.org/10.48550/arXiv.1606.06565">https://doi.org/10.48550/arXiv.1606.06565</a>.
</div>
<div id="ref-anderson2007" class="csl-entry" role="listitem">
Anderson, Terence J. 2007. <span>“Visualization Tools and Argument
Schemes: A Question of Standpoint.”</span> <em>Law, Prob. &amp;
Risk</em> 6: 97. <a href="https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/lawprisk6&amp;section=9">https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/lawprisk6&amp;section=9</a>.
</div>
<div id="ref-askell2021" class="csl-entry" role="listitem">
Askell, Amanda, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom
Henighan, Andy Jones, et al. 2021. <span>“A <span>General Language
Assistant</span> as a <span>Laboratory</span> for
<span>Alignment</span>.”</span> December 9, 2021. <a href="https://doi.org/10.48550/arXiv.2112.00861">https://doi.org/10.48550/arXiv.2112.00861</a>.
</div>
<div id="ref-benn2011" class="csl-entry" role="listitem">
Benn, Neil, and Ann Macintosh. 2011. <span>“Argument
<span>Visualization</span> for <span class="nocase">eParticipation</span>: <span>Towards</span> a
<span>Research Agenda</span> and <span>Prototype Tool</span>.”</span> In
<em>Electronic <span>Participation</span></em>, edited by Efthimios
Tambouris, Ann Macintosh, and Hans De Bruijn, 6847:60–73. Berlin,
Heidelberg: Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-642-23333-3_6">https://doi.org/10.1007/978-3-642-23333-3_6</a>.
</div>
<div id="ref-bostrom2014" class="csl-entry" role="listitem">
Bostrom, Nick. 2014. <em>Superintelligence: <span>Paths</span>,
Strategies, Dangers</em>. Oxford: Oxford University Press. <a href="https://scholar.dominican.edu/cynthia-stokes-brown-books-big-history/47">https://scholar.dominican.edu/cynthia-stokes-brown-books-big-history/47</a>.
</div>
<div id="ref-box1976" class="csl-entry" role="listitem">
Box, George E. P. 1976. <span>“Science and
<span>Statistics</span>.”</span> <em>Journal of the American Statistical
Association</em> 71 (356): 791–99. <a href="https://doi.org/10.1080/01621459.1976.10480949">https://doi.org/10.1080/01621459.1976.10480949</a>.
</div>
<div id="ref-brundage2018" class="csl-entry" role="listitem">
Brundage, Miles, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley,
Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar,
Hyrum Anderson, et al. 2018. <span>“The <span>Malicious Use</span> of
<span>Artificial Intelligence</span>: <span>Forecasting</span>,
<span>Prevention</span>, and <span>Mitigation</span>.”</span> 2018. <a href="https://doi.org/10.48550/ARXIV.1802.07228">https://doi.org/10.48550/ARXIV.1802.07228</a>.
</div>
<div id="ref-brundage2018a" class="csl-entry" role="listitem">
Brundage, Miles, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley,
Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, and Bobby
Filar. 2018. <span>“The Malicious Use of Artificial Intelligence:
<span>Forecasting</span>, Prevention, and Mitigation.”</span> <a href="https://arxiv.org/abs/1802.07228">https://arxiv.org/abs/1802.07228</a>.
</div>
<div id="ref-carlsmith2021" class="csl-entry" role="listitem">
Carlsmith, Joseph. 2021. <span>“Is <span>Power-Seeking AI</span> an
<span>Existential Risk</span>?”</span> 2021. <a href="https://doi.org/10.48550/arXiv.2206.13353">https://doi.org/10.48550/arXiv.2206.13353</a>.
</div>
<div id="ref-carlsmith2022" class="csl-entry" role="listitem">
———. 2022. <span>“Is Power-Seeking <span>AI</span> an Existential
Risk?”</span> <a href="https://arxiv.org/abs/2206.13353">https://arxiv.org/abs/2206.13353</a>.
</div>
<div id="ref-carlsmith2024" class="csl-entry" role="listitem">
———. 2024. <span>“Is <span>Power-Seeking AI</span> an <span>Existential
Risk</span>?”</span> August 13, 2024. <a href="https://doi.org/10.48550/arXiv.2206.13353">https://doi.org/10.48550/arXiv.2206.13353</a>.
</div>
<div id="ref-christiano2019" class="csl-entry" role="listitem">
Christiano, Paul F. 2019. <span>“What Failure Looks Like,”</span> March.
<a href="https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like">https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like</a>.
</div>
<div id="ref-clarke2022" class="csl-entry" role="listitem">
Clarke, Sam, Ben Cottier, Aryeh Englander, Daniel Eth, David Manheim,
Samuel Dylan Martin, and Issa Rice. 2022. <span>“Modeling
<span>Transformative AI Risks</span> (<span>MTAIR</span>)
<span>Project</span> – <span>Summary Report</span>.”</span> 2022. <a href="https://doi.org/10.48550/ARXIV.2206.09360">https://doi.org/10.48550/ARXIV.2206.09360</a>.
</div>
<div id="ref-cuomo2016" class="csl-entry" role="listitem">
Cuomo, Francesca, Christine Mallin, and Alessandro Zattoni. 2016.
<span>“Corporate Governance Codes: <span>A</span> Review and Research
Agenda.”</span> <em>Corporate Governance: An International Review</em>
24 (3): 222–41. <a href="https://ueaeprints.uea.ac.uk/id/eprint/57664/">https://ueaeprints.uea.ac.uk/id/eprint/57664/</a>.
</div>
<div id="ref-dafoe2018" class="csl-entry" role="listitem">
Dafoe, Allan. 2018. <span>“<span>AI</span> Governance: A Research
Agenda.”</span> <em>Governance of AI Program, Future of Humanity
Institute, University of Oxford: Oxford, UK</em> 1442: 1443. <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf">https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf</a>.
</div>
<div id="ref-dafoe2021" class="csl-entry" role="listitem">
———. 2021. <span>“<span>AI</span> Governance: A Research Agenda.”</span>
2021. <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf">https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf</a>.
</div>
<div id="ref-devilliers2021" class="csl-entry" role="listitem">
De Villiers, Charl, and Ruth Dimes. 2021. <span>“Determinants,
Mechanisms and Consequences of Corporate Governance Reporting: A
Research Framework.”</span> <em>Journal of Management and
Governance</em> 25 (1): 7–26. <a href="https://doi.org/10.1007/s10997-020-09530-0">https://doi.org/10.1007/s10997-020-09530-0</a>.
</div>
<div id="ref-demirag2000" class="csl-entry" role="listitem">
Demirag, Istemi, Sudi Sudarsanam, and MIKE WRIGHT. 2000.
<span>“Corporate Governance: Overview and Research Agenda.”</span>
<em>The British Accounting Review</em> 32 (4): 341–54. <a href="https://www.academia.edu/download/49469624/bare.2000.014620161009-3955-1dt4aq5.pdf">https://www.academia.edu/download/49469624/bare.2000.014620161009-3955-1dt4aq5.pdf</a>.
</div>
<div id="ref-divito2022" class="csl-entry" role="listitem">
Di Vito, Jackie, and Kim Trottier. 2022. <span>“A <span>Literature
Review</span> on <span>Corporate Governance Mechanisms</span>:
<span>Past</span>, <span>Present</span>, and
<span>Future</span>*.”</span> <em>Accounting Perspectives</em> 21 (2):
207–35. <a href="https://doi.org/10.1111/1911-3838.12279">https://doi.org/10.1111/1911-3838.12279</a>.
</div>
<div id="ref-drexler2019a" class="csl-entry" role="listitem">
Drexler, K. Eric. 2019. <span>“Reframing Superintelligence:
<span>Comprehensive AI</span> Services as General Intelligence.”</span>
</div>
<div id="ref-drexler2019" class="csl-entry" role="listitem">
Drexler, KE. 2019. <span>“Reframing Superintelligence: Comprehensive
<span>AI</span> Services as General Intelligence.”</span> Technical
Report. Future of Humanity Institute. <a href="https://owainevans.github.io/pdfs/Reframing_Superintelligence_FHI-TR-2019.pdf">https://owainevans.github.io/pdfs/Reframing_Superintelligence_FHI-TR-2019.pdf</a>.
</div>
<div id="ref-european2024" class="csl-entry" role="listitem">
European, Union. 2024. <span>“The <span>Act Texts</span> | <span>EU
Artificial Intelligence Act</span>.”</span> 2024. <a href="https://artificialintelligenceact.eu/the-act/">https://artificialintelligenceact.eu/the-act/</a>.
</div>
<div id="ref-good1966" class="csl-entry" role="listitem">
Good, Irving John. 1966. <span>“Speculations <span>Concerning</span> the
<span>First Ultraintelligent Machine</span>.”</span> <em>Advances in
Computers</em>, 31. <a href="https://doi.org/10.1016/S0065-2458(08)60418-0">https://doi.org/10.1016/S0065-2458(08)60418-0</a>.
</div>
<div id="ref-growiec2024" class="csl-entry" role="listitem">
Growiec, Jakub. 2024. <span>“Existential Risk from Transformative
<span>AI</span>: An Economic Perspective.”</span> <em>Technological and
Economic Development of Economy</em> 30 (6): 1682–1708.
</div>
<div id="ref-hallegatte2012" class="csl-entry" role="listitem">
Hallegatte, Stéphane, Ankur Shah, Robert Lempert, Casey Brown, and
Stuart Gill. 2012. <span>“Investment Decision-Making Under Deep
Uncertainty-Application to Climate Change.”</span> <em>Policy Research
Working Paper</em> 6193. <a href="https://enpc.hal.science/hal-00802049/document">https://enpc.hal.science/hal-00802049/document</a>.
</div>
<div id="ref-hendrycks2021" class="csl-entry" role="listitem">
Hendrycks, Dan, Nicholas Carlini, John Schulman, and Jacob Steinhardt.
2021a. <span>“Unsolved <span>Problems</span> in <span>ML
Safety</span>.”</span> 2021. <a href="https://doi.org/10.48550/ARXIV.2109.13916">https://doi.org/10.48550/ARXIV.2109.13916</a>.
</div>
<div id="ref-hendrycks2021a" class="csl-entry" role="listitem">
———. 2021b. <span>“Unsolved Problems in Ml Safety.”</span> <a href="https://arxiv.org/abs/2109.13916">https://arxiv.org/abs/2109.13916</a>.
</div>
<div id="ref-jaynes2003" class="csl-entry" role="listitem">
Jaynes, Edwin T. 2003. <em>Probability Theory: <span>The</span> Logic of
Science</em>. Cambridge university press.
</div>
<div id="ref-kaur2024" class="csl-entry" role="listitem">
Kaur, Kawaljit. 2024. <span>“Corporate <span>Governance</span> and
<span>Legal Accountability</span>: <span>A Critical Review</span> of
<span>Global Practices</span>.”</span> <em>Journal of Law</em> 2 (6):
1–7. <a href="https://joi.shodhsagar.org/index.php/SSJOI/article/view/16">https://joi.shodhsagar.org/index.php/SSJOI/article/view/16</a>.
</div>
<div id="ref-khartabil2020" class="csl-entry" role="listitem">
Khartabil, Dana. 2020. <span>“Visualisation Techniques to Facilitate
Argument Exploration.”</span> PhD thesis. <a href="https://napier-repository.worktribe.com/output/2694675">https://napier-repository.worktribe.com/output/2694675</a>.
</div>
<div id="ref-khartabil2021" class="csl-entry" role="listitem">
Khartabil, D., C. Collins, S. Wells, B. Bach, and J. Kennedy. 2021.
<span>“Design and <span>Evaluation</span> of <span>Visualization
Techniques</span> to <span>Facilitate Argument
Exploration</span>.”</span> <em>Computer Graphics Forum</em> 40 (6):
447–65. <a href="https://doi.org/10.1111/cgf.14389">https://doi.org/10.1111/cgf.14389</a>.
</div>
<div id="ref-koller2009" class="csl-entry" role="listitem">
Koller, Daphne, and Nir Friedman. 2009. <em>Probabilistic Graphical
Models: Principles and Techniques</em>. MIT press. <a href="https://books.google.ca/books?hl=en&amp;lr=&amp;id=7dzpHCHzNQ4C&amp;oi=fnd&amp;pg=PR9&amp;dq=Koller,+D.,+%26+Friedman,+N.+(2009).+Probabilistic+Graphical+Models&amp;ots=py2HAh0VAL&amp;sig=gpaID3x6-TY8x5SOopuXpZDXfzs">https://books.google.ca/books?hl=en&amp;lr=&amp;id=7dzpHCHzNQ4C&amp;oi=fnd&amp;pg=PR9&amp;dq=Koller,+D.,+%26+Friedman,+N.+(2009).+Probabilistic+Graphical+Models&amp;ots=py2HAh0VAL&amp;sig=gpaID3x6-TY8x5SOopuXpZDXfzs</a>.
</div>
<div id="ref-kumar2019a" class="csl-entry" role="listitem">
Kumar, Ram Shankar Siva, David O. Brien, Kendra Albert, Salomé Viljöen,
and Jeffrey Snover. 2019a. <span>“Failure Modes in Machine Learning
Systems.”</span> <a href="https://arxiv.org/abs/1911.11034">https://arxiv.org/abs/1911.11034</a>.
</div>
<div id="ref-kumar2019" class="csl-entry" role="listitem">
Kumar, Ram Shankar Siva, David O Brien, Kendra Albert, Salomé Viljöen,
and Jeffrey Snover. 2019b. <span>“Failure <span>Modes</span> in
<span>Machine Learning Systems</span>.”</span> 2019. <a href="https://doi.org/10.48550/ARXIV.1911.11034">https://doi.org/10.48550/ARXIV.1911.11034</a>.
</div>
<div id="ref-lempert2003" class="csl-entry" role="listitem">
Lempert, Robert J, Steven W Popper, and Steven C Bankes. 2003.
<em>Shaping the Next One Hundred Years: <span>New</span> Methods for
Quantitative, Long-Term Policy Analysis</em>. RAND Corporation.
</div>
<div id="ref-lindley2013" class="csl-entry" role="listitem">
Lindley, Dennis V. 2013. <em>Understanding Uncertainty</em>. John Wiley
&amp; Sons. <a href="https://books.google.ca/books?hl=en&amp;lr=&amp;id=Tfk8AgAAQBAJ&amp;oi=fnd&amp;pg=PR11&amp;dq=Lindley,+D.+(2006).+Understanding+Uncertainty&amp;ots=55HS6lTOVP&amp;sig=0gKCDvRu5rUKhuPyJqhOzW23upU">https://books.google.ca/books?hl=en&amp;lr=&amp;id=Tfk8AgAAQBAJ&amp;oi=fnd&amp;pg=PR11&amp;dq=Lindley,+D.+(2006).+Understanding+Uncertainty&amp;ots=55HS6lTOVP&amp;sig=0gKCDvRu5rUKhuPyJqhOzW23upU</a>.
</div>
<div id="ref-list2011" class="csl-entry" role="listitem">
List, Christian, and Philip Pettit. 2011. <em>Group <span>Agency</span>:
<span>The Possibility</span>, <span>Design</span>, and
<span>Status</span> of <span>Corporate Agents</span></em>. Oxford
University Press.
</div>
<div id="ref-maslej2025" class="csl-entry" role="listitem">
Maslej, Nestor. 2025. <span>“Artificial <span>Intelligence Index
Report</span> 2025.”</span> <em>Artificial Intelligence</em>.
</div>
<div id="ref-nelson2006" class="csl-entry" role="listitem">
Nelson, Roger B. 2006. <em>An <span>Introduction</span> to
<span>Copulas</span></em>. Springer <span>Series</span> in
<span>Statistics</span>. New York, NY: Springer New York. <a href="https://doi.org/10.1007/0-387-28678-0">https://doi.org/10.1007/0-387-28678-0</a>.
</div>
<div id="ref-ngajie2020" class="csl-entry" role="listitem">
Ngajie, Berty Nsolly, Yan Li, Dawit Tibebu Tiruneh, and Mengmeng Cheng.
2020. <span>“Investigating the Effects of a Systematic and Model-Based
Design of Computer-Supported Argument Visualization on Critical
Thinking.”</span> <em>Thinking Skills and Creativity</em> 38: 100742. <a href="https://www.sciencedirect.com/science/article/pii/S1871187120302169">https://www.sciencedirect.com/science/article/pii/S1871187120302169</a>.
</div>
<div id="ref-paul2023" class="csl-entry" role="listitem">
Paul. 2023. <span>“The <span class="nocase">elephAInt</span> –
<span>Are</span> We All Like the Six Blind Men When It Comes to
<span>AI</span>? | <span>PRISMAGuard LLC</span>.”</span> 2023. <a href="https://www.prismaguard.com/the-elephaint-are-we-all-like-the-six-blind-men-when-it-comes-to-ai/">https://www.prismaguard.com/the-elephaint-are-we-all-like-the-six-blind-men-when-it-comes-to-ai/</a>.
</div>
<div id="ref-pearl2000" class="csl-entry" role="listitem">
Pearl, Judea. 2000. <em>Causality: Models, Reasoning, and
Inference</em>. Cambridge, U.K. ; New York: Cambridge University Press.
</div>
<div id="ref-pearl2009" class="csl-entry" role="listitem">
———. 2009. <em>Causality: <span>Models</span>, Reasoning and
Inference</em>. 2nd ed. Cambridge University Press.
</div>
<div id="ref-pearl2014" class="csl-entry" role="listitem">
———. 2014. <em>Probabilistic Reasoning in Intelligent Systems: Networks
of Plausible Inference</em>. Elsevier. <a href="https://books.google.ca/books?hl=en&amp;lr=&amp;id=mn2jBQAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=Pearl,+J.+(1988).+Probabilistic+Reasoning+in+Intelligent+Systems&amp;ots=4tEX2A4Ha8&amp;sig=lgUs_RCoeXEEuGwM5xMEoyJy4HI">https://books.google.ca/books?hl=en&amp;lr=&amp;id=mn2jBQAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=Pearl,+J.+(1988).+Probabilistic+Reasoning+in+Intelligent+Systems&amp;ots=4tEX2A4Ha8&amp;sig=lgUs_RCoeXEEuGwM5xMEoyJy4HI</a>.
</div>
<div id="ref-pollock1995" class="csl-entry" role="listitem">
Pollock, John L. 1995. <em>Cognitive Carpentry: <span>A</span> Blueprint
for How to Build a Person</em>. Mit Press. <a href="https://books.google.ca/books?hl=en&amp;lr=&amp;id=JAfHrHTqswAC&amp;oi=fnd&amp;pg=PA1&amp;dq=Pollock,+J.+(1995).+Cognitive+Carpentry&amp;ots=rq-qSCBcxV&amp;sig=aAfHGsGUosxl_1-JuxIEA7C2QO4">https://books.google.ca/books?hl=en&amp;lr=&amp;id=JAfHrHTqswAC&amp;oi=fnd&amp;pg=PA1&amp;dq=Pollock,+J.+(1995).+Cognitive+Carpentry&amp;ots=rq-qSCBcxV&amp;sig=aAfHGsGUosxl_1-JuxIEA7C2QO4</a>.
</div>
<div id="ref-prokudin2024" class="csl-entry" role="listitem">
Prokudin, D. E., E. N. Lisanyuk, and I. R. Baymuratov. 2024.
<span>“Visualization <span>Functions</span> in <span>Argumentation
Representation Software</span>.”</span> <em>Scientific
Visualization</em> 16 (3). <a href="https://sv-journal.org/2024-3/11/en.pdf">https://sv-journal.org/2024-3/11/en.pdf</a>.
</div>
<div id="ref-rehman2025" class="csl-entry" role="listitem">
Rehman, Iskander. 2025. <span>“The <span>Battle</span> for
<span>Brilliant Minds</span>: <span>From</span> the <span>Nuclear
Age</span> to <span>AI</span>.”</span> War on the Rocks. January 13,
2025. <a href="https://warontherocks.com/2025/01/the-battle-for-brilliant-minds-from-the-nuclear-age-to-ai/">https://warontherocks.com/2025/01/the-battle-for-brilliant-minds-from-the-nuclear-age-to-ai/</a>.
</div>
<div id="ref-russell2015" class="csl-entry" role="listitem">
Russell, Stuart, Tom Dietterich, Eric Horvitz, Bart Selman, Francesca
Rossi, Demis Hassabis, Shane Legg, et al. 2015. <span>“Research
Priorities for Robust and Beneficial Artificial Intelligence:
<span>An</span> Open Letter.”</span> <em>AI Magazine</em> 36 (4): 3–4.
<a href="https://doi.org/10.1609/aimag.v36i4.2621">https://doi.org/10.1609/aimag.v36i4.2621</a>.
</div>
<div id="ref-samborska2025" class="csl-entry" role="listitem">
Samborska, Veronika. 2025. <span>“Scaling up: How Increasing Inputs Has
Made Artificial Intelligence More Capable.”</span> <em>Our World in
Data</em>, January. <a href="https://ourworldindata.org/scaling-up-ai">https://ourworldindata.org/scaling-up-ai</a>.
</div>
<div id="ref-schelling1960" class="csl-entry" role="listitem">
Schelling, Thomas C. 1960. <span>“I960. <span>The</span> Strategy of
Conflict.”</span> <em>Cambridge, Mass</em>.
</div>
<div id="ref-scheuer2010" class="csl-entry" role="listitem">
Scheuer, Oliver, Frank Loll, Niels Pinkwart, and Bruce M. McLaren. 2010.
<span>“Computer-Supported Argumentation: <span>A</span> Review of the
State of the Art.”</span> <em>International Journal of
Computer-Supported Collaborative Learning</em> 5 (1): 43–102. <a href="https://doi.org/10.1007/s11412-009-9080-x">https://doi.org/10.1007/s11412-009-9080-x</a>.
</div>
<div id="ref-solomon2020" class="csl-entry" role="listitem">
Solomon, Jill. 2020. <em>Corporate Governance and Accountability</em>.
John Wiley &amp; Sons. <a href="https://books.google.ca/books?hl=en&amp;lr=&amp;id=JAX9DwAAQBAJ&amp;oi=fnd&amp;pg=PR1&amp;dq=review+of+the+effects+of+liability+frameworks+on+corporate+governance+&amp;ots=ny23_vd-U0&amp;sig=3LuNNhvSWXriEeg-ipAdDIQGAgo">https://books.google.ca/books?hl=en&amp;lr=&amp;id=JAX9DwAAQBAJ&amp;oi=fnd&amp;pg=PR1&amp;dq=review+of+the+effects+of+liability+frameworks+on+corporate+governance+&amp;ots=ny23_vd-U0&amp;sig=3LuNNhvSWXriEeg-ipAdDIQGAgo</a>.
</div>
<div id="ref-tegmark2024" class="csl-entry" role="listitem">
Tegmark, Max. 2024. <span>“Asilomar <span>AI Principles</span>.”</span>
Future of Life Institute. 2024. <a href="https://futureoflife.org/open-letter/ai-principles/">https://futureoflife.org/open-letter/ai-principles/</a>.
</div>
<div id="ref-tetlock2015" class="csl-entry" role="listitem">
Tetlock, Philip E., and Dan Gardner. 2015. <em>Superforecasting: The Art
and Science of Prediction</em>. First paperback edition. New York:
Broadway Books.
</div>
<div id="ref-todd2024" class="csl-entry" role="listitem">
Todd, Benjamin. 2024. <span>“It Looks Like There Are Some Good Funding
Opportunities in <span>AI</span> Safety Right Now.”</span> Substack
newsletter. Benjamin Todd. December 21, 2024. <a href="https://benjamintodd.substack.com/p/looks-like-there-are-some-good-funding">https://benjamintodd.substack.com/p/looks-like-there-are-some-good-funding</a>.
</div>
<div id="ref-voigt2025" class="csl-entry" role="listitem">
Voigt, Christian. (2014) 2025. <span>“Christianvoigt/Argdown.”</span> <a href="https://github.com/christianvoigt/argdown">https://github.com/christianvoigt/argdown</a>.
</div>
<div id="ref-walton2009" class="csl-entry" role="listitem">
Walton, Douglas. 2009. <span>“Argument Visualization Tools for
Corroborative Evidence.”</span> In <em>Proc. Of the 2nd
<span>International Conference</span> on <span>Evidence Law</span> and
<span>Forensic Science</span></em>, 32–49. <a href="https://www.academia.edu/download/37718171/09ArguVis.pdf">https://www.academia.edu/download/37718171/09ArguVis.pdf</a>.
</div>
<div id="ref-wilson2023" class="csl-entry" role="listitem">
Wilson, Nick, Matt Boyd, John Kerr, Amanda Kvalsvig, and Michael Baker.
2023. <span>“The Need for Long-Term Thinking–<span>Especially</span> for
Preventing Catastrophic Risks.”</span> <em>Public Health Expert
Briefing</em>.
</div>
<div id="ref-yudkowsky2008" class="csl-entry" role="listitem">
Yudkowsky, Eliezer. 2008. <span>“Artificial <span>Intelligence</span> as
a Positive and Negative Factor in Global Risk.”</span> In <em>Global
<span>Catastrophic Risks</span></em>, by Eliezer Yudkowsky. Oxford
University Press. <a href="https://doi.org/10.1093/oso/9780198570509.003.0021">https://doi.org/10.1093/oso/9780198570509.003.0021</a>.
</div>
</div>
</section>
<section id="sec-appendices" class="level1 unnumbered">
<h1 class="unnumbered">Appendices</h1>
<section id="sec-appendix-technical" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-technical">Appendix A: Technical Implementation Details</h2>
<!-- [ ] CREATE: Complete technical documentation -->
<pre><code>
Contents:

- Full API specifications
- Architectural diagrams with component details
- Code structure and organization
- Deployment instructions
- Performance optimization guides
</code></pre>
<section id="sec-data-structures" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-data-structures">A.1 Core Data Structures</h3>
<p>The AMTAIR system employs several custom data structures optimized for representing hierarchical arguments with probabilistic metadata:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BayesDownNode:</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Represents a single node in the BayesDown format"""</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    title: <span class="bu">str</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    description: <span class="bu">str</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    instantiations: List[<span class="bu">str</span>]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    priors: Dict[<span class="bu">str</span>, <span class="bu">float</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">dict</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    posteriors: Dict[<span class="bu">str</span>, <span class="bu">float</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">dict</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    parents: List[<span class="bu">str</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    children: List[<span class="bu">str</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    metadata: Dict[<span class="bu">str</span>, Any] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">dict</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-extraction-details" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-extraction-details">A.2 Extraction Algorithm Details</h3>
<!-- [ ] TODO: Document complete extraction pipeline with pseudocode -->
<!-- [ ] TODO: Include prompt templates and engineering decisions -->
</section>
<section id="sec-api-specs" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-api-specs">A.3 API Specifications</h3>
<!-- [ ] TODO: Complete API documentation for each component -->
<!-- [ ] TODO: Include example usage and error handling -->
</section>
</section>
<section id="sec-appendix-validation" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-validation">Appendix B: Validation Datasets and Procedures</h2>
<!-- [ ] CREATE: Validation materials -->
<pre><code>Contents:
- Benchmark dataset descriptions
- Annotation guidelines
- Inter-rater reliability protocols
- Statistical analysis procedures
- Replication instructions</code></pre>
<section id="sec-annotation-protocol" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-annotation-protocol">B.1 Expert Annotation Protocol</h3>
<!-- [ ] TODO: Detailed instructions for manual extraction -->
<!-- [ ] TODO: Inter-annotator agreement methodology -->
</section>
<section id="sec-benchmark-construction" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-benchmark-construction">B.2 Benchmark Dataset Construction</h3>
<!-- [ ] TODO: Dataset statistics and characteristics -->
<!-- [ ] TODO: Quality control procedures -->
</section>
<section id="sec-validation-results" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-validation-results">B.3 Validation Results</h3>
<!-- [ ] TODO: Complete precision/recall tables -->
<!-- [ ] TODO: Error analysis breakdowns -->
</section>
</section>
<section id="sec-appendix-cases" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-cases">Appendix C: Extended Case Studies</h2>
<!-- [ ] IMPLEMENT: Additional extraction examples -->
<pre><code>Include:
- Christiano's "What failure looks like"
- Critch's ARCHES model
- Additional policy evaluation scenarios
- Comparative analysis across models</code></pre>
<section id="sec-christiano-extraction" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-christiano-extraction">C.1 Christiano’s “What Failure Looks Like” Extraction</h3>
<!-- [ ] TODO: Complete manual extraction for comparison -->
<!-- [ ] TODO: Document extraction decisions and challenges -->
</section>
<section id="sec-critch-extraction" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-critch-extraction">C.2 Critch’s ARCHES Model</h3>
<!-- [ ] TODO: Full model extraction and formalization -->
<!-- [ ] TODO: Comparison with original presentation -->
</section>
<section id="sec-narrow-path-evaluation" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-narrow-path-evaluation">C.3 Policy Evaluation: A Narrow Path</h3>
<!-- [ ] TODO: Detailed analysis of policy proposals -->
<!-- [ ] TODO: Sensitivity to implementation specifics -->
</section>
</section>
<section id="sec-appendix-bayesdown" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-bayesdown">Appendix D: BayesDown Syntax Specification</h2>
<!-- [ ] CREATE: Complete formal specification -->
<pre><code>Contents:
- Full syntax definition
- Validation rules
- Example transformations
- Implementation notes
- Extension possibilities</code></pre>
</section>
<section id="sec-appendix-prompts" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-prompts">Appendix E: Prompt Engineering Details</h2>
<!-- [ ] CREATE: Prompt documentation -->
<pre><code>Include:
- Full extraction prompts with annotations
- Iterative refinement history
- Ablation study results
- Best practices guide
- Common failure patterns</code></pre>
</section>
<section id="sec-appendix-userguide" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-userguide">Appendix F: User Guide</h2>
<!-- [ ] CREATE: Practical user guide -->
<pre><code>Sections:
- Getting started with AMTAIR
- Creating your first extraction
- Interpreting visualizations
- Policy evaluation walkthrough
- Troubleshooting common issues</code></pre>
</section>
<section id="sec-appendix-notebook" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-notebook">Appendix G: Jupyter Notebook Implementation</h2>
<!-- Link to embedded notebook as per _quarto.yml configuration -->
<p>The complete implementation is available as an interactive Jupyter notebook demonstrating:</p>
<ul>
<li>Environment setup and configuration</li>
<li>Step-by-step extraction pipeline</li>
<li>Visualization generation</li>
<li>Policy evaluation examples</li>
<li>Performance benchmarking</li>
</ul>
<!-- {{< embed /AMTAIR_Prototype/data/example_carlsmith/AMTAIR_Prototype_example_carlsmith.ipynb >}} -->
</section>
<section id="sec-appendix-ethical" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-ethical">Appendix H: Ethical Considerations and Governance</h2>
<!-- [ ] TODO: Analyze ethical dimensions comprehensively -->
<!-- [ ] TODO: Include misuse analysis and mitigations -->
<!-- [ ] TODO: Document responsibility frameworks -->
<section id="sec-misuse-scenarios" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-misuse-scenarios">H.1 Potential Misuse Scenarios</h3>
<!-- [ ] TODO: Strategic manipulation of models -->
<!-- [ ] TODO: Technocratic capture risks -->
<!-- [ ] TODO: Mitigation strategies -->
</section>
<section id="sec-democratic-frameworks" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-democratic-frameworks">H.2 Democratic Participation Frameworks</h3>
<!-- [ ] TODO: Inclusivity design principles -->
<!-- [ ] TODO: Stakeholder engagement protocols -->
</section>
<section id="sec-responsibility" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-responsibility">H.3 Responsibility Assignment</h3>
<!-- [ ] TODO: Model developer obligations -->
<!-- [ ] TODO: User responsibilities -->
<!-- [ ] TODO: Institutional oversight needs -->
</section>
</section>
<section id="sec-appendix-examples" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-examples">Appendix I: Full Extraction Examples</h2>
<!-- [ ] TODO: Include complete worked examples -->
<!-- [ ] TODO: Show all intermediate steps -->
<!-- [ ] TODO: Document decision points -->
</section>
<section id="sec-appendix-software" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-appendix-software">Appendix J: Software Installation and Usage Guide</h2>
<!-- [ ] TODO: Complete installation instructions -->
<!-- [ ] TODO: Tutorial walkthrough -->
<!-- [ ] TODO: Troubleshooting guide -->
<!-- [ ] FINAL CHECKLIST: -->
<!-- [ ] TODO: American spelling check throughout -->
<!-- [ ] TODO: Word count verification (~30,000 words) -->
<!-- [ ] TODO: All figures and tables properly labeled -->
<!-- [ ] TODO: All citations verified -->
<!-- [ ] TODO: Task management system fully implemented -->
<!-- [ ] TODO: Quarto syntax validated -->
<!-- [ ] TODO: Final review for consistency and completeness -->


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The probability estimates vary between outlines; using more conservative estimates from 12.2<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This example, while simple, demonstrates all essential features of Bayesian networks and serves as the foundation for understanding more complex applications<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Pearl’s causal framework revolutionized how we think about causation in complex systems<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Copulas provide a mathematically elegant way to separate marginal behavior from dependence structure<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>This reflects how LLMs inherit human cognitive biases from training data<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../index.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../ref/references.html" class="pagination-link" aria-label="References (.md)">
        <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">References (.md)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/VJMeyer/submission/edit/main/chapters/Outlines/Outline_13.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>