# Main Argument: Current AI as Existential Risk Factor

<AI_Existential_Risk_Factor>: Current and near-term AI technologies can contribute to existential risk by acting as intermediate risk factors.
  + <AI_Intermediate_Factor>: Current and near-term AI technologies can act as intermediate risk factors, magnifying the likelihood of previously identified sources of existential risk.
  + <Risk_Not_Limited_To_AGI>: This potential contribution to existential risk is not limited to the unaligned AGI scenario.
  + <Causal_Pathways_Exist>: There exist causal pathways from AI systems to existential risks that do not presuppose hypothetical future AI capabilities.


# Power Dynamics Arguments

[AI_Affects_Power_Dynamics]: AI can shift or strengthen existing power dynamics between different actors.
  + [AI_State_State_Risk]: AI can disturb relationships between nation states, potentially leading to an "AI arms race."
  + [AI_State_Corporation_Risk]: The rise of tech corporations affects their relationships with states, creating power imbalances.
  + [AI_State_Citizen_Risk]: AI surveillance technologies change the dynamics between states and citizens.
   /* => [AI_Existential_Risk_Factor]*/

## State-State Relationships

[AI_State_State_Risk]: AI affects relationships between states in ways that can increase existential risk./*=> [AI_Affects_Power_Dynamics]*/
 + <AI_Global_Power_Shift>: AI development contributes to a global shift in power towards nations like China ("Easternisation").
 + <AI_Arms_Race>: Concerns about technological competition can lead to an "AI arms race" between nations.
 + <Competition_Inhibits_Coordination>: Such competitive dynamics may inhibit international coordination and incentivize against AI safety precautions.


## State-Corporation Relationships

[AI_State_Corporation_Risk]: AI affects relationships between states and corporations in ways that can increase existential risk.
 + <Tech_Corporation_Rise>: The past two decades have seen a monumental rise of private corporations in the technology sector with revenues comparable to national GDPs.
 + <Corporate_Global_Reach>: These corporations operate globally, making state regulation challenging.
 + <Profit_Driven_Goals>: Many corporate goals are profit-driven and can be misaligned with wider societal interests.


## State-Citizen Relationships

[AI_State_Citizen_Risk]: AI surveillance technologies enable potential stable repressive regimes that could constitute existential catastrophes.
/* 
(4) => [AI_Affects_Power_Dynamics]
*/
 + <AI_Surveillance_Growth>: There's been a rapid increase in the use of AI surveillance systems by states across different political systems.
 + <Insufficient_Ethical_Frameworks>: Ethical and legal frameworks for these technologies are lagging behind their deployment.
 + <Surveillance_Privatization>: Private companies develop and sell surveillance technologies to governments, further normalizing mass surveillance.

<AI_Existential_Risk_Factor>

(1) [AI_State_State_Risk]
(2) [AI_Affects_Power_Dynamics]
--
Some inference rule: p .^. (p .->. q) .->. q {some_additional_data: [1,2]}
--
(3) [AI_Existential_Risk_Factor_Final]
  -> Outgoing relations of the conclusion, are also interpreted as outgoing relations of the whole argument.









# Specific Existential Risk Pathways

## Nuclear Risk

<Nuclear_Risk_Argument>:
  + <AI_State_State_Risk>: AI affects relationships between states, potentially creating tensions.
  +  <AI_Arms_Race_To_Military>: An AI arms race could become military in nature.
  + <Cybersecurity_Intelligence_Impact>: Changes in cybersecurity could affect a state's intelligence capabilities.
/* 
(4) [AI_Nuclear_Risk]: AI could increase the probability of a nuclear conflict leading to "nuclear winter" and potential human extinction. => [AI_Existential_Risk_Factor]
*/

## Pandemic Risk

<Pandemic_Risk_Argument>: 
  + <AI_Information_Ecosystem_Risk>: AI threatens the information ecosystem, as seen with COVID-19 misinformation.
  + <Response_Requires_Trust>: Effective pandemic response requires public trust in political systems.
  + <AI_Biological_Weapons>: AI could be used to design and produce dangerous pathogens.

/* 
(4) [AI_Pandemic_Risk]: AI could increase the risk from engineered pandemics and biotechnology. => [AI_Existential_Risk_Factor]
*/

## Climate Risk

<Climate_Risk_Argument>:
  + <AI_Information_Ecosystem_Risk>: AI threatens the information ecosystem through misinformation.
  +  <Climate_Misinformation>: Climate change has a history of being clouded by misinformation.
  +  <AI_Energy_Consumption>: AI development and training has a significant carbon footprint, with a single NLP model producing 300,000kg of CO2 emissions.

/* 
(4) [AI_Climate_Risk]: AI could increase the risk from climate change through both information distortion and direct emissions. => [AI_Existential_Risk_Factor]
*/


## Unaligned AGI Risk

<AGI_Risk_Argument>:
  + <AI_Arms_Race>: Concerns about technological competition can lead to an "AI arms race" between nations or corporations.
  + <Safety_Corner_Cutting>: Competitive dynamics may incentivize corner-cutting on AI safety.

/* 
(3) [AI_AGI_Risk]: Current AI development patterns could increase future risks from unaligned AGI. => [AI_Existential_Risk_Factor]
*/


## Stable Repressive Regime Risk


<Repressive_Regime_Argument>:
  + <AI_Surveillance_Growth>: Rapid increase in AI surveillance technologies globally.
  + <Lagging_Ethical_Frameworks>: Even democratic countries fail to meet regulatory standards for surveillance.
  + <Surveillance_Privatization>: Private-public partnerships normalize surveillance beyond legal limits.

/* 
(4) [AI_Repressive_Regime_Risk]: AI surveillance enables potentially stable, global repressive autocracies constituting existential catastrophes. => [AI_Existential_Risk_Factor]
*/




/* 
From: Current and Near-Term AI as a Potential Existential Risk Factor by Benjamin S. Bucknallâˆ—
*/
